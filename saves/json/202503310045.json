[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.18869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v2",
                "updated": "2025-03-27T17:48:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    48,
                    14,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21725v1",
                "updated": "2025-03-27T17:37:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:37:12Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "title": "Low-noise environment for probing fundamental symmetries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-noise environment for probing fundamental symmetries"
                },
                "summary": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons."
                },
                "authors": [
                    {
                        "name": "F. J. Collings"
                    },
                    {
                        "name": "N. J. Fitch"
                    },
                    {
                        "name": "J. M. Dyne"
                    },
                    {
                        "name": "R. A. Jenkins"
                    },
                    {
                        "name": "E. Wursten"
                    },
                    {
                        "name": "M. T. Ziemba"
                    },
                    {
                        "name": "X. S. Zheng"
                    },
                    {
                        "name": "F. Castellini"
                    },
                    {
                        "name": "J. Lim"
                    },
                    {
                        "name": "B. E. Sauer"
                    },
                    {
                        "name": "M. R. Tarbutt"
                    }
                ],
                "author_detail": {
                    "name": "M. R. Tarbutt"
                },
                "author": "M. R. Tarbutt",
                "arxiv_comment": "34 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v4",
                "updated": "2025-03-27T15:21:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    21,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17922v2",
                "updated": "2025-03-27T14:11:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    11,
                    37,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-23T03:36:52Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    36,
                    52,
                    6,
                    82,
                    0
                ],
                "title": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference"
                },
                "summary": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness."
                },
                "authors": [
                    {
                        "name": "Youhui Zuo"
                    },
                    {
                        "name": "Sibo Wei"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhuorui Liu"
                    },
                    {
                        "name": "Wenpeng Lu"
                    },
                    {
                        "name": "Dawei Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Song"
                },
                "author": "Dawei Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17038v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17038v3",
                "updated": "2025-03-27T12:14:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    14,
                    56,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-21T10:48:35Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    48,
                    35,
                    4,
                    80,
                    0
                ],
                "title": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation"
                },
                "summary": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference."
                },
                "authors": [
                    {
                        "name": "Ashutosh Pradhan"
                    },
                    {
                        "name": "Daniele Ottaviano"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Haozheng Huang"
                    },
                    {
                        "name": "Alexander Zuepke"
                    },
                    {
                        "name": "Andrea Bastoni"
                    },
                    {
                        "name": "Marco Caccamo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Caccamo"
                },
                "author": "Marco Caccamo",
                "arxiv_comment": "Accepted for publication in the Proceedings of the 31st IEEE\n  Real-Time and Embedded Technology and Applications Symposium (RTAS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17038v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17038v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.3; C.4; D.4.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03708v3",
                "updated": "2025-03-27T11:46:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    46,
                    22,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-05T17:59:19Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "title": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach"
                },
                "summary": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights are available at\nhttps://github.com/ali-vilab/CDT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights are available at\nhttps://github.com/ali-vilab/CDT."
                },
                "authors": [
                    {
                        "name": "Nianzu Yang"
                    },
                    {
                        "name": "Pandeng Li"
                    },
                    {
                        "name": "Liming Zhao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chen-Wei Xie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17606v2",
                "updated": "2025-03-27T09:53:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    53,
                    15,
                    3,
                    86,
                    0
                ],
                "published": "2024-09-26T07:44:47Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "title": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support"
                },
                "summary": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan."
                },
                "authors": [
                    {
                        "name": "Tim Fischer"
                    },
                    {
                        "name": "Michael Rogenmoser"
                    },
                    {
                        "name": "Thomas Benz"
                    },
                    {
                        "name": "Frank K. GÃ¼rkaynak"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_doi": "10.1109/TVLSI.2025.3527225",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVLSI.2025.3527225",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.17606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems (\n  Volume: 33, Issue: 4, April 2025)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11108v2",
                "updated": "2025-03-27T07:02:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    2,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-14T06:01:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer\n  Decoding"
                },
                "summary": "The key-value (KV) cache in the tensor version of transformers presents a\nsignificant bottleneck during inference. While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanisms [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a reduction\nfrom communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. Furthermore,\nwe introduce two types of tensor attention cache and present a trade-off\nbetween time and memory for two scenarios. Overall, our work provides a\ntheoretical foundation for us to understand the time-memory tradeoff of\nKV-Cache compression in tensor attention decoding and offers more perspectives\nin developing more memory-efficient tensor attention Transformer architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache in the tensor version of transformers presents a\nsignificant bottleneck during inference. While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanisms [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a reduction\nfrom communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. Furthermore,\nwe introduce two types of tensor attention cache and present a trade-off\nbetween time and memory for two scenarios. Overall, our work provides a\ntheoretical foundation for us to understand the time-memory tradeoff of\nKV-Cache compression in tensor attention decoding and offers more perspectives\nin developing more memory-efficient tensor attention Transformer architectures."
                },
                "authors": [
                    {
                        "name": "Yifang Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tian"
                },
                "author": "Yu Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v4",
                "updated": "2025-03-26T17:42:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    42,
                    17,
                    2,
                    85,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks."
                },
                "authors": [
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16302v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16302v2",
                "updated": "2025-03-26T15:08:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    8,
                    12,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-20T16:23:44Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    23,
                    44,
                    3,
                    79,
                    0
                ],
                "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Vecset Diffusion Model for Fast Shape Generation"
                },
                "summary": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM."
                },
                "authors": [
                    {
                        "name": "Zeqiang Lai"
                    },
                    {
                        "name": "Yunfei Zhao"
                    },
                    {
                        "name": "Zibo Zhao"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Fuyun Wang"
                    },
                    {
                        "name": "Huiwen Shi"
                    },
                    {
                        "name": "Xianghui Yang"
                    },
                    {
                        "name": "Qingxiang Lin"
                    },
                    {
                        "name": "Jingwei Huang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16302v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v3",
                "updated": "2025-03-26T13:59:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    59,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN\n  Inference on NVIDIA GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN\n  Inference on NVIDIA GPUs"
                },
                "summary": "Cloud service providers heavily colocate high-priority, latency-sensitive\n(LS), and low-priority, best-effort (BE) DNN inference services on the same GPU\nto improve resource utilization in data centers. Among the critical shared GPU\nresources, there has been very limited analysis on the dynamic allocation of\ncompute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU\nresource management solutions are either hardware-specific, or unable to\ndynamically allocate resources to different tenants, or both; (2) NVIDIA\ndoesn't expose interfaces for VRAM bandwidth allocation, and the software stack\nand VRAM channel architectures are black-box, both of which limit the\nsoftware-level resource management. These drive prior work to design either\nconservative sharing policies detrimental to throughput, or static resource\npartitioning only applicable to a few GPU models.\n  To bridge this gap, this paper proposes SGDRC, a fully software-defined\ndynamic VRAM bandwidth and compute unit management solution for concurrent DNN\ninference services. SGDRC aims at guaranteeing service quality, maximizing the\noverall throughput, and providing general applicability to NVIDIA GPUs. SGDRC\nfirst reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs\nthrough comprehensive reverse engineering and eliminates VRAM channel conflicts\nusing software-level cache coloring. SGDRC applies bimodal tensors and tidal SM\nmasking to dynamically allocate VRAM bandwidth and compute units, and guides\nthe allocation of resources based on offline profiling. We evaluate 11\nmainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show\nthat compared with the state-of-the-art GPU sharing solutions, SGDRC achieves\nthe highest SLO attainment rates (99.0% on average), and improves overall\nthroughput by up to 1.47x and BE job throughput by up to 2.36x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud service providers heavily colocate high-priority, latency-sensitive\n(LS), and low-priority, best-effort (BE) DNN inference services on the same GPU\nto improve resource utilization in data centers. Among the critical shared GPU\nresources, there has been very limited analysis on the dynamic allocation of\ncompute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU\nresource management solutions are either hardware-specific, or unable to\ndynamically allocate resources to different tenants, or both; (2) NVIDIA\ndoesn't expose interfaces for VRAM bandwidth allocation, and the software stack\nand VRAM channel architectures are black-box, both of which limit the\nsoftware-level resource management. These drive prior work to design either\nconservative sharing policies detrimental to throughput, or static resource\npartitioning only applicable to a few GPU models.\n  To bridge this gap, this paper proposes SGDRC, a fully software-defined\ndynamic VRAM bandwidth and compute unit management solution for concurrent DNN\ninference services. SGDRC aims at guaranteeing service quality, maximizing the\noverall throughput, and providing general applicability to NVIDIA GPUs. SGDRC\nfirst reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs\nthrough comprehensive reverse engineering and eliminates VRAM channel conflicts\nusing software-level cache coloring. SGDRC applies bimodal tensors and tidal SM\nmasking to dynamically allocate VRAM bandwidth and compute units, and guides\nthe allocation of resources based on offline profiling. We evaluate 11\nmainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show\nthat compared with the state-of-the-art GPU sharing solutions, SGDRC achieves\nthe highest SLO attainment rates (99.0% on average), and improves overall\nthroughput by up to 1.47x and BE job throughput by up to 2.36x."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yunzhe Li"
                    },
                    {
                        "name": "Zhifeng Jiang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_doi": "10.1145/3710848.3710863",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3710848.3710863",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13996v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 19 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20481v1",
                "updated": "2025-03-26T12:10:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    10,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T12:10:53Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    10,
                    53,
                    2,
                    85,
                    0
                ],
                "title": "Analyzing Modern NVIDIA GPU cores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Modern NVIDIA GPU cores"
                },
                "summary": "GPUs are the most popular platform for accelerating HPC workloads, such as\nartificial intelligence and science simulations. However, most\nmicroarchitectural research in academia relies on GPU core pipeline designs\nbased on architectures that are more than 15 years old.\n  This paper reverse engineers modern NVIDIA GPU cores, unveiling many key\naspects of its design and explaining how GPUs leverage hardware-compiler\ntechniques where the compiler guides hardware during execution. In particular,\nit reveals how the issue logic works including the policy of the issue\nscheduler, the structure of the register file and its associated cache, and\nmultiple features of the memory pipeline. Moreover, it analyses how a simple\ninstruction prefetcher based on a stream buffer fits well with modern NVIDIA\nGPUs and is likely to be used. Furthermore, we investigate the impact of the\nregister file cache and the number of register file read ports on both\nsimulation accuracy and performance.\n  By modeling all these new discovered microarchitectural details, we achieve\n18.24% lower mean absolute percentage error (MAPE) in execution cycles than\nprevious state-of-the-art simulators, resulting in an average of 13.98% MAPE\nwith respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that\nthis new model stands for other NVIDIA architectures, such as Turing. Finally,\nwe show that the software-based dependence management mechanism included in\nmodern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in\nterms of performance and area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPUs are the most popular platform for accelerating HPC workloads, such as\nartificial intelligence and science simulations. However, most\nmicroarchitectural research in academia relies on GPU core pipeline designs\nbased on architectures that are more than 15 years old.\n  This paper reverse engineers modern NVIDIA GPU cores, unveiling many key\naspects of its design and explaining how GPUs leverage hardware-compiler\ntechniques where the compiler guides hardware during execution. In particular,\nit reveals how the issue logic works including the policy of the issue\nscheduler, the structure of the register file and its associated cache, and\nmultiple features of the memory pipeline. Moreover, it analyses how a simple\ninstruction prefetcher based on a stream buffer fits well with modern NVIDIA\nGPUs and is likely to be used. Furthermore, we investigate the impact of the\nregister file cache and the number of register file read ports on both\nsimulation accuracy and performance.\n  By modeling all these new discovered microarchitectural details, we achieve\n18.24% lower mean absolute percentage error (MAPE) in execution cycles than\nprevious state-of-the-art simulators, resulting in an average of 13.98% MAPE\nwith respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that\nthis new model stands for other NVIDIA architectures, such as Turing. Finally,\nwe show that the software-based dependence management mechanism included in\nmodern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in\nterms of performance and area."
                },
                "authors": [
                    {
                        "name": "Rodrigo Huerta"
                    },
                    {
                        "name": "Mojtaba Abaie Shoushtary"
                    },
                    {
                        "name": "JosÃ©-Lorenzo Cruz"
                    },
                    {
                        "name": "Antonio GonzÃ¡lez"
                    }
                ],
                "author_detail": {
                    "name": "Antonio GonzÃ¡lez"
                },
                "author": "Antonio GonzÃ¡lez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v2",
                "updated": "2025-03-26T11:08:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    8,
                    20,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20174v1",
                "updated": "2025-03-26T02:58:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    2,
                    58,
                    41,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T02:58:41Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    2,
                    58,
                    41,
                    2,
                    85,
                    0
                ],
                "title": "Devil is in the Uniformity: Exploring Diverse Learners within\n  Transformer for Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Devil is in the Uniformity: Exploring Diverse Learners within\n  Transformer for Image Restoration"
                },
                "summary": "Transformer-based approaches have gained significant attention in image\nrestoration, where the core component, i.e, Multi-Head Attention (MHA), plays a\ncrucial role in capturing diverse features and recovering high-quality results.\nIn MHA, heads perform attention calculation independently from uniform split\nsubspaces, and a redundancy issue is triggered to hinder the model from\nachieving satisfactory outputs. In this paper, we propose to improve MHA by\nexploring diverse learners and introducing various interactions between heads,\nwhich results in a Hierarchical multI-head atteNtion driven Transformer model,\ntermed HINT, for image restoration. HINT contains two modules, i.e., the\nHierarchical Multi-Head Attention (HMHA) and the Query-Key Cache Updating\n(QKCU) module, to address the redundancy problem that is rooted in vanilla MHA.\nSpecifically, HMHA extracts diverse contextual features by employing heads to\nlearn from subspaces of varying sizes and containing different information.\nMoreover, QKCU, comprising intra- and inter-layer schemes, further reduces the\nredundancy problem by facilitating enhanced interactions between attention\nheads within and across layers. Extensive experiments are conducted on 12\nbenchmarks across 5 image restoration tasks, including low-light enhancement,\ndehazing, desnowing, denoising, and deraining, to demonstrate the superiority\nof HINT. The source code is available in the supplementary materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based approaches have gained significant attention in image\nrestoration, where the core component, i.e, Multi-Head Attention (MHA), plays a\ncrucial role in capturing diverse features and recovering high-quality results.\nIn MHA, heads perform attention calculation independently from uniform split\nsubspaces, and a redundancy issue is triggered to hinder the model from\nachieving satisfactory outputs. In this paper, we propose to improve MHA by\nexploring diverse learners and introducing various interactions between heads,\nwhich results in a Hierarchical multI-head atteNtion driven Transformer model,\ntermed HINT, for image restoration. HINT contains two modules, i.e., the\nHierarchical Multi-Head Attention (HMHA) and the Query-Key Cache Updating\n(QKCU) module, to address the redundancy problem that is rooted in vanilla MHA.\nSpecifically, HMHA extracts diverse contextual features by employing heads to\nlearn from subspaces of varying sizes and containing different information.\nMoreover, QKCU, comprising intra- and inter-layer schemes, further reduces the\nredundancy problem by facilitating enhanced interactions between attention\nheads within and across layers. Extensive experiments are conducted on 12\nbenchmarks across 5 image restoration tasks, including low-light enhancement,\ndehazing, desnowing, denoising, and deraining, to demonstrate the superiority\nof HINT. The source code is available in the supplementary materials."
                },
                "authors": [
                    {
                        "name": "Shihao Zhou"
                    },
                    {
                        "name": "Dayu Li"
                    },
                    {
                        "name": "Jinshan Pan"
                    },
                    {
                        "name": "Juncheng Zhou"
                    },
                    {
                        "name": "Jinglei Shi"
                    },
                    {
                        "name": "Jufeng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jufeng Yang"
                },
                "author": "Jufeng Yang",
                "arxiv_comment": "11 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v2",
                "updated": "2025-03-26T01:58:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    1,
                    58,
                    40,
                    2,
                    85,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\ninference requests for context lengths in the range of millions of tokens\npresents unique challenges. While existing techniques are effective for\ntraining, they fail to address the unique challenges of inference, such as\nvarying prefill and decode phases and their associated latency constraints --\nlike Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore,\nno long-context inference solutions address head-of-line blocking today.\n  We present Medha, a system for efficient long-context LLM inference that\nintroduces three key innovations: adaptive chunking with slack-aware scheduling\nto prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce\nTTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into\na novel 3D parallelism serving engine, Medha achieves unprecedented scale --\nsupporting contexts up to 10M tokens with production-grade latency. Our\nevaluation shows Medha reduces median latency by up to 30x compared to\nstate-of-the-art systems when serving a mix of short and long requests, while\nimproving throughput by upwards of 5x. This enables, for the first time,\nefficient long-context LLM inference at scale without compromising on shorter\nrequest latencies or system efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\ninference requests for context lengths in the range of millions of tokens\npresents unique challenges. While existing techniques are effective for\ntraining, they fail to address the unique challenges of inference, such as\nvarying prefill and decode phases and their associated latency constraints --\nlike Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore,\nno long-context inference solutions address head-of-line blocking today.\n  We present Medha, a system for efficient long-context LLM inference that\nintroduces three key innovations: adaptive chunking with slack-aware scheduling\nto prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce\nTTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into\na novel 3D parallelism serving engine, Medha achieves unprecedented scale --\nsupporting contexts up to 10M tokens with production-grade latency. Our\nevaluation shows Medha reduces median latency by up to 30x compared to\nstate-of-the-art systems when serving a mix of short and long requests, while\nimproving throughput by upwards of 5x. This enables, for the first time,\nefficient long-context LLM inference at scale without compromising on shorter\nrequest latencies or system efficiency."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "ÃÃ±igo Goiri"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v2",
                "updated": "2025-03-25T17:56:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    56,
                    1,
                    1,
                    84,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Visualizing the Invisible: A Generative AR System for Intuitive\n  Multi-Modal Sensor Data Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visualizing the Invisible: A Generative AR System for Intuitive\n  Multi-Modal Sensor Data Presentation"
                },
                "summary": "Understanding sensor data can be difficult for non-experts because of the\ncomplexity and different semantic meanings of sensor modalities. This leads to\na need for intuitive and effective methods to present sensor information.\nHowever, creating intuitive sensor data visualizations presents three key\nchallenges: the variability of sensor readings, gaps in domain comprehension,\nand the dynamic nature of sensor data. To address these issues, we propose\nVivar, a novel system that integrates multi-modal sensor data and presents 3D\nvolumetric content for AR visualization. In particular, we introduce a\ncross-modal embedding approach that maps sensor data into a pre-trained visual\nembedding space through barycentric interpolation. This approach accurately\nreflects value changes in multi-modal sensor information, ensuring that sensor\nvariations are properly shown in visualization outcomes. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation, demonstrating 11x latency reduction without compromising\nquality. A user study involving over 503 participants, including domain\nexperts, demonstrates Vivar's effectiveness in accuracy, consistency, and\nreal-world applicability, paving the way for more intuitive sensor data\nvisualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be difficult for non-experts because of the\ncomplexity and different semantic meanings of sensor modalities. This leads to\na need for intuitive and effective methods to present sensor information.\nHowever, creating intuitive sensor data visualizations presents three key\nchallenges: the variability of sensor readings, gaps in domain comprehension,\nand the dynamic nature of sensor data. To address these issues, we propose\nVivar, a novel system that integrates multi-modal sensor data and presents 3D\nvolumetric content for AR visualization. In particular, we introduce a\ncross-modal embedding approach that maps sensor data into a pre-trained visual\nembedding space through barycentric interpolation. This approach accurately\nreflects value changes in multi-modal sensor information, ensuring that sensor\nvariations are properly shown in visualization outcomes. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation, demonstrating 11x latency reduction without compromising\nquality. A user study involving over 503 participants, including domain\nexperts, demonstrates Vivar's effectiveness in accuracy, consistency, and\nreal-world applicability, paving the way for more intuitive sensor data\nvisualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19950v1",
                "updated": "2025-03-25T16:24:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    24,
                    45,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T16:24:45Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    24,
                    45,
                    1,
                    84,
                    0
                ],
                "title": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation"
                },
                "summary": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV."
                },
                "authors": [
                    {
                        "name": "Han Chen"
                    },
                    {
                        "name": "Zicong Jiang"
                    },
                    {
                        "name": "Zining Zhang"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Pingyi Luo"
                    },
                    {
                        "name": "Mian Lu"
                    },
                    {
                        "name": "Yuqiang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yuqiang Chen"
                },
                "author": "Yuqiang Chen",
                "arxiv_comment": "Accepted by ICLR 2025 Workshop on Sparsity in LLMs (SLLM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19786v1",
                "updated": "2025-03-25T15:52:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    52,
                    34,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T15:52:34Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    52,
                    34,
                    1,
                    84,
                    0
                ],
                "title": "Gemma 3 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gemma 3 Technical Report"
                },
                "summary": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community."
                },
                "authors": [
                    {
                        "name": "Gemma Team"
                    },
                    {
                        "name": "Aishwarya Kamath"
                    },
                    {
                        "name": "Johan Ferret"
                    },
                    {
                        "name": "Shreya Pathak"
                    },
                    {
                        "name": "Nino Vieillard"
                    },
                    {
                        "name": "Ramona Merhej"
                    },
                    {
                        "name": "Sarah Perrin"
                    },
                    {
                        "name": "Tatiana Matejovicova"
                    },
                    {
                        "name": "Alexandre RamÃ©"
                    },
                    {
                        "name": "Morgane RiviÃ¨re"
                    },
                    {
                        "name": "Louis Rouillard"
                    },
                    {
                        "name": "Thomas Mesnard"
                    },
                    {
                        "name": "Geoffrey Cideron"
                    },
                    {
                        "name": "Jean-bastien Grill"
                    },
                    {
                        "name": "Sabela Ramos"
                    },
                    {
                        "name": "Edouard Yvinec"
                    },
                    {
                        "name": "Michelle Casbon"
                    },
                    {
                        "name": "Etienne Pot"
                    },
                    {
                        "name": "Ivo Penchev"
                    },
                    {
                        "name": "GaÃ«l Liu"
                    },
                    {
                        "name": "Francesco Visin"
                    },
                    {
                        "name": "Kathleen Kenealy"
                    },
                    {
                        "name": "Lucas Beyer"
                    },
                    {
                        "name": "Xiaohai Zhai"
                    },
                    {
                        "name": "Anton Tsitsulin"
                    },
                    {
                        "name": "Robert Busa-Fekete"
                    },
                    {
                        "name": "Alex Feng"
                    },
                    {
                        "name": "Noveen Sachdeva"
                    },
                    {
                        "name": "Benjamin Coleman"
                    },
                    {
                        "name": "Yi Gao"
                    },
                    {
                        "name": "Basil Mustafa"
                    },
                    {
                        "name": "Iain Barr"
                    },
                    {
                        "name": "Emilio Parisotto"
                    },
                    {
                        "name": "David Tian"
                    },
                    {
                        "name": "Matan Eyal"
                    },
                    {
                        "name": "Colin Cherry"
                    },
                    {
                        "name": "Jan-Thorsten Peter"
                    },
                    {
                        "name": "Danila Sinopalnikov"
                    },
                    {
                        "name": "Surya Bhupatiraju"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Mehran Kazemi"
                    },
                    {
                        "name": "Dan Malkin"
                    },
                    {
                        "name": "Ravin Kumar"
                    },
                    {
                        "name": "David Vilar"
                    },
                    {
                        "name": "Idan Brusilovsky"
                    },
                    {
                        "name": "Jiaming Luo"
                    },
                    {
                        "name": "Andreas Steiner"
                    },
                    {
                        "name": "Abe Friesen"
                    },
                    {
                        "name": "Abhanshu Sharma"
                    },
                    {
                        "name": "Abheesht Sharma"
                    },
                    {
                        "name": "Adi Mayrav Gilady"
                    },
                    {
                        "name": "Adrian Goedeckemeyer"
                    },
                    {
                        "name": "Alaa Saade"
                    },
                    {
                        "name": "Alex Feng"
                    },
                    {
                        "name": "Alexander Kolesnikov"
                    },
                    {
                        "name": "Alexei Bendebury"
                    },
                    {
                        "name": "Alvin Abdagic"
                    },
                    {
                        "name": "Amit Vadi"
                    },
                    {
                        "name": "AndrÃ¡s GyÃ¶rgy"
                    },
                    {
                        "name": "AndrÃ© Susano Pinto"
                    },
                    {
                        "name": "Anil Das"
                    },
                    {
                        "name": "Ankur Bapna"
                    },
                    {
                        "name": "Antoine Miech"
                    },
                    {
                        "name": "Antoine Yang"
                    },
                    {
                        "name": "Antonia Paterson"
                    },
                    {
                        "name": "Ashish Shenoy"
                    },
                    {
                        "name": "Ayan Chakrabarti"
                    },
                    {
                        "name": "Bilal Piot"
                    },
                    {
                        "name": "Bo Wu"
                    },
                    {
                        "name": "Bobak Shahriari"
                    },
                    {
                        "name": "Bryce Petrini"
                    },
                    {
                        "name": "Charlie Chen"
                    },
                    {
                        "name": "Charline Le Lan"
                    },
                    {
                        "name": "Christopher A. Choquette-Choo"
                    },
                    {
                        "name": "CJ Carey"
                    },
                    {
                        "name": "Cormac Brick"
                    },
                    {
                        "name": "Daniel Deutsch"
                    },
                    {
                        "name": "Danielle Eisenbud"
                    },
                    {
                        "name": "Dee Cattle"
                    },
                    {
                        "name": "Derek Cheng"
                    },
                    {
                        "name": "Dimitris Paparas"
                    },
                    {
                        "name": "Divyashree Shivakumar Sreepathihalli"
                    },
                    {
                        "name": "Doug Reid"
                    },
                    {
                        "name": "Dustin Tran"
                    },
                    {
                        "name": "Dustin Zelle"
                    },
                    {
                        "name": "Eric Noland"
                    },
                    {
                        "name": "Erwin Huizenga"
                    },
                    {
                        "name": "Eugene Kharitonov"
                    },
                    {
                        "name": "Frederick Liu"
                    },
                    {
                        "name": "Gagik Amirkhanyan"
                    },
                    {
                        "name": "Glenn Cameron"
                    },
                    {
                        "name": "Hadi Hashemi"
                    },
                    {
                        "name": "Hanna Klimczak-PluciÅska"
                    },
                    {
                        "name": "Harman Singh"
                    },
                    {
                        "name": "Harsh Mehta"
                    },
                    {
                        "name": "Harshal Tushar Lehri"
                    },
                    {
                        "name": "Hussein Hazimeh"
                    },
                    {
                        "name": "Ian Ballantyne"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Ivan Nardini"
                    },
                    {
                        "name": "Jean Pouget-Abadie"
                    },
                    {
                        "name": "Jetha Chan"
                    },
                    {
                        "name": "Joe Stanton"
                    },
                    {
                        "name": "John Wieting"
                    },
                    {
                        "name": "Jonathan Lai"
                    },
                    {
                        "name": "Jordi Orbay"
                    },
                    {
                        "name": "Joseph Fernandez"
                    },
                    {
                        "name": "Josh Newlan"
                    },
                    {
                        "name": "Ju-yeong Ji"
                    },
                    {
                        "name": "Jyotinder Singh"
                    },
                    {
                        "name": "Kat Black"
                    },
                    {
                        "name": "Kathy Yu"
                    },
                    {
                        "name": "Kevin Hui"
                    },
                    {
                        "name": "Kiran Vodrahalli"
                    },
                    {
                        "name": "Klaus Greff"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Marcella Valentine"
                    },
                    {
                        "name": "Marina Coelho"
                    },
                    {
                        "name": "Marvin Ritter"
                    },
                    {
                        "name": "Matt Hoffman"
                    },
                    {
                        "name": "Matthew Watson"
                    },
                    {
                        "name": "Mayank Chaturvedi"
                    },
                    {
                        "name": "Michael Moynihan"
                    },
                    {
                        "name": "Min Ma"
                    },
                    {
                        "name": "Nabila Babar"
                    },
                    {
                        "name": "Natasha Noy"
                    },
                    {
                        "name": "Nathan Byrd"
                    },
                    {
                        "name": "Nick Roy"
                    },
                    {
                        "name": "Nikola Momchev"
                    },
                    {
                        "name": "Nilay Chauhan"
                    },
                    {
                        "name": "Noveen Sachdeva"
                    },
                    {
                        "name": "Oskar Bunyan"
                    },
                    {
                        "name": "Pankil Botarda"
                    },
                    {
                        "name": "Paul Caron"
                    },
                    {
                        "name": "Paul Kishan Rubenstein"
                    },
                    {
                        "name": "Phil Culliton"
                    },
                    {
                        "name": "Philipp Schmid"
                    },
                    {
                        "name": "Pier Giuseppe Sessa"
                    },
                    {
                        "name": "Pingmei Xu"
                    },
                    {
                        "name": "Piotr Stanczyk"
                    },
                    {
                        "name": "Pouya Tafti"
                    },
                    {
                        "name": "Rakesh Shivanna"
                    },
                    {
                        "name": "Renjie Wu"
                    },
                    {
                        "name": "Renke Pan"
                    },
                    {
                        "name": "Reza Rokni"
                    },
                    {
                        "name": "Rob Willoughby"
                    },
                    {
                        "name": "Rohith Vallu"
                    },
                    {
                        "name": "Ryan Mullins"
                    },
                    {
                        "name": "Sammy Jerome"
                    },
                    {
                        "name": "Sara Smoot"
                    },
                    {
                        "name": "Sertan Girgin"
                    },
                    {
                        "name": "Shariq Iqbal"
                    },
                    {
                        "name": "Shashir Reddy"
                    },
                    {
                        "name": "Shruti Sheth"
                    },
                    {
                        "name": "Siim PÃµder"
                    },
                    {
                        "name": "Sijal Bhatnagar"
                    },
                    {
                        "name": "Sindhu Raghuram Panyam"
                    },
                    {
                        "name": "Sivan Eiger"
                    },
                    {
                        "name": "Susan Zhang"
                    },
                    {
                        "name": "Tianqi Liu"
                    },
                    {
                        "name": "Trevor Yacovone"
                    },
                    {
                        "name": "Tyler Liechty"
                    },
                    {
                        "name": "Uday Kalra"
                    },
                    {
                        "name": "Utku Evci"
                    },
                    {
                        "name": "Vedant Misra"
                    },
                    {
                        "name": "Vincent Roseberry"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Vlad Kolesnikov"
                    },
                    {
                        "name": "Woohyun Han"
                    },
                    {
                        "name": "Woosuk Kwon"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Yinlam Chow"
                    },
                    {
                        "name": "Yuvein Zhu"
                    },
                    {
                        "name": "Zichuan Wei"
                    },
                    {
                        "name": "Zoltan Egyed"
                    },
                    {
                        "name": "Victor Cotruta"
                    },
                    {
                        "name": "Minh Giang"
                    },
                    {
                        "name": "Phoebe Kirk"
                    },
                    {
                        "name": "Anand Rao"
                    },
                    {
                        "name": "Kat Black"
                    },
                    {
                        "name": "Nabila Babar"
                    },
                    {
                        "name": "Jessica Lo"
                    },
                    {
                        "name": "Erica Moreira"
                    },
                    {
                        "name": "Luiz Gustavo Martins"
                    },
                    {
                        "name": "Omar Sanseviero"
                    },
                    {
                        "name": "Lucas Gonzalez"
                    },
                    {
                        "name": "Zach Gleicher"
                    },
                    {
                        "name": "Tris Warkentin"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    },
                    {
                        "name": "Evan Senter"
                    },
                    {
                        "name": "Eli Collins"
                    },
                    {
                        "name": "Joelle Barral"
                    },
                    {
                        "name": "Zoubin Ghahramani"
                    },
                    {
                        "name": "Raia Hadsell"
                    },
                    {
                        "name": "Yossi Matias"
                    },
                    {
                        "name": "D. Sculley"
                    },
                    {
                        "name": "Slav Petrov"
                    },
                    {
                        "name": "Noah Fiedel"
                    },
                    {
                        "name": "Noam Shazeer"
                    },
                    {
                        "name": "Oriol Vinyals"
                    },
                    {
                        "name": "Jeff Dean"
                    },
                    {
                        "name": "Demis Hassabis"
                    },
                    {
                        "name": "Koray Kavukcuoglu"
                    },
                    {
                        "name": "Clement Farabet"
                    },
                    {
                        "name": "Elena Buchatskaya"
                    },
                    {
                        "name": "Jean-Baptiste Alayrac"
                    },
                    {
                        "name": "Rohan Anil"
                    },
                    {
                        "name": "Dmitry"
                    },
                    {
                        "name": "Lepikhin"
                    },
                    {
                        "name": "Sebastian Borgeaud"
                    },
                    {
                        "name": "Olivier Bachem"
                    },
                    {
                        "name": "Armand Joulin"
                    },
                    {
                        "name": "Alek Andreev"
                    },
                    {
                        "name": "Cassidy Hardin"
                    },
                    {
                        "name": "Robert Dadashi"
                    },
                    {
                        "name": "LÃ©onard Hussenot"
                    }
                ],
                "author_detail": {
                    "name": "LÃ©onard Hussenot"
                },
                "author": "LÃ©onard Hussenot",
                "arxiv_affiliation": "Dima",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19390v1",
                "updated": "2025-03-25T06:45:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    45,
                    13,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T06:45:13Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    45,
                    13,
                    1,
                    84,
                    0
                ],
                "title": "Integrating Prefetcher Selection with Dynamic Request Allocation\n  Improves Prefetching Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Prefetcher Selection with Dynamic Request Allocation\n  Improves Prefetching Efficiency"
                },
                "summary": "Hardware prefetching plays a critical role in hiding the off-chip DRAM\nlatency. The complexity of applications results in a wide variety of memory\naccess patterns, prompting the development of numerous cache-prefetching\nalgorithms. Consequently, commercial processors often employ a hybrid of these\nalgorithms to enhance the overall prefetching performance. Nonetheless, since\nthese prefetchers share hardware resources, conflicts arising from competing\nprefetching requests can negate the benefits of hardware prefetching. Under\nsuch circumstances, several prefetcher selection algorithms have been proposed\nto mitigate conflicts between prefetchers. However, these prior solutions\nsuffer from two limitations. First, the input demand request allocation is\ninaccurate. Second, the prefetcher selection criteria are coarse-grained.\n  In this paper, we address both limitations by introducing an efficient and\nwidely applicable prefetcher selection algorithm--Alecto, which tailors the\ndemand requests for each prefetcher. Every demand request is first sent to\nAlecto to identify suitable prefetchers before being routed to prefetchers for\ntraining and prefetching. Our analysis shows that Alecto is adept at not only\nharmonizing prefetching accuracy, coverage, and timeliness but also\nsignificantly enhancing the utilization of the prefetcher table, which is vital\nfor temporal prefetching. Alecto outperforms the state-of-the-art RL-based\nprefetcher selection algorithm--Bandit by 2.76% in single-core, and 7.56% in\neight-core. For memory-intensive benchmarks, Alecto outperforms Bandit by\n5.25%. Alecto consistently delivers state-of-the-art performance in scheduling\nvarious types of cache prefetchers. In addition to the performance improvement,\nAlecto can reduce the energy consumption associated with accessing the\nprefetchers' table by 48%, while only adding less than 1 KB of storage\noverhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware prefetching plays a critical role in hiding the off-chip DRAM\nlatency. The complexity of applications results in a wide variety of memory\naccess patterns, prompting the development of numerous cache-prefetching\nalgorithms. Consequently, commercial processors often employ a hybrid of these\nalgorithms to enhance the overall prefetching performance. Nonetheless, since\nthese prefetchers share hardware resources, conflicts arising from competing\nprefetching requests can negate the benefits of hardware prefetching. Under\nsuch circumstances, several prefetcher selection algorithms have been proposed\nto mitigate conflicts between prefetchers. However, these prior solutions\nsuffer from two limitations. First, the input demand request allocation is\ninaccurate. Second, the prefetcher selection criteria are coarse-grained.\n  In this paper, we address both limitations by introducing an efficient and\nwidely applicable prefetcher selection algorithm--Alecto, which tailors the\ndemand requests for each prefetcher. Every demand request is first sent to\nAlecto to identify suitable prefetchers before being routed to prefetchers for\ntraining and prefetching. Our analysis shows that Alecto is adept at not only\nharmonizing prefetching accuracy, coverage, and timeliness but also\nsignificantly enhancing the utilization of the prefetcher table, which is vital\nfor temporal prefetching. Alecto outperforms the state-of-the-art RL-based\nprefetcher selection algorithm--Bandit by 2.76% in single-core, and 7.56% in\neight-core. For memory-intensive benchmarks, Alecto outperforms Bandit by\n5.25%. Alecto consistently delivers state-of-the-art performance in scheduling\nvarious types of cache prefetchers. In addition to the performance improvement,\nAlecto can reduce the energy consumption associated with accessing the\nprefetchers' table by 48%, while only adding less than 1 KB of storage\noverhead."
                },
                "authors": [
                    {
                        "name": "Mengming Li"
                    },
                    {
                        "name": "Qijun Zhang"
                    },
                    {
                        "name": "Yongqing Ren"
                    },
                    {
                        "name": "Zhiyao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyao Xie"
                },
                "author": "Zhiyao Xie",
                "arxiv_comment": "In 31th IEEE International Symposium on High-Performance Computer\n  Architecture (HPCA 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14882v2",
                "updated": "2025-03-24T23:47:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    23,
                    47,
                    51,
                    0,
                    83,
                    0
                ],
                "published": "2025-02-15T05:08:01Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    5,
                    8,
                    1,
                    5,
                    46,
                    0
                ],
                "title": "CalibQuant: 1-Bit KV Cache Quantization for Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CalibQuant: 1-Bit KV Cache Quantization for Multimodal LLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance across diverse applications. However, their computational overhead\nduring deployment remains a critical bottleneck. While Key-Value (KV) caching\neffectively trades memory for computation to enhance inference efficiency, the\ngrowing memory footprint from extensive KV caches significantly reduces\nthroughput and restricts prolonged deployment on memory-constrained GPU\ndevices. To address this challenge, we propose CalibQuant, a simple yet highly\neffective visual quantization strategy that drastically reduces both memory and\ncomputational overhead. Specifically, CalibQuant introduces an extreme 1-bit\nquantization scheme, complemented by novel post-scaling and calibration\ntechniques tailored to the intrinsic patterns of KV caches, thereby ensuring\nhigh efficiency without compromising model performance. Leveraging Triton for\nruntime optimization, we achieve a 10x throughput increase on InternVL models.\nOur method is designed to be plug-and-play, seamlessly integrating with various\nexisting MLLMs without requiring architectural changes. Extensive experiments\nconfirm that our approach significantly reduces memory usage while maintaining\ncomputational efficiency and preserving multimodal capabilities. Codes are\navailable at https://github.com/insuhan/calibquant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance across diverse applications. However, their computational overhead\nduring deployment remains a critical bottleneck. While Key-Value (KV) caching\neffectively trades memory for computation to enhance inference efficiency, the\ngrowing memory footprint from extensive KV caches significantly reduces\nthroughput and restricts prolonged deployment on memory-constrained GPU\ndevices. To address this challenge, we propose CalibQuant, a simple yet highly\neffective visual quantization strategy that drastically reduces both memory and\ncomputational overhead. Specifically, CalibQuant introduces an extreme 1-bit\nquantization scheme, complemented by novel post-scaling and calibration\ntechniques tailored to the intrinsic patterns of KV caches, thereby ensuring\nhigh efficiency without compromising model performance. Leveraging Triton for\nruntime optimization, we achieve a 10x throughput increase on InternVL models.\nOur method is designed to be plug-and-play, seamlessly integrating with various\nexisting MLLMs without requiring architectural changes. Extensive experiments\nconfirm that our approach significantly reduces memory usage while maintaining\ncomputational efficiency and preserving multimodal capabilities. Codes are\navailable at https://github.com/insuhan/calibquant."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Susan Liang"
                    },
                    {
                        "name": "Jiani Liu"
                    },
                    {
                        "name": "Haiting Lin"
                    },
                    {
                        "name": "Mingjie Zhao"
                    },
                    {
                        "name": "Chenliang Xu"
                    },
                    {
                        "name": "Kun Wan"
                    },
                    {
                        "name": "Wentian Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wentian Zhao"
                },
                "author": "Wentian Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09859v2",
                "updated": "2025-03-24T21:27:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    27,
                    53,
                    0,
                    83,
                    0
                ],
                "published": "2024-11-15T00:37:31Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "title": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures"
                },
                "summary": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra, particularly in comparison to that of general and\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. This\nwork examines the factorization of a skew-symmetric matrix $X$ into its\n$LTL^\\mathrm{T}$ decomposition, where $L$ is unit lower triangular and $T$ is\ntridiagonal. This is also known as a triangular tridiagonalization. This\noperation is a means for computing the determinant of $X$ as the square of the\n(cheaply-computed) Pfaffian of the skew-symmetric tridiagonal matrix $T$ as\nwell as for solving systems of equations, across fields such as quantum\nelectronic structure and machine learning. Its application also often requires\npivoting in order to improve numerical stability. We compare and contrast\npreviously-published algorithms with those systematically derived using the\nFLAME methodology. Performant parallel CPU implementations are achieved by\nfusing operations at multiple levels in order to reduce memory traffic\noverhead. A key factor is the employment of new capabilities of the BLAS-like\nLibrary Instantiation Software (BLIS) framework, which now supports casting\nlevel-2 and level-3 BLAS-like operations by leveraging its gemm and other\nkernels, hierarchical parallelism, and cache blocking. A prototype, concise C++\nAPI facilitates the translation of correct-by-construction algorithms into\ncorrect code. Experiments verify that the resulting implementations greatly\nexceed the performance of previous work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra, particularly in comparison to that of general and\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. This\nwork examines the factorization of a skew-symmetric matrix $X$ into its\n$LTL^\\mathrm{T}$ decomposition, where $L$ is unit lower triangular and $T$ is\ntridiagonal. This is also known as a triangular tridiagonalization. This\noperation is a means for computing the determinant of $X$ as the square of the\n(cheaply-computed) Pfaffian of the skew-symmetric tridiagonal matrix $T$ as\nwell as for solving systems of equations, across fields such as quantum\nelectronic structure and machine learning. Its application also often requires\npivoting in order to improve numerical stability. We compare and contrast\npreviously-published algorithms with those systematically derived using the\nFLAME methodology. Performant parallel CPU implementations are achieved by\nfusing operations at multiple levels in order to reduce memory traffic\noverhead. A key factor is the employment of new capabilities of the BLAS-like\nLibrary Instantiation Software (BLIS) framework, which now supports casting\nlevel-2 and level-3 BLAS-like operations by leveraging its gemm and other\nkernels, hierarchical parallelism, and cache blocking. A prototype, concise C++\nAPI facilitates the translation of correct-by-construction algorithms into\ncorrect code. Experiments verify that the resulting implementations greatly\nexceed the performance of previous work."
                },
                "authors": [
                    {
                        "name": "Ishna Satyarth"
                    },
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "Devin A. Matthews"
                    },
                    {
                        "name": "Maggie Myers"
                    },
                    {
                        "name": "Robert van de Geijn"
                    },
                    {
                        "name": "RuQing G. Xu"
                    }
                ],
                "author_detail": {
                    "name": "RuQing G. Xu"
                },
                "author": "RuQing G. Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19145v1",
                "updated": "2025-03-24T21:00:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    0,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T21:00:37Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    0,
                    37,
                    0,
                    83,
                    0
                ],
                "title": "Compositional Caching for Training-free Open-vocabulary Attribute\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Caching for Training-free Open-vocabulary Attribute\n  Detection"
                },
                "summary": "Attribute detection is crucial for many computer vision tasks, as it enables\nsystems to describe properties such as color, texture, and material. Current\napproaches often rely on labor-intensive annotation processes which are\ninherently limited: objects can be described at an arbitrary level of detail\n(e.g., color vs. color shades), leading to ambiguities when the annotators are\nnot instructed carefully. Furthermore, they operate within a predefined set of\nattributes, reducing scalability and adaptability to unforeseen downstream\napplications. We present Compositional Caching (ComCa), a training-free method\nfor open-vocabulary attribute detection that overcomes these constraints. ComCa\nrequires only the list of target attributes and objects as input, using them to\npopulate an auxiliary cache of images by leveraging web-scale databases and\nLarge Language Models to determine attribute-object compatibility. To account\nfor the compositional nature of attributes, cache images receive soft attribute\nlabels. Those are aggregated at inference time based on the similarity between\nthe input and cache images, refining the predictions of underlying\nVision-Language Models (VLMs). Importantly, our approach is model-agnostic,\ncompatible with various VLMs. Experiments on public datasets demonstrate that\nComCa significantly outperforms zero-shot and cache-based baselines, competing\nwith recent training-based methods, proving that a carefully designed\ntraining-free approach can successfully address open-vocabulary attribute\ndetection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribute detection is crucial for many computer vision tasks, as it enables\nsystems to describe properties such as color, texture, and material. Current\napproaches often rely on labor-intensive annotation processes which are\ninherently limited: objects can be described at an arbitrary level of detail\n(e.g., color vs. color shades), leading to ambiguities when the annotators are\nnot instructed carefully. Furthermore, they operate within a predefined set of\nattributes, reducing scalability and adaptability to unforeseen downstream\napplications. We present Compositional Caching (ComCa), a training-free method\nfor open-vocabulary attribute detection that overcomes these constraints. ComCa\nrequires only the list of target attributes and objects as input, using them to\npopulate an auxiliary cache of images by leveraging web-scale databases and\nLarge Language Models to determine attribute-object compatibility. To account\nfor the compositional nature of attributes, cache images receive soft attribute\nlabels. Those are aggregated at inference time based on the similarity between\nthe input and cache images, refining the predictions of underlying\nVision-Language Models (VLMs). Importantly, our approach is model-agnostic,\ncompatible with various VLMs. Experiments on public datasets demonstrate that\nComCa significantly outperforms zero-shot and cache-based baselines, competing\nwith recent training-based methods, proving that a carefully designed\ntraining-free approach can successfully address open-vocabulary attribute\ndetection."
                },
                "authors": [
                    {
                        "name": "Marco Garosi"
                    },
                    {
                        "name": "Alessandro Conti"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Elisa Ricci"
                    },
                    {
                        "name": "Massimiliano Mancini"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Mancini"
                },
                "author": "Massimiliano Mancini",
                "arxiv_comment": "CVPR 2025. Project website at https://comca-attributes.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13773v2",
                "updated": "2025-03-24T18:50:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    18,
                    50,
                    9,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-17T23:38:29Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    23,
                    38,
                    29,
                    0,
                    76,
                    0
                ],
                "title": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference"
                },
                "summary": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    },
                    {
                        "name": "Masahiro Tanaka"
                    }
                ],
                "author_detail": {
                    "name": "Masahiro Tanaka"
                },
                "author": "Masahiro Tanaka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06364v2",
                "updated": "2025-03-24T18:16:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    18,
                    16,
                    58,
                    0,
                    83,
                    0
                ],
                "published": "2024-11-10T05:12:51Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "title": "EconoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EconoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving"
                },
                "summary": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EconoServe.\nEconoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EconoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EconoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EconoServe.\nEconoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EconoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EconoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18893v1",
                "updated": "2025-03-24T17:06:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    6,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:06:37Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    6,
                    37,
                    0,
                    83,
                    0
                ],
                "title": "xKV: Cross-Layer SVD for KV-Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xKV: Cross-Layer SVD for KV-Cache Compression"
                },
                "summary": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13064v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13064v2",
                "updated": "2025-03-24T16:47:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    47,
                    48,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-17T11:10:49Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    10,
                    49,
                    0,
                    76,
                    0
                ],
                "title": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads"
                },
                "summary": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols equipped with specialized pathways to deep-learning\naccelerators such as Gemmini. Simulation tools like Gem5 and DRAMSim2 were used\nto evaluate baseline performance and scalability under representative ML\nworkloads. The findings of this study highlight the design choices, and the\nanticipated challenges, paving the way for low-latency scalable memory\noperations for ML applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols equipped with specialized pathways to deep-learning\naccelerators such as Gemmini. Simulation tools like Gem5 and DRAMSim2 were used\nto evaluate baseline performance and scalability under representative ML\nworkloads. The findings of this study highlight the design choices, and the\nanticipated challenges, paving the way for low-latency scalable memory\noperations for ML applications."
                },
                "authors": [
                    {
                        "name": "Pranav Suryadevara"
                    }
                ],
                "author_detail": {
                    "name": "Pranav Suryadevara"
                },
                "author": "Pranav Suryadevara",
                "arxiv_comment": "5 pages, 5 figures. Individual Project",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13064v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13064v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.2; C.1.3; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18862v1",
                "updated": "2025-03-24T16:38:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    38,
                    31,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:38:31Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    38,
                    31,
                    0,
                    83,
                    0
                ],
                "title": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid\n  Transformers for Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid\n  Transformers for Semantic Segmentation"
                },
                "summary": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation."
                },
                "authors": [
                    {
                        "name": "DeShin Hwa"
                    },
                    {
                        "name": "Tobias Holmes"
                    },
                    {
                        "name": "Klaus Drechsler"
                    }
                ],
                "author_detail": {
                    "name": "Klaus Drechsler"
                },
                "author": "Klaus Drechsler",
                "arxiv_doi": "10.1007/978-3-658-47422-5_71",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-658-47422-5_71",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, 3 figures, Preprint. Final version published in:\n  Bildverarbeitung f\\\"ur die Medizin 2025, Springer. DOI:\n  https://doi.org/10.1007/978-3-658-47422-5_71",
                "arxiv_journal_ref": "Bildverarbeitung f\\\"ur die Medizin 2025. BVM 2025. Informatik\n  aktuell. Springer Vieweg, Wiesbaden, pp 305-310",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18773v1",
                "updated": "2025-03-24T15:22:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T15:22:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache"
                },
                "summary": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding."
                },
                "authors": [
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05941v2",
                "updated": "2025-03-24T13:09:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    9,
                    3,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-07T21:16:41Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "title": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions"
                },
                "summary": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Avinash Kumar"
                },
                "author": "Avinash Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18599v1",
                "updated": "2025-03-24T11:56:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:56:50Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "title": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization"
                },
                "summary": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques."
                },
                "authors": [
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Seongmin Hong"
                    },
                    {
                        "name": "RyeoWook Ko"
                    },
                    {
                        "name": "Soongyu Choi"
                    },
                    {
                        "name": "Hunjong Lee"
                    },
                    {
                        "name": "Junsoo Kim"
                    },
                    {
                        "name": "Joo-Young Kim"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "arxiv_comment": "15 pages, 14 figures, and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17333v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17333v2",
                "updated": "2025-03-24T11:00:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    0,
                    35,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-21T17:33:03Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    33,
                    3,
                    4,
                    80,
                    0
                ],
                "title": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs"
                },
                "summary": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical."
                },
                "authors": [
                    {
                        "name": "Vasileios Titopoulos"
                    },
                    {
                        "name": "George Alexakis"
                    },
                    {
                        "name": "Kosmas Alexandridis"
                    },
                    {
                        "name": "Chrysostomos Nicopoulos"
                    },
                    {
                        "name": "Giorgos Dimitrakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Dimitrakopoulos"
                },
                "author": "Giorgos Dimitrakopoulos",
                "arxiv_comment": "22nd ACM International Conference on Computing Frontiers (CF' 25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17333v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18334v1",
                "updated": "2025-03-24T04:32:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    4,
                    32,
                    35,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T04:32:35Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    4,
                    32,
                    35,
                    0,
                    83,
                    0
                ],
                "title": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models"
                },
                "summary": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n``Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n``Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability."
                },
                "authors": [
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Tianming Sha"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICME 2025 and ICLR 2025 Workshop on Foundation Models in\n  the Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16653v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16653v2",
                "updated": "2025-03-24T03:18:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    18,
                    49,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-20T19:10:37Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    19,
                    10,
                    37,
                    3,
                    79,
                    0
                ],
                "title": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation"
                },
                "summary": "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse."
                },
                "authors": [
                    {
                        "name": "Hanxiao Wang"
                    },
                    {
                        "name": "Biao Zhang"
                    },
                    {
                        "name": "Weize Quan"
                    },
                    {
                        "name": "Dong-Ming Yan"
                    },
                    {
                        "name": "Peter Wonka"
                    }
                ],
                "author_detail": {
                    "name": "Peter Wonka"
                },
                "author": "Peter Wonka",
                "arxiv_comment": "Project website: https://wanghanxiao123.github.io/iFa/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16653v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18292v1",
                "updated": "2025-03-24T02:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    28,
                    4,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T02:28:04Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    28,
                    4,
                    0,
                    83,
                    0
                ],
                "title": "Jenga: Effective Memory Management for Serving LLM with Heterogeneity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jenga: Effective Memory Management for Serving LLM with Heterogeneity"
                },
                "summary": "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average)."
                },
                "authors": [
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Woosuk Kwon"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Yufeng Wang"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Kaichao You"
                    },
                    {
                        "name": "Zhuohan Li"
                    },
                    {
                        "name": "Mingsheng Long"
                    },
                    {
                        "name": "Jidong Zhai"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_comment": "16 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v5",
                "updated": "2025-03-24T02:17:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    17,
                    34,
                    0,
                    83,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe."
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Rewrite the methods section. Add more ablation studies and results in\n  LongVideoBench. Update metadata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18278v1",
                "updated": "2025-03-24T01:47:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    47,
                    26,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T01:47:26Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    47,
                    26,
                    0,
                    83,
                    0
                ],
                "title": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model"
                },
                "summary": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach."
                },
                "authors": [
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Jinqi Xiao"
                    },
                    {
                        "name": "Lingyi Huang"
                    },
                    {
                        "name": "Yu Gong"
                    },
                    {
                        "name": "Chendi Li"
                    },
                    {
                        "name": "Jinghua Yan"
                    },
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Ponnuswamy Sadayappan"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Bo Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Bo Yuan"
                },
                "author": "Bo Yuan",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18265v1",
                "updated": "2025-03-24T01:15:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    15,
                    43,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T01:15:43Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    15,
                    43,
                    0,
                    83,
                    0
                ],
                "title": "Risk Management for Distributed Arbitrage Systems: Integrating\n  Artificial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Risk Management for Distributed Arbitrage Systems: Integrating\n  Artificial Intelligence"
                },
                "summary": "Effective risk management solutions become absolutely crucial when financial\nmarkets embrace distributed technology and decentralized financing (DeFi). This\nstudy offers a thorough survey and comparative analysis of the integration of\nartificial intelligence (AI) in risk management for distributed arbitrage\nsystems. We examine several modern caching techniques namely in memory caching,\ndistributed caching, and proxy caching and their functions in enhancing\nperformance in decentralized settings. Through literature review we examine the\nutilization of AI techniques for alleviating risks related to market\nvolatility, liquidity challenges, operational failures, regulatory compliance,\nand security threats. This comparison research evaluates various case studies\nfrom prominent DeFi technologies, emphasizing critical performance metrics like\nlatency reduction, load balancing, and system resilience. Additionally, we\nexamine the problems and trade offs associated with these technologies,\nemphasizing their effects on consistency, scalability, and fault tolerance. By\nmeticulously analyzing real world applications, specifically centering on the\nAave platform as our principal case study, we illustrate how the purposeful\namalgamation of AI with contemporary caching methodologies has revolutionized\nrisk management in distributed arbitrage systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective risk management solutions become absolutely crucial when financial\nmarkets embrace distributed technology and decentralized financing (DeFi). This\nstudy offers a thorough survey and comparative analysis of the integration of\nartificial intelligence (AI) in risk management for distributed arbitrage\nsystems. We examine several modern caching techniques namely in memory caching,\ndistributed caching, and proxy caching and their functions in enhancing\nperformance in decentralized settings. Through literature review we examine the\nutilization of AI techniques for alleviating risks related to market\nvolatility, liquidity challenges, operational failures, regulatory compliance,\nand security threats. This comparison research evaluates various case studies\nfrom prominent DeFi technologies, emphasizing critical performance metrics like\nlatency reduction, load balancing, and system resilience. Additionally, we\nexamine the problems and trade offs associated with these technologies,\nemphasizing their effects on consistency, scalability, and fault tolerance. By\nmeticulously analyzing real world applications, specifically centering on the\nAave platform as our principal case study, we illustrate how the purposeful\namalgamation of AI with contemporary caching methodologies has revolutionized\nrisk management in distributed arbitrage systems."
                },
                "authors": [
                    {
                        "name": "Akaash Vishal Hazarika"
                    },
                    {
                        "name": "Mahak Shah"
                    },
                    {
                        "name": "Swapnil Patil"
                    },
                    {
                        "name": "Pradyumna Shukla"
                    }
                ],
                "author_detail": {
                    "name": "Pradyumna Shukla"
                },
                "author": "Pradyumna Shukla",
                "arxiv_comment": "International Conference on AI and Financial Innovation AIFI-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18191v1",
                "updated": "2025-03-23T20:18:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T20:18:16Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "title": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems"
                },
                "summary": "The large-scale, multi-tenant nature of cloud computing requires distributed\nfile systems that offer stability, adaptability, and compatibility. FUSE-based\ndistributed file systems have emerged as a popular solution for the cloud,\noffering fast deployment, fault isolation, and POSIX compliance. However,\nFUSE's performance limitations, particularly its inability to reconcile page\ncaching with strong consistency in distributed environments, remain a\npersistent problem. Existing approaches either sacrifice consistency for\nperformance or rely on inefficient caching, limiting their practicality.\n  To this end, we present DistFUSE, the first FUSE-based distributed file\nsystem that relies on a write-back kernel-based page cache for performance and\nprovides strong consistency. DistFUSE achieves this by offloading userspace\nlock management to the kernel driver, allowing coordinated access to the\nkernel's page cache across nodes. This design eliminates blind local cache\nupdates and ensures cluster-wide consistency without compromising performance.\nOur evaluation shows DistFUSE improves throughput by up to 75% compared to\nbaseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The large-scale, multi-tenant nature of cloud computing requires distributed\nfile systems that offer stability, adaptability, and compatibility. FUSE-based\ndistributed file systems have emerged as a popular solution for the cloud,\noffering fast deployment, fault isolation, and POSIX compliance. However,\nFUSE's performance limitations, particularly its inability to reconcile page\ncaching with strong consistency in distributed environments, remain a\npersistent problem. Existing approaches either sacrifice consistency for\nperformance or rely on inefficient caching, limiting their practicality.\n  To this end, we present DistFUSE, the first FUSE-based distributed file\nsystem that relies on a write-back kernel-based page cache for performance and\nprovides strong consistency. DistFUSE achieves this by offloading userspace\nlock management to the kernel driver, allowing coordinated access to the\nkernel's page cache across nodes. This design eliminates blind local cache\nupdates and ensures cluster-wide consistency without compromising performance.\nOur evaluation shows DistFUSE improves throughput by up to 75% compared to\nbaseline approaches."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingkai Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Windsor Hsu"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18030v1",
                "updated": "2025-03-23T11:07:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    11,
                    7,
                    24,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T11:07:24Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    11,
                    7,
                    24,
                    6,
                    82,
                    0
                ],
                "title": "Formal Verification of Parameterized Systems based on Induction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal Verification of Parameterized Systems based on Induction"
                },
                "summary": "Parameterized systems play a crucial role in the computer field, and their\nsecurity is of great significance. Formal verification of parameterized\nprotocols is especially challenging due to its \"parameterized\" feature, which\nbrings complexity and undecidability. Existing automated parameterized\nverification methods have limitations, such as facing difficulties in\nautomatically deriving parameterized invariants constrained by mixed Forall and\nExists quantifiers, or having challenges in completing the parameterized\nverification of large and complex protocols. This paper proposes a formal\nverification framework for parameterized systems based on induction, named\nwiseParaverifier. It starts from small concretizations of protocols, analyzes\ninductive counterexamples, and constructs counterexample formulas to guide the\nentire process of parameterized verification. It also presents a heuristic\nGeneralize method to quickly find auxiliary invariants, a method for promoting\ncomplex mixed quantifiers and merging parameterized invariants, and uses\nsymmetric reduction ideas to accelerate the verification process. Experimental\nresults show that wiseParaverifier can successfully complete automatic\ninductive verification on 7 cache coherence protocols and 10 distributed\nprotocols. It has strong verification capabilities and migration capabilities,\nand can provide concise and readable verification results, which is helpful for\nlearners to understand protocol behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterized systems play a crucial role in the computer field, and their\nsecurity is of great significance. Formal verification of parameterized\nprotocols is especially challenging due to its \"parameterized\" feature, which\nbrings complexity and undecidability. Existing automated parameterized\nverification methods have limitations, such as facing difficulties in\nautomatically deriving parameterized invariants constrained by mixed Forall and\nExists quantifiers, or having challenges in completing the parameterized\nverification of large and complex protocols. This paper proposes a formal\nverification framework for parameterized systems based on induction, named\nwiseParaverifier. It starts from small concretizations of protocols, analyzes\ninductive counterexamples, and constructs counterexample formulas to guide the\nentire process of parameterized verification. It also presents a heuristic\nGeneralize method to quickly find auxiliary invariants, a method for promoting\ncomplex mixed quantifiers and merging parameterized invariants, and uses\nsymmetric reduction ideas to accelerate the verification process. Experimental\nresults show that wiseParaverifier can successfully complete automatic\ninductive verification on 7 cache coherence protocols and 10 distributed\nprotocols. It has strong verification capabilities and migration capabilities,\nand can provide concise and readable verification results, which is helpful for\nlearners to understand protocol behaviors."
                },
                "authors": [
                    {
                        "name": "Jiaqi Xiu"
                    },
                    {
                        "name": "Yongjian Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongjian Li"
                },
                "author": "Yongjian Li",
                "arxiv_comment": "9 pages,2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.10425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.10425v2",
                "updated": "2025-03-23T06:14:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    6,
                    14,
                    35,
                    6,
                    82,
                    0
                ],
                "published": "2023-12-16T11:40:49Z",
                "published_parsed": [
                    2023,
                    12,
                    16,
                    11,
                    40,
                    49,
                    5,
                    350,
                    0
                ],
                "title": "Knowledge Rumination for Client Utility Evaluation in Heterogeneous\n  Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Rumination for Client Utility Evaluation in Heterogeneous\n  Federated Learning"
                },
                "summary": "Federated Learning (FL) allows several clients to cooperatively train machine\nlearning models without disclosing the raw data. In practical applications,\nasynchronous FL (AFL) can address the straggler effect compared to synchronous\nFL. However, Non-IID data and stale models pose significant challenges to AFL,\nas they can diminish the practicality of the global model and even lead to\ntraining failures. In this work, we propose a novel AFL framework called\nFederated Historical Learning (FedHist), which effectively addresses the\nchallenges posed by both Non-IID data and gradient staleness based on the\nconcept of knowledge rumination. FedHist enhances the stability of local\ngradients by performing weighted fusion with historical global gradients cached\non the server. Relying on hindsight, it assigns aggregation weights to each\nparticipant in a multi-dimensional manner during each communication round. To\nfurther enhance the efficiency and stability of the training process, we\nintroduce an intelligent $\\ell_2$-norm amplification scheme, which dynamically\nregulates the learning progress based on the $\\ell_2$-norms of the submitted\ngradients. Extensive experiments indicate FedHist outperforms state-of-the-art\nmethods in terms of convergence performance and test accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) allows several clients to cooperatively train machine\nlearning models without disclosing the raw data. In practical applications,\nasynchronous FL (AFL) can address the straggler effect compared to synchronous\nFL. However, Non-IID data and stale models pose significant challenges to AFL,\nas they can diminish the practicality of the global model and even lead to\ntraining failures. In this work, we propose a novel AFL framework called\nFederated Historical Learning (FedHist), which effectively addresses the\nchallenges posed by both Non-IID data and gradient staleness based on the\nconcept of knowledge rumination. FedHist enhances the stability of local\ngradients by performing weighted fusion with historical global gradients cached\non the server. Relying on hindsight, it assigns aggregation weights to each\nparticipant in a multi-dimensional manner during each communication round. To\nfurther enhance the efficiency and stability of the training process, we\nintroduce an intelligent $\\ell_2$-norm amplification scheme, which dynamically\nregulates the learning progress based on the $\\ell_2$-norms of the submitted\ngradients. Extensive experiments indicate FedHist outperforms state-of-the-art\nmethods in terms of convergence performance and test accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaorui Jiang"
                    },
                    {
                        "name": "Yu Gao"
                    },
                    {
                        "name": "Hengwei Xu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Yong Liao"
                    },
                    {
                        "name": "Pengyuan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Pengyuan Zhou"
                },
                "author": "Pengyuan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.10425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.10425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17913v1",
                "updated": "2025-03-23T03:20:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    20,
                    25,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T03:20:25Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    20,
                    25,
                    6,
                    82,
                    0
                ],
                "title": "Cache-Aware Cooperative Multicast Beamforming in Dynamic\n  Satellite-Terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aware Cooperative Multicast Beamforming in Dynamic\n  Satellite-Terrestrial Networks"
                },
                "summary": "With the burgeoning demand for data-intensive services, satellite-terrestrial\nnetworks (STNs) face increasing backhaul link congestion, deteriorating user\nquality of service (QoS), and escalating power consumption. Cache-aided STNs\nare acknowledged as a promising paradigm for accelerating content delivery to\nusers and alleviating the load of backhaul links. However, the dynamic nature\nof low earth orbit (LEO) satellites and the complex interference among\nsatellite beams and terrestrial base stations pose challenges in effectively\nmanaging limited edge resources. To address these issues, this paper proposes a\nmethod for dynamically scheduling caching and communication resources, aiming\nto reduce network costs in terms of transmission power consumption and backhaul\ntraffic, while meeting user QoS demands and resource constraints. We formulate\na mixed timescale problem to jointly optimize cache placement, LEO satellite\nbeam direction, and cooperative multicast beamforming among satellite beams and\nbase stations. To tackle this intricate problem, we propose a two-stage\nsolution framework, where the primary problem is decoupled into a short-term\ncontent delivery subproblem and a long-term cache placement subproblem. The\nformer subproblem is solved by designing an alternating optimization approach\nwith whale optimization and successive convex approximation methods according\nto the cache placement state, while cache content in STNs is updated using an\niterative algorithm that utilizes historical information. Simulation results\ndemonstrate the effectiveness of our proposed algorithms, showcasing their\nconvergence and significantly reducing transmission power consumption and\nbackhaul traffic by up to 52%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the burgeoning demand for data-intensive services, satellite-terrestrial\nnetworks (STNs) face increasing backhaul link congestion, deteriorating user\nquality of service (QoS), and escalating power consumption. Cache-aided STNs\nare acknowledged as a promising paradigm for accelerating content delivery to\nusers and alleviating the load of backhaul links. However, the dynamic nature\nof low earth orbit (LEO) satellites and the complex interference among\nsatellite beams and terrestrial base stations pose challenges in effectively\nmanaging limited edge resources. To address these issues, this paper proposes a\nmethod for dynamically scheduling caching and communication resources, aiming\nto reduce network costs in terms of transmission power consumption and backhaul\ntraffic, while meeting user QoS demands and resource constraints. We formulate\na mixed timescale problem to jointly optimize cache placement, LEO satellite\nbeam direction, and cooperative multicast beamforming among satellite beams and\nbase stations. To tackle this intricate problem, we propose a two-stage\nsolution framework, where the primary problem is decoupled into a short-term\ncontent delivery subproblem and a long-term cache placement subproblem. The\nformer subproblem is solved by designing an alternating optimization approach\nwith whale optimization and successive convex approximation methods according\nto the cache placement state, while cache content in STNs is updated using an\niterative algorithm that utilizes historical information. Simulation results\ndemonstrate the effectiveness of our proposed algorithms, showcasing their\nconvergence and significantly reducing transmission power consumption and\nbackhaul traffic by up to 52%."
                },
                "authors": [
                    {
                        "name": "Shuo Yuan"
                    },
                    {
                        "name": "Yaohua Sun"
                    },
                    {
                        "name": "Mugen Peng"
                    }
                ],
                "author_detail": {
                    "name": "Mugen Peng"
                },
                "author": "Mugen Peng",
                "arxiv_doi": "10.1109/TVT.2024.3463548",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVT.2024.3463548",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.17913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by IEEE Transactions on Vehicular Technology",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v1",
                "updated": "2025-03-23T03:16:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "16 pages, the report of open-source library VSAG\n  (https://github.com/antgroup/vsag)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17895v1",
                "updated": "2025-03-23T01:17:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    1,
                    17,
                    8,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T01:17:08Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    1,
                    17,
                    8,
                    6,
                    82,
                    0
                ],
                "title": "Orientation-Dependent \\b{eta}-Ga2O3 Heterojunction Diode with Atomic\n  Layer Deposition (ALD) Grown NiO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orientation-Dependent \\b{eta}-Ga2O3 Heterojunction Diode with Atomic\n  Layer Deposition (ALD) Grown NiO"
                },
                "summary": "This work reports the demonstration of ALD-deposited NiO/\\b{eta}-Ga2O3\nheterojunction diodes (HJDs) on low doped drift layer and highly doped (001) &\n(100) n+ substrates with experimental observation of a parallel-plane junction\nelectric field as high as 7.5 MV/cm, revealing a crystal orientation dependence\nin \\b{eta}-Ga2O3. We use a novel metalorganic precursor\nbis(1,4-di-tert-butyl-1,3-diazadienyl) (nickel Ni(tBu2DAD)2) with ozone (O3) to\ndeposit NiO. The NiO/\\b{eta}-Ga2O3 HJD on 7.7 {\\mu}m-thick HVPE-grown drift\nregion exhibited an on-state current density of ~20 A/cm2 at 5 V, ~10-8 A/cm2\nreverse leakage at low reverse bias(-5 V), and a rectifying ratio(Jon/Joff) of\n~109. The HJD broke down at ~2.2 kV reverse bias, corresponding to a ~3.4 MV/cm\nparallel-plane junction electric field, with a noise floor reverse leakage\n(10-8~10-6 A/cm2, nA) at 80% of the device catastrophic breakdown voltage. The\nNiO/\\b{eta}-Ga2O3 HJDs on n+ (001) & (100) highly-doped substrates exhibited\nbreakdown voltages at 12.5-16.0 V and 28.5-70.5 V, respectively, with extracted\ncritical electric field (EC) at 2.30-2.76 MV/cm, and 4.33-7.50 MV/cm, revealing\na substrate crystal orientation dependence on breakdown electric field for\n\\b{eta}-Ga2O3. The 7.5 MV/cm EC reported here is one of the highest\nparallel-plane junction electric fields reported in literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reports the demonstration of ALD-deposited NiO/\\b{eta}-Ga2O3\nheterojunction diodes (HJDs) on low doped drift layer and highly doped (001) &\n(100) n+ substrates with experimental observation of a parallel-plane junction\nelectric field as high as 7.5 MV/cm, revealing a crystal orientation dependence\nin \\b{eta}-Ga2O3. We use a novel metalorganic precursor\nbis(1,4-di-tert-butyl-1,3-diazadienyl) (nickel Ni(tBu2DAD)2) with ozone (O3) to\ndeposit NiO. The NiO/\\b{eta}-Ga2O3 HJD on 7.7 {\\mu}m-thick HVPE-grown drift\nregion exhibited an on-state current density of ~20 A/cm2 at 5 V, ~10-8 A/cm2\nreverse leakage at low reverse bias(-5 V), and a rectifying ratio(Jon/Joff) of\n~109. The HJD broke down at ~2.2 kV reverse bias, corresponding to a ~3.4 MV/cm\nparallel-plane junction electric field, with a noise floor reverse leakage\n(10-8~10-6 A/cm2, nA) at 80% of the device catastrophic breakdown voltage. The\nNiO/\\b{eta}-Ga2O3 HJDs on n+ (001) & (100) highly-doped substrates exhibited\nbreakdown voltages at 12.5-16.0 V and 28.5-70.5 V, respectively, with extracted\ncritical electric field (EC) at 2.30-2.76 MV/cm, and 4.33-7.50 MV/cm, revealing\na substrate crystal orientation dependence on breakdown electric field for\n\\b{eta}-Ga2O3. The 7.5 MV/cm EC reported here is one of the highest\nparallel-plane junction electric fields reported in literature."
                },
                "authors": [
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Shane M. W. Witsell"
                    },
                    {
                        "name": "John F. Conley"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17603v1",
                "updated": "2025-03-22T01:17:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    17,
                    56,
                    5,
                    81,
                    0
                ],
                "published": "2025-03-22T01:17:56Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    17,
                    56,
                    5,
                    81,
                    0
                ],
                "title": "A Generative Caching System for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generative Caching System for Large Language Models"
                },
                "summary": "Caching has the potential to be of significant benefit for accessing large\nlanguage models (LLMs) due to their high latencies which typically range from a\nsmall number of seconds to well over a minute. Furthermore, many LLMs charge\nmoney for queries; caching thus has a clear monetary benefit. This paper\npresents a new caching system for improving user experiences with LLMs. In\naddition to reducing both latencies and monetary costs for accessing LLMs, our\nsystem also provides important features that go beyond the performance benefits\ntypically associated with caches. A key feature we provide is generative\ncaching, wherein multiple cached responses can be synthesized to provide\nanswers to queries which have never been seen before. Our generative caches\nfunction as repositories of valuable information which can be mined and\nanalyzed. We also improve upon past semantic caching techniques by tailoring\nthe caching algorithms to optimally balance cost and latency reduction with the\nquality of responses provided. Performance tests indicate that our caches are\nconsiderably faster than GPTcache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching has the potential to be of significant benefit for accessing large\nlanguage models (LLMs) due to their high latencies which typically range from a\nsmall number of seconds to well over a minute. Furthermore, many LLMs charge\nmoney for queries; caching thus has a clear monetary benefit. This paper\npresents a new caching system for improving user experiences with LLMs. In\naddition to reducing both latencies and monetary costs for accessing LLMs, our\nsystem also provides important features that go beyond the performance benefits\ntypically associated with caches. A key feature we provide is generative\ncaching, wherein multiple cached responses can be synthesized to provide\nanswers to queries which have never been seen before. Our generative caches\nfunction as repositories of valuable information which can be mined and\nanalyzed. We also improve upon past semantic caching techniques by tailoring\nthe caching algorithms to optimally balance cost and latency reduction with the\nquality of responses provided. Performance tests indicate that our caches are\nconsiderably faster than GPTcache."
                },
                "authors": [
                    {
                        "name": "Arun Iyengar"
                    },
                    {
                        "name": "Ashish Kundu"
                    },
                    {
                        "name": "Ramana Kompella"
                    },
                    {
                        "name": "Sai Nandan Mamidi"
                    }
                ],
                "author_detail": {
                    "name": "Sai Nandan Mamidi"
                },
                "author": "Sai Nandan Mamidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17602v1",
                "updated": "2025-03-22T01:16:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    16,
                    24,
                    5,
                    81,
                    0
                ],
                "published": "2025-03-22T01:16:24Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    16,
                    24,
                    5,
                    81,
                    0
                ],
                "title": "Multiport Support for Vortex OpenGPU Memory Hierarchy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiport Support for Vortex OpenGPU Memory Hierarchy"
                },
                "summary": "Modern day applications have grown in size and require more computational\npower. The rise of machine learning and AI increased the need for parallel\ncomputation, which has increased the need for GPGPUs. With the increasing\ndemand for computational power, GPGPUs' SIMT architecture has solved this with\nan increase in the number of threads and the number of cores in a GPU,\nincreasing the throughput of these processors to match the demand of the\napplications. However, this created a larger demand for the memory, making the\nmemory bandwidth a bottleneck. The introduction of High-Bandwidth Memory (HBM)\nwith its increased number of memory ports offers a potential solution for the\nGPU to exploit its memory parallelism to increase the memory bandwidth.\nHowever, effectively leveraging HBM's memory parallelism to maximize bandwidth\npresents a unique and complex challenge for GPU architectures on how to\ndistribute those ports among the streaming multiprocessors in the GPGPU. In\nthis work, we extend the Vortex OpenGPU microarchitecture to incorporate a\nmultiport memory hierarchy, spanning from the L1 cache to the last-level cache\n(LLC). In addition, we propose various arbitration strategies to optimize\nmemory transfers across the cache hierarchy. The results have shown that an\nincrease in memory ports increases IPC, achieving an average speedup of 2.34x\nwith 8 memory ports in the tested configuration while showing relatively small\narea overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern day applications have grown in size and require more computational\npower. The rise of machine learning and AI increased the need for parallel\ncomputation, which has increased the need for GPGPUs. With the increasing\ndemand for computational power, GPGPUs' SIMT architecture has solved this with\nan increase in the number of threads and the number of cores in a GPU,\nincreasing the throughput of these processors to match the demand of the\napplications. However, this created a larger demand for the memory, making the\nmemory bandwidth a bottleneck. The introduction of High-Bandwidth Memory (HBM)\nwith its increased number of memory ports offers a potential solution for the\nGPU to exploit its memory parallelism to increase the memory bandwidth.\nHowever, effectively leveraging HBM's memory parallelism to maximize bandwidth\npresents a unique and complex challenge for GPU architectures on how to\ndistribute those ports among the streaming multiprocessors in the GPGPU. In\nthis work, we extend the Vortex OpenGPU microarchitecture to incorporate a\nmultiport memory hierarchy, spanning from the L1 cache to the last-level cache\n(LLC). In addition, we propose various arbitration strategies to optimize\nmemory transfers across the cache hierarchy. The results have shown that an\nincrease in memory ports increases IPC, achieving an average speedup of 2.34x\nwith 8 memory ports in the tested configuration while showing relatively small\narea overhead."
                },
                "authors": [
                    {
                        "name": "Injae Shin"
                    },
                    {
                        "name": "Blaise Tine"
                    }
                ],
                "author_detail": {
                    "name": "Blaise Tine"
                },
                "author": "Blaise Tine",
                "arxiv_comment": "OSSMPIC2025, 1st workshop on Open Source Solutions for Massively\n  Parallel Integrated Circuits",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v2",
                "updated": "2025-03-21T21:10:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    21,
                    10,
                    2,
                    4,
                    80,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09003v2",
                "updated": "2025-03-21T19:26:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    19,
                    26,
                    12,
                    4,
                    80,
                    0
                ],
                "published": "2025-02-13T06:44:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models"
                },
                "summary": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia, Qwen and\nLlama models of different sizes demonstrate the effectiveness of RoSTE.\nCompared to existing post-SFT quantization baselines, our method consistently\nachieves superior performances across various tasks and different LLM\narchitectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia, Qwen and\nLlama models of different sizes demonstrate the effectiveness of RoSTE.\nCompared to existing post-SFT quantization baselines, our method consistently\nachieves superior performances across various tasks and different LLM\narchitectures."
                },
                "authors": [
                    {
                        "name": "Quan Wei"
                    },
                    {
                        "name": "Chung-Yiu Yau"
                    },
                    {
                        "name": "Hoi-To Wai"
                    },
                    {
                        "name": "Yang Katie Zhao"
                    },
                    {
                        "name": "Dongyeop Kang"
                    },
                    {
                        "name": "Youngsuk Park"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "author": "Mingyi Hong",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v3",
                "updated": "2025-03-21T15:52:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    52,
                    39,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency. Code: https://github.com/shawnricecake/lazydit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency. Code: https://github.com/shawnricecake/lazydit"
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v3",
                "updated": "2025-03-21T15:47:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    47,
                    53,
                    4,
                    80,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "24 pages, 11 figures, ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v4",
                "updated": "2025-03-21T13:30:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    13,
                    30,
                    33,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Accepted to ICLR 2025. Code is available at\n  https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v3",
                "updated": "2025-03-21T12:51:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    51,
                    15,
                    4,
                    80,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16870v1",
                "updated": "2025-03-21T05:58:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T05:58:18Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs"
                },
                "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B."
                },
                "authors": [
                    {
                        "name": "Anshumann"
                    },
                    {
                        "name": "Mohd Abbas Zaidi"
                    },
                    {
                        "name": "Akhil Kedia"
                    },
                    {
                        "name": "Jinwoo Ahn"
                    },
                    {
                        "name": "Taehwak Kwon"
                    },
                    {
                        "name": "Kangwook Lee"
                    },
                    {
                        "name": "Haejun Lee"
                    },
                    {
                        "name": "Joohyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Lee"
                },
                "author": "Joohyung Lee",
                "arxiv_comment": "Anshumann, Mohd Abbas Zaidi and Akhil Kedia have Equal Contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16131v2",
                "updated": "2025-03-21T01:59:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    1,
                    59,
                    12,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-20T13:25:03Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    25,
                    3,
                    3,
                    79,
                    0
                ],
                "title": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 35.03% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 35.03% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds."
                },
                "authors": [
                    {
                        "name": "Feiyang Li"
                    },
                    {
                        "name": "Yingjian Chen"
                    },
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Han Yuan"
                    },
                    {
                        "name": "Yuang Jiang"
                    },
                    {
                        "name": "Tianxiao Li"
                    },
                    {
                        "name": "Edison Marrese Taylor"
                    },
                    {
                        "name": "Hossein Rouhizadeh"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Douglas Teodoro"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02430v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02430v3",
                "updated": "2025-03-20T21:49:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    21,
                    49,
                    15,
                    3,
                    79,
                    0
                ],
                "published": "2025-02-04T15:55:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals"
                },
                "summary": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach."
                },
                "authors": [
                    {
                        "name": "RÃ³bert Busa-Fekete"
                    },
                    {
                        "name": "Julian Zimmert"
                    },
                    {
                        "name": "AndrÃ¡s GyÃ¶rgy"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Tzu-Wei Sung"
                    },
                    {
                        "name": "Hao Shen"
                    },
                    {
                        "name": "Hyomin Choi"
                    },
                    {
                        "name": "Sharmila Subramaniam"
                    },
                    {
                        "name": "Li Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiao"
                },
                "author": "Li Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02430v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02430v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v1",
                "updated": "2025-03-20T17:37:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to\nblock-wise competitiveness and systematically analyze the competitiveness and\nblock competitiveness of FIFO and MRU relative to LRU for arbitrary\nassociativities. We show how competitiveness and block competitiveness can be\nexploited in state-of-the-art WCET analysis based on the results of existing\npersistence analyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to\nblock-wise competitiveness and systematically analyze the competitiveness and\nblock competitiveness of FIFO and MRU relative to LRU for arbitrary\nassociativities. We show how competitiveness and block competitiveness can be\nexploited in state-of-the-art WCET analysis based on the results of existing\npersistence analyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16257v1",
                "updated": "2025-03-20T15:52:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T15:52:43Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models"
                },
                "summary": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16163v1",
                "updated": "2025-03-20T14:01:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    1,
                    56,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T14:01:56Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    1,
                    56,
                    3,
                    79,
                    0
                ],
                "title": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs"
                },
                "summary": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio."
                },
                "authors": [
                    {
                        "name": "Shibo Jie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Zhi-Hong Deng"
                    },
                    {
                        "name": "Jing Han"
                    }
                ],
                "author_detail": {
                    "name": "Jing Han"
                },
                "author": "Jing Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16112v1",
                "updated": "2025-03-20T13:00:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    36,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T13:00:36Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    36,
                    3,
                    79,
                    0
                ],
                "title": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming"
                },
                "summary": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6\\%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60\\% of severely distorted\nframes (compared to VQGAN).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6\\%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60\\% of severely distorted\nframes (compared to VQGAN)."
                },
                "authors": [
                    {
                        "name": "Liming Liu"
                    },
                    {
                        "name": "Jiangkai Wu"
                    },
                    {
                        "name": "Haoyang Wang"
                    },
                    {
                        "name": "Peiheng Wang"
                    },
                    {
                        "name": "Xinggong Zhang"
                    },
                    {
                        "name": "Zongming Guo"
                    }
                ],
                "author_detail": {
                    "name": "Zongming Guo"
                },
                "author": "Zongming Guo",
                "arxiv_comment": "7 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15927v1",
                "updated": "2025-03-20T08:07:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    7,
                    31,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T08:07:31Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    7,
                    31,
                    3,
                    79,
                    0
                ],
                "title": "BlockDance: Reuse Structurally Similar Spatio-Temporal Features to\n  Accelerate Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockDance: Reuse Structurally Similar Spatio-Temporal Features to\n  Accelerate Diffusion Transformers"
                },
                "summary": "Diffusion models have demonstrated impressive generation capabilities,\nparticularly with recent advancements leveraging transformer architectures to\nimprove both visual and artistic quality. However, Diffusion Transformers\n(DiTs) continue to encounter challenges related to low inference speed,\nprimarily due to the iterative denoising process. To address this issue, we\npropose BlockDance, a training-free approach that explores feature similarities\nat adjacent time steps to accelerate DiTs. Unlike previous feature-reuse\nmethods that lack tailored reuse strategies for features at different scales,\nBlockDance prioritizes the identification of the most structurally similar\nfeatures, referred to as Structurally Similar Spatio-Temporal (STSS) features.\nThese features are primarily located within the structure-focused blocks of the\ntransformer during the later stages of denoising. BlockDance caches and reuses\nthese highly similar features to mitigate redundant computation, thereby\naccelerating DiTs while maximizing consistency with the generated results of\nthe original model. Furthermore, considering the diversity of generated content\nand the varying distributions of redundant features, we introduce\nBlockDance-Ada, a lightweight decision-making network tailored for\ninstance-specific acceleration. BlockDance-Ada dynamically allocates resources\nand provides superior content quality. Both BlockDance and BlockDance-Ada have\nproven effective across various generation tasks and models, achieving\naccelerations between 25% and 50% while maintaining generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated impressive generation capabilities,\nparticularly with recent advancements leveraging transformer architectures to\nimprove both visual and artistic quality. However, Diffusion Transformers\n(DiTs) continue to encounter challenges related to low inference speed,\nprimarily due to the iterative denoising process. To address this issue, we\npropose BlockDance, a training-free approach that explores feature similarities\nat adjacent time steps to accelerate DiTs. Unlike previous feature-reuse\nmethods that lack tailored reuse strategies for features at different scales,\nBlockDance prioritizes the identification of the most structurally similar\nfeatures, referred to as Structurally Similar Spatio-Temporal (STSS) features.\nThese features are primarily located within the structure-focused blocks of the\ntransformer during the later stages of denoising. BlockDance caches and reuses\nthese highly similar features to mitigate redundant computation, thereby\naccelerating DiTs while maximizing consistency with the generated results of\nthe original model. Furthermore, considering the diversity of generated content\nand the varying distributions of redundant features, we introduce\nBlockDance-Ada, a lightweight decision-making network tailored for\ninstance-specific acceleration. BlockDance-Ada dynamically allocates resources\nand provides superior content quality. Both BlockDance and BlockDance-Ada have\nproven effective across various generation tasks and models, achieving\naccelerations between 25% and 50% while maintaining generation quality."
                },
                "authors": [
                    {
                        "name": "Hui Zhang"
                    },
                    {
                        "name": "Tingwei Gao"
                    },
                    {
                        "name": "Jie Shao"
                    },
                    {
                        "name": "Zuxuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zuxuan Wu"
                },
                "author": "Zuxuan Wu",
                "arxiv_comment": "Accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18921v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18921v2",
                "updated": "2025-03-20T05:23:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    5,
                    23,
                    42,
                    3,
                    79,
                    0
                ],
                "published": "2024-07-09T13:47:05Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    13,
                    47,
                    5,
                    1,
                    191,
                    0
                ],
                "title": "Mobile Edge Intelligence for Large Language Models: A Contemporary\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Intelligence for Large Language Models: A Contemporary\n  Survey"
                },
                "summary": "On-device large language models (LLMs), referring to running LLMs on edge\ndevices, have raised considerable interest since they are more cost-effective,\nlatency-efficient, and privacy-preserving compared with the cloud paradigm.\nNonetheless, the performance of on-device LLMs is intrinsically constrained by\nresource limitations on edge devices. Sitting between cloud and on-device AI,\nmobile edge intelligence (MEI) presents a viable solution by provisioning AI\ncapabilities at the edge of mobile networks, enabling end users to offload\nheavy AI computation to capable edge servers nearby. This article provides a\ncontemporary survey on harnessing MEI for LLMs. We begin by illustrating\nseveral killer applications to demonstrate the urgent need for deploying LLMs\nat the network edge. Next, we present the preliminaries of LLMs and MEI,\nfollowed by resource-efficient LLM techniques. We then present an architectural\noverview of MEI for LLMs (MEI4LLM), outlining its core components and how it\nsupports the deployment of LLMs. Subsequently, we delve into various aspects of\nMEI4LLM, extensively covering edge LLM caching and delivery, edge LLM training,\nand edge LLM inference. Finally, we identify future research opportunities. We\nhope this article inspires researchers in the field to leverage mobile edge\ncomputing to facilitate LLM deployment, thereby unleashing the potential of\nLLMs across various privacy- and delay-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device large language models (LLMs), referring to running LLMs on edge\ndevices, have raised considerable interest since they are more cost-effective,\nlatency-efficient, and privacy-preserving compared with the cloud paradigm.\nNonetheless, the performance of on-device LLMs is intrinsically constrained by\nresource limitations on edge devices. Sitting between cloud and on-device AI,\nmobile edge intelligence (MEI) presents a viable solution by provisioning AI\ncapabilities at the edge of mobile networks, enabling end users to offload\nheavy AI computation to capable edge servers nearby. This article provides a\ncontemporary survey on harnessing MEI for LLMs. We begin by illustrating\nseveral killer applications to demonstrate the urgent need for deploying LLMs\nat the network edge. Next, we present the preliminaries of LLMs and MEI,\nfollowed by resource-efficient LLM techniques. We then present an architectural\noverview of MEI for LLMs (MEI4LLM), outlining its core components and how it\nsupports the deployment of LLMs. Subsequently, we delve into various aspects of\nMEI4LLM, extensively covering edge LLM caching and delivery, edge LLM training,\nand edge LLM inference. Finally, we identify future research opportunities. We\nhope this article inspires researchers in the field to leverage mobile edge\ncomputing to facilitate LLM deployment, thereby unleashing the potential of\nLLMs across various privacy- and delay-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Guanqiao Qu"
                    },
                    {
                        "name": "Qiyuan Chen"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "arxiv_comment": "42 pages, 17 figures. This paper has been accepted by IEEE\n  Communications Surveys & Tutorials",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18921v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18921v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15908v2",
                "updated": "2025-03-19T10:19:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    19,
                    30,
                    2,
                    78,
                    0
                ],
                "published": "2024-10-21T11:29:49Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "title": "Formalising CXL Cache Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalising CXL Cache Coherence"
                },
                "summary": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs."
                },
                "authors": [
                    {
                        "name": "Chengsong Tan"
                    },
                    {
                        "name": "Alastair F. Donaldson"
                    },
                    {
                        "name": "John Wickerson"
                    }
                ],
                "author_detail": {
                    "name": "John Wickerson"
                },
                "author": "John Wickerson",
                "arxiv_doi": "10.1145/3676641.3715999",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3715999",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.15908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages",
                "arxiv_journal_ref": "Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, ASPLOS\n  2025, Rotterdam, Netherlands",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68 (Primary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1; F.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14881v1",
                "updated": "2025-03-19T04:18:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    4,
                    18,
                    57,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T04:18:57Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    4,
                    18,
                    57,
                    2,
                    78,
                    0
                ],
                "title": "Exploring the Limits of KV Cache Compression in Visual Autoregressive\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Limits of KV Cache Compression in Visual Autoregressive\n  Transformers"
                },
                "summary": "A fundamental challenge in Visual Autoregressive models is the substantial\nmemory overhead required during inference to store previously generated\nrepresentations. Despite various attempts to mitigate this issue through\ncompression techniques, prior works have not explicitly formalized the problem\nof KV-cache compression in this context. In this work, we take the first step\nin formally defining the KV-cache compression problem for Visual Autoregressive\ntransformers. We then establish a fundamental negative result, proving that any\nmechanism for sequential visual token generation under attention-based\narchitectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log\nn)$, where $n$ is the number of tokens generated and $d$ is the embedding\ndimensionality. This result demonstrates that achieving truly sub-quadratic\nmemory usage is impossible without additional structural constraints. Our proof\nis constructed via a reduction from a computational lower bound problem,\nleveraging randomized embedding techniques inspired by dimensionality reduction\nprinciples. Finally, we discuss how sparsity priors on visual representations\ncan influence memory efficiency, presenting both impossibility results and\npotential directions for mitigating memory overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fundamental challenge in Visual Autoregressive models is the substantial\nmemory overhead required during inference to store previously generated\nrepresentations. Despite various attempts to mitigate this issue through\ncompression techniques, prior works have not explicitly formalized the problem\nof KV-cache compression in this context. In this work, we take the first step\nin formally defining the KV-cache compression problem for Visual Autoregressive\ntransformers. We then establish a fundamental negative result, proving that any\nmechanism for sequential visual token generation under attention-based\narchitectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log\nn)$, where $n$ is the number of tokens generated and $d$ is the embedding\ndimensionality. This result demonstrates that achieving truly sub-quadratic\nmemory usage is impossible without additional structural constraints. Our proof\nis constructed via a reduction from a computational lower bound problem,\nleveraging randomized embedding techniques inspired by dimensionality reduction\nprinciples. Finally, we discuss how sparsity priors on visual representations\ncan influence memory efficiency, presenting both impossibility results and\npotential directions for mitigating memory overhead."
                },
                "authors": [
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yekun Ke"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Song"
                },
                "author": "Zhao Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14805v1",
                "updated": "2025-03-19T00:30:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    0,
                    30,
                    43,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T00:30:43Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    0,
                    30,
                    43,
                    2,
                    78,
                    0
                ],
                "title": "Degradation of 2.4-kV $Ga_{2}O_{3}$ Schottky Barrier Diode at High\n  Temperatures up to 500 Â°C",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degradation of 2.4-kV $Ga_{2}O_{3}$ Schottky Barrier Diode at High\n  Temperatures up to 500 Â°C"
                },
                "summary": "Ga2O3 Schottky barrier diodes featuring a field plate and a composite\nSiO2/SiNx dielectric layer beneath the field plate were fabricated, achieving a\nbreakdown voltage of 2.4 kV at room temperature. Electrical performance and\ndegradation were analyzed via I-V and C-V measurements from 25 {\\deg}C to 500\n{\\deg}C, revealing temperature-dependent transport, interface stability, and\ndevice stability. Upon returning to room temperature, the diodes exhibited\nnearly unchanged forward characteristics, while the breakdown voltage declined\nsignificantly from 2.4 kV to 700 V. This behavior indicates a\ntemperature-induced reduction in the barrier height. Detailed analysis revealed\nthat variable range hopping (VRH) dominated the leakage mechanism at moderate\ntemperatures, while thermal emission (TE) became increasingly significant at\ntemperatures exceeding 400 {\\deg}C.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ga2O3 Schottky barrier diodes featuring a field plate and a composite\nSiO2/SiNx dielectric layer beneath the field plate were fabricated, achieving a\nbreakdown voltage of 2.4 kV at room temperature. Electrical performance and\ndegradation were analyzed via I-V and C-V measurements from 25 {\\deg}C to 500\n{\\deg}C, revealing temperature-dependent transport, interface stability, and\ndevice stability. Upon returning to room temperature, the diodes exhibited\nnearly unchanged forward characteristics, while the breakdown voltage declined\nsignificantly from 2.4 kV to 700 V. This behavior indicates a\ntemperature-induced reduction in the barrier height. Detailed analysis revealed\nthat variable range hopping (VRH) dominated the leakage mechanism at moderate\ntemperatures, while thermal emission (TE) became increasingly significant at\ntemperatures exceeding 400 {\\deg}C."
                },
                "authors": [
                    {
                        "name": "Hunter Ellis"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Imteaz Rahaman"
                    },
                    {
                        "name": "Apostoli Hillas"
                    },
                    {
                        "name": "Botong Li"
                    },
                    {
                        "name": "Michael A. Scarpulla"
                    },
                    {
                        "name": "Berardi Sensale Rodriguez"
                    },
                    {
                        "name": "Kai Fu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Fu"
                },
                "author": "Kai Fu",
                "arxiv_comment": "7 Pages, 9 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14708v1",
                "updated": "2025-03-18T20:16:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    20,
                    16,
                    50,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T20:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    20,
                    16,
                    50,
                    1,
                    77,
                    0
                ],
                "title": "NeCTAr: A Heterogeneous RISC-V SoC for Language Model Inference in Intel\n  16",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeCTAr: A Heterogeneous RISC-V SoC for Language Model Inference in Intel\n  16"
                },
                "summary": "This paper introduces NeCTAr (Near-Cache Transformer Accelerator), a 16nm\nheterogeneous multicore RISC-V SoC for sparse and dense machine learning\nkernels with both near-core and near-memory accelerators. A prototype chip runs\nat 400MHz at 0.85V and performs matrix-vector multiplications with 109 GOPs/W.\nThe effectiveness of the design is demonstrated by running inference on a\nsparse language model, ReLU-Llama.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces NeCTAr (Near-Cache Transformer Accelerator), a 16nm\nheterogeneous multicore RISC-V SoC for sparse and dense machine learning\nkernels with both near-core and near-memory accelerators. A prototype chip runs\nat 400MHz at 0.85V and performs matrix-vector multiplications with 109 GOPs/W.\nThe effectiveness of the design is demonstrated by running inference on a\nsparse language model, ReLU-Llama."
                },
                "authors": [
                    {
                        "name": "Viansa Schmulbach"
                    },
                    {
                        "name": "Jason Kim"
                    },
                    {
                        "name": "Ethan Gao"
                    },
                    {
                        "name": "Lucy Revina"
                    },
                    {
                        "name": "Nikhil Jha"
                    },
                    {
                        "name": "Ethan Wu"
                    },
                    {
                        "name": "Borivoje Nikolic"
                    }
                ],
                "author_detail": {
                    "name": "Borivoje Nikolic"
                },
                "author": "Borivoje Nikolic",
                "arxiv_doi": "10.1109/HCS61935.2024.10665203",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/HCS61935.2024.10665203",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.14708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14647v1",
                "updated": "2025-03-18T18:52:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    18,
                    52,
                    3,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T18:52:03Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    18,
                    52,
                    3,
                    1,
                    77,
                    0
                ],
                "title": "Towards More Economical Context-Augmented LLM Generation by Reusing\n  Stored KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards More Economical Context-Augmented LLM Generation by Reusing\n  Stored KV Cache"
                },
                "summary": "Across large language model (LLM) applications, we observe an emerging trend\nfor reusing KV caches to save the prefill delays of processing repeated input\ntexts in different LLM inputs. This has led to a broad design space, including\ncolocating stored KV caches with (or close to) GPUs to various KV cache\ncompression. However, a key question remains unanswered: can these delay\nreductions also be economically favorable? Specifically, we ask whether a\ndeveloper can use public cloud services to store precomputed KV caches and\nreuse them to save delay without incurring more costs in terms of compute,\nstorage, and network. To answer this question, we propose an validated\nanalytical model for the cloud cost (in compute, storage, and network) of\nstoring and reusing KV caches based on various workload parameters, such as\nreuse frequency, generated text lengths, model sizes, etc. Preliminary results\nshow that KV cache reusing is able to save both delay and cloud cost across a\nrange of workloads with long context. And we call more efforts on building more\neconomical context augmented LLM by KV cache reusing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Across large language model (LLM) applications, we observe an emerging trend\nfor reusing KV caches to save the prefill delays of processing repeated input\ntexts in different LLM inputs. This has led to a broad design space, including\ncolocating stored KV caches with (or close to) GPUs to various KV cache\ncompression. However, a key question remains unanswered: can these delay\nreductions also be economically favorable? Specifically, we ask whether a\ndeveloper can use public cloud services to store precomputed KV caches and\nreuse them to save delay without incurring more costs in terms of compute,\nstorage, and network. To answer this question, we propose an validated\nanalytical model for the cloud cost (in compute, storage, and network) of\nstoring and reusing KV caches based on various workload parameters, such as\nreuse frequency, generated text lengths, model sizes, etc. Preliminary results\nshow that KV cache reusing is able to save both delay and cloud cost across a\nrange of workloads with long context. And we call more efforts on building more\neconomical context augmented LLM by KV cache reusing."
                },
                "authors": [
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08640v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08640v2",
                "updated": "2025-03-18T17:13:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    13,
                    42,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-11T17:30:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    30,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention"
                },
                "summary": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale."
                },
                "authors": [
                    {
                        "name": "Emily Xiao"
                    },
                    {
                        "name": "Chin-Jou Li"
                    },
                    {
                        "name": "Yilin Zhang"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Amanda Bertsch"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Bertsch"
                },
                "author": "Amanda Bertsch",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08640v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08640v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09573v2",
                "updated": "2025-03-18T15:58:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    58,
                    18,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-12T17:43:40Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    43,
                    40,
                    2,
                    71,
                    0
                ],
                "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models"
                },
                "summary": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/"
                },
                "authors": [
                    {
                        "name": "Marianne Arriola"
                    },
                    {
                        "name": "Aaron Gokaslan"
                    },
                    {
                        "name": "Justin T Chiu"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Zhixuan Qi"
                    },
                    {
                        "name": "Jiaqi Han"
                    },
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Volodymyr Kuleshov"
                    }
                ],
                "author_detail": {
                    "name": "Volodymyr Kuleshov"
                },
                "author": "Volodymyr Kuleshov",
                "arxiv_comment": "ICLR 2025 Oral. We provide the code at\n  https://github.com/kuleshov-group/bd3lms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18753v2",
                "updated": "2025-03-18T09:43:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    9,
                    43,
                    33,
                    1,
                    77,
                    0
                ],
                "published": "2024-07-26T14:08:53Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    14,
                    8,
                    53,
                    4,
                    208,
                    0
                ],
                "title": "Suffixient Arrays: a New Efficient Suffix Array Compression Technique",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Suffixient Arrays: a New Efficient Suffix Array Compression Technique"
                },
                "summary": "The Suffix Array is a classic text index enabling on-line pattern matching\nqueries via simple binary search. The main drawback of the Suffix Array is that\nit takes linear space in the text's length, even if the text itself is\nextremely compressible. Several works in the literature showed that the Suffix\nArray can be compressed, but they all rely on complex succinct data structures\nwhich in practice tend to exhibit poor cache locality and thus significantly\nslow down queries. In this paper, we propose a new simple and very efficient\nsolution to this problem by presenting the \\emph{Suffixient Array}: a tiny\nsubset of the Suffix Array \\emph{sufficient} to locate on-line one pattern\noccurrence (in general, all its Maximal Exact Matches) via binary search,\nprovided that random access to the text is available. We prove that: (i) the\nSuffixient Array length $\\chi$ is a strong repetitiveness measure, (ii) unlike\nmost existing repetition-aware indexes such as the $r$-index, our new index is\nefficient in the I/O model, and (iii) Suffixient Arrays can be computed in\nlinear time and compressed working space. We show experimentally that, when\nusing well-established compressed random access data structures on repetitive\ncollections, the Suffixient Array $\\SuA$ is \\emph{simultaneously} (i) faster\nand orders of magnitude smaller than the Suffix Array $\\SA$ and (ii) smaller\nand \\emph{one to two orders of magnitude faster} than the $r$-index. With an\naverage pattern matching query time as low as 3.5 ns per character, our new\nindex gets very close to the ultimate lower bound: the RAM throughput of our\nworkstation (1.18 ns per character).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Suffix Array is a classic text index enabling on-line pattern matching\nqueries via simple binary search. The main drawback of the Suffix Array is that\nit takes linear space in the text's length, even if the text itself is\nextremely compressible. Several works in the literature showed that the Suffix\nArray can be compressed, but they all rely on complex succinct data structures\nwhich in practice tend to exhibit poor cache locality and thus significantly\nslow down queries. In this paper, we propose a new simple and very efficient\nsolution to this problem by presenting the \\emph{Suffixient Array}: a tiny\nsubset of the Suffix Array \\emph{sufficient} to locate on-line one pattern\noccurrence (in general, all its Maximal Exact Matches) via binary search,\nprovided that random access to the text is available. We prove that: (i) the\nSuffixient Array length $\\chi$ is a strong repetitiveness measure, (ii) unlike\nmost existing repetition-aware indexes such as the $r$-index, our new index is\nefficient in the I/O model, and (iii) Suffixient Arrays can be computed in\nlinear time and compressed working space. We show experimentally that, when\nusing well-established compressed random access data structures on repetitive\ncollections, the Suffixient Array $\\SuA$ is \\emph{simultaneously} (i) faster\nand orders of magnitude smaller than the Suffix Array $\\SA$ and (ii) smaller\nand \\emph{one to two orders of magnitude faster} than the $r$-index. With an\naverage pattern matching query time as low as 3.5 ns per character, our new\nindex gets very close to the ultimate lower bound: the RAM throughput of our\nworkstation (1.18 ns per character)."
                },
                "authors": [
                    {
                        "name": "Davide Cenzato"
                    },
                    {
                        "name": "Lore Depuydt"
                    },
                    {
                        "name": "Travis Gagie"
                    },
                    {
                        "name": "Sung-Hwan Kim"
                    },
                    {
                        "name": "Giovanni Manzini"
                    },
                    {
                        "name": "Francisco Olivares"
                    },
                    {
                        "name": "Nicola Prezza"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Prezza"
                },
                "author": "Nicola Prezza",
                "arxiv_comment": "40 pages, 7 figure, 1 table and 7 pseudocodes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13145v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13145v2",
                "updated": "2025-03-18T07:02:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    7,
                    2,
                    33,
                    1,
                    77,
                    0
                ],
                "published": "2025-02-18T18:59:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba"
                },
                "authors": [
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Yingyue Li"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Code and model are available at https://github.com/hustvl/mmMamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13145v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13145v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19108v2",
                "updated": "2025-03-18T04:49:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    4,
                    49,
                    23,
                    1,
                    77,
                    0
                ],
                "published": "2024-11-28T12:50:05Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model"
                },
                "summary": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality."
                },
                "authors": [
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Yuzhong Zhao"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Qixiang Ye"
                    },
                    {
                        "name": "Fang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Wan"
                },
                "author": "Fang Wan",
                "arxiv_comment": "Accepted in CVPR 2025. Project: https://liewfeng.github.io/TeaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10511v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10511v3",
                "updated": "2025-03-18T01:58:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    1,
                    58,
                    36,
                    1,
                    77,
                    0
                ],
                "published": "2024-06-15T05:28:55Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    5,
                    28,
                    55,
                    5,
                    167,
                    0
                ],
                "title": "Efficient Hardware Accelerator Based on Medium Granularity Dataflow for\n  SpTRSV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Hardware Accelerator Based on Medium Granularity Dataflow for\n  SpTRSV"
                },
                "summary": "Sparse triangular solve (SpTRSV) is widely used in various domains. Numerous\nstudies have been conducted using CPUs, GPUs, and specific hardware\naccelerators, where dataflows can be categorized into coarse and fine\ngranularity. Coarse dataflows offer good spatial locality but suffer from low\nparallelism, while fine dataflows provide high parallelism but disrupt the\nspatial structure, leading to increased nodes and poor data reuse. This paper\nproposes a novel hardware accelerator for SpTRSV or SpTRSV-like DAGs. The\naccelerator implements a medium granularity dataflow through hardware-software\ncodesign and achieves both excellent spatial locality and high parallelism.\nAdditionally, a partial sum caching mechanism is introduced to reduce the\nblocking frequency of processing elements (PEs), and a reordering algorithm of\nintra-node edges computation is developed to enhance data reuse. Experimental\nresults on 245 benchmarks with node counts reaching up to 85,392 demonstrate\nthat this work achieves average performance improvements of 7.0$\\times$ (up to\n27.8$\\times$) over CPUs and 5.8$\\times$ (up to 98.8$\\times$) over GPUs.\nCompared to the state-of-the-art technique (DPU-v2), this work shows a\n2.5$\\times$ (up to 5.9$\\times$) average performance improvement and 1.7$\\times$\n(up to 4.1$\\times$) average energy efficiency enhancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse triangular solve (SpTRSV) is widely used in various domains. Numerous\nstudies have been conducted using CPUs, GPUs, and specific hardware\naccelerators, where dataflows can be categorized into coarse and fine\ngranularity. Coarse dataflows offer good spatial locality but suffer from low\nparallelism, while fine dataflows provide high parallelism but disrupt the\nspatial structure, leading to increased nodes and poor data reuse. This paper\nproposes a novel hardware accelerator for SpTRSV or SpTRSV-like DAGs. The\naccelerator implements a medium granularity dataflow through hardware-software\ncodesign and achieves both excellent spatial locality and high parallelism.\nAdditionally, a partial sum caching mechanism is introduced to reduce the\nblocking frequency of processing elements (PEs), and a reordering algorithm of\nintra-node edges computation is developed to enhance data reuse. Experimental\nresults on 245 benchmarks with node counts reaching up to 85,392 demonstrate\nthat this work achieves average performance improvements of 7.0$\\times$ (up to\n27.8$\\times$) over CPUs and 5.8$\\times$ (up to 98.8$\\times$) over GPUs.\nCompared to the state-of-the-art technique (DPU-v2), this work shows a\n2.5$\\times$ (up to 5.9$\\times$) average performance improvement and 1.7$\\times$\n(up to 4.1$\\times$) average energy efficiency enhancement."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    },
                    {
                        "name": "Shengli Lu"
                    }
                ],
                "author_detail": {
                    "name": "Shengli Lu"
                },
                "author": "Shengli Lu",
                "arxiv_doi": "10.1109/TVLSI.2024.3497166",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVLSI.2024.3497166",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.10511v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10511v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Trans. Very Large Scale Integr. (VLSI) Syst. 33 (2025)\n  807-820",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13737v1",
                "updated": "2025-03-17T21:47:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    47,
                    43,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T21:47:43Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    47,
                    43,
                    0,
                    76,
                    0
                ],
                "title": "AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference\n  Serving for Diverse Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference\n  Serving for Diverse Applications"
                },
                "summary": "In this paper, we consider a mixed-prompt scenario for a large language model\n(LLM) inference serving system that supports diverse applications with both\nshort prompts and long prompts and heterogeneous SLOs for iteration time. To\nimprove throughput when handling long prompts, previous research introduces a\nchunking method, but has not addressed heterogeneous SLOs. To address the\nlimitation, we propose AccelGen, a high-throughput LLM inference serving system\nwith heterogeneous SLO guarantees for diverse applications. AccelGen introduces\nfour core components: (1) SLO-guaranteed dynamic chunking, which dynamically\nadjusts chunk sizes to maximize GPU compute utilization while meeting\niteration-level SLOs; (2) Iteration-level SLO-based task prioritization, which\nprioritizes tight-SLO requests and batches requests with similar SLOs; (3)\nMulti-resource-aware batching, which selects queued requests to maximize the\nutilizations of both GPU compute resource and key-value cache (KVC).\nTrace-driven real experiments demonstrate that AccelGen achieves 1.42-11.21X\nhigher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment,\nand 1.61-12.22X lower response latency compared to the state-of-the-art\napproaches. It achieves performance near the Oracle, which optimally maximizes\ngoodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we consider a mixed-prompt scenario for a large language model\n(LLM) inference serving system that supports diverse applications with both\nshort prompts and long prompts and heterogeneous SLOs for iteration time. To\nimprove throughput when handling long prompts, previous research introduces a\nchunking method, but has not addressed heterogeneous SLOs. To address the\nlimitation, we propose AccelGen, a high-throughput LLM inference serving system\nwith heterogeneous SLO guarantees for diverse applications. AccelGen introduces\nfour core components: (1) SLO-guaranteed dynamic chunking, which dynamically\nadjusts chunk sizes to maximize GPU compute utilization while meeting\niteration-level SLOs; (2) Iteration-level SLO-based task prioritization, which\nprioritizes tight-SLO requests and batches requests with similar SLOs; (3)\nMulti-resource-aware batching, which selects queued requests to maximize the\nutilizations of both GPU compute resource and key-value cache (KVC).\nTrace-driven real experiments demonstrate that AccelGen achieves 1.42-11.21X\nhigher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment,\nand 1.61-12.22X lower response latency compared to the state-of-the-art\napproaches. It achieves performance near the Oracle, which optimally maximizes\ngoodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13723v1",
                "updated": "2025-03-17T21:11:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    11,
                    30,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T21:11:30Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    11,
                    30,
                    0,
                    76,
                    0
                ],
                "title": "Fast Maximum Likelihood Positioning for a Staggered Layer Scintillation\n  PET Detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Maximum Likelihood Positioning for a Staggered Layer Scintillation\n  PET Detector"
                },
                "summary": "In this study, we propose a fast implementation of a Maximum Likelihood\nPositioning (MLP) algorithm to estimate the energy and identify the active\nscintillator pixel in staggered layer scintillation detectors for PET. The\nstaggered layer design with pixelated scintillators enables the determination\nof the gamma's depth of interaction and facilitates an iteration-free\nformulation of the MLP algorithm. The efficacy of the algorithm optimization\nwas tested on a scintillation detector block designed for an ultra-high field\nBrainPET 7T, comprising three scintillator pixel layers. The three layers\ncontain 24 x 24, 24 x 23 and 23 x 22 scintillator pixels, respectively, with a\npixel pitch of 2 mm in both directions and layer thicknesses of 9, 8 and 7 mm.\nCalibration measurements, in combination with an automated calibration script,\nwere used to obtain the expected counts of scintillation photons required in\nthe MLP algorithm. Using Single-Instruction-Multiple-Data parallelization,\nmulti-threading and optimized cache lines, a maximum processing speed of\napproximately 22.5 million singles per second was achieved on a platform with\nfour Intel Xeon Platinum 8168 CPUs and 60 threads, encompassing all required\nprocessing steps. The automatic calibration failed for 1 to 15 individual\nscintillator pixels in approximately 10 per cent of the 120 scintillation\ndetector blocks, necessitating manual correction. After applying the energy\ncorrection to the positioned single events, an energy resolution of of 12 +/- 2\nper cent FWHM was obtained for the entire scintillation block. This value is\nvery close to the energy resolutions measured for the individual scintillator\npixels, proving that the MLP accurately identifies the scintillating pixel and\nthat the energy correction method effectively compensates for the light\ncollection variations of the SiPM array.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we propose a fast implementation of a Maximum Likelihood\nPositioning (MLP) algorithm to estimate the energy and identify the active\nscintillator pixel in staggered layer scintillation detectors for PET. The\nstaggered layer design with pixelated scintillators enables the determination\nof the gamma's depth of interaction and facilitates an iteration-free\nformulation of the MLP algorithm. The efficacy of the algorithm optimization\nwas tested on a scintillation detector block designed for an ultra-high field\nBrainPET 7T, comprising three scintillator pixel layers. The three layers\ncontain 24 x 24, 24 x 23 and 23 x 22 scintillator pixels, respectively, with a\npixel pitch of 2 mm in both directions and layer thicknesses of 9, 8 and 7 mm.\nCalibration measurements, in combination with an automated calibration script,\nwere used to obtain the expected counts of scintillation photons required in\nthe MLP algorithm. Using Single-Instruction-Multiple-Data parallelization,\nmulti-threading and optimized cache lines, a maximum processing speed of\napproximately 22.5 million singles per second was achieved on a platform with\nfour Intel Xeon Platinum 8168 CPUs and 60 threads, encompassing all required\nprocessing steps. The automatic calibration failed for 1 to 15 individual\nscintillator pixels in approximately 10 per cent of the 120 scintillation\ndetector blocks, necessitating manual correction. After applying the energy\ncorrection to the positioned single events, an energy resolution of of 12 +/- 2\nper cent FWHM was obtained for the entire scintillation block. This value is\nvery close to the energy resolutions measured for the individual scintillator\npixels, proving that the MLP accurately identifies the scintillating pixel and\nthat the energy correction method effectively compensates for the light\ncollection variations of the SiPM array."
                },
                "authors": [
                    {
                        "name": "Christoph W. Lerche"
                    },
                    {
                        "name": "Wenwei Bi"
                    },
                    {
                        "name": "Mirjam Schoeneck"
                    },
                    {
                        "name": "Debora Niekaemper"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Elisabeth Pfaehler"
                    },
                    {
                        "name": "Lutz Tellmann"
                    },
                    {
                        "name": "Juergen J. Scheins"
                    },
                    {
                        "name": "N. Jon Shah"
                    }
                ],
                "author_detail": {
                    "name": "N. Jon Shah"
                },
                "arxiv_affiliation": "Department of Neurology RWTH Aachen University Aachen Germany",
                "author": "N. Jon Shah",
                "arxiv_comment": "20 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92C55 (Primary) 94A08 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13873v2",
                "updated": "2025-03-17T20:31:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    20,
                    31,
                    46,
                    0,
                    76,
                    0
                ],
                "published": "2025-02-19T16:54:58Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "title": "NVR: Vector Runahead on NPUs for Sparse Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVR: Vector Runahead on NPUs for Sparse Memory Access"
                },
                "summary": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zhengpeng Zhao"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Bing Guo"
                    },
                    {
                        "name": "He Xiao"
                    },
                    {
                        "name": "Chenhao Ma"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13679v1",
                "updated": "2025-03-17T19:32:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    19,
                    32,
                    26,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T19:32:26Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    19,
                    32,
                    26,
                    0,
                    76,
                    0
                ],
                "title": "PrETi: Predicting Execution Time in Early Stage with LLVM and Machine\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrETi: Predicting Execution Time in Early Stage with LLVM and Machine\n  Learning"
                },
                "summary": "We introduce preti, a novel framework for predicting software execution time\nduring the early stages of development. preti leverages an LLVM-based\nsimulation environment to extract timing-related runtime information, such as\nthe count of executed LLVM IR instructions. This information, combined with\nhistorical execution time data, is utilized to train machine learning models\nfor accurate time prediction. To further enhance prediction accuracy, our\napproach incorporates simulations of cache accesses and branch prediction. The\nevaluations on public benchmarks demonstrate that preti achieves an average\nAbsolute Percentage Error (APE) of 11.98\\%, surpassing state-of-the-art\nmethods. These results underscore the effectiveness and efficiency of preti as\na robust solution for early-stage timing analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce preti, a novel framework for predicting software execution time\nduring the early stages of development. preti leverages an LLVM-based\nsimulation environment to extract timing-related runtime information, such as\nthe count of executed LLVM IR instructions. This information, combined with\nhistorical execution time data, is utilized to train machine learning models\nfor accurate time prediction. To further enhance prediction accuracy, our\napproach incorporates simulations of cache accesses and branch prediction. The\nevaluations on public benchmarks demonstrate that preti achieves an average\nAbsolute Percentage Error (APE) of 11.98\\%, surpassing state-of-the-art\nmethods. These results underscore the effectiveness and efficiency of preti as\na robust solution for early-stage timing analysis."
                },
                "authors": [
                    {
                        "name": "Risheng Xu"
                    },
                    {
                        "name": "Philipp Sieweck"
                    },
                    {
                        "name": "Hermann von Hasseln"
                    },
                    {
                        "name": "Dirk Nowotka"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Nowotka"
                },
                "author": "Dirk Nowotka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16525v1",
                "updated": "2025-03-17T16:43:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    43,
                    35,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T16:43:35Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    43,
                    35,
                    0,
                    76,
                    0
                ],
                "title": "KVShare: Semantic-Aware Key-Value Cache Sharing for Efficient Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVShare: Semantic-Aware Key-Value Cache Sharing for Efficient Large\n  Language Model Inference"
                },
                "summary": "This paper presents KVShare, a multi-user Key-Value (KV) Cache sharing\ntechnology based on semantic similarity, designed to enhance the inference\nefficiency of Large Language Models (LLMs) and Multimodal Large Language Models\n(MLLMs). Addressing the limitations of existing prefix caching (strict text\nprefix matching) and semantic caching (loss of response diversity), KVShare\nachieves fine-grained KV cache reuse through semantic alignment algorithms and\ndifferential editing operations. Experiments on real-world user conversation\ndatasets demonstrate that KVShare improves KV cache hit rates by over 60%,\nwhile maintaining output quality comparable to full computation (no significant\ndegradation in BLEU and Rouge-L metrics). This approach effectively reduces GPU\nresource consumption and is applicable to scenarios with repetitive queries,\nsuch as healthcare and education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents KVShare, a multi-user Key-Value (KV) Cache sharing\ntechnology based on semantic similarity, designed to enhance the inference\nefficiency of Large Language Models (LLMs) and Multimodal Large Language Models\n(MLLMs). Addressing the limitations of existing prefix caching (strict text\nprefix matching) and semantic caching (loss of response diversity), KVShare\nachieves fine-grained KV cache reuse through semantic alignment algorithms and\ndifferential editing operations. Experiments on real-world user conversation\ndatasets demonstrate that KVShare improves KV cache hit rates by over 60%,\nwhile maintaining output quality comparable to full computation (no significant\ndegradation in BLEU and Rouge-L metrics). This approach effectively reduces GPU\nresource consumption and is applicable to scenarios with repetitive queries,\nsuch as healthcare and education."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Renji Zhang"
                    },
                    {
                        "name": "Deyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhang"
                },
                "author": "Deyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13275v1",
                "updated": "2025-03-17T15:27:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"
                },
                "summary": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    }
                ],
                "author_detail": {
                    "name": "Seyoung Song"
                },
                "author": "Seyoung Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.11; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12991v1",
                "updated": "2025-03-17T09:46:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    46,
                    35,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T09:46:35Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    46,
                    35,
                    0,
                    76,
                    0
                ],
                "title": "Tuning the CMS Coffea-casa facility for 200 Gbps Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuning the CMS Coffea-casa facility for 200 Gbps Challenge"
                },
                "summary": "As a part of the IRIS-HEP \"Analysis Grand Challenge\" activities, the\nCoffea-casa AF team executed a \"200 Gbps Challenge\". One of the goals of this\nchallenge was to provide a setup for execution of a test notebook-style\nanalysis on the facility that could process a 200 TB CMS NanoAOD dataset in 20\nminutes.\n  We describe the solutions we deployed at the facility to execute the\nchallenge tasks. The facility was configured to provide 2000+ cores for quick\nturn-around, low-latency analysis. To reach the highest event processing rates\nwe tested different scaling backends, both scaling over HTCondor and Kubernetes\nresources and using Dask and Taskvine schedulers. This configuration also\nallowed us to compare two different services for managing Dask clusters, Dask\nlabextention, and Dask Gateway server, under extreme conditions.\n  A robust set of XCache servers with a redirector were deployed in Kubernetes\nto cache the dataset to minimize wide-area network traffic. The XCache servers\nwere backed with solid-state NVME drives deployed within the Kubernetes cluster\nnodes. All data access was authenticated using scitokens and was transparent to\nthe user. To ensure we could track and measure data throughput precisely, we\nused our existing Prometheus monitoring stack to monitor the XCache pod\nthroughput on the Kubernetes network layer. Using the rate query across all of\nthe 8 XCache pods we were able to view a stacked cumulative graph of the total\nthroughput for each XCache. This monitoring setup allowed us to ensure uniform\ndata rates across all nodes while verifying we had reached the 200 Gbps\nbenchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a part of the IRIS-HEP \"Analysis Grand Challenge\" activities, the\nCoffea-casa AF team executed a \"200 Gbps Challenge\". One of the goals of this\nchallenge was to provide a setup for execution of a test notebook-style\nanalysis on the facility that could process a 200 TB CMS NanoAOD dataset in 20\nminutes.\n  We describe the solutions we deployed at the facility to execute the\nchallenge tasks. The facility was configured to provide 2000+ cores for quick\nturn-around, low-latency analysis. To reach the highest event processing rates\nwe tested different scaling backends, both scaling over HTCondor and Kubernetes\nresources and using Dask and Taskvine schedulers. This configuration also\nallowed us to compare two different services for managing Dask clusters, Dask\nlabextention, and Dask Gateway server, under extreme conditions.\n  A robust set of XCache servers with a redirector were deployed in Kubernetes\nto cache the dataset to minimize wide-area network traffic. The XCache servers\nwere backed with solid-state NVME drives deployed within the Kubernetes cluster\nnodes. All data access was authenticated using scitokens and was transparent to\nthe user. To ensure we could track and measure data throughput precisely, we\nused our existing Prometheus monitoring stack to monitor the XCache pod\nthroughput on the Kubernetes network layer. Using the rate query across all of\nthe 8 XCache pods we were able to view a stacked cumulative graph of the total\nthroughput for each XCache. This monitoring setup allowed us to ensure uniform\ndata rates across all nodes while verifying we had reached the 200 Gbps\nbenchmark."
                },
                "authors": [
                    {
                        "name": "Sam Albin"
                    },
                    {
                        "name": "Garhan Attebury"
                    },
                    {
                        "name": "Kenneth Bloom"
                    },
                    {
                        "name": "Brian Paul Bockelman"
                    },
                    {
                        "name": "Benjamin Tovar Lopez"
                    },
                    {
                        "name": "Carl Lundstedt"
                    },
                    {
                        "name": "Oksana Shadura"
                    },
                    {
                        "name": "John Thiltges"
                    },
                    {
                        "name": "Derek Weitzel"
                    },
                    {
                        "name": "Andrew Wightman"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Wightman"
                },
                "arxiv_affiliation": "University of Nebraska-Lincoln",
                "author": "Andrew Wightman",
                "arxiv_comment": "Draft submitted to EPJ journal (CHEP 2024 conference proceedings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12988v1",
                "updated": "2025-03-17T09:44:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    44,
                    17,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T09:44:17Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    44,
                    17,
                    0,
                    76,
                    0
                ],
                "title": "ROMA: a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROMA: a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM"
                },
                "summary": "As large language models (LLMs) demonstrate powerful capabilities, deploying\nthem on edge devices has become increasingly crucial, offering advantages in\nprivacy and real-time interaction. QLoRA has emerged as the standard approach\nfor on-device LLMs, leveraging quantized models to reduce memory and\ncomputational costs while utilizing LoRA for task-specific adaptability. In\nthis work, we propose ROMA, a QLoRA accelerator with a hybrid storage\narchitecture that uses ROM for quantized base models and SRAM for LoRA weights\nand KV cache. Our insight is that the quantized base model is stable and\nconverged, making it well-suited for ROM storage. Meanwhile, LoRA modules offer\nthe flexibility to adapt to new data without requiring updates to the base\nmodel. To further reduce the area cost of ROM, we introduce a novel B-ROM\ndesign and integrate it with the compute unit to form a fused cell for\nefficient use of chip resources. ROMA can effectively store both a 4-bit 3B and\na 2-bit 8B LLaMA model entirely on-chip, achieving a notable generation speed\nexceeding 20,000 tokens/s without requiring external memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) demonstrate powerful capabilities, deploying\nthem on edge devices has become increasingly crucial, offering advantages in\nprivacy and real-time interaction. QLoRA has emerged as the standard approach\nfor on-device LLMs, leveraging quantized models to reduce memory and\ncomputational costs while utilizing LoRA for task-specific adaptability. In\nthis work, we propose ROMA, a QLoRA accelerator with a hybrid storage\narchitecture that uses ROM for quantized base models and SRAM for LoRA weights\nand KV cache. Our insight is that the quantized base model is stable and\nconverged, making it well-suited for ROM storage. Meanwhile, LoRA modules offer\nthe flexibility to adapt to new data without requiring updates to the base\nmodel. To further reduce the area cost of ROM, we introduce a novel B-ROM\ndesign and integrate it with the compute unit to form a fused cell for\nefficient use of chip resources. ROMA can effectively store both a 4-bit 3B and\na 2-bit 8B LLaMA model entirely on-chip, achieving a notable generation speed\nexceeding 20,000 tokens/s without requiring external memory."
                },
                "authors": [
                    {
                        "name": "Wenqiang Wang"
                    },
                    {
                        "name": "Yijia Zhang"
                    },
                    {
                        "name": "Zikai Zhang"
                    },
                    {
                        "name": "Guanting Huo"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Ningyi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ningyi Xu"
                },
                "author": "Ningyi Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08407v2",
                "updated": "2025-03-17T03:30:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    3,
                    30,
                    29,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-11T13:10:41Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    10,
                    41,
                    1,
                    70,
                    0
                ],
                "title": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images"
                },
                "summary": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Yansong Guo"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yansong Qu"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12491v1",
                "updated": "2025-03-16T12:49:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    16,
                    12,
                    49,
                    44,
                    6,
                    75,
                    0
                ],
                "published": "2025-03-16T12:49:44Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    12,
                    49,
                    44,
                    6,
                    75,
                    0
                ],
                "title": "CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences"
                },
                "summary": "Large language models (LLMs) excel at processing long sequences, boosting\ndemand for key-value (KV) caching. While recent efforts to evict KV cache have\nalleviated the inference burden, they often fail to allocate resources\nrationally across layers with different attention patterns. In this paper, we\nintroduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach\nthat frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses\nlayer-specific preferences by considering attention dynamics in both spatial\nand temporal dimensions, allocates rational cache size for layers accordingly,\nand manages memory constraints in a cascading manner. This approach enables a\nglobal view of cache allocation, adaptively distributing resources across\ndiverse attention mechanisms while maintaining memory budgets. CAKE also\nemploys a new eviction indicator that considers the shifting importance of\ntokens over time, addressing limitations in existing methods that overlook\ntemporal dynamics. Comprehensive experiments on LongBench and NeedleBench show\nthat CAKE maintains model performance with only 3.2% of the KV cache and\nconsistently outperforms current baselines across various models and memory\nconstraints, particularly in low-memory settings. Additionally, CAKE achieves\nover 10x speedup in decoding latency compared to full cache when processing\ncontexts of 128K tokens with FlashAttention-2. Our code is available at\nhttps://github.com/antgroup/cakekv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at processing long sequences, boosting\ndemand for key-value (KV) caching. While recent efforts to evict KV cache have\nalleviated the inference burden, they often fail to allocate resources\nrationally across layers with different attention patterns. In this paper, we\nintroduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach\nthat frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses\nlayer-specific preferences by considering attention dynamics in both spatial\nand temporal dimensions, allocates rational cache size for layers accordingly,\nand manages memory constraints in a cascading manner. This approach enables a\nglobal view of cache allocation, adaptively distributing resources across\ndiverse attention mechanisms while maintaining memory budgets. CAKE also\nemploys a new eviction indicator that considers the shifting importance of\ntokens over time, addressing limitations in existing methods that overlook\ntemporal dynamics. Comprehensive experiments on LongBench and NeedleBench show\nthat CAKE maintains model performance with only 3.2% of the KV cache and\nconsistently outperforms current baselines across various models and memory\nconstraints, particularly in low-memory settings. Additionally, CAKE achieves\nover 10x speedup in decoding latency compared to full cache when processing\ncontexts of 128K tokens with FlashAttention-2. Our code is available at\nhttps://github.com/antgroup/cakekv."
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Yuchen Cao"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Wen Hu"
                    },
                    {
                        "name": "Shixuan Fan"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Weiyao Lin"
                    },
                    {
                        "name": "Jianguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Li"
                },
                "author": "Jianguo Li",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12450v1",
                "updated": "2025-03-16T10:54:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    16,
                    10,
                    54,
                    59,
                    6,
                    75,
                    0
                ],
                "published": "2025-03-16T10:54:59Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    10,
                    54,
                    59,
                    6,
                    75,
                    0
                ],
                "title": "LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching"
                },
                "summary": "Masked Autoregressive (MAR) models have emerged as a promising approach in\nimage generation, expected to surpass traditional autoregressive models in\ncomputational efficiency by leveraging the capability of parallel decoding.\nHowever, their dependence on bidirectional self-attention inherently conflicts\nwith conventional KV caching mechanisms, creating unexpected computational\nbottlenecks that undermine their expected efficiency. To address this problem,\nthis paper studies the caching mechanism for MAR by leveraging two types of\nredundancy: Token Redundancy indicates that a large portion of tokens have very\nsimilar representations in the adjacent decoding steps, which allows us to\nfirst cache them in previous steps and then reuse them in the later steps.\nCondition Redundancy indicates that the difference between conditional and\nunconditional output in classifier-free guidance exhibits very similar values\nin adjacent steps. Based on these two redundancies, we propose LazyMAR, which\nintroduces two caching mechanisms to handle them one by one. LazyMAR is\ntraining-free and plug-and-play for all MAR models. Experimental results\ndemonstrate that our method achieves 2.83 times acceleration with almost no\ndrop in generation quality. Our codes will be released in\nhttps://github.com/feihongyan1/LazyMAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Autoregressive (MAR) models have emerged as a promising approach in\nimage generation, expected to surpass traditional autoregressive models in\ncomputational efficiency by leveraging the capability of parallel decoding.\nHowever, their dependence on bidirectional self-attention inherently conflicts\nwith conventional KV caching mechanisms, creating unexpected computational\nbottlenecks that undermine their expected efficiency. To address this problem,\nthis paper studies the caching mechanism for MAR by leveraging two types of\nredundancy: Token Redundancy indicates that a large portion of tokens have very\nsimilar representations in the adjacent decoding steps, which allows us to\nfirst cache them in previous steps and then reuse them in the later steps.\nCondition Redundancy indicates that the difference between conditional and\nunconditional output in classifier-free guidance exhibits very similar values\nin adjacent steps. Based on these two redundancies, we propose LazyMAR, which\nintroduces two caching mechanisms to handle them one by one. LazyMAR is\ntraining-free and plug-and-play for all MAR models. Experimental results\ndemonstrate that our method achieves 2.83 times acceleration with almost no\ndrop in generation quality. Our codes will be released in\nhttps://github.com/feihongyan1/LazyMAR."
                },
                "authors": [
                    {
                        "name": "Feihong Yan"
                    },
                    {
                        "name": "Qingyan Wei"
                    },
                    {
                        "name": "Jiayi Tang"
                    },
                    {
                        "name": "Jiajun Li"
                    },
                    {
                        "name": "Yulin Wang"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Huiqi Li"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11972v1",
                "updated": "2025-03-15T02:48:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "published": "2025-03-15T02:48:27Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "title": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models"
                },
                "summary": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment."
                },
                "authors": [
                    {
                        "name": "Yuchen Xia"
                    },
                    {
                        "name": "Divyam Sharma"
                    },
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11946v1",
                "updated": "2025-03-15T01:35:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    1,
                    35,
                    53,
                    5,
                    74,
                    0
                ],
                "published": "2025-03-15T01:35:53Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    1,
                    35,
                    53,
                    5,
                    74,
                    0
                ],
                "title": "CCRSat: A Collaborative Computation Reuse Framework for Satellite Edge\n  Computing Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCRSat: A Collaborative Computation Reuse Framework for Satellite Edge\n  Computing Networks"
                },
                "summary": "In satellite computing applications, such as remote sensing, tasks often\ninvolve similar or identical input data, leading to the same processing\nresults. Computation reuse is an emerging paradigm that leverages the execution\nresults of previous tasks to enhance the utilization of computational\nresources. While this paradigm has been extensively studied in terrestrial\nnetworks with abundant computing and caching resources, such as named data\nnetworking (NDN), it is essential to develop a framework appropriate for\nresource-constrained satellite networks, which are expected to have longer task\ncompletion times. In this paper, we propose CCRSat, a collaborative computation\nreuse framework for satellite edge computing networks. CCRSat initially\nimplements local computation reuse on an independent satellite, utilizing a\nsatellite reuse state (SRS) to assess the efficiency of computation reuse.\nAdditionally, an inter-satellite computation reuse algorithm is introduced,\nwhich utilizes the collaborative sharing of similarity in previously processed\ndata among multiple satellites. The evaluation results tested on real-world\ndatasets demonstrate that, compared to comparative scenarios, our proposed\nCCRSat can significantly reduce task completion time by up to 62.1% and\ncomputational resource consumption by up to 28.8%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In satellite computing applications, such as remote sensing, tasks often\ninvolve similar or identical input data, leading to the same processing\nresults. Computation reuse is an emerging paradigm that leverages the execution\nresults of previous tasks to enhance the utilization of computational\nresources. While this paradigm has been extensively studied in terrestrial\nnetworks with abundant computing and caching resources, such as named data\nnetworking (NDN), it is essential to develop a framework appropriate for\nresource-constrained satellite networks, which are expected to have longer task\ncompletion times. In this paper, we propose CCRSat, a collaborative computation\nreuse framework for satellite edge computing networks. CCRSat initially\nimplements local computation reuse on an independent satellite, utilizing a\nsatellite reuse state (SRS) to assess the efficiency of computation reuse.\nAdditionally, an inter-satellite computation reuse algorithm is introduced,\nwhich utilizes the collaborative sharing of similarity in previously processed\ndata among multiple satellites. The evaluation results tested on real-world\ndatasets demonstrate that, compared to comparative scenarios, our proposed\nCCRSat can significantly reduce task completion time by up to 62.1% and\ncomputational resource consumption by up to 28.8%."
                },
                "authors": [
                    {
                        "name": "Ye Zhang"
                    },
                    {
                        "name": "Zhishu Shen"
                    },
                    {
                        "name": "Dawen Jiang"
                    },
                    {
                        "name": "Xiangrui Liu"
                    },
                    {
                        "name": "Qiushi Zheng"
                    },
                    {
                        "name": "Jiong Jin"
                    }
                ],
                "author_detail": {
                    "name": "Jiong Jin"
                },
                "author": "Jiong Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06348v2",
                "updated": "2025-03-15T00:49:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    0,
                    49,
                    55,
                    5,
                    74,
                    0
                ],
                "published": "2024-03-11T00:30:25Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    0,
                    30,
                    25,
                    0,
                    71,
                    0
                ],
                "title": "Accelerating Sparse Tensor Decomposition Using Adaptive Linearized\n  Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Sparse Tensor Decomposition Using Adaptive Linearized\n  Representation"
                },
                "summary": "High-dimensional sparse data emerge in many critical application domains such\nas healthcare and cybersecurity. To extract meaningful insights from massive\nvolumes of these multi-dimensional data, scientists employ unsupervised\nanalysis tools based on tensor decomposition (TD) methods. However, real-world\nsparse tensors exhibit highly irregular shapes and data distributions, which\npose significant challenges for making efficient use of modern parallel\nprocessors. This study breaks the prevailing assumption that compressing sparse\ntensors into coarse-grained structures or along a particular dimension/mode is\nmore efficient than keeping them in a fine-grained, mode-agnostic form. Our\nnovel sparse tensor representation, Adaptive Linearized Tensor Order (ALTO),\nencodes tensors in a compact format that can be easily streamed from memory and\nis amenable to both caching and parallel execution. In contrast to existing\ncompressed tensor formats, ALTO constructs one tensor copy that is agnostic to\nboth the mode orientation and the irregular distribution of nonzero elements.\nTo demonstrate the efficacy of ALTO, we propose a set of parallel TD algorithms\nthat exploit the inherent data reuse of tensor computations to substantially\nreduce synchronization overhead, decrease memory footprint, and improve\nparallel performance. Additionally, we characterize the major execution\nbottlenecks of TD methods on the latest Intel Xeon Scalable processors and\nintroduce dynamic adaptation heuristics to automatically select the best\nalgorithm based on the sparse tensor characteristics. Across a diverse set of\nreal-world data sets, ALTO outperforms the state-of-the-art approaches,\nachieving more than an order-of-magnitude speedup over the best mode-agnostic\nformats. Compared to the best mode-specific formats, ALTO achieves 5.1X\ngeometric mean speedup at a fraction (25%) of their storage costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-dimensional sparse data emerge in many critical application domains such\nas healthcare and cybersecurity. To extract meaningful insights from massive\nvolumes of these multi-dimensional data, scientists employ unsupervised\nanalysis tools based on tensor decomposition (TD) methods. However, real-world\nsparse tensors exhibit highly irregular shapes and data distributions, which\npose significant challenges for making efficient use of modern parallel\nprocessors. This study breaks the prevailing assumption that compressing sparse\ntensors into coarse-grained structures or along a particular dimension/mode is\nmore efficient than keeping them in a fine-grained, mode-agnostic form. Our\nnovel sparse tensor representation, Adaptive Linearized Tensor Order (ALTO),\nencodes tensors in a compact format that can be easily streamed from memory and\nis amenable to both caching and parallel execution. In contrast to existing\ncompressed tensor formats, ALTO constructs one tensor copy that is agnostic to\nboth the mode orientation and the irregular distribution of nonzero elements.\nTo demonstrate the efficacy of ALTO, we propose a set of parallel TD algorithms\nthat exploit the inherent data reuse of tensor computations to substantially\nreduce synchronization overhead, decrease memory footprint, and improve\nparallel performance. Additionally, we characterize the major execution\nbottlenecks of TD methods on the latest Intel Xeon Scalable processors and\nintroduce dynamic adaptation heuristics to automatically select the best\nalgorithm based on the sparse tensor characteristics. Across a diverse set of\nreal-world data sets, ALTO outperforms the state-of-the-art approaches,\nachieving more than an order-of-magnitude speedup over the best mode-agnostic\nformats. Compared to the best mode-specific formats, ALTO achieves 5.1X\ngeometric mean speedup at a fraction (25%) of their storage costs."
                },
                "authors": [
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Ahmed E. Helal"
                    },
                    {
                        "name": "S. Isaac Geronimo Anderson"
                    },
                    {
                        "name": "Fabio Checconi"
                    },
                    {
                        "name": "Yongseok Soh"
                    },
                    {
                        "name": "Jesmin Jahan Tithi"
                    },
                    {
                        "name": "Teresa Ranadive"
                    },
                    {
                        "name": "Brian J Gravelle"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    },
                    {
                        "name": "Jee Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jee Choi"
                },
                "author": "Jee Choi",
                "arxiv_comment": "Accepted to TPDS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v1",
                "updated": "2025-03-14T19:02:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Invited paper to IEEE Custom Integrated Circuits Conference (CICC)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11750v1",
                "updated": "2025-03-14T17:57:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    57,
                    42,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T17:57:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    57,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Making Every Step Effective: Jailbreaking Large Vision-Language Models\n  Through Hierarchical KV Equalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Every Step Effective: Jailbreaking Large Vision-Language Models\n  Through Hierarchical KV Equalization"
                },
                "summary": "In the realm of large vision-language models (LVLMs), adversarial jailbreak\nattacks serve as a red-teaming approach to identify safety vulnerabilities of\nthese models and their associated defense mechanisms. However, we identify a\ncritical limitation: not every adversarial optimization step leads to a\npositive outcome, and indiscriminately accepting optimization results at each\nstep may reduce the overall attack success rate. To address this challenge, we\nintroduce HKVE (Hierarchical Key-Value Equalization), an innovative\njailbreaking framework that selectively accepts gradient optimization results\nbased on the distribution of attention scores across different layers, ensuring\nthat every optimization step positively contributes to the attack. Extensive\nexperiments demonstrate HKVE's significant effectiveness, achieving attack\nsuccess rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL,\nsubstantially outperforming existing methods by margins of 20.43\\%, 21.01\\% and\n26.43\\% respectively. Furthermore, making every step effective not only leads\nto an increase in attack success rate but also allows for a reduction in the\nnumber of iterations, thereby lowering computational costs. Warning: This paper\ncontains potentially harmful example data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of large vision-language models (LVLMs), adversarial jailbreak\nattacks serve as a red-teaming approach to identify safety vulnerabilities of\nthese models and their associated defense mechanisms. However, we identify a\ncritical limitation: not every adversarial optimization step leads to a\npositive outcome, and indiscriminately accepting optimization results at each\nstep may reduce the overall attack success rate. To address this challenge, we\nintroduce HKVE (Hierarchical Key-Value Equalization), an innovative\njailbreaking framework that selectively accepts gradient optimization results\nbased on the distribution of attention scores across different layers, ensuring\nthat every optimization step positively contributes to the attack. Extensive\nexperiments demonstrate HKVE's significant effectiveness, achieving attack\nsuccess rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL,\nsubstantially outperforming existing methods by margins of 20.43\\%, 21.01\\% and\n26.43\\% respectively. Furthermore, making every step effective not only leads\nto an increase in attack success rate but also allows for a reduction in the\nnumber of iterations, thereby lowering computational costs. Warning: This paper\ncontains potentially harmful example data."
                },
                "authors": [
                    {
                        "name": "Shuyang Hao"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Muhao Chen"
                    },
                    {
                        "name": "Zi Huang"
                    },
                    {
                        "name": "Yujun Cai"
                    }
                ],
                "author_detail": {
                    "name": "Yujun Cai"
                },
                "author": "Yujun Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01066v2",
                "updated": "2025-03-14T16:57:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    57,
                    12,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-03T00:14:34Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    0,
                    14,
                    34,
                    0,
                    62,
                    0
                ],
                "title": "Alchemist: Towards the Design of Efficient Online Continual Learning\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alchemist: Towards the Design of Efficient Online Continual Learning\n  System"
                },
                "summary": "Continual learning has become a promising solution to refine large language\nmodels incrementally by leveraging user feedback. In particular, online\ncontinual learning - iteratively training the model with small batches of user\nfeedback - has demonstrated notable performance improvements. However, the\nexisting practice of separating training and serving processes forces the\nonline trainer to recompute the intermediate results already done during\nserving. Such redundant computations can account for 30%-42% of total training\ntime.\n  In this paper, we propose Alchemist, to the best of our knowledge, the first\nonline continual learning system that efficiently reuses serving activations to\nincrease training throughput. Alchemist introduces two key techniques: (1)\nrecording and storing activations and KV cache only during the prefill phase to\nminimize latency and memory overhead; and (2) smart activation offloading and\nhedging. Evaluations with inputs of varied token length sampled from ShareGPT\ndataset show that compared with a separate training cluster, Alchemist\nsignificantly increases training throughput by up to 1.72x, reduces up to 47%\nmemory usage during training, and supports up to 2x more training tokens - all\nwhile maintaining negligible impact on serving latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning has become a promising solution to refine large language\nmodels incrementally by leveraging user feedback. In particular, online\ncontinual learning - iteratively training the model with small batches of user\nfeedback - has demonstrated notable performance improvements. However, the\nexisting practice of separating training and serving processes forces the\nonline trainer to recompute the intermediate results already done during\nserving. Such redundant computations can account for 30%-42% of total training\ntime.\n  In this paper, we propose Alchemist, to the best of our knowledge, the first\nonline continual learning system that efficiently reuses serving activations to\nincrease training throughput. Alchemist introduces two key techniques: (1)\nrecording and storing activations and KV cache only during the prefill phase to\nminimize latency and memory overhead; and (2) smart activation offloading and\nhedging. Evaluations with inputs of varied token length sampled from ShareGPT\ndataset show that compared with a separate training cluster, Alchemist\nsignificantly increases training throughput by up to 1.72x, reduces up to 47%\nmemory usage during training, and supports up to 2x more training tokens - all\nwhile maintaining negligible impact on serving latency."
                },
                "authors": [
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Haryadi S. Gunawi"
                    },
                    {
                        "name": "Beibin Li"
                    },
                    {
                        "name": "Changho Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Changho Hwang"
                },
                "author": "Changho Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11460v1",
                "updated": "2025-03-14T14:47:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    47,
                    55,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:47:55Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    47,
                    55,
                    4,
                    73,
                    0
                ],
                "title": "ARCAS: Adaptive Runtime System for Chiplet-Aware Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCAS: Adaptive Runtime System for Chiplet-Aware Scheduling"
                },
                "summary": "The growing disparity between CPU core counts and available memory bandwidth\nhas intensified memory contention in servers. This particularly affects highly\nparallelizable applications, which must achieve efficient cache utilization to\nmaintain performance as CPU core counts grow. Optimizing cache utilization,\nhowever, is complex for recent chiplet-based CPUs, whose partitioned L3 caches\nlead to varying latencies and bandwidths, even within a single NUMA domain.\nClassical NUMA optimizations and task scheduling approaches unfortunately fail\nto address the performance issues of chiplet-based CPUs.\n  We describe Adaptive Runtime system for Chiplet-Aware Scheduling (ARCAS), a\nnew runtime system designed for chiplet-based CPUs. ARCAS combines\nchiplet-aware task scheduling heuristics, hardware-aware memory allocation, and\nfine-grained performance monitoring to optimize workload execution. It\nimplements a lightweight concurrency model that combines user-level thread\nfeatures-such as individual stacks, per-task scheduling, and state\nmanagement-with coroutine-like behavior, allowing tasks to suspend and resume\nexecution at defined points while efficiently managing task migration across\nchiplets. Our evaluation across diverse scenarios shows ARCAS's effectiveness\nfor optimizing the performance of memory-intensive parallel applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing disparity between CPU core counts and available memory bandwidth\nhas intensified memory contention in servers. This particularly affects highly\nparallelizable applications, which must achieve efficient cache utilization to\nmaintain performance as CPU core counts grow. Optimizing cache utilization,\nhowever, is complex for recent chiplet-based CPUs, whose partitioned L3 caches\nlead to varying latencies and bandwidths, even within a single NUMA domain.\nClassical NUMA optimizations and task scheduling approaches unfortunately fail\nto address the performance issues of chiplet-based CPUs.\n  We describe Adaptive Runtime system for Chiplet-Aware Scheduling (ARCAS), a\nnew runtime system designed for chiplet-based CPUs. ARCAS combines\nchiplet-aware task scheduling heuristics, hardware-aware memory allocation, and\nfine-grained performance monitoring to optimize workload execution. It\nimplements a lightweight concurrency model that combines user-level thread\nfeatures-such as individual stacks, per-task scheduling, and state\nmanagement-with coroutine-like behavior, allowing tasks to suspend and resume\nexecution at defined points while efficiently managing task migration across\nchiplets. Our evaluation across diverse scenarios shows ARCAS's effectiveness\nfor optimizing the performance of memory-intensive parallel applications."
                },
                "authors": [
                    {
                        "name": "Alessandro Fogli"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Peter Pietzuch"
                    },
                    {
                        "name": "Jana Giceva"
                    }
                ],
                "author_detail": {
                    "name": "Jana Giceva"
                },
                "author": "Jana Giceva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11426v1",
                "updated": "2025-03-14T14:14:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    14,
                    5,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:14:05Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    14,
                    5,
                    4,
                    73,
                    0
                ],
                "title": "Text Compression for Efficient Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Compression for Efficient Language Generation"
                },
                "summary": "We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork."
                },
                "authors": [
                    {
                        "name": "David Gu"
                    },
                    {
                        "name": "Peter Belcak"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Roger Wattenhofer"
                },
                "author": "Roger Wattenhofer",
                "arxiv_comment": "accepted to NAACL SRW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v1",
                "updated": "2025-03-14T06:49:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid (i.e., combination of regular attention and MLA\nlayers) or full MLA variant through lightweight post-training adaptation,\nbypassing the need for extensive pre-training. We demonstrate that leveraging\nthe dark knowledge of a well-trained model can enhance training accuracy and\nenable extreme KV cache compression in MLA without compromising model\nperformance. Our results show that using an 8B teacher model allows us to\ncompress the KV cache size of the Llama3.2-1B-Inst baseline by 6.4x while\npreserving 100% of its average score across multiple tasks on the LM Harness\nEvaluation benchmark. This is achieved with only 3.6B training tokens and about\n70 GPU hours on AMD MI300 GPUs, compared to the 370K GPU hours required for\npre-training the Llama3.2-1B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid (i.e., combination of regular attention and MLA\nlayers) or full MLA variant through lightweight post-training adaptation,\nbypassing the need for extensive pre-training. We demonstrate that leveraging\nthe dark knowledge of a well-trained model can enhance training accuracy and\nenable extreme KV cache compression in MLA without compromising model\nperformance. Our results show that using an 8B teacher model allows us to\ncompress the KV cache size of the Llama3.2-1B-Inst baseline by 6.4x while\npreserving 100% of its average score across multiple tasks on the LM Harness\nEvaluation benchmark. This is achieved with only 3.6B training tokens and about\n70 GPU hours on AMD MI300 GPUs, compared to the 370K GPU hours required for\npre-training the Llama3.2-1B model."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10589v1",
                "updated": "2025-03-13T17:40:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    40,
                    7,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:40:07Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    40,
                    7,
                    3,
                    72,
                    0
                ],
                "title": "Long Context Tuning for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Context Tuning for Video Generation"
                },
                "summary": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details."
                },
                "authors": [
                    {
                        "name": "Yuwei Guo"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Ziyan Yang"
                    },
                    {
                        "name": "Zhibei Ma"
                    },
                    {
                        "name": "Zhijie Lin"
                    },
                    {
                        "name": "Zhenheng Yang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "arxiv_comment": "Project Page: https://guoyww.github.io/projects/long-context-video/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10568v1",
                "updated": "2025-03-13T17:19:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:19:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Randomized Parallel Decoding"
                },
                "summary": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale."
                },
                "authors": [
                    {
                        "name": "Haopeng Li"
                    },
                    {
                        "name": "Jinyue Yang"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v2",
                "updated": "2025-03-13T16:29:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    29,
                    17,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion\nTransformer, that innovatively combines autoregressive and diffusion paradigms\nfor modeling continuous visual information. By introducing a block-wise\nautoregressive unit, ACDiT offers a flexible interpolation between token-wise\nautoregression and full-sequence diffusion, bypassing the limitations of\ndiscrete tokenization. The generation of each block is formulated as a\nconditional diffusion process, conditioned on prior blocks. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) on\nstandard diffusion transformer during training. During inference, the process\niterates between diffusion denoising and autoregressive decoding that can make\nfull use of KV-Cache. We show that ACDiT performs best among all autoregressive\nbaselines under similar model scales on image and video generation tasks. We\nalso demonstrate that benefiting from autoregressive modeling, pretrained ACDiT\ncan be transferred in visual understanding tasks despite being trained with the\ndiffusion objective. The analysis of the trade-off between autoregressive\nmodeling and diffusion demonstrates the potential of ACDiT to be used in\nlong-horizon visual generation tasks. We hope that ACDiT offers a novel\nperspective on visual autoregressive generation and unlocks new avenues for\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion\nTransformer, that innovatively combines autoregressive and diffusion paradigms\nfor modeling continuous visual information. By introducing a block-wise\nautoregressive unit, ACDiT offers a flexible interpolation between token-wise\nautoregression and full-sequence diffusion, bypassing the limitations of\ndiscrete tokenization. The generation of each block is formulated as a\nconditional diffusion process, conditioned on prior blocks. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) on\nstandard diffusion transformer during training. During inference, the process\niterates between diffusion denoising and autoregressive decoding that can make\nfull use of KV-Cache. We show that ACDiT performs best among all autoregressive\nbaselines under similar model scales on image and video generation tasks. We\nalso demonstrate that benefiting from autoregressive modeling, pretrained ACDiT\ncan be transferred in visual understanding tasks despite being trained with the\ndiffusion objective. The analysis of the trade-off between autoregressive\nmodeling and diffusion demonstrates the potential of ACDiT to be used in\nlong-horizon visual generation tasks. We hope that ACDiT offers a novel\nperspective on visual autoregressive generation and unlocks new avenues for\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10501v1",
                "updated": "2025-03-13T16:04:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    4,
                    31,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T16:04:31Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    4,
                    31,
                    3,
                    72,
                    0
                ],
                "title": "TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps://github.com/ShawnTan86/TokenCarve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps://github.com/ShawnTan86/TokenCarve."
                },
                "authors": [
                    {
                        "name": "Xudong Tan"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Chongjun Tu"
                    },
                    {
                        "name": "Jianjian Cao"
                    },
                    {
                        "name": "Yaoxin Yang"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10494v1",
                "updated": "2025-03-13T15:57:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    57,
                    50,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T15:57:50Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    57,
                    50,
                    3,
                    72,
                    0
                ],
                "title": "Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents"
                },
                "summary": "LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs."
                },
                "authors": [
                    {
                        "name": "Hanxu Hu"
                    },
                    {
                        "name": "Jannis Vamvas"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10337v1",
                "updated": "2025-03-13T13:15:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    15,
                    28,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T13:15:28Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    15,
                    28,
                    3,
                    72,
                    0
                ],
                "title": "KV-Distill: Nearly Lossless Learnable Context Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Distill: Nearly Lossless Learnable Context Compression for LLMs"
                },
                "summary": "Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures."
                },
                "authors": [
                    {
                        "name": "Vivek Chari"
                    },
                    {
                        "name": "Guanghui Qin"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v1",
                "updated": "2025-03-13T11:26:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit : Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit : Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v3",
                "updated": "2025-03-13T11:14:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    14,
                    49,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: https://github.com/NX-AI/flashrnn",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: https://github.com/NX-AI/flashrnn"
                },
                "authors": [
                    {
                        "name": "Korbinian PÃ¶ppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10074v1",
                "updated": "2025-03-13T05:43:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T05:43:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension"
                },
                "summary": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions."
                },
                "authors": [
                    {
                        "name": "Taehun Kim"
                    },
                    {
                        "name": "Hyerean Jang"
                    },
                    {
                        "name": "Youngjoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Shin"
                },
                "author": "Youngjoo Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.21782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21782v1",
                "updated": "2025-03-27T17:59:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    59,
                    58,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:59:58Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    59,
                    58,
                    3,
                    86,
                    0
                ],
                "title": "Mobile-VideoGPT: Fast and Accurate Video Understanding Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile-VideoGPT: Fast and Accurate Video Understanding Language Model"
                },
                "summary": "Video understanding models often struggle with high computational\nrequirements, extensive parameter counts, and slow inference speed, making them\ninefficient for practical use. To tackle these challenges, we propose\nMobile-VideoGPT, an efficient multimodal framework designed to operate with\nfewer than a billion parameters. Unlike traditional video large multimodal\nmodels (LMMs), Mobile-VideoGPT consists of lightweight dual visual encoders,\nefficient projectors, and a small language model (SLM), enabling real-time\nthroughput. To further improve efficiency, we present an Attention-Based Frame\nScoring mechanism to select the key-frames, along with an efficient token\nprojector that prunes redundant visual tokens and preserves essential\ncontextual cues. We evaluate our model across well-established six video\nunderstanding benchmarks (e.g., MVBench, EgoSchema, NextQA, and PercepTest).\nOur results show that Mobile-VideoGPT-0.5B can generate up to 46 tokens per\nsecond while outperforming existing state-of-the-art 0.5B-parameter models by 6\npoints on average with 40% fewer parameters and more than 2x higher throughput.\nOur code and models are publicly available at:\nhttps://github.com/Amshaker/Mobile-VideoGPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video understanding models often struggle with high computational\nrequirements, extensive parameter counts, and slow inference speed, making them\ninefficient for practical use. To tackle these challenges, we propose\nMobile-VideoGPT, an efficient multimodal framework designed to operate with\nfewer than a billion parameters. Unlike traditional video large multimodal\nmodels (LMMs), Mobile-VideoGPT consists of lightweight dual visual encoders,\nefficient projectors, and a small language model (SLM), enabling real-time\nthroughput. To further improve efficiency, we present an Attention-Based Frame\nScoring mechanism to select the key-frames, along with an efficient token\nprojector that prunes redundant visual tokens and preserves essential\ncontextual cues. We evaluate our model across well-established six video\nunderstanding benchmarks (e.g., MVBench, EgoSchema, NextQA, and PercepTest).\nOur results show that Mobile-VideoGPT-0.5B can generate up to 46 tokens per\nsecond while outperforming existing state-of-the-art 0.5B-parameter models by 6\npoints on average with 40% fewer parameters and more than 2x higher throughput.\nOur code and models are publicly available at:\nhttps://github.com/Amshaker/Mobile-VideoGPT."
                },
                "authors": [
                    {
                        "name": "Abdelrahman Shaker"
                    },
                    {
                        "name": "Muhammad Maaz"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Hamid Rezatofighi"
                    },
                    {
                        "name": "Salman Khan"
                    },
                    {
                        "name": "Fahad Shahbaz Khan"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Shahbaz Khan"
                },
                "author": "Fahad Shahbaz Khan",
                "arxiv_comment": "Technical Report. Project Page:\n  https://amshaker.github.io/Mobile-VideoGPT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15341v2",
                "updated": "2025-03-27T17:59:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    59,
                    22,
                    3,
                    86,
                    0
                ],
                "published": "2024-06-21T17:55:24Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    17,
                    55,
                    24,
                    4,
                    173,
                    0
                ],
                "title": "GenoTEX: A Benchmark for Automated Gene Expression Data Analysis in\n  Alignment with Bioinformaticians",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenoTEX: A Benchmark for Automated Gene Expression Data Analysis in\n  Alignment with Bioinformaticians"
                },
                "summary": "Recent advancements in machine learning have significantly improved the\nidentification of disease-associated genes from gene expression datasets.\nHowever, these processes often require extensive expertise and manual effort,\nlimiting their scalability. Large Language Model (LLM)-based agents have shown\npromise in automating these tasks due to their increasing problem-solving\nabilities. To support the evaluation and development of such methods, we\nintroduce GenoTEX, a benchmark dataset for the automated analysis of gene\nexpression data. GenoTEX provides annotated code and results for solving a wide\nrange of gene identification problems, encompassing dataset selection,\npreprocessing, and statistical analysis, in a pipeline that follows\ncomputational genomics standards. The benchmark includes expert-curated\nannotations from bioinformaticians to ensure accuracy and reliability. To\nprovide baselines for these tasks, we present GenoAgent, a team of LLM-based\nagents that adopt a multi-step programming workflow with flexible\nself-correction, to collaboratively analyze gene expression datasets. Our\nexperiments demonstrate the potential of LLM-based methods in analyzing genomic\ndata, while error analysis highlights the challenges and areas for future\nimprovement. We propose GenoTEX as a promising resource for benchmarking and\nenhancing automated methods for gene expression data analysis. The benchmark is\navailable at https://github.com/Liu-Hy/GenoTex.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in machine learning have significantly improved the\nidentification of disease-associated genes from gene expression datasets.\nHowever, these processes often require extensive expertise and manual effort,\nlimiting their scalability. Large Language Model (LLM)-based agents have shown\npromise in automating these tasks due to their increasing problem-solving\nabilities. To support the evaluation and development of such methods, we\nintroduce GenoTEX, a benchmark dataset for the automated analysis of gene\nexpression data. GenoTEX provides annotated code and results for solving a wide\nrange of gene identification problems, encompassing dataset selection,\npreprocessing, and statistical analysis, in a pipeline that follows\ncomputational genomics standards. The benchmark includes expert-curated\nannotations from bioinformaticians to ensure accuracy and reliability. To\nprovide baselines for these tasks, we present GenoAgent, a team of LLM-based\nagents that adopt a multi-step programming workflow with flexible\nself-correction, to collaboratively analyze gene expression datasets. Our\nexperiments demonstrate the potential of LLM-based methods in analyzing genomic\ndata, while error analysis highlights the challenges and areas for future\nimprovement. We propose GenoTEX as a promising resource for benchmarking and\nenhancing automated methods for gene expression data analysis. The benchmark is\navailable at https://github.com/Liu-Hy/GenoTex."
                },
                "authors": [
                    {
                        "name": "Haoyang Liu"
                    },
                    {
                        "name": "Shuyu Chen"
                    },
                    {
                        "name": "Ye Zhang"
                    },
                    {
                        "name": "Haohan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haohan Wang"
                },
                "author": "Haohan Wang",
                "arxiv_comment": "29 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21760v1",
                "updated": "2025-03-27T17:57:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    57,
                    28,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:57:28Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    57,
                    28,
                    3,
                    86,
                    0
                ],
                "title": "MemInsight: Autonomous Memory Augmentation for LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemInsight: Autonomous Memory Augmentation for LLM Agents"
                },
                "summary": "Large language model (LLM) agents have evolved to intelligently process\ninformation, make decisions, and interact with users or tools. A key capability\nis the integration of long-term memory capabilities, enabling these agents to\ndraw upon historical interactions and knowledge. However, the growing memory\nsize and need for semantic structuring pose significant challenges. In this\nwork, we propose an autonomous memory augmentation approach, MemInsight, to\nenhance semantic data representation and retrieval mechanisms. By leveraging\nautonomous augmentation to historical interactions, LLM agents are shown to\ndeliver more accurate and contextualized responses. We empirically validate the\nefficacy of our proposed approach in three task scenarios; conversational\nrecommendation, question answering and event summarization. On the LLM-REDIAL\ndataset, MemInsight boosts persuasiveness of recommendations by up to 14%.\nMoreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval.\nOur empirical results show the potential of MemInsight to enhance the\ncontextual performance of LLM agents across multiple tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents have evolved to intelligently process\ninformation, make decisions, and interact with users or tools. A key capability\nis the integration of long-term memory capabilities, enabling these agents to\ndraw upon historical interactions and knowledge. However, the growing memory\nsize and need for semantic structuring pose significant challenges. In this\nwork, we propose an autonomous memory augmentation approach, MemInsight, to\nenhance semantic data representation and retrieval mechanisms. By leveraging\nautonomous augmentation to historical interactions, LLM agents are shown to\ndeliver more accurate and contextualized responses. We empirically validate the\nefficacy of our proposed approach in three task scenarios; conversational\nrecommendation, question answering and event summarization. On the LLM-REDIAL\ndataset, MemInsight boosts persuasiveness of recommendations by up to 14%.\nMoreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval.\nOur empirical results show the potential of MemInsight to enhance the\ncontextual performance of LLM agents across multiple tasks."
                },
                "authors": [
                    {
                        "name": "Rana Salama"
                    },
                    {
                        "name": "Jason Cai"
                    },
                    {
                        "name": "Michelle Yuan"
                    },
                    {
                        "name": "Anna Currey"
                    },
                    {
                        "name": "Monica Sunkara"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Yassine Benajiba"
                    }
                ],
                "author_detail": {
                    "name": "Yassine Benajiba"
                },
                "author": "Yassine Benajiba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21757v1",
                "updated": "2025-03-27T17:57:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    57,
                    7,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:57:07Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    57,
                    7,
                    3,
                    86,
                    0
                ],
                "title": "Fwd2Bot: LVLM Visual Token Compression with Double Forward Bottleneck",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fwd2Bot: LVLM Visual Token Compression with Double Forward Bottleneck"
                },
                "summary": "In this work, we aim to compress the vision tokens of a Large Vision Language\nModel (LVLM) into a representation that is simultaneously suitable for (a)\ngenerative and (b) discriminative tasks, (c) is nearly lossless, and (d) is\nstorage-efficient. We propose a novel compression approach, called Fwd2Bot,\nthat uses the LVLM itself to compress the visual information in a task-agnostic\nmanner. At the core of Fwd2bot there exists a \"double-forward pass\" training\nstrategy, whereby, during the first forward pass, the LLM (of the LVLM) creates\na bottleneck by condensing the visual information into a small number of\nsummary tokens. Then, using the same LLM, the second forward pass processes the\nlanguage instruction(s) alongside the summary tokens, used as a direct\nreplacement for the image ones. The training signal is provided by two losses:\nan autoregressive one applied after the second pass that provides a direct\noptimization objective for compression, and a contrastive loss, applied after\nthe first pass, that further boosts the representation strength, especially for\ndiscriminative tasks. The training is further enhanced by stage-specific\nadapters. We accompany the proposed method by an in-depth ablation study.\nOverall, Fwd2Bot results in highly-informative compressed representations\nsuitable for both generative and discriminative tasks. For generative tasks, we\noffer a 2x higher compression rate without compromising the generative\ncapabilities, setting a new state-of-the-art result. For discriminative tasks,\nwe set a new state-of-the-art on image retrieval and compositionality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we aim to compress the vision tokens of a Large Vision Language\nModel (LVLM) into a representation that is simultaneously suitable for (a)\ngenerative and (b) discriminative tasks, (c) is nearly lossless, and (d) is\nstorage-efficient. We propose a novel compression approach, called Fwd2Bot,\nthat uses the LVLM itself to compress the visual information in a task-agnostic\nmanner. At the core of Fwd2bot there exists a \"double-forward pass\" training\nstrategy, whereby, during the first forward pass, the LLM (of the LVLM) creates\na bottleneck by condensing the visual information into a small number of\nsummary tokens. Then, using the same LLM, the second forward pass processes the\nlanguage instruction(s) alongside the summary tokens, used as a direct\nreplacement for the image ones. The training signal is provided by two losses:\nan autoregressive one applied after the second pass that provides a direct\noptimization objective for compression, and a contrastive loss, applied after\nthe first pass, that further boosts the representation strength, especially for\ndiscriminative tasks. The training is further enhanced by stage-specific\nadapters. We accompany the proposed method by an in-depth ablation study.\nOverall, Fwd2Bot results in highly-informative compressed representations\nsuitable for both generative and discriminative tasks. For generative tasks, we\noffer a 2x higher compression rate without compromising the generative\ncapabilities, setting a new state-of-the-art result. For discriminative tasks,\nwe set a new state-of-the-art on image retrieval and compositionality."
                },
                "authors": [
                    {
                        "name": "Adrian Bulat"
                    },
                    {
                        "name": "Yassine Ouali"
                    },
                    {
                        "name": "Georgios Tzimiropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Tzimiropoulos"
                },
                "author": "Georgios Tzimiropoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21758v1",
                "updated": "2025-03-27T17:57:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    57,
                    7,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:57:07Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    57,
                    7,
                    3,
                    86,
                    0
                ],
                "title": "Lumina-Image 2.0: A Unified and Efficient Image Generative Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lumina-Image 2.0: A Unified and Efficient Image Generative Framework"
                },
                "summary": "We introduce Lumina-Image 2.0, an advanced text-to-image generation framework\nthat achieves significant progress compared to previous work, Lumina-Next.\nLumina-Image 2.0 is built upon two key principles: (1) Unification - it adopts\na unified architecture (Unified Next-DiT) that treats text and image tokens as\na joint sequence, enabling natural cross-modal interactions and allowing\nseamless task expansion. Besides, since high-quality captioners can provide\nsemantically well-aligned text-image training pairs, we introduce a unified\ncaptioning system, Unified Captioner (UniCap), specifically designed for T2I\ngeneration tasks. UniCap excels at generating comprehensive and accurate\ncaptions, accelerating convergence and enhancing prompt adherence. (2)\nEfficiency - to improve the efficiency of our proposed model, we develop\nmulti-stage progressive training strategies and introduce inference\nacceleration techniques without compromising image quality. Extensive\nevaluations on academic benchmarks and public text-to-image arenas show that\nLumina-Image 2.0 delivers strong performances even with only 2.6B parameters,\nhighlighting its scalability and design efficiency. We have released our\ntraining details, code, and models at\nhttps://github.com/Alpha-VLLM/Lumina-Image-2.0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Lumina-Image 2.0, an advanced text-to-image generation framework\nthat achieves significant progress compared to previous work, Lumina-Next.\nLumina-Image 2.0 is built upon two key principles: (1) Unification - it adopts\na unified architecture (Unified Next-DiT) that treats text and image tokens as\na joint sequence, enabling natural cross-modal interactions and allowing\nseamless task expansion. Besides, since high-quality captioners can provide\nsemantically well-aligned text-image training pairs, we introduce a unified\ncaptioning system, Unified Captioner (UniCap), specifically designed for T2I\ngeneration tasks. UniCap excels at generating comprehensive and accurate\ncaptions, accelerating convergence and enhancing prompt adherence. (2)\nEfficiency - to improve the efficiency of our proposed model, we develop\nmulti-stage progressive training strategies and introduce inference\nacceleration techniques without compromising image quality. Extensive\nevaluations on academic benchmarks and public text-to-image arenas show that\nLumina-Image 2.0 delivers strong performances even with only 2.6B parameters,\nhighlighting its scalability and design efficiency. We have released our\ntraining details, code, and models at\nhttps://github.com/Alpha-VLLM/Lumina-Image-2.0."
                },
                "authors": [
                    {
                        "name": "Qi Qin"
                    },
                    {
                        "name": "Le Zhuo"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Ruoyi Du"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Bin Fu"
                    },
                    {
                        "name": "Yiting Lu"
                    },
                    {
                        "name": "Jiakang Yuan"
                    },
                    {
                        "name": "Xinyue Li"
                    },
                    {
                        "name": "Dongyang Liu"
                    },
                    {
                        "name": "Xiangyang Zhu"
                    },
                    {
                        "name": "Manyuan Zhang"
                    },
                    {
                        "name": "Will Beddow"
                    },
                    {
                        "name": "Erwann Millon"
                    },
                    {
                        "name": "Victor Perez"
                    },
                    {
                        "name": "Wenhai Wang"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Xiaohong Liu"
                    },
                    {
                        "name": "Hongsheng Li"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Chang Xu"
                    },
                    {
                        "name": "Peng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Peng Gao"
                },
                "author": "Peng Gao",
                "arxiv_comment": "Tech Report, 21 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21755v1",
                "updated": "2025-03-27T17:57:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    57,
                    1,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:57:01Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    57,
                    1,
                    3,
                    86,
                    0
                ],
                "title": "VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic\n  Faithfulness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic\n  Faithfulness"
                },
                "summary": "Video generation has advanced significantly, evolving from producing\nunrealistic outputs to generating videos that appear visually convincing and\ntemporally coherent. To evaluate these video generative models, benchmarks such\nas VBench have been developed to assess their faithfulness, measuring factors\nlike per-frame aesthetics, temporal consistency, and basic prompt adherence.\nHowever, these aspects mainly represent superficial faithfulness, which focus\non whether the video appears visually convincing rather than whether it adheres\nto real-world principles. While recent models perform increasingly well on\nthese metrics, they still struggle to generate videos that are not just\nvisually plausible but fundamentally realistic. To achieve real \"world models\"\nthrough video generation, the next frontier lies in intrinsic faithfulness to\nensure that generated videos adhere to physical laws, commonsense reasoning,\nanatomical correctness, and compositional integrity. Achieving this level of\nrealism is essential for applications such as AI-assisted filmmaking and\nsimulated world modeling. To bridge this gap, we introduce VBench-2.0, a\nnext-generation benchmark designed to automatically evaluate video generative\nmodels for their intrinsic faithfulness. VBench-2.0 assesses five key\ndimensions: Human Fidelity, Controllability, Creativity, Physics, and\nCommonsense, each further broken down into fine-grained capabilities. Tailored\nfor individual dimensions, our evaluation framework integrates generalists such\nas state-of-the-art VLMs and LLMs, and specialists, including anomaly detection\nmethods proposed for video generation. We conduct extensive annotations to\nensure alignment with human judgment. By pushing beyond superficial\nfaithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new\nstandard for the next generation of video generative models in pursuit of\nintrinsic faithfulness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation has advanced significantly, evolving from producing\nunrealistic outputs to generating videos that appear visually convincing and\ntemporally coherent. To evaluate these video generative models, benchmarks such\nas VBench have been developed to assess their faithfulness, measuring factors\nlike per-frame aesthetics, temporal consistency, and basic prompt adherence.\nHowever, these aspects mainly represent superficial faithfulness, which focus\non whether the video appears visually convincing rather than whether it adheres\nto real-world principles. While recent models perform increasingly well on\nthese metrics, they still struggle to generate videos that are not just\nvisually plausible but fundamentally realistic. To achieve real \"world models\"\nthrough video generation, the next frontier lies in intrinsic faithfulness to\nensure that generated videos adhere to physical laws, commonsense reasoning,\nanatomical correctness, and compositional integrity. Achieving this level of\nrealism is essential for applications such as AI-assisted filmmaking and\nsimulated world modeling. To bridge this gap, we introduce VBench-2.0, a\nnext-generation benchmark designed to automatically evaluate video generative\nmodels for their intrinsic faithfulness. VBench-2.0 assesses five key\ndimensions: Human Fidelity, Controllability, Creativity, Physics, and\nCommonsense, each further broken down into fine-grained capabilities. Tailored\nfor individual dimensions, our evaluation framework integrates generalists such\nas state-of-the-art VLMs and LLMs, and specialists, including anomaly detection\nmethods proposed for video generation. We conduct extensive annotations to\nensure alignment with human judgment. By pushing beyond superficial\nfaithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new\nstandard for the next generation of video generative models in pursuit of\nintrinsic faithfulness."
                },
                "authors": [
                    {
                        "name": "Dian Zheng"
                    },
                    {
                        "name": "Ziqi Huang"
                    },
                    {
                        "name": "Hongbo Liu"
                    },
                    {
                        "name": "Kai Zou"
                    },
                    {
                        "name": "Yinan He"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Yuanhan Zhang"
                    },
                    {
                        "name": "Jingwen He"
                    },
                    {
                        "name": "Wei-Shi Zheng"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziwei Liu"
                },
                "author": "Ziwei Liu",
                "arxiv_comment": "Equal contributions from first two authors. Project page:\n  https://vchitect.github.io/VBench-2.0-project/ Code:\n  https://github.com/Vchitect/VBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20769v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20769v2",
                "updated": "2025-03-27T17:53:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    53,
                    5,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-26T17:53:06Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    53,
                    6,
                    2,
                    85,
                    0
                ],
                "title": "Inferring Treatment Effects in Large Panels by Uncovering Latent\n  Similarities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Treatment Effects in Large Panels by Uncovering Latent\n  Similarities"
                },
                "summary": "The presence of unobserved confounders is one of the main challenges in\nidentifying treatment effects. In this paper, we propose a new approach to\ncausal inference using panel data with large large $N$ and $T$. Our approach\nimputes the untreated potential outcomes for treated units using the outcomes\nfor untreated individuals with similar values of the latent confounders. In\norder to find units with similar latent characteristics, we utilize long\npre-treatment histories of the outcomes. Our analysis is based on a\nnonparametric, nonlinear, and nonseparable factor model for untreated potential\noutcomes and treatments. The model satisfies minimal smoothness requirements.\nWe impute both missing counterfactual outcomes and propensity scores using\nkernel smoothing based on the constructed measure of latent similarity between\nunits, and demonstrate that our estimates can achieve the optimal nonparametric\nrate of convergence up to log terms. Using these estimates, we construct a\ndoubly robust estimator of the period-specifc average treatment effect on the\ntreated (ATT), and provide conditions, under which this estimator is\n$\\sqrt{N}$-consistent, and asymptotically normal and unbiased. Our simulation\nstudy demonstrates that our method provides accurate inference for a wide range\nof data generating processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The presence of unobserved confounders is one of the main challenges in\nidentifying treatment effects. In this paper, we propose a new approach to\ncausal inference using panel data with large large $N$ and $T$. Our approach\nimputes the untreated potential outcomes for treated units using the outcomes\nfor untreated individuals with similar values of the latent confounders. In\norder to find units with similar latent characteristics, we utilize long\npre-treatment histories of the outcomes. Our analysis is based on a\nnonparametric, nonlinear, and nonseparable factor model for untreated potential\noutcomes and treatments. The model satisfies minimal smoothness requirements.\nWe impute both missing counterfactual outcomes and propensity scores using\nkernel smoothing based on the constructed measure of latent similarity between\nunits, and demonstrate that our estimates can achieve the optimal nonparametric\nrate of convergence up to log terms. Using these estimates, we construct a\ndoubly robust estimator of the period-specifc average treatment effect on the\ntreated (ATT), and provide conditions, under which this estimator is\n$\\sqrt{N}$-consistent, and asymptotically normal and unbiased. Our simulation\nstudy demonstrates that our method provides accurate inference for a wide range\nof data generating processes."
                },
                "authors": [
                    {
                        "name": "Ben Deaner"
                    },
                    {
                        "name": "Chen-Wei Hsiang"
                    },
                    {
                        "name": "Andrei Zeleneev"
                    }
                ],
                "author_detail": {
                    "name": "Andrei Zeleneev"
                },
                "author": "Andrei Zeleneev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20769v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20769v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21735v1",
                "updated": "2025-03-27T17:48:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    48,
                    32,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:48:32Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    48,
                    32,
                    3,
                    86,
                    0
                ],
                "title": "GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release\n  Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release\n  Analytics"
                },
                "summary": "Ensuring the reliability and effectiveness of software release decisions is\ncritical, particularly in safety-critical domains like automotive systems.\nPrecise analysis of release validation data, often presented in tabular form,\nplays a pivotal role in this process. However, traditional methods that rely on\nmanual analysis of extensive test datasets and validation metrics are prone to\ndelays and high costs. Large Language Models (LLMs) offer a promising\nalternative but face challenges in analytical reasoning, contextual\nunderstanding, handling out-of-scope queries, and processing structured test\ndata consistently; limitations that hinder their direct application in\nsafety-critical scenarios. This paper introduces GateLens, an LLM-based tool\nfor analyzing tabular data in the automotive domain. GateLens translates\nnatural language queries into Relational Algebra (RA) expressions and then\ngenerates optimized Python code. It outperforms the baseline system on\nbenchmarking datasets, achieving higher F1 scores and handling complex and\nambiguous queries with greater robustness. Ablation studies confirm the\ncritical role of the RA module, with performance dropping sharply when omitted.\nIndustrial evaluations reveal that GateLens reduces analysis time by over 80%\nwhile maintaining high accuracy and reliability. As demonstrated by presented\nresults, GateLens achieved high performance without relying on few-shot\nexamples, showcasing strong generalization across various query types from\ndiverse company roles. Insights from deploying GateLens with a partner\nautomotive company offer practical guidance for integrating AI into critical\nworkflows such as release validation. Results show that by automating test\nresult analysis, GateLens enables faster, more informed, and dependable release\ndecisions, and can thus advance software scalability and reliability in\nautomotive systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the reliability and effectiveness of software release decisions is\ncritical, particularly in safety-critical domains like automotive systems.\nPrecise analysis of release validation data, often presented in tabular form,\nplays a pivotal role in this process. However, traditional methods that rely on\nmanual analysis of extensive test datasets and validation metrics are prone to\ndelays and high costs. Large Language Models (LLMs) offer a promising\nalternative but face challenges in analytical reasoning, contextual\nunderstanding, handling out-of-scope queries, and processing structured test\ndata consistently; limitations that hinder their direct application in\nsafety-critical scenarios. This paper introduces GateLens, an LLM-based tool\nfor analyzing tabular data in the automotive domain. GateLens translates\nnatural language queries into Relational Algebra (RA) expressions and then\ngenerates optimized Python code. It outperforms the baseline system on\nbenchmarking datasets, achieving higher F1 scores and handling complex and\nambiguous queries with greater robustness. Ablation studies confirm the\ncritical role of the RA module, with performance dropping sharply when omitted.\nIndustrial evaluations reveal that GateLens reduces analysis time by over 80%\nwhile maintaining high accuracy and reliability. As demonstrated by presented\nresults, GateLens achieved high performance without relying on few-shot\nexamples, showcasing strong generalization across various query types from\ndiverse company roles. Insights from deploying GateLens with a partner\nautomotive company offer practical guidance for integrating AI into critical\nworkflows such as release validation. Results show that by automating test\nresult analysis, GateLens enables faster, more informed, and dependable release\ndecisions, and can thus advance software scalability and reliability in\nautomotive systems."
                },
                "authors": [
                    {
                        "name": "Arsham Gholamzadeh Khoee"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Yinan Yu"
                    },
                    {
                        "name": "Robert Feldt"
                    },
                    {
                        "name": "Dhasarathy Parthasarathy"
                    }
                ],
                "author_detail": {
                    "name": "Dhasarathy Parthasarathy"
                },
                "author": "Dhasarathy Parthasarathy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v2",
                "updated": "2025-03-27T17:48:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    48,
                    14,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21730v1",
                "updated": "2025-03-27T17:45:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    45,
                    6,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:45:06Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    45,
                    6,
                    3,
                    86,
                    0
                ],
                "title": "Effective Skill Unlearning through Intervention and Abstention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective Skill Unlearning through Intervention and Abstention"
                },
                "summary": "Large language Models (LLMs) have demonstrated remarkable skills across\nvarious domains. Understanding the mechanisms behind their abilities and\nimplementing controls over them is becoming increasingly important for\ndeveloping better models. In this paper, we focus on skill unlearning in LLMs,\nspecifically unlearning a particular skill while retaining their overall\ncapabilities. We introduce two lightweight, training-free machine skill\nunlearning techniques for LLMs. First, we observe that the pre-activation\ndistribution of neurons in each Feed-Forward Layer (FFL) differs when the model\ndemonstrates different skills. Additionally, we find that queries triggering\nthe same skill cluster within the FFL key space and can be separated from other\nqueries using a hypercube. Based on these observations, we propose two\nlightweight, training-free skill unlearning methods via \\textit{intervention}\nand \\textit{abstention} respectively: \\texttt{Neuron Adjust} and \\texttt{Key\nSpace Detection}. We evaluate our methods on unlearning math-solving,\nPython-coding, and comprehension skills across seven different languages. The\nresults demonstrate their strong unlearning capabilities for the designated\nskills. Specifically, \\texttt{Key Space Detection} achieves over 80\\% relative\nperformance drop on the forgetting skill and less than 10\\% relative\nperformance drop on other skills and the model's general knowledge (MMLU) for\nmost unlearning tasks. Our code is available at\nhttps://github.com/Trustworthy-ML-Lab/effective_skill_unlearning",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language Models (LLMs) have demonstrated remarkable skills across\nvarious domains. Understanding the mechanisms behind their abilities and\nimplementing controls over them is becoming increasingly important for\ndeveloping better models. In this paper, we focus on skill unlearning in LLMs,\nspecifically unlearning a particular skill while retaining their overall\ncapabilities. We introduce two lightweight, training-free machine skill\nunlearning techniques for LLMs. First, we observe that the pre-activation\ndistribution of neurons in each Feed-Forward Layer (FFL) differs when the model\ndemonstrates different skills. Additionally, we find that queries triggering\nthe same skill cluster within the FFL key space and can be separated from other\nqueries using a hypercube. Based on these observations, we propose two\nlightweight, training-free skill unlearning methods via \\textit{intervention}\nand \\textit{abstention} respectively: \\texttt{Neuron Adjust} and \\texttt{Key\nSpace Detection}. We evaluate our methods on unlearning math-solving,\nPython-coding, and comprehension skills across seven different languages. The\nresults demonstrate their strong unlearning capabilities for the designated\nskills. Specifically, \\texttt{Key Space Detection} achieves over 80\\% relative\nperformance drop on the forgetting skill and less than 10\\% relative\nperformance drop on other skills and the model's general knowledge (MMLU) for\nmost unlearning tasks. Our code is available at\nhttps://github.com/Trustworthy-ML-Lab/effective_skill_unlearning"
                },
                "authors": [
                    {
                        "name": "Yongce Li"
                    },
                    {
                        "name": "Chung-En Sun"
                    },
                    {
                        "name": "Tsui-Wei Weng"
                    }
                ],
                "author_detail": {
                    "name": "Tsui-Wei Weng"
                },
                "author": "Tsui-Wei Weng",
                "arxiv_comment": "Accepted to NAACL 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05510v2",
                "updated": "2025-03-27T17:40:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    40,
                    9,
                    3,
                    86,
                    0
                ],
                "published": "2025-01-09T19:00:01Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    19,
                    0,
                    1,
                    3,
                    9,
                    0
                ],
                "title": "OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video\n  Understanding?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video\n  Understanding?"
                },
                "summary": "Temporal Awareness, the ability to reason dynamically based on the timestamp\nwhen a question is raised, is the key distinction between offline and online\nvideo LLMs. Unlike offline models, which rely on complete videos for static,\npost hoc analysis, online models process video streams incrementally and\ndynamically adapt their responses based on the timestamp at which the question\nis posed. Despite its significance, temporal awareness has not been adequately\nevaluated in existing benchmarks. To fill this gap, we present OVO-Bench\n(Online-VideO-Benchmark), a novel video benchmark that emphasizes the\nimportance of timestamps for advanced online video understanding capability\nbenchmarking. OVO-Bench evaluates the ability of video LLMs to reason and\nrespond to events occurring at specific timestamps under three distinct\nscenarios: (1) Backward tracing: trace back to past events to answer the\nquestion. (2) Real-time understanding: understand and respond to events as they\nunfold at the current timestamp. (3) Forward active responding: delay the\nresponse until sufficient future information becomes available to answer the\nquestion accurately. OVO-Bench comprises 12 tasks, featuring 644 unique videos\nand approximately human-curated 2,800 fine-grained meta-annotations with\nprecise timestamps. We combine automated generation pipelines with human\ncuration. With these high-quality samples, we further developed an evaluation\npipeline to systematically query video LLMs along the video timeline.\nEvaluations of nine Video-LLMs reveal that, despite advancements on traditional\nbenchmarks, current models struggle with online video understanding, showing a\nsignificant gap compared to human agents. We hope OVO-Bench will drive progress\nin video LLMs and inspire future research in online video reasoning. Our\nbenchmark and code can be accessed at https://github.com/JoeLeelyf/OVO-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Awareness, the ability to reason dynamically based on the timestamp\nwhen a question is raised, is the key distinction between offline and online\nvideo LLMs. Unlike offline models, which rely on complete videos for static,\npost hoc analysis, online models process video streams incrementally and\ndynamically adapt their responses based on the timestamp at which the question\nis posed. Despite its significance, temporal awareness has not been adequately\nevaluated in existing benchmarks. To fill this gap, we present OVO-Bench\n(Online-VideO-Benchmark), a novel video benchmark that emphasizes the\nimportance of timestamps for advanced online video understanding capability\nbenchmarking. OVO-Bench evaluates the ability of video LLMs to reason and\nrespond to events occurring at specific timestamps under three distinct\nscenarios: (1) Backward tracing: trace back to past events to answer the\nquestion. (2) Real-time understanding: understand and respond to events as they\nunfold at the current timestamp. (3) Forward active responding: delay the\nresponse until sufficient future information becomes available to answer the\nquestion accurately. OVO-Bench comprises 12 tasks, featuring 644 unique videos\nand approximately human-curated 2,800 fine-grained meta-annotations with\nprecise timestamps. We combine automated generation pipelines with human\ncuration. With these high-quality samples, we further developed an evaluation\npipeline to systematically query video LLMs along the video timeline.\nEvaluations of nine Video-LLMs reveal that, despite advancements on traditional\nbenchmarks, current models struggle with online video understanding, showing a\nsignificant gap compared to human agents. We hope OVO-Bench will drive progress\nin video LLMs and inspire future research in online video reasoning. Our\nbenchmark and code can be accessed at https://github.com/JoeLeelyf/OVO-Bench."
                },
                "authors": [
                    {
                        "name": "Yifei Li"
                    },
                    {
                        "name": "Junbo Niu"
                    },
                    {
                        "name": "Ziyang Miao"
                    },
                    {
                        "name": "Chunjiang Ge"
                    },
                    {
                        "name": "Yuanhang Zhou"
                    },
                    {
                        "name": "Qihao He"
                    },
                    {
                        "name": "Xiaoyi Dong"
                    },
                    {
                        "name": "Haodong Duan"
                    },
                    {
                        "name": "Shuangrui Ding"
                    },
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Pan Zhang"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Yuhang Cao"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21720v1",
                "updated": "2025-03-27T17:34:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    34,
                    25,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:34:25Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    34,
                    25,
                    3,
                    86,
                    0
                ],
                "title": "Collab: Controlled Decoding using Mixture of Agents for LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collab: Controlled Decoding using Mixture of Agents for LLM Alignment"
                },
                "summary": "Alignment of Large Language models (LLMs) is crucial for safe and trustworthy\ndeployment in applications. Reinforcement learning from human feedback (RLHF)\nhas emerged as an effective technique to align LLMs to human preferences and\nbroader utilities, but it requires updating billions of model parameters, which\nis computationally expensive. Controlled Decoding, by contrast, provides a\nmechanism for aligning a model at inference time without retraining. However,\nsingle-agent decoding approaches often struggle to adapt to diverse tasks due\nto the complexity and variability inherent in these tasks. To strengthen the\ntest-time performance w.r.t the target task, we propose a mixture of\nagent-based decoding strategies leveraging the existing off-the-shelf aligned\nLLM policies. Treating each prior policy as an agent in the spirit of mixture\nof agent collaboration, we develop a decoding method that allows for\ninference-time alignment through a token-level selection strategy among\nmultiple agents. For each token, the most suitable LLM is dynamically chosen\nfrom a pool of models based on a long-term utility metric. This\npolicy-switching mechanism ensures optimal model selection at each step,\nenabling efficient collaboration and alignment among LLMs during decoding.\nTheoretical analysis of our proposed algorithm establishes optimal performance\nwith respect to the target task represented via a target reward for the given\noff-the-shelf models. We conduct comprehensive empirical evaluations with\nopen-source aligned models on diverse tasks and preferences, which demonstrates\nthe merits of this approach over single-agent decoding baselines. Notably,\nCollab surpasses the current SoTA decoding strategy, achieving an improvement\nof up to 1.56x in average reward and 71.89% in GPT-4 based win-tie rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment of Large Language models (LLMs) is crucial for safe and trustworthy\ndeployment in applications. Reinforcement learning from human feedback (RLHF)\nhas emerged as an effective technique to align LLMs to human preferences and\nbroader utilities, but it requires updating billions of model parameters, which\nis computationally expensive. Controlled Decoding, by contrast, provides a\nmechanism for aligning a model at inference time without retraining. However,\nsingle-agent decoding approaches often struggle to adapt to diverse tasks due\nto the complexity and variability inherent in these tasks. To strengthen the\ntest-time performance w.r.t the target task, we propose a mixture of\nagent-based decoding strategies leveraging the existing off-the-shelf aligned\nLLM policies. Treating each prior policy as an agent in the spirit of mixture\nof agent collaboration, we develop a decoding method that allows for\ninference-time alignment through a token-level selection strategy among\nmultiple agents. For each token, the most suitable LLM is dynamically chosen\nfrom a pool of models based on a long-term utility metric. This\npolicy-switching mechanism ensures optimal model selection at each step,\nenabling efficient collaboration and alignment among LLMs during decoding.\nTheoretical analysis of our proposed algorithm establishes optimal performance\nwith respect to the target task represented via a target reward for the given\noff-the-shelf models. We conduct comprehensive empirical evaluations with\nopen-source aligned models on diverse tasks and preferences, which demonstrates\nthe merits of this approach over single-agent decoding baselines. Notably,\nCollab surpasses the current SoTA decoding strategy, achieving an improvement\nof up to 1.56x in average reward and 71.89% in GPT-4 based win-tie rate."
                },
                "authors": [
                    {
                        "name": "Souradip Chakraborty"
                    },
                    {
                        "name": "Sujay Bhatt"
                    },
                    {
                        "name": "Udari Madhushani Sehwag"
                    },
                    {
                        "name": "Soumya Suvra Ghosal"
                    },
                    {
                        "name": "Jiahao Qiu"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Dinesh Manocha"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Alec Koppel"
                    },
                    {
                        "name": "Sumitra Ganesh"
                    }
                ],
                "author_detail": {
                    "name": "Sumitra Ganesh"
                },
                "author": "Sumitra Ganesh",
                "arxiv_comment": "Accepted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18943v2",
                "updated": "2025-03-27T17:34:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    34,
                    6,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-24T17:59:07Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    59,
                    7,
                    0,
                    83,
                    0
                ],
                "title": "SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language\n  Models for Long-Form Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language\n  Models for Long-Form Video Understanding"
                },
                "summary": "We introduce SlowFast-LLaVA-1.5 (abbreviated as SF-LLaVA-1.5), a family of\nvideo large language models (LLMs) offering a token-efficient solution for\nlong-form video understanding. We incorporate the two-stream SlowFast mechanism\ninto a streamlined training pipeline, and perform joint video-image training on\na carefully curated data mixture of only publicly available datasets. Our\nprimary focus is on highly efficient model scales (1B and 3B), demonstrating\nthat even relatively small Video LLMs can achieve state-of-the-art performance\non video understanding, meeting the demand for mobile-friendly models.\nExperimental results demonstrate that SF-LLaVA-1.5 achieves superior\nperformance on a wide range of video and image tasks, with robust results at\nall model sizes (ranging from 1B to 7B). Notably, SF-LLaVA-1.5 achieves\nstate-of-the-art results in long-form video understanding (e.g., LongVideoBench\nand MLVU) and excels at small scales across various video benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SlowFast-LLaVA-1.5 (abbreviated as SF-LLaVA-1.5), a family of\nvideo large language models (LLMs) offering a token-efficient solution for\nlong-form video understanding. We incorporate the two-stream SlowFast mechanism\ninto a streamlined training pipeline, and perform joint video-image training on\na carefully curated data mixture of only publicly available datasets. Our\nprimary focus is on highly efficient model scales (1B and 3B), demonstrating\nthat even relatively small Video LLMs can achieve state-of-the-art performance\non video understanding, meeting the demand for mobile-friendly models.\nExperimental results demonstrate that SF-LLaVA-1.5 achieves superior\nperformance on a wide range of video and image tasks, with robust results at\nall model sizes (ranging from 1B to 7B). Notably, SF-LLaVA-1.5 achieves\nstate-of-the-art results in long-form video understanding (e.g., LongVideoBench\nand MLVU) and excels at small scales across various video benchmarks."
                },
                "authors": [
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Jiasen Lu"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Zhengfeng Lai"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Kai Kang"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Afshin Dehghan"
                    }
                ],
                "author_detail": {
                    "name": "Afshin Dehghan"
                },
                "author": "Afshin Dehghan",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21717v1",
                "updated": "2025-03-27T17:29:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    29,
                    45,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:29:45Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    29,
                    45,
                    3,
                    86,
                    0
                ],
                "title": "CLAIMCHECK: How Grounded are LLM Critiques of Scientific Papers?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLAIMCHECK: How Grounded are LLM Critiques of Scientific Papers?"
                },
                "summary": "A core part of scientific peer review involves providing expert critiques\nthat directly assess the scientific claims a paper makes. While it is now\npossible to automatically generate plausible (if generic) reviews, ensuring\nthat these reviews are sound and grounded in the papers' claims remains\nchallenging. To facilitate LLM benchmarking on these challenges, we introduce\nCLAIMCHECK, an annotated dataset of NeurIPS 2023 and 2024 submissions and\nreviews mined from OpenReview. CLAIMCHECK is richly annotated by ML experts for\nweakness statements in the reviews and the paper claims that they dispute, as\nwell as fine-grained labels of the validity, objectivity, and type of the\nidentified weaknesses. We benchmark several LLMs on three claim-centric tasks\nsupported by CLAIMCHECK, requiring models to (1) associate weaknesses with the\nclaims they dispute, (2) predict fine-grained labels for weaknesses and rewrite\nthe weaknesses to enhance their specificity, and (3) verify a paper's claims\nwith grounded reasoning. Our experiments reveal that cutting-edge LLMs, while\ncapable of predicting weakness labels in (2), continue to underperform relative\nto human experts on all other tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A core part of scientific peer review involves providing expert critiques\nthat directly assess the scientific claims a paper makes. While it is now\npossible to automatically generate plausible (if generic) reviews, ensuring\nthat these reviews are sound and grounded in the papers' claims remains\nchallenging. To facilitate LLM benchmarking on these challenges, we introduce\nCLAIMCHECK, an annotated dataset of NeurIPS 2023 and 2024 submissions and\nreviews mined from OpenReview. CLAIMCHECK is richly annotated by ML experts for\nweakness statements in the reviews and the paper claims that they dispute, as\nwell as fine-grained labels of the validity, objectivity, and type of the\nidentified weaknesses. We benchmark several LLMs on three claim-centric tasks\nsupported by CLAIMCHECK, requiring models to (1) associate weaknesses with the\nclaims they dispute, (2) predict fine-grained labels for weaknesses and rewrite\nthe weaknesses to enhance their specificity, and (3) verify a paper's claims\nwith grounded reasoning. Our experiments reveal that cutting-edge LLMs, while\ncapable of predicting weakness labels in (2), continue to underperform relative\nto human experts on all other tasks."
                },
                "authors": [
                    {
                        "name": "Jiefu Ou"
                    },
                    {
                        "name": "William Gantt Walden"
                    },
                    {
                        "name": "Kate Sanders"
                    },
                    {
                        "name": "Zhengping Jiang"
                    },
                    {
                        "name": "Kaiser Sun"
                    },
                    {
                        "name": "Jeffrey Cheng"
                    },
                    {
                        "name": "William Jurayj"
                    },
                    {
                        "name": "Miriam Wanner"
                    },
                    {
                        "name": "Shaobo Liang"
                    },
                    {
                        "name": "Candice Morgan"
                    },
                    {
                        "name": "Seunghoon Han"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Chandler May"
                    },
                    {
                        "name": "Hannah Recknor"
                    },
                    {
                        "name": "Daniel Khashabi"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21714v1",
                "updated": "2025-03-27T17:26:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    26,
                    32,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:26:32Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    26,
                    32,
                    3,
                    86,
                    0
                ],
                "title": "As easy as PIE: understanding when pruning causes language models to\n  disagree",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As easy as PIE: understanding when pruning causes language models to\n  disagree"
                },
                "summary": "Language Model (LM) pruning compresses the model by removing weights, nodes,\nor other parts of its architecture. Typically, pruning focuses on the resulting\nefficiency gains at the cost of effectiveness. However, when looking at how\nindividual data points are affected by pruning, it turns out that a particular\nsubset of data points always bears most of the brunt (in terms of reduced\naccuracy) when pruning, but this effect goes unnoticed when reporting the mean\naccuracy of all data points. These data points are called PIEs and have been\nstudied in image processing, but not in NLP. In a study of various NLP\ndatasets, pruning methods, and levels of compression, we find that PIEs impact\ninference quality considerably, regardless of class frequency, and that BERT is\nmore prone to this than BiLSTM. We also find that PIEs contain a high amount of\ndata points that have the largest influence on how well the model generalises\nto unseen data. This means that when pruning, with seemingly moderate loss to\naccuracy across all data points, we in fact hurt tremendously those data points\nthat matter the most. We trace what makes PIEs both hard and impactful to\ninference to their overall longer and more semantically complex text. These\nfindings are novel and contribute to understanding how LMs are affected by\npruning. The code is available at: https://github.com/pietrotrope/AsEasyAsPIE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Model (LM) pruning compresses the model by removing weights, nodes,\nor other parts of its architecture. Typically, pruning focuses on the resulting\nefficiency gains at the cost of effectiveness. However, when looking at how\nindividual data points are affected by pruning, it turns out that a particular\nsubset of data points always bears most of the brunt (in terms of reduced\naccuracy) when pruning, but this effect goes unnoticed when reporting the mean\naccuracy of all data points. These data points are called PIEs and have been\nstudied in image processing, but not in NLP. In a study of various NLP\ndatasets, pruning methods, and levels of compression, we find that PIEs impact\ninference quality considerably, regardless of class frequency, and that BERT is\nmore prone to this than BiLSTM. We also find that PIEs contain a high amount of\ndata points that have the largest influence on how well the model generalises\nto unseen data. This means that when pruning, with seemingly moderate loss to\naccuracy across all data points, we in fact hurt tremendously those data points\nthat matter the most. We trace what makes PIEs both hard and impactful to\ninference to their overall longer and more semantically complex text. These\nfindings are novel and contribute to understanding how LMs are affected by\npruning. The code is available at: https://github.com/pietrotrope/AsEasyAsPIE"
                },
                "authors": [
                    {
                        "name": "Pietro Tropeano"
                    },
                    {
                        "name": "Maria Maistro"
                    },
                    {
                        "name": "Tuukka Ruotsalo"
                    },
                    {
                        "name": "Christina Lioma"
                    }
                ],
                "author_detail": {
                    "name": "Christina Lioma"
                },
                "author": "Christina Lioma",
                "arxiv_comment": "Accepted to NAACL 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21710v1",
                "updated": "2025-03-27T17:21:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    21,
                    47,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:21:47Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    21,
                    47,
                    3,
                    86,
                    0
                ],
                "title": "Enhancing Repository-Level Software Repair via Repository-Aware\n  Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Repository-Level Software Repair via Repository-Aware\n  Knowledge Graphs"
                },
                "summary": "Repository-level software repair faces challenges in bridging semantic gaps\nbetween issue descriptions and code patches. Existing approaches, which mostly\ndepend on large language models (LLMs), suffer from semantic ambiguities,\nlimited structural context understanding, and insufficient reasoning\ncapability. To address these limitations, we propose KGCompass with two\ninnovations: (1) a novel repository-aware knowledge graph (KG) that accurately\nlinks repository artifacts (issues and pull requests) and codebase entities\n(files, classes, and functions), allowing us to effectively narrow down the\nvast search space to only 20 most relevant functions with accurate candidate\nbug locations and contextual information, and (2) a path-guided repair\nmechanism that leverages KG-mined entity path, tracing through which allows us\nto augment LLMs with relevant contextual information to generate precise\npatches along with their explanations. Experimental results in the\nSWE-Bench-Lite demonstrate that KGCompass achieves state-of-the-art repair\nperformance (45.67%) and function-level localization accuracy (51.33%) across\nopen-source approaches, costing only $0.20 per repair. Our analysis reveals\nthat among successfully localized bugs, 69.7% require multi-hop traversals\nthrough the knowledge graph, without which LLM-based approaches struggle to\naccurately locate bugs. The knowledge graph built in KGCompass is language\nagnostic and can be incrementally updated, making it a practical solution for\nreal-world development environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-level software repair faces challenges in bridging semantic gaps\nbetween issue descriptions and code patches. Existing approaches, which mostly\ndepend on large language models (LLMs), suffer from semantic ambiguities,\nlimited structural context understanding, and insufficient reasoning\ncapability. To address these limitations, we propose KGCompass with two\ninnovations: (1) a novel repository-aware knowledge graph (KG) that accurately\nlinks repository artifacts (issues and pull requests) and codebase entities\n(files, classes, and functions), allowing us to effectively narrow down the\nvast search space to only 20 most relevant functions with accurate candidate\nbug locations and contextual information, and (2) a path-guided repair\nmechanism that leverages KG-mined entity path, tracing through which allows us\nto augment LLMs with relevant contextual information to generate precise\npatches along with their explanations. Experimental results in the\nSWE-Bench-Lite demonstrate that KGCompass achieves state-of-the-art repair\nperformance (45.67%) and function-level localization accuracy (51.33%) across\nopen-source approaches, costing only $0.20 per repair. Our analysis reveals\nthat among successfully localized bugs, 69.7% require multi-hop traversals\nthrough the knowledge graph, without which LLM-based approaches struggle to\naccurately locate bugs. The knowledge graph built in KGCompass is language\nagnostic and can be incrementally updated, making it a practical solution for\nreal-world development environments."
                },
                "authors": [
                    {
                        "name": "Boyang Yang"
                    },
                    {
                        "name": "Haoye Tian"
                    },
                    {
                        "name": "Jiadong Ren"
                    },
                    {
                        "name": "Shunfu Jin"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Bach Le"
                    }
                ],
                "author_detail": {
                    "name": "Bach Le"
                },
                "author": "Bach Le",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21706v1",
                "updated": "2025-03-27T17:20:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    20,
                    35,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:20:35Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    20,
                    35,
                    3,
                    86,
                    0
                ],
                "title": "Flashlights: Prospects for constraining the Initial Mass Function around\n  cosmic noon with caustic-crossing events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flashlights: Prospects for constraining the Initial Mass Function around\n  cosmic noon with caustic-crossing events"
                },
                "summary": "The Flashlights program with the Hubble Space Telescope imaged the six Hubble\nFrontier Fields galaxy clusters in two epochs and detected twenty transients.\nThese are primarily expected to be caustic-crossing events (CCEs) where bright\nstars in distant lensed galaxies, typically at redshift $z\\approx1$--3, get\ntemporarily magnified close to cluster caustics. Since CCEs are generally\nbiased toward more massive and luminous stars, they offer a unique route for\nprobing the high end of the stellar mass function. We take advantage of the\nFlashlights event statistics to place preliminary constraints on the stellar\ninitial mass function (IMF) around cosmic noon. The photometry (along with\nspectral information) of lensed arcs is used to infer their various stellar\nproperties, and stellar synthesis models are used to evolve a recent stellar\npopulation in them. We estimate the microlens surface density near each arc\nand, together with existing lens models and simple formalism for CCEs,\ncalculate the expected rate for a given IMF. We find that, on average, a\nSalpeter-like IMF ($\\alpha=2.35$) underpredicts the number of observed CCEs by\na factor of ${\\sim}0.7$, and a top-heavy IMF ($\\alpha=1.00$) overpredicts by a\nfactor of ${\\sim}1.7$, suggesting that the average IMF slope may lie somewhere\nin between. However, given the large uncertainties associated with estimating\nthe stellar populations, these results are strongly model-dependent.\nNevertheless, we introduce a useful framework for constraining the IMF using\nCCEs. Observations with JWST are already yielding many more CCEs and will soon\nenable more stringent constraints on the IMF at a range of redshifts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Flashlights program with the Hubble Space Telescope imaged the six Hubble\nFrontier Fields galaxy clusters in two epochs and detected twenty transients.\nThese are primarily expected to be caustic-crossing events (CCEs) where bright\nstars in distant lensed galaxies, typically at redshift $z\\approx1$--3, get\ntemporarily magnified close to cluster caustics. Since CCEs are generally\nbiased toward more massive and luminous stars, they offer a unique route for\nprobing the high end of the stellar mass function. We take advantage of the\nFlashlights event statistics to place preliminary constraints on the stellar\ninitial mass function (IMF) around cosmic noon. The photometry (along with\nspectral information) of lensed arcs is used to infer their various stellar\nproperties, and stellar synthesis models are used to evolve a recent stellar\npopulation in them. We estimate the microlens surface density near each arc\nand, together with existing lens models and simple formalism for CCEs,\ncalculate the expected rate for a given IMF. We find that, on average, a\nSalpeter-like IMF ($\\alpha=2.35$) underpredicts the number of observed CCEs by\na factor of ${\\sim}0.7$, and a top-heavy IMF ($\\alpha=1.00$) overpredicts by a\nfactor of ${\\sim}1.7$, suggesting that the average IMF slope may lie somewhere\nin between. However, given the large uncertainties associated with estimating\nthe stellar populations, these results are strongly model-dependent.\nNevertheless, we introduce a useful framework for constraining the IMF using\nCCEs. Observations with JWST are already yielding many more CCEs and will soon\nenable more stringent constraints on the IMF at a range of redshifts."
                },
                "authors": [
                    {
                        "name": "Ashish Kumar Meena"
                    },
                    {
                        "name": "Sung Kei Li"
                    },
                    {
                        "name": "Adi Zitrin"
                    },
                    {
                        "name": "Patrick L. Kelly"
                    },
                    {
                        "name": "Tom Broadhurst"
                    },
                    {
                        "name": "Wenlei Chen"
                    },
                    {
                        "name": "Jose M. Diego"
                    },
                    {
                        "name": "Alexei V. Filippenko"
                    },
                    {
                        "name": "Lukas J. Furtak"
                    },
                    {
                        "name": "Liliya L. R. Williams"
                    }
                ],
                "author_detail": {
                    "name": "Liliya L. R. Williams"
                },
                "author": "Liliya L. R. Williams",
                "arxiv_comment": "14 pages. 6 figures. Comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20074v2",
                "updated": "2025-03-27T17:16:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    16,
                    44,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-25T21:20:11Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    21,
                    20,
                    11,
                    1,
                    84,
                    0
                ],
                "title": "Adaptive Orchestration for Large-Scale Inference on Heterogeneous\n  Accelerator Systems Balancing Cost, Performance, and Resilience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Orchestration for Large-Scale Inference on Heterogeneous\n  Accelerator Systems Balancing Cost, Performance, and Resilience"
                },
                "summary": "The surge in generative AI workloads has created a need for scalable\ninference systems that can flexibly harness both GPUs and specialized\naccelerators while containing operational costs. This paper proposes a\nhardware-agnostic control loop that adaptively allocates requests across\nheterogeneous accelerators based on real-time cost and capacity signals. The\napproach sustains low latency and high throughput by dynamically shifting\nbetween cost-optimized and capacity-optimized modes, ensuring the most\nefficient use of expensive compute resources under fluctuating availability.\nEvaluated using the Stable Diffusion model, the framework consistently meets\nlatency targets, automatically redirects traffic during capacity shortfalls,\nand capitalizes on lower-cost accelerators when possible. These results\nhighlight how a feedback-driven deployment strategy, spanning the entire\nsoftware and hardware stack, can help organizations efficiently scale\ngenerative AI workloads while maintaining resilience in the face of limited\naccelerator capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The surge in generative AI workloads has created a need for scalable\ninference systems that can flexibly harness both GPUs and specialized\naccelerators while containing operational costs. This paper proposes a\nhardware-agnostic control loop that adaptively allocates requests across\nheterogeneous accelerators based on real-time cost and capacity signals. The\napproach sustains low latency and high throughput by dynamically shifting\nbetween cost-optimized and capacity-optimized modes, ensuring the most\nefficient use of expensive compute resources under fluctuating availability.\nEvaluated using the Stable Diffusion model, the framework consistently meets\nlatency targets, automatically redirects traffic during capacity shortfalls,\nand capitalizes on lower-cost accelerators when possible. These results\nhighlight how a feedback-driven deployment strategy, spanning the entire\nsoftware and hardware stack, can help organizations efficiently scale\ngenerative AI workloads while maintaining resilience in the face of limited\naccelerator capacity."
                },
                "authors": [
                    {
                        "name": "Yahav Biran"
                    },
                    {
                        "name": "Imry Kissos"
                    }
                ],
                "author_detail": {
                    "name": "Imry Kissos"
                },
                "author": "Imry Kissos",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68U01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21702v1",
                "updated": "2025-03-27T17:06:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    6,
                    9,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:06:09Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    6,
                    9,
                    3,
                    86,
                    0
                ],
                "title": "Enabling Robust Exoplanet Atmospheric Retrievals with Gaussian Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Robust Exoplanet Atmospheric Retrievals with Gaussian Processes"
                },
                "summary": "Atmospheric retrievals are essential tools for interpreting exoplanet\ntransmission and eclipse spectra, enabling quantitative constraints on the\nchemical composition, aerosol properties, and thermal structure of planetary\natmospheres. The James Webb Space Telescope (JWST) offers unprecedented\nspectral precision, resolution, and wavelength coverage, unlocking\ntransformative insights into the formation, evolution, climate, and potential\nhabitability of planetary systems. However, this opportunity is accompanied by\nchallenges: modeling assumptions and unaccounted-for noise or signal sources\ncan bias retrieval outcomes and their interpretation. To address these\nlimitations, we introduce a Gaussian Process (GP)-aided atmospheric retrieval\nframework that flexibly accounts for unmodeled features in exoplanet spectra,\nwhether global or localized. We validate this method on synthetic JWST\nobservations and show that GP-aided retrievals reduce bias in inferred\nabundances and better capture model-data mismatches than traditional\napproaches. We also introduce the concept of mean squared error to quantify the\ntrade-off between bias and variance, arguing that this metric more accurately\nreflects retrieval performance than bias alone. We then reanalyze the\nNIRISS/SOSS JWST transmission spectrum of WASP-96 b, finding that GP-aided\nretrievals yield broader constraints on CO$_2$ and H$_2$O, alleviating tension\nbetween previous retrieval results and equilibrium predictions. Our GP\nframework provides precise and accurate constraints while highlighting regions\nwhere models fail to explain the data. As JWST matures and future facilities\ncome online, a deeper understanding of the limitations of both data and models\nwill be essential, and GP-enabled retrievals like the one presented here offer\na principled path forward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atmospheric retrievals are essential tools for interpreting exoplanet\ntransmission and eclipse spectra, enabling quantitative constraints on the\nchemical composition, aerosol properties, and thermal structure of planetary\natmospheres. The James Webb Space Telescope (JWST) offers unprecedented\nspectral precision, resolution, and wavelength coverage, unlocking\ntransformative insights into the formation, evolution, climate, and potential\nhabitability of planetary systems. However, this opportunity is accompanied by\nchallenges: modeling assumptions and unaccounted-for noise or signal sources\ncan bias retrieval outcomes and their interpretation. To address these\nlimitations, we introduce a Gaussian Process (GP)-aided atmospheric retrieval\nframework that flexibly accounts for unmodeled features in exoplanet spectra,\nwhether global or localized. We validate this method on synthetic JWST\nobservations and show that GP-aided retrievals reduce bias in inferred\nabundances and better capture model-data mismatches than traditional\napproaches. We also introduce the concept of mean squared error to quantify the\ntrade-off between bias and variance, arguing that this metric more accurately\nreflects retrieval performance than bias alone. We then reanalyze the\nNIRISS/SOSS JWST transmission spectrum of WASP-96 b, finding that GP-aided\nretrievals yield broader constraints on CO$_2$ and H$_2$O, alleviating tension\nbetween previous retrieval results and equilibrium predictions. Our GP\nframework provides precise and accurate constraints while highlighting regions\nwhere models fail to explain the data. As JWST matures and future facilities\ncome online, a deeper understanding of the limitations of both data and models\nwill be essential, and GP-enabled retrievals like the one presented here offer\na principled path forward."
                },
                "authors": [
                    {
                        "name": "Yoav Rotman"
                    },
                    {
                        "name": "Luis Welbanks"
                    },
                    {
                        "name": "Michael R. Line"
                    },
                    {
                        "name": "Peter McGill"
                    },
                    {
                        "name": "Michael Radica"
                    },
                    {
                        "name": "Matthew C. Nixon"
                    }
                ],
                "author_detail": {
                    "name": "Matthew C. Nixon"
                },
                "author": "Matthew C. Nixon",
                "arxiv_comment": "Submitted to AAS Journals, 25 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21699v1",
                "updated": "2025-03-27T17:04:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    4,
                    33,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:04:33Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    4,
                    33,
                    3,
                    86,
                    0
                ],
                "title": "MAVERIX: Multimodal Audio-Visual Evaluation Reasoning IndeX",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAVERIX: Multimodal Audio-Visual Evaluation Reasoning IndeX"
                },
                "summary": "Frontier models have either been language-only or have primarily focused on\nvision and language modalities. Although recent advancements in models with\nvision and audio understanding capabilities have shown substantial progress,\nthe field lacks a standardized evaluation framework for thoroughly assessing\ntheir cross-modality perception performance. We introduce MAVERIX~(Multimodal\nAudio-Visual Evaluation Reasoning IndeX), a novel benchmark with 700 videos and\n2,556 questions explicitly designed to evaluate multimodal models through tasks\nthat necessitate close integration of video and audio information. MAVERIX\nuniquely provides models with audiovisual tasks, closely mimicking the\nmultimodal perceptual experiences available to humans during inference and\ndecision-making processes. To our knowledge, MAVERIX is the first benchmark\naimed explicitly at assessing comprehensive audiovisual integration.\nExperiments with state-of-the-art models, including Gemini 1.5 Pro and o1, show\nperformance approaching human levels (around 70% accuracy), while human experts\nreach near-ceiling performance (95.1%). With standardized evaluation protocols,\na rigorously annotated pipeline, and a public toolkit, MAVERIX establishes a\nchallenging testbed for advancing audiovisual multimodal intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frontier models have either been language-only or have primarily focused on\nvision and language modalities. Although recent advancements in models with\nvision and audio understanding capabilities have shown substantial progress,\nthe field lacks a standardized evaluation framework for thoroughly assessing\ntheir cross-modality perception performance. We introduce MAVERIX~(Multimodal\nAudio-Visual Evaluation Reasoning IndeX), a novel benchmark with 700 videos and\n2,556 questions explicitly designed to evaluate multimodal models through tasks\nthat necessitate close integration of video and audio information. MAVERIX\nuniquely provides models with audiovisual tasks, closely mimicking the\nmultimodal perceptual experiences available to humans during inference and\ndecision-making processes. To our knowledge, MAVERIX is the first benchmark\naimed explicitly at assessing comprehensive audiovisual integration.\nExperiments with state-of-the-art models, including Gemini 1.5 Pro and o1, show\nperformance approaching human levels (around 70% accuracy), while human experts\nreach near-ceiling performance (95.1%). With standardized evaluation protocols,\na rigorously annotated pipeline, and a public toolkit, MAVERIX establishes a\nchallenging testbed for advancing audiovisual multimodal intelligence."
                },
                "authors": [
                    {
                        "name": "Liuyue Xie"
                    },
                    {
                        "name": "George Z. Wei"
                    },
                    {
                        "name": "Avik Kuthiala"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Ananya Bal"
                    },
                    {
                        "name": "Mosam Dabhi"
                    },
                    {
                        "name": "Liting Wen"
                    },
                    {
                        "name": "Taru Rustagi"
                    },
                    {
                        "name": "Ethan Lai"
                    },
                    {
                        "name": "Sushil Khyalia"
                    },
                    {
                        "name": "Rohan Choudhury"
                    },
                    {
                        "name": "Morteza Ziyadi"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Hao Yang"
                    },
                    {
                        "name": "LÃ¡szlÃ³ A. Jeni"
                    }
                ],
                "author_detail": {
                    "name": "LÃ¡szlÃ³ A. Jeni"
                },
                "author": "LÃ¡szlÃ³ A. Jeni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21694v1",
                "updated": "2025-03-27T16:59:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    59,
                    15,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T16:59:15Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    59,
                    15,
                    3,
                    86,
                    0
                ],
                "title": "Progressive Rendering Distillation: Adapting Stable Diffusion for\n  Instant Text-to-Mesh Generation without 3D Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Rendering Distillation: Adapting Stable Diffusion for\n  Instant Text-to-Mesh Generation without 3D Data"
                },
                "summary": "It is highly desirable to obtain a model that can generate high-quality 3D\nmeshes from text prompts in just seconds. While recent attempts have adapted\npre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into\ngenerators of 3D representations (e.g., Triplane), they often suffer from poor\nquality due to the lack of sufficient high-quality 3D training data. Aiming at\novercoming the data shortage, we propose a novel training scheme, termed as\nProgressive Rendering Distillation (PRD), eliminating the need for 3D\nground-truths by distilling multi-view diffusion models and adapting SD into a\nnative 3D generator. In each iteration of training, PRD uses the U-Net to\nprogressively denoise the latent from random noise for a few steps, and in each\nstep it decodes the denoised latent into 3D output. Multi-view diffusion\nmodels, including MVDream and RichDreamer, are used in joint with SD to distill\ntext-consistent textures and geometries into the 3D outputs through score\ndistillation. Since PRD supports training without 3D ground-truths, we can\neasily scale up the training data and improve generation quality for\nchallenging text prompts with creative concepts. Meanwhile, PRD can accelerate\nthe inference speed of the generation model in just a few steps. With PRD, we\ntrain a Triplane generator, namely TriplaneTurbo, which adds only $2.5\\%$\ntrainable parameters to adapt SD for Triplane generation. TriplaneTurbo\noutperforms previous text-to-3D generators in both efficiency and quality.\nSpecifically, it can produce high-quality 3D meshes in 1.2 seconds and\ngeneralize well for challenging text input. The code is available at\nhttps://github.com/theEricMa/TriplaneTurbo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is highly desirable to obtain a model that can generate high-quality 3D\nmeshes from text prompts in just seconds. While recent attempts have adapted\npre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into\ngenerators of 3D representations (e.g., Triplane), they often suffer from poor\nquality due to the lack of sufficient high-quality 3D training data. Aiming at\novercoming the data shortage, we propose a novel training scheme, termed as\nProgressive Rendering Distillation (PRD), eliminating the need for 3D\nground-truths by distilling multi-view diffusion models and adapting SD into a\nnative 3D generator. In each iteration of training, PRD uses the U-Net to\nprogressively denoise the latent from random noise for a few steps, and in each\nstep it decodes the denoised latent into 3D output. Multi-view diffusion\nmodels, including MVDream and RichDreamer, are used in joint with SD to distill\ntext-consistent textures and geometries into the 3D outputs through score\ndistillation. Since PRD supports training without 3D ground-truths, we can\neasily scale up the training data and improve generation quality for\nchallenging text prompts with creative concepts. Meanwhile, PRD can accelerate\nthe inference speed of the generation model in just a few steps. With PRD, we\ntrain a Triplane generator, namely TriplaneTurbo, which adds only $2.5\\%$\ntrainable parameters to adapt SD for Triplane generation. TriplaneTurbo\noutperforms previous text-to-3D generators in both efficiency and quality.\nSpecifically, it can produce high-quality 3D meshes in 1.2 seconds and\ngeneralize well for challenging text input. The code is available at\nhttps://github.com/theEricMa/TriplaneTurbo."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Ma"
                    },
                    {
                        "name": "Xinyue Liang"
                    },
                    {
                        "name": "Rongyuan Wu"
                    },
                    {
                        "name": "Xiangyu Zhu"
                    },
                    {
                        "name": "Zhen Lei"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "Accepted to CVPR 2025.\n  Code:https://github.com/theEricMa/TriplaneTurbo.\n  Demo:https://huggingface.co/spaces/ZhiyuanthePony/TriplaneTurbo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21687v1",
                "updated": "2025-03-27T16:54:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    54,
                    22,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T16:54:22Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    54,
                    22,
                    3,
                    86,
                    0
                ],
                "title": "Exploiting synergies between JWST and cosmic 21-cm observations to\n  uncover star formation in the early Universe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting synergies between JWST and cosmic 21-cm observations to\n  uncover star formation in the early Universe"
                },
                "summary": "In the current era of JWST, we continue to uncover a wealth of information\nabout the Universe deep into the Epoch of Reionization. In this work, we run a\nsuite of simulations using the code 21cmSPACE, to explore the astrophysical\nproperties of galaxies in the early Universe, and their impact on high-redshift\nobservables. We use multi-wavelength observational data including the global\n21-cm signal and power spectrum limits from SARAS~3 and HERA respectively,\npresent-day diffuse X-ray and radio backgrounds, and UV luminosity functions\n(UVLFs) from HST and JWST in the range $z=6-14.5$ to derive our constraints. We\nconstrain a flexible model of halo-mass and redshift dependent star-formation\nefficiency (SFE), defined as the gas fraction converted into stars, and find\nthat it is best described by little to no redshift evolution at $z\\approx6-10$\nand rapid evolution at $z\\approx10-15$. We derive Bayesian functional posterior\ndistributions for the SFE across this redshift range, inferring that a halo of\nmass $M_h=10^{10}\\text{M}_\\odot$ has an efficiency of $2-3\\%$ at $z\\lesssim10$,\n$12\\%$ at $z=12$ and $26\\%$ at $z=15$. We also find, through synergy between\nSARAS~3 and UVLFs, that the minimum circular velocity for star-formation in\nhalos is $V_c = 16.9^{+25.7}_{-9.5}\\text{km s}^{-1}$ or equivalently\n$\\log_{10}(M_\\text{crit}/\\text{M}_\\odot) = 8.29^{+1.21}_{-1.08}$ at $z=6$.\nAlongside these star-formation constraints, we find the X-ray and radio\nefficiencies of early galaxies to be $f_X = 0.5^{+6.3}_{-0.3}$ and $f_r\n\\lesssim 11.7$ respectively, improving upon existing works that do not use UVLF\ndata. Our results demonstrate the critical role of UVLFs in constraining the\nearly Universe, and its synergies with 21-cm observations, alongside other\nmulti-wavelength observational datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the current era of JWST, we continue to uncover a wealth of information\nabout the Universe deep into the Epoch of Reionization. In this work, we run a\nsuite of simulations using the code 21cmSPACE, to explore the astrophysical\nproperties of galaxies in the early Universe, and their impact on high-redshift\nobservables. We use multi-wavelength observational data including the global\n21-cm signal and power spectrum limits from SARAS~3 and HERA respectively,\npresent-day diffuse X-ray and radio backgrounds, and UV luminosity functions\n(UVLFs) from HST and JWST in the range $z=6-14.5$ to derive our constraints. We\nconstrain a flexible model of halo-mass and redshift dependent star-formation\nefficiency (SFE), defined as the gas fraction converted into stars, and find\nthat it is best described by little to no redshift evolution at $z\\approx6-10$\nand rapid evolution at $z\\approx10-15$. We derive Bayesian functional posterior\ndistributions for the SFE across this redshift range, inferring that a halo of\nmass $M_h=10^{10}\\text{M}_\\odot$ has an efficiency of $2-3\\%$ at $z\\lesssim10$,\n$12\\%$ at $z=12$ and $26\\%$ at $z=15$. We also find, through synergy between\nSARAS~3 and UVLFs, that the minimum circular velocity for star-formation in\nhalos is $V_c = 16.9^{+25.7}_{-9.5}\\text{km s}^{-1}$ or equivalently\n$\\log_{10}(M_\\text{crit}/\\text{M}_\\odot) = 8.29^{+1.21}_{-1.08}$ at $z=6$.\nAlongside these star-formation constraints, we find the X-ray and radio\nefficiencies of early galaxies to be $f_X = 0.5^{+6.3}_{-0.3}$ and $f_r\n\\lesssim 11.7$ respectively, improving upon existing works that do not use UVLF\ndata. Our results demonstrate the critical role of UVLFs in constraining the\nearly Universe, and its synergies with 21-cm observations, alongside other\nmulti-wavelength observational datasets."
                },
                "authors": [
                    {
                        "name": "Jiten Dhandha"
                    },
                    {
                        "name": "Thomas Gessey-Jones"
                    },
                    {
                        "name": "Harry T. J. Bevins"
                    },
                    {
                        "name": "Simon Pochinda"
                    },
                    {
                        "name": "Anastasia Fialkov"
                    },
                    {
                        "name": "Sandro Tacchella"
                    },
                    {
                        "name": "Eloy de Lera Acedo"
                    },
                    {
                        "name": "Saurabh Singh"
                    },
                    {
                        "name": "Rennan Barkana"
                    }
                ],
                "author_detail": {
                    "name": "Rennan Barkana"
                },
                "author": "Rennan Barkana",
                "arxiv_comment": "28 pages, 13 figures, 6 tables. Submitted to MNRAS. All comments\n  welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21683v1",
                "updated": "2025-03-27T16:52:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    52,
                    25,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T16:52:25Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    52,
                    25,
                    3,
                    86,
                    0
                ],
                "title": "LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku\n  with Self-Play and Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku\n  with Self-Play and Reinforcement Learning"
                },
                "summary": "In recent years, large language models (LLMs) have shown significant\nadvancements in natural language processing (NLP), with strong capa-bilities in\ngeneration, comprehension, and rea-soning. These models have found applications\nin education, intelligent decision-making, and gaming. However, effectively\nutilizing LLMs for strategic planning and decision-making in the game of Gomoku\nremains a challenge. This study aims to develop a Gomoku AI system based on\nLLMs, simulating the human learning process of playing chess. The system is\nde-signed to understand and apply Gomoku strat-egies and logic to make rational\ndecisions. The research methods include enabling the model to \"read the board,\"\n\"understand the rules,\" \"select strategies,\" and \"evaluate positions,\" while\nen-hancing its abilities through self-play and rein-forcement learning. The\nresults demonstrate that this approach significantly improves the se-lection of\nmove positions, resolves the issue of generating illegal positions, and reduces\npro-cess time through parallel position evaluation. After extensive self-play\ntraining, the model's Gomoku-playing capabilities have been notably enhanced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have shown significant\nadvancements in natural language processing (NLP), with strong capa-bilities in\ngeneration, comprehension, and rea-soning. These models have found applications\nin education, intelligent decision-making, and gaming. However, effectively\nutilizing LLMs for strategic planning and decision-making in the game of Gomoku\nremains a challenge. This study aims to develop a Gomoku AI system based on\nLLMs, simulating the human learning process of playing chess. The system is\nde-signed to understand and apply Gomoku strat-egies and logic to make rational\ndecisions. The research methods include enabling the model to \"read the board,\"\n\"understand the rules,\" \"select strategies,\" and \"evaluate positions,\" while\nen-hancing its abilities through self-play and rein-forcement learning. The\nresults demonstrate that this approach significantly improves the se-lection of\nmove positions, resolves the issue of generating illegal positions, and reduces\npro-cess time through parallel position evaluation. After extensive self-play\ntraining, the model's Gomoku-playing capabilities have been notably enhanced."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hui Wang"
                },
                "author": "Hui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21673v1",
                "updated": "2025-03-27T16:41:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    41,
                    14,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T16:41:14Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    41,
                    14,
                    3,
                    86,
                    0
                ],
                "title": "A friendly introduction to triangular transport",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A friendly introduction to triangular transport"
                },
                "summary": "Decision making under uncertainty is a cross-cutting challenge in science and\nengineering. Most approaches to this challenge employ probabilistic\nrepresentations of uncertainty. In complicated systems accessible only via data\nor black-box models, however, these representations are rarely known. We\ndiscuss how to characterize and manipulate such representations using\ntriangular transport maps, which approximate any complex probability\ndistribution as a transformation of a simple, well-understood distribution. The\nparticular structure of triangular transport guarantees many desirable\nmathematical and computational properties that translate well into solving\npractical problems. Triangular maps are actively used for density estimation,\n(conditional) generative modelling, Bayesian inference, data assimilation,\noptimal experimental design, and related tasks. While there is ample literature\non the development and theory of triangular transport methods, this manuscript\nprovides a detailed introduction for scientists interested in employing measure\ntransport without assuming a formal mathematical background. We build intuition\nfor the key foundations of triangular transport, discuss many aspects of its\npractical implementation, and outline the frontiers of this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision making under uncertainty is a cross-cutting challenge in science and\nengineering. Most approaches to this challenge employ probabilistic\nrepresentations of uncertainty. In complicated systems accessible only via data\nor black-box models, however, these representations are rarely known. We\ndiscuss how to characterize and manipulate such representations using\ntriangular transport maps, which approximate any complex probability\ndistribution as a transformation of a simple, well-understood distribution. The\nparticular structure of triangular transport guarantees many desirable\nmathematical and computational properties that translate well into solving\npractical problems. Triangular maps are actively used for density estimation,\n(conditional) generative modelling, Bayesian inference, data assimilation,\noptimal experimental design, and related tasks. While there is ample literature\non the development and theory of triangular transport methods, this manuscript\nprovides a detailed introduction for scientists interested in employing measure\ntransport without assuming a formal mathematical background. We build intuition\nfor the key foundations of triangular transport, discuss many aspects of its\npractical implementation, and outline the frontiers of this field."
                },
                "authors": [
                    {
                        "name": "Maximilian Ramgraber"
                    },
                    {
                        "name": "Daniel Sharp"
                    },
                    {
                        "name": "Mathieu Le Provost"
                    },
                    {
                        "name": "Youssef Marzouk"
                    }
                ],
                "author_detail": {
                    "name": "Youssef Marzouk"
                },
                "author": "Youssef Marzouk",
                "arxiv_comment": "46 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62-01, 62-02, 60-08, 65C05, 62F15, 65C20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03006v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03006v2",
                "updated": "2025-03-27T16:36:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    36,
                    58,
                    3,
                    86,
                    0
                ],
                "published": "2024-07-03T11:05:19Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    11,
                    5,
                    19,
                    2,
                    185,
                    0
                ],
                "title": "Frequency-Controlled Diffusion Model for Versatile Text-Guided\n  Image-to-Image Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frequency-Controlled Diffusion Model for Versatile Text-Guided\n  Image-to-Image Translation"
                },
                "summary": "Recently, large-scale text-to-image (T2I) diffusion models have emerged as a\npowerful tool for image-to-image translation (I2I), allowing open-domain image\ntranslation via user-provided text prompts. This paper proposes\nfrequency-controlled diffusion model (FCDiffusion), an end-to-end\ndiffusion-based framework that contributes a novel solution to text-guided I2I\nfrom a frequency-domain perspective. At the heart of our framework is a\nfeature-space frequency-domain filtering module based on Discrete Cosine\nTransform, which filters the latent features of the source image in the DCT\ndomain, yielding filtered image features bearing different DCT spectral bands\nas different control signals to the pre-trained Latent Diffusion Model. We\nreveal that control signals of different DCT spectral bands bridge the source\nimage and the T2I generated image in different correlations (e.g., style,\nstructure, layout, contour, etc.), and thus enable versatile I2I applications\nemphasizing different I2I correlations, including style-guided content\ncreation, image semantic manipulation, image scene translation, and image style\ntranslation. Different from related approaches, FCDiffusion establishes a\nunified text-guided I2I framework suitable for diverse image translation tasks\nsimply by switching among different frequency control branches at inference\ntime. The effectiveness and superiority of our method for text-guided I2I are\ndemonstrated with extensive experiments both qualitatively and quantitatively.\nOur project is publicly available at:\nhttps://xianggao1102.github.io/FCDiffusion/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large-scale text-to-image (T2I) diffusion models have emerged as a\npowerful tool for image-to-image translation (I2I), allowing open-domain image\ntranslation via user-provided text prompts. This paper proposes\nfrequency-controlled diffusion model (FCDiffusion), an end-to-end\ndiffusion-based framework that contributes a novel solution to text-guided I2I\nfrom a frequency-domain perspective. At the heart of our framework is a\nfeature-space frequency-domain filtering module based on Discrete Cosine\nTransform, which filters the latent features of the source image in the DCT\ndomain, yielding filtered image features bearing different DCT spectral bands\nas different control signals to the pre-trained Latent Diffusion Model. We\nreveal that control signals of different DCT spectral bands bridge the source\nimage and the T2I generated image in different correlations (e.g., style,\nstructure, layout, contour, etc.), and thus enable versatile I2I applications\nemphasizing different I2I correlations, including style-guided content\ncreation, image semantic manipulation, image scene translation, and image style\ntranslation. Different from related approaches, FCDiffusion establishes a\nunified text-guided I2I framework suitable for diverse image translation tasks\nsimply by switching among different frequency control branches at inference\ntime. The effectiveness and superiority of our method for text-guided I2I are\ndemonstrated with extensive experiments both qualitatively and quantitatively.\nOur project is publicly available at:\nhttps://xianggao1102.github.io/FCDiffusion/."
                },
                "authors": [
                    {
                        "name": "Xiang Gao"
                    },
                    {
                        "name": "Zhengbo Xu"
                    },
                    {
                        "name": "Junhan Zhao"
                    },
                    {
                        "name": "Jiaying Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaying Liu"
                },
                "author": "Jiaying Liu",
                "arxiv_doi": "10.1609/aaai.v38i3.27951",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1609/aaai.v38i3.27951",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.03006v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03006v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Proceedings of the 38th AAAI Conference on Artificial Intelligence\n  (AAAI 2024)",
                "arxiv_journal_ref": "Proceedings of the AAAI Conference on Artificial Intelligence,\n  2024, 38(3), 1824-1832",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21670v1",
                "updated": "2025-03-27T16:36:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    36,
                    39,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T16:36:39Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    36,
                    39,
                    3,
                    86,
                    0
                ],
                "title": "COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in\n  Hindi-English Code-Mixing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in\n  Hindi-English Code-Mixing"
                },
                "summary": "The rapid growth of digital communication has driven the widespread use of\ncode-mixing, particularly Hindi-English, in multilingual communities. Existing\ndatasets often focus on romanized text, have limited scope, or rely on\nsynthetic data, which fails to capture realworld language nuances. Human\nannotations are crucial for assessing the naturalness and acceptability of\ncode-mixed text. To address these challenges, We introduce COMI-LINGUA, the\nlargest manually annotated dataset for code-mixed text, comprising 100,970\ninstances evaluated by three expert annotators in both Devanagari and Roman\nscripts. The dataset supports five fundamental NLP tasks: Language\nIdentification, Matrix Language Identification, Part-of-Speech Tagging, Named\nEntity Recognition, and Translation. We evaluate LLMs on these tasks using\nCOMILINGUA, revealing limitations in current multilingual modeling strategies\nand emphasizing the need for improved code-mixed text processing capabilities.\nCOMI-LINGUA is publically availabe at:\nhttps://huggingface.co/datasets/LingoIITGN/COMI-LINGUA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of digital communication has driven the widespread use of\ncode-mixing, particularly Hindi-English, in multilingual communities. Existing\ndatasets often focus on romanized text, have limited scope, or rely on\nsynthetic data, which fails to capture realworld language nuances. Human\nannotations are crucial for assessing the naturalness and acceptability of\ncode-mixed text. To address these challenges, We introduce COMI-LINGUA, the\nlargest manually annotated dataset for code-mixed text, comprising 100,970\ninstances evaluated by three expert annotators in both Devanagari and Roman\nscripts. The dataset supports five fundamental NLP tasks: Language\nIdentification, Matrix Language Identification, Part-of-Speech Tagging, Named\nEntity Recognition, and Translation. We evaluate LLMs on these tasks using\nCOMILINGUA, revealing limitations in current multilingual modeling strategies\nand emphasizing the need for improved code-mixed text processing capabilities.\nCOMI-LINGUA is publically availabe at:\nhttps://huggingface.co/datasets/LingoIITGN/COMI-LINGUA."
                },
                "authors": [
                    {
                        "name": "Rajvee Sheth"
                    },
                    {
                        "name": "Himanshu Beniwal"
                    },
                    {
                        "name": "Mayank Singh"
                    }
                ],
                "author_detail": {
                    "name": "Mayank Singh"
                },
                "author": "Mayank Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19105v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19105v2",
                "updated": "2025-03-27T16:29:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    29,
                    2,
                    3,
                    86,
                    0
                ],
                "published": "2024-11-28T12:39:50Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    39,
                    50,
                    3,
                    333,
                    0
                ],
                "title": "Mapping the Milky Way with Gaia Bp/Rp spectra I: Systematic flux\n  corrections and atmospheric parameters for 68 million stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping the Milky Way with Gaia Bp/Rp spectra I: Systematic flux\n  corrections and atmospheric parameters for 68 million stars"
                },
                "summary": "Gaia Bp/Rp spectra for over two hundred million stars have great potential\nfor mapping metallicity across the Milky Way. We aim to construct an\nalternative catalog of atmospheric parameters from Gaia Bp/Rp spectra by\nfitting them with synthetic spectra based on model atmospheres, and provide\ncorrections to the Bp/Rp fluxes according to stellar colors, magnitudes, and\nextinction. We use GaiaXPy to obtain calibrated spectra and apply FERRE to\nmatch the corrected Bp/Rp spectra with models and infer atmospheric parameters.\nWe train a neural network using stars in APOGEE to predict flux corrections as\na function of wavelength for each target. Based on the comparison with APOGEE\nparameters, we conclude that our estimated parameters have systematic errors\nand uncertainties in $T_{\\mathrm{eff}}$, $\\log g$, and [M/H] about $-38 \\pm\n167$ K, $0.05 \\pm 0.40$ dex, and $-0.12 \\pm 0.19$ dex, respectively, for stars\nin the range $4000 \\le T_{\\mathrm{eff}} \\le 7000$ K. The corrected Bp/Rp\nspectra show better agreement with both models and Hubble Space Telescope\nCALSPEC data. Our correction increases the precision of the relative\nspectrophotometry of the Bp/Rp data from $3.2\\% - 3.7\\%$ to $1.2\\% - 2.4\\%$.\nFinally, we have built a catalog of atmospheric parameters for stars within\n$4000 \\le T_{\\mathrm{eff}} \\le 7000$ K, comprising $68,394,431$ sources, along\nwith a subset of $124,188$ stars with $\\mathrm{[M/H]} \\le -2.5$. Our results\nconfirm that the Gaia Bp/Rp flux calibrated spectra show systematic patterns as\na function of wavelength that are tightly related to colors, magnitudes, and\nextinction. Our optimization algorithm can give us accurate atmospheric\nparameters of stars with a clear and direct link to models of stellar\natmospheres, and can be used to efficiently search for extremely metal-poor\nstars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaia Bp/Rp spectra for over two hundred million stars have great potential\nfor mapping metallicity across the Milky Way. We aim to construct an\nalternative catalog of atmospheric parameters from Gaia Bp/Rp spectra by\nfitting them with synthetic spectra based on model atmospheres, and provide\ncorrections to the Bp/Rp fluxes according to stellar colors, magnitudes, and\nextinction. We use GaiaXPy to obtain calibrated spectra and apply FERRE to\nmatch the corrected Bp/Rp spectra with models and infer atmospheric parameters.\nWe train a neural network using stars in APOGEE to predict flux corrections as\na function of wavelength for each target. Based on the comparison with APOGEE\nparameters, we conclude that our estimated parameters have systematic errors\nand uncertainties in $T_{\\mathrm{eff}}$, $\\log g$, and [M/H] about $-38 \\pm\n167$ K, $0.05 \\pm 0.40$ dex, and $-0.12 \\pm 0.19$ dex, respectively, for stars\nin the range $4000 \\le T_{\\mathrm{eff}} \\le 7000$ K. The corrected Bp/Rp\nspectra show better agreement with both models and Hubble Space Telescope\nCALSPEC data. Our correction increases the precision of the relative\nspectrophotometry of the Bp/Rp data from $3.2\\% - 3.7\\%$ to $1.2\\% - 2.4\\%$.\nFinally, we have built a catalog of atmospheric parameters for stars within\n$4000 \\le T_{\\mathrm{eff}} \\le 7000$ K, comprising $68,394,431$ sources, along\nwith a subset of $124,188$ stars with $\\mathrm{[M/H]} \\le -2.5$. Our results\nconfirm that the Gaia Bp/Rp flux calibrated spectra show systematic patterns as\na function of wavelength that are tightly related to colors, magnitudes, and\nextinction. Our optimization algorithm can give us accurate atmospheric\nparameters of stars with a clear and direct link to models of stellar\natmospheres, and can be used to efficiently search for extremely metal-poor\nstars."
                },
                "authors": [
                    {
                        "name": "Xianhao Ye"
                    },
                    {
                        "name": "Wenbo Wu"
                    },
                    {
                        "name": "Carlos Allende Prieto"
                    },
                    {
                        "name": "David S. Aguado"
                    },
                    {
                        "name": "Jingkun Zhao"
                    },
                    {
                        "name": "Jonay I. GonzÃ¡lez HernÃ¡ndez"
                    },
                    {
                        "name": "Rafael Rebolo"
                    },
                    {
                        "name": "Gang Zhao"
                    },
                    {
                        "name": "Zhuohan Li"
                    },
                    {
                        "name": "Carlos del Burgo"
                    },
                    {
                        "name": "Yuqin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yuqin Chen"
                },
                "author": "Yuqin Chen",
                "arxiv_doi": "10.1051/0004-6361/202452871",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202452871",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.19105v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19105v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "17 pages, 12 figures, revised version: incorporated referee comments,\n  minor revisions and corrected typos, published in A&A, catalogs and code in\n  https://doi.org/10.5281/zenodo.14028588",
                "arxiv_journal_ref": "A&A 695, A75 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16655v2",
                "updated": "2025-03-27T16:26:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    26,
                    55,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-20T19:12:32Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    19,
                    12,
                    32,
                    3,
                    79,
                    0
                ],
                "title": "Accelerating Antibiotic Discovery with Large Language Models and\n  Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Antibiotic Discovery with Large Language Models and\n  Knowledge Graphs"
                },
                "summary": "The discovery of novel antibiotics is critical to address the growing\nantimicrobial resistance (AMR). However, pharmaceutical industries face high\ncosts (over $1 billion), long timelines, and a high failure rate, worsened by\nthe rediscovery of known compounds. We propose an LLM-based pipeline that acts\nas an alarm system, detecting prior evidence of antibiotic activity to prevent\ncostly rediscoveries. The system integrates organism and chemical literature\ninto a Knowledge Graph (KG), ensuring taxonomic resolution, synonym handling,\nand multi-level evidence classification. We tested the pipeline on a private\nlist of 73 potential antibiotic-producing organisms, disclosing 12 negative\nhits for evaluation. The results highlight the effectiveness of the pipeline\nfor evidence reviewing, reducing false negatives, and accelerating\ndecision-making. The KG for negative hits and the user interface for\ninteractive exploration will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The discovery of novel antibiotics is critical to address the growing\nantimicrobial resistance (AMR). However, pharmaceutical industries face high\ncosts (over $1 billion), long timelines, and a high failure rate, worsened by\nthe rediscovery of known compounds. We propose an LLM-based pipeline that acts\nas an alarm system, detecting prior evidence of antibiotic activity to prevent\ncostly rediscoveries. The system integrates organism and chemical literature\ninto a Knowledge Graph (KG), ensuring taxonomic resolution, synonym handling,\nand multi-level evidence classification. We tested the pipeline on a private\nlist of 73 potential antibiotic-producing organisms, disclosing 12 negative\nhits for evaluation. The results highlight the effectiveness of the pipeline\nfor evidence reviewing, reducing false negatives, and accelerating\ndecision-making. The KG for negative hits and the user interface for\ninteractive exploration will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Maxime Delmas"
                    },
                    {
                        "name": "Magdalena Wysocka"
                    },
                    {
                        "name": "Danilo Gusicuma"
                    },
                    {
                        "name": "AndrÃ© Freitas"
                    }
                ],
                "author_detail": {
                    "name": "AndrÃ© Freitas"
                },
                "author": "AndrÃ© Freitas",
                "arxiv_comment": "11 pages, 9 figures, 3 tables fix: table, typos and error analysis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12257v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12257v3",
                "updated": "2025-03-27T16:21:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    21,
                    2,
                    3,
                    86,
                    0
                ],
                "published": "2024-06-18T04:10:38Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    4,
                    10,
                    38,
                    1,
                    170,
                    0
                ],
                "title": "CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large\n  Language Models"
                },
                "summary": "The remarkable performance of large language models (LLMs) in generation\ntasks has enabled practitioners to leverage publicly available models to power\ncustom applications, such as chatbots and virtual assistants. However, the data\nused to train or fine-tune these LLMs is often undisclosed, allowing an\nattacker to compromise the data and inject backdoors into the models. In this\npaper, we develop a novel inference time defense, named CLEANGEN, to mitigate\nbackdoor attacks for generation tasks in LLMs. CLEANGEN is a lightweight and\neffective decoding strategy that is compatible with the state-of-the-art (SOTA)\nLLMs. Our insight behind CLEANGEN is that compared to other LLMs, backdoored\nLLMs assign significantly higher probabilities to tokens representing the\nattacker-desired contents. These discrepancies in token probabilities enable\nCLEANGEN to identify suspicious tokens favored by the attacker and replace them\nwith tokens generated by another LLM that is not compromised by the same\nattacker, thereby avoiding generation of attacker-desired content. We evaluate\nCLEANGEN against five SOTA backdoor attacks. Our results show that CLEANGEN\nachieves lower attack success rates (ASR) compared to five SOTA baseline\ndefenses for all five backdoor attacks. Moreover, LLMs deploying CLEANGEN\nmaintain helpfulness in their responses when serving benign user queries with\nminimal added computational overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable performance of large language models (LLMs) in generation\ntasks has enabled practitioners to leverage publicly available models to power\ncustom applications, such as chatbots and virtual assistants. However, the data\nused to train or fine-tune these LLMs is often undisclosed, allowing an\nattacker to compromise the data and inject backdoors into the models. In this\npaper, we develop a novel inference time defense, named CLEANGEN, to mitigate\nbackdoor attacks for generation tasks in LLMs. CLEANGEN is a lightweight and\neffective decoding strategy that is compatible with the state-of-the-art (SOTA)\nLLMs. Our insight behind CLEANGEN is that compared to other LLMs, backdoored\nLLMs assign significantly higher probabilities to tokens representing the\nattacker-desired contents. These discrepancies in token probabilities enable\nCLEANGEN to identify suspicious tokens favored by the attacker and replace them\nwith tokens generated by another LLM that is not compromised by the same\nattacker, thereby avoiding generation of attacker-desired content. We evaluate\nCLEANGEN against five SOTA backdoor attacks. Our results show that CLEANGEN\nachieves lower attack success rates (ASR) compared to five SOTA baseline\ndefenses for all five backdoor attacks. Moreover, LLMs deploying CLEANGEN\nmaintain helpfulness in their responses when serving benign user queries with\nminimal added computational overhead."
                },
                "authors": [
                    {
                        "name": "Yuetai Li"
                    },
                    {
                        "name": "Zhangchen Xu"
                    },
                    {
                        "name": "Fengqing Jiang"
                    },
                    {
                        "name": "Luyao Niu"
                    },
                    {
                        "name": "Dinuka Sahabandu"
                    },
                    {
                        "name": "Bhaskar Ramasubramanian"
                    },
                    {
                        "name": "Radha Poovendran"
                    }
                ],
                "author_detail": {
                    "name": "Radha Poovendran"
                },
                "author": "Radha Poovendran",
                "arxiv_comment": "This paper is presented at EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12257v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12257v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08180v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08180v2",
                "updated": "2025-03-27T16:07:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    7,
                    18,
                    3,
                    86,
                    0
                ],
                "published": "2025-02-12T07:37:39Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    37,
                    39,
                    2,
                    43,
                    0
                ],
                "title": "Enhancing LLM Character-Level Manipulation via Divide and Conquer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Character-Level Manipulation via Divide and Conquer"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong generalization\ncapabilities across a wide range of natural language processing (NLP) tasks.\nHowever, they exhibit notable weaknesses in character-level string\nmanipulation, struggling with fundamental operations such as character\ndeletion, insertion, and substitution. These challenges stem primarily from\ntokenization constraints, despite the critical role of such operations in data\npreprocessing and code generation. Through systematic analysis, we derive two\nkey insights: (1) LLMs face significant difficulties in leveraging intrinsic\ntoken knowledge for character-level reasoning, and (2) atomized word structures\ncan substantially enhance LLMs' ability to process token-level structural\ninformation. Building on these insights, we propose Character-Level\nManipulation via Divide and Conquer, a novel approach designed to bridge the\ngap between token-level processing and character-level manipulation. Our method\ndecomposes complex operations into explicit character-level subtasks coupled\nwith controlled token reconstruction phases, leading to significant\nimprovements in accuracy. Without additional training, our method significantly\nimproves accuracies on the $\\texttt{Deletion}$, $\\texttt{Insertion}$, and\n$\\texttt{Substitution}$ tasks. To support further research, we open-source our\nimplementation and benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong generalization\ncapabilities across a wide range of natural language processing (NLP) tasks.\nHowever, they exhibit notable weaknesses in character-level string\nmanipulation, struggling with fundamental operations such as character\ndeletion, insertion, and substitution. These challenges stem primarily from\ntokenization constraints, despite the critical role of such operations in data\npreprocessing and code generation. Through systematic analysis, we derive two\nkey insights: (1) LLMs face significant difficulties in leveraging intrinsic\ntoken knowledge for character-level reasoning, and (2) atomized word structures\ncan substantially enhance LLMs' ability to process token-level structural\ninformation. Building on these insights, we propose Character-Level\nManipulation via Divide and Conquer, a novel approach designed to bridge the\ngap between token-level processing and character-level manipulation. Our method\ndecomposes complex operations into explicit character-level subtasks coupled\nwith controlled token reconstruction phases, leading to significant\nimprovements in accuracy. Without additional training, our method significantly\nimproves accuracies on the $\\texttt{Deletion}$, $\\texttt{Insertion}$, and\n$\\texttt{Substitution}$ tasks. To support further research, we open-source our\nimplementation and benchmarks."
                },
                "authors": [
                    {
                        "name": "Zhen Xiong"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Zhecheng Li"
                    },
                    {
                        "name": "Yiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yiwei Wang"
                },
                "author": "Yiwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08180v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08180v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21639v1",
                "updated": "2025-03-27T16:06:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    6,
                    7,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T16:06:07Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    6,
                    7,
                    3,
                    86,
                    0
                ],
                "title": "Locally minimax optimal and dimension-agnostic discrete argmin inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locally minimax optimal and dimension-agnostic discrete argmin inference"
                },
                "summary": "We revisit the discrete argmin inference problem in high-dimensional\nsettings. Given $n$ observations from a $d$ dimensional vector, the goal is to\ntest whether the $r$th component of the mean vector is the smallest among all\ncomponents. We propose dimension-agnostic tests that maintain validity\nregardless of how $d$ scales with $n$, and regardless of arbitrary ties in the\nmean vector. Notably, our validity holds under mild moment conditions,\nrequiring little more than finiteness of a second moment, and permitting\npossibly strong dependence between coordinates. In addition, we establish the\nlocal minimax separation rate for this problem, which adapts to the cardinality\nof a confusion set, and show that the proposed tests attain this rate. Our\nmethod uses the sample splitting and self-normalization approach of Kim and\nRamdas (2024). Our tests can be easily inverted to yield confidence sets for\nthe argmin index. Empirical results illustrate the strong performance of our\napproach in terms of type I error control and power compared to existing\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We revisit the discrete argmin inference problem in high-dimensional\nsettings. Given $n$ observations from a $d$ dimensional vector, the goal is to\ntest whether the $r$th component of the mean vector is the smallest among all\ncomponents. We propose dimension-agnostic tests that maintain validity\nregardless of how $d$ scales with $n$, and regardless of arbitrary ties in the\nmean vector. Notably, our validity holds under mild moment conditions,\nrequiring little more than finiteness of a second moment, and permitting\npossibly strong dependence between coordinates. In addition, we establish the\nlocal minimax separation rate for this problem, which adapts to the cardinality\nof a confusion set, and show that the proposed tests attain this rate. Our\nmethod uses the sample splitting and self-normalization approach of Kim and\nRamdas (2024). Our tests can be easily inverted to yield confidence sets for\nthe argmin index. Empirical results illustrate the strong performance of our\napproach in terms of type I error control and power compared to existing\nmethods."
                },
                "authors": [
                    {
                        "name": "Ilmun Kim"
                    },
                    {
                        "name": "Aaditya Ramdas"
                    }
                ],
                "author_detail": {
                    "name": "Aaditya Ramdas"
                },
                "author": "Aaditya Ramdas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.14203v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.14203v5",
                "updated": "2025-03-27T16:00:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    0,
                    13,
                    3,
                    86,
                    0
                ],
                "published": "2023-07-26T14:02:24Z",
                "published_parsed": [
                    2023,
                    7,
                    26,
                    14,
                    2,
                    24,
                    2,
                    207,
                    0
                ],
                "title": "Dynamic Regression Discontinuity: An Event-Study Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Regression Discontinuity: An Event-Study Approach"
                },
                "summary": "I propose a novel argument to identify economically interpretable\nintertemporal treatment effects in dynamic regression discontinuity designs\n(RDDs). Specifically, I develop a dynamic potential outcomes model and\nreformulate two assumptions from the difference-in-differences literature, no\nanticipation and common trends, to attain point identification of\ncutoff-specific impulse responses. The estimand of each target parameter can be\nexpressed as the sum of two static RDD contrasts, thereby allowing for\nnonparametric estimation and inference with standard local polynomial methods.\nI also propose a nonparametric approach to aggregate treatment effects across\ncalendar time and treatment paths, leveraging a limited path independence\nrestriction to reduce the dimensionality of the parameter space. I apply this\nmethod to estimate the dynamic effects of school district expenditure\nauthorizations on housing prices in Wisconsin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I propose a novel argument to identify economically interpretable\nintertemporal treatment effects in dynamic regression discontinuity designs\n(RDDs). Specifically, I develop a dynamic potential outcomes model and\nreformulate two assumptions from the difference-in-differences literature, no\nanticipation and common trends, to attain point identification of\ncutoff-specific impulse responses. The estimand of each target parameter can be\nexpressed as the sum of two static RDD contrasts, thereby allowing for\nnonparametric estimation and inference with standard local polynomial methods.\nI also propose a nonparametric approach to aggregate treatment effects across\ncalendar time and treatment paths, leveraging a limited path independence\nrestriction to reduce the dimensionality of the parameter space. I apply this\nmethod to estimate the dynamic effects of school district expenditure\nauthorizations on housing prices in Wisconsin."
                },
                "authors": [
                    {
                        "name": "Francesco Ruggieri"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Ruggieri"
                },
                "author": "Francesco Ruggieri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.14203v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.14203v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11593v2",
                "updated": "2025-03-27T15:57:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    57,
                    57,
                    3,
                    86,
                    0
                ],
                "published": "2024-09-17T22:58:20Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    22,
                    58,
                    20,
                    1,
                    261,
                    0
                ],
                "title": "Self-Contrastive Forward-Forward Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Contrastive Forward-Forward Algorithm"
                },
                "summary": "Agents that operate autonomously benefit from lifelong learning capabilities.\nHowever, compatible training algorithms must comply with the decentralized\nnature of these systems, which imposes constraints on both the parameter counts\nand the computational resources. The Forward-Forward (FF) algorithm is one of\nthese. FF relies only on feedforward operations, the same used for inference,\nfor optimizing layer-wise objectives. This purely forward approach eliminates\nthe need for transpose operations required in traditional backpropagation.\nDespite its potential, FF has failed to reach state-of-the-art performance on\nmost standard benchmark tasks, in part due to unreliable negative data\ngeneration methods for unsupervised learning.\n  In this work, we propose the Self-Contrastive Forward-Forward (SCFF)\nalgorithm, a competitive training method aimed at closing this performance gap.\nInspired by standard self-supervised contrastive learning for vision tasks,\nSCFF generates positive and negative inputs applicable across various datasets.\nThe method demonstrates superior performance compared to existing unsupervised\nlocal learning algorithms on several benchmark datasets, including MNIST,\nCIFAR-10, STL-10, and Tiny ImageNet. We extend FF's application to training\nrecurrent neural networks, expanding its utility to sequential data tasks.\nThese findings pave the way for high-accuracy, real-time learning on\nresource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents that operate autonomously benefit from lifelong learning capabilities.\nHowever, compatible training algorithms must comply with the decentralized\nnature of these systems, which imposes constraints on both the parameter counts\nand the computational resources. The Forward-Forward (FF) algorithm is one of\nthese. FF relies only on feedforward operations, the same used for inference,\nfor optimizing layer-wise objectives. This purely forward approach eliminates\nthe need for transpose operations required in traditional backpropagation.\nDespite its potential, FF has failed to reach state-of-the-art performance on\nmost standard benchmark tasks, in part due to unreliable negative data\ngeneration methods for unsupervised learning.\n  In this work, we propose the Self-Contrastive Forward-Forward (SCFF)\nalgorithm, a competitive training method aimed at closing this performance gap.\nInspired by standard self-supervised contrastive learning for vision tasks,\nSCFF generates positive and negative inputs applicable across various datasets.\nThe method demonstrates superior performance compared to existing unsupervised\nlocal learning algorithms on several benchmark datasets, including MNIST,\nCIFAR-10, STL-10, and Tiny ImageNet. We extend FF's application to training\nrecurrent neural networks, expanding its utility to sequential data tasks.\nThese findings pave the way for high-accuracy, real-time learning on\nresource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Xing Chen"
                    },
                    {
                        "name": "Dongshu Liu"
                    },
                    {
                        "name": "Jeremie Laydevant"
                    },
                    {
                        "name": "Julie Grollier"
                    }
                ],
                "author_detail": {
                    "name": "Julie Grollier"
                },
                "author": "Julie Grollier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21629v1",
                "updated": "2025-03-27T15:50:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    50,
                    32,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T15:50:32Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    50,
                    32,
                    3,
                    86,
                    0
                ],
                "title": "ClusterSC: Advancing Synthetic Control with Donor Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterSC: Advancing Synthetic Control with Donor Selection"
                },
                "summary": "In causal inference with observational studies, synthetic control (SC) has\nemerged as a prominent tool. SC has traditionally been applied to\naggregate-level datasets, but more recent work has extended its use to\nindividual-level data. As they contain a greater number of observed units, this\nshift introduces the curse of dimensionality to SC. To address this, we propose\nCluster Synthetic Control (ClusterSC), based on the idea that groups of\nindividuals may exist where behavior aligns internally but diverges between\ngroups. ClusterSC incorporates a clustering step to select only the relevant\ndonors for the target. We provide theoretical guarantees on the improvements\ninduced by ClusterSC, supported by empirical demonstrations on synthetic and\nreal-world datasets. The results indicate that ClusterSC consistently\noutperforms classical SC approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In causal inference with observational studies, synthetic control (SC) has\nemerged as a prominent tool. SC has traditionally been applied to\naggregate-level datasets, but more recent work has extended its use to\nindividual-level data. As they contain a greater number of observed units, this\nshift introduces the curse of dimensionality to SC. To address this, we propose\nCluster Synthetic Control (ClusterSC), based on the idea that groups of\nindividuals may exist where behavior aligns internally but diverges between\ngroups. ClusterSC incorporates a clustering step to select only the relevant\ndonors for the target. We provide theoretical guarantees on the improvements\ninduced by ClusterSC, supported by empirical demonstrations on synthetic and\nreal-world datasets. The results indicate that ClusterSC consistently\noutperforms classical SC approaches."
                },
                "authors": [
                    {
                        "name": "Saeyoung Rho"
                    },
                    {
                        "name": "Andrew Tang"
                    },
                    {
                        "name": "Noah Bergam"
                    },
                    {
                        "name": "Rachel Cummings"
                    },
                    {
                        "name": "Vishal Misra"
                    }
                ],
                "author_detail": {
                    "name": "Vishal Misra"
                },
                "author": "Vishal Misra",
                "arxiv_comment": "35 pages, 11 figures, to be published in Proceedings of The 28th\n  International Conference on Artificial Intelligence and Statistics (AIStats)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16822v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16822v2",
                "updated": "2025-03-27T15:42:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    42,
                    18,
                    3,
                    86,
                    0
                ],
                "published": "2024-12-22T02:04:17Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    2,
                    4,
                    17,
                    6,
                    357,
                    0
                ],
                "title": "Layer- and Timestep-Adaptive Differentiable Token Compression Ratios for\n  Efficient Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layer- and Timestep-Adaptive Differentiable Token Compression Ratios for\n  Efficient Diffusion Transformers"
                },
                "summary": "Diffusion Transformers (DiTs) have achieved state-of-the-art (SOTA) image\ngeneration quality but suffer from high latency and memory inefficiency, making\nthem difficult to deploy on resource-constrained devices. One major efficiency\nbottleneck is that existing DiTs apply equal computation across all regions of\nan image. However, not all image tokens are equally important, and certain\nlocalized areas require more computation, such as objects. To address this, we\npropose DiffCR, a dynamic DiT inference framework with differentiable\ncompression ratios, which automatically learns to dynamically route computation\nacross layers and timesteps for each image token, resulting in efficient DiTs.\nSpecifically, DiffCR integrates three features: (1) A token-level routing\nscheme where each DiT layer includes a router that is fine-tuned jointly with\nmodel weights to predict token importance scores. In this way, unimportant\ntokens bypass the entire layer's computation; (2) A layer-wise differentiable\nratio mechanism where different DiT layers automatically learn varying\ncompression ratios from a zero initialization, resulting in large compression\nratios in redundant layers while others remain less compressed or even\nuncompressed; (3) A timestep-wise differentiable ratio mechanism where each\ndenoising timestep learns its own compression ratio. The resulting pattern\nshows higher ratios for noisier timesteps and lower ratios as the image becomes\nclearer. Extensive experiments on text-to-image and inpainting tasks show that\nDiffCR effectively captures dynamism across token, layer, and timestep axes,\nachieving superior trade-offs between generation quality and efficiency\ncompared to prior works. The project website is available at\nhttps://www.haoranyou.com/diffcr.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have achieved state-of-the-art (SOTA) image\ngeneration quality but suffer from high latency and memory inefficiency, making\nthem difficult to deploy on resource-constrained devices. One major efficiency\nbottleneck is that existing DiTs apply equal computation across all regions of\nan image. However, not all image tokens are equally important, and certain\nlocalized areas require more computation, such as objects. To address this, we\npropose DiffCR, a dynamic DiT inference framework with differentiable\ncompression ratios, which automatically learns to dynamically route computation\nacross layers and timesteps for each image token, resulting in efficient DiTs.\nSpecifically, DiffCR integrates three features: (1) A token-level routing\nscheme where each DiT layer includes a router that is fine-tuned jointly with\nmodel weights to predict token importance scores. In this way, unimportant\ntokens bypass the entire layer's computation; (2) A layer-wise differentiable\nratio mechanism where different DiT layers automatically learn varying\ncompression ratios from a zero initialization, resulting in large compression\nratios in redundant layers while others remain less compressed or even\nuncompressed; (3) A timestep-wise differentiable ratio mechanism where each\ndenoising timestep learns its own compression ratio. The resulting pattern\nshows higher ratios for noisier timesteps and lower ratios as the image becomes\nclearer. Extensive experiments on text-to-image and inpainting tasks show that\nDiffCR effectively captures dynamism across token, layer, and timestep axes,\nachieving superior trade-offs between generation quality and efficiency\ncompared to prior works. The project website is available at\nhttps://www.haoranyou.com/diffcr."
                },
                "authors": [
                    {
                        "name": "Haoran You"
                    },
                    {
                        "name": "Connelly Barnes"
                    },
                    {
                        "name": "Yuqian Zhou"
                    },
                    {
                        "name": "Yan Kang"
                    },
                    {
                        "name": "Zhenbang Du"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Lingzhi Zhang"
                    },
                    {
                        "name": "Yotam Nitzan"
                    },
                    {
                        "name": "Xiaoyang Liu"
                    },
                    {
                        "name": "Zhe Lin"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Sohrab Amirghodsi"
                    },
                    {
                        "name": "Yingyan Celine Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yingyan Celine Lin"
                },
                "author": "Yingyan Celine Lin",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16822v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16822v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12802v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12802v2",
                "updated": "2025-03-27T15:40:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    40,
                    0,
                    3,
                    86,
                    0
                ],
                "published": "2024-05-21T13:53:58Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    13,
                    53,
                    58,
                    1,
                    142,
                    0
                ],
                "title": "Stochastic Inference of Plate Bending from Heterogeneous Data:\n  Physics-informed Gaussian Processes via Kirchhoff-Love Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Inference of Plate Bending from Heterogeneous Data:\n  Physics-informed Gaussian Processes via Kirchhoff-Love Theory"
                },
                "summary": "Advancements in machine learning and an abundance of structural monitoring\ndata have inspired the integration of mechanical models with probabilistic\nmodels to identify a structure's state and quantify the uncertainty of its\nphysical parameters and response. In this paper, we propose an inference\nmethodology for classical Kirchhoff-Love plates via physics-informed Gaussian\nProcesses (GP). A probabilistic model is formulated as a multi-output GP by\nplacing a GP prior on the deflection and deriving the covariance function using\nthe linear differential operators of the plate governing equations. The\nposteriors of the flexural rigidity, hyperparameters, and plate response are\ninferred in a Bayesian manner using Markov chain Monte Carlo (MCMC) sampling\nfrom noisy measurements. We demonstrate the applicability with two examples: a\nsimply supported plate subjected to a sinusoidal load and a fixed plate\nsubjected to a uniform load. The results illustrate how the proposed\nmethodology can be employed to perform stochastic inference for plate rigidity\nand physical quantities by integrating measurements from various sensor types\nand qualities. Potential applications of the presented methodology are in\nstructural health monitoring and uncertainty quantification of plate-like\nstructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in machine learning and an abundance of structural monitoring\ndata have inspired the integration of mechanical models with probabilistic\nmodels to identify a structure's state and quantify the uncertainty of its\nphysical parameters and response. In this paper, we propose an inference\nmethodology for classical Kirchhoff-Love plates via physics-informed Gaussian\nProcesses (GP). A probabilistic model is formulated as a multi-output GP by\nplacing a GP prior on the deflection and deriving the covariance function using\nthe linear differential operators of the plate governing equations. The\nposteriors of the flexural rigidity, hyperparameters, and plate response are\ninferred in a Bayesian manner using Markov chain Monte Carlo (MCMC) sampling\nfrom noisy measurements. We demonstrate the applicability with two examples: a\nsimply supported plate subjected to a sinusoidal load and a fixed plate\nsubjected to a uniform load. The results illustrate how the proposed\nmethodology can be employed to perform stochastic inference for plate rigidity\nand physical quantities by integrating measurements from various sensor types\nand qualities. Potential applications of the presented methodology are in\nstructural health monitoring and uncertainty quantification of plate-like\nstructures."
                },
                "authors": [
                    {
                        "name": "Igor Kavrakov"
                    },
                    {
                        "name": "Gledson Rodrigo Tondo"
                    },
                    {
                        "name": "Guido Morgenthal"
                    }
                ],
                "author_detail": {
                    "name": "Guido Morgenthal"
                },
                "author": "Guido Morgenthal",
                "arxiv_doi": "10.1061/JENMDT.EMENG-7558",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1061/JENMDT.EMENG-7558",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.12802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12802v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 11 figures",
                "arxiv_journal_ref": "ASCE J. Eng. Mech. 151(4) (2025) 04025005",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21620v1",
                "updated": "2025-03-27T15:39:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    39,
                    30,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T15:39:30Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    39,
                    30,
                    3,
                    86,
                    0
                ],
                "title": "UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement\n  Learning"
                },
                "summary": "The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities\nin LLMs through reinforcement learning (RL) with rule-based rewards. Building\non this idea, we are the first to explore how rule-based RL can enhance the\nreasoning capabilities of multimodal large language models (MLLMs) for graphic\nuser interface (GUI) action prediction tasks. To this end, we curate a small\nyet high-quality dataset of 136 challenging tasks, encompassing five common\naction types on mobile devices. We also introduce a unified rule-based action\nreward, enabling model optimization via policy-based algorithms such as Group\nRelative Policy Optimization (GRPO). Experimental results demonstrate that our\nproposed data-efficient model, UI-R1-3B, achieves substantial improvements on\nboth in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the ID\nbenchmark AndroidControl, the action type accuracy improves by 15%, while\ngrounding accuracy increases by 10.3%, compared with the base model (i.e.\nQwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our model\nsurpasses the base model by 6.0% and achieves competitive performance with\nlarger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning\n(SFT) on 76K data. These results underscore the potential of rule-based\nreinforcement learning to advance GUI understanding and control, paving the way\nfor future research in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities\nin LLMs through reinforcement learning (RL) with rule-based rewards. Building\non this idea, we are the first to explore how rule-based RL can enhance the\nreasoning capabilities of multimodal large language models (MLLMs) for graphic\nuser interface (GUI) action prediction tasks. To this end, we curate a small\nyet high-quality dataset of 136 challenging tasks, encompassing five common\naction types on mobile devices. We also introduce a unified rule-based action\nreward, enabling model optimization via policy-based algorithms such as Group\nRelative Policy Optimization (GRPO). Experimental results demonstrate that our\nproposed data-efficient model, UI-R1-3B, achieves substantial improvements on\nboth in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the ID\nbenchmark AndroidControl, the action type accuracy improves by 15%, while\ngrounding accuracy increases by 10.3%, compared with the base model (i.e.\nQwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our model\nsurpasses the base model by 6.0% and achieves competitive performance with\nlarger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning\n(SFT) on 76K data. These results underscore the potential of rule-based\nreinforcement learning to advance GUI understanding and control, paving the way\nfor future research in this domain."
                },
                "authors": [
                    {
                        "name": "Zhengxi Lu"
                    },
                    {
                        "name": "Yuxiang Chai"
                    },
                    {
                        "name": "Yaxuan Guo"
                    },
                    {
                        "name": "Xi Yin"
                    },
                    {
                        "name": "Liang Liu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Guanjing Xiong"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21615v1",
                "updated": "2025-03-27T15:36:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    36,
                    49,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T15:36:49Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    36,
                    49,
                    3,
                    86,
                    0
                ],
                "title": "A Measure Based Generalizable Approach to Understandability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Measure Based Generalizable Approach to Understandability"
                },
                "summary": "Successful agent-human partnerships require that any agent generated\ninformation is understandable to the human, and that the human can easily steer\nthe agent towards a goal. Such effective communication requires the agent to\ndevelop a finer-level notion of what is understandable to the human.\nState-of-the-art agents, including LLMs, lack this detailed notion of\nunderstandability because they only capture average human sensibilities from\nthe training data, and therefore afford limited steerability (e.g., requiring\nnon-trivial prompt engineering).\n  In this paper, instead of only relying on data, we argue for developing\ngeneralizable, domain-agnostic measures of understandability that can be used\nas directives for these agents. Existing research on understandability measures\nis fragmented, we survey various such efforts across domains, and lay a\ncognitive-science-rooted groundwork for more coherent and domain-agnostic\nresearch investigations in future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Successful agent-human partnerships require that any agent generated\ninformation is understandable to the human, and that the human can easily steer\nthe agent towards a goal. Such effective communication requires the agent to\ndevelop a finer-level notion of what is understandable to the human.\nState-of-the-art agents, including LLMs, lack this detailed notion of\nunderstandability because they only capture average human sensibilities from\nthe training data, and therefore afford limited steerability (e.g., requiring\nnon-trivial prompt engineering).\n  In this paper, instead of only relying on data, we argue for developing\ngeneralizable, domain-agnostic measures of understandability that can be used\nas directives for these agents. Existing research on understandability measures\nis fragmented, we survey various such efforts across domains, and lay a\ncognitive-science-rooted groundwork for more coherent and domain-agnostic\nresearch investigations in future."
                },
                "authors": [
                    {
                        "name": "Vikas Kushwaha"
                    },
                    {
                        "name": "Sruti Srinivasa Ragavan"
                    },
                    {
                        "name": "Subhajit Roy"
                    }
                ],
                "author_detail": {
                    "name": "Subhajit Roy"
                },
                "author": "Subhajit Roy",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21614v1",
                "updated": "2025-03-27T15:36:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    36,
                    30,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T15:36:30Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    36,
                    30,
                    3,
                    86,
                    0
                ],
                "title": "A Survey of Efficient Reasoning for Large Reasoning Models: Language,\n  Multimodality, and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Efficient Reasoning for Large Reasoning Models: Language,\n  Multimodality, and Beyond"
                },
                "summary": "Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have\ndemonstrated strong performance gains by scaling up the length of\nChain-of-Thought (CoT) reasoning during inference. However, a growing concern\nlies in their tendency to produce excessively long reasoning traces, which are\noften filled with redundant content (e.g., repeated definitions), over-analysis\nof simple problems, and superficial exploration of multiple reasoning paths for\nharder tasks. This inefficiency introduces significant challenges for training,\ninference, and real-world deployment (e.g., in agent-based systems), where\ntoken economy is critical. In this survey, we provide a comprehensive overview\nof recent efforts aimed at improving reasoning efficiency in LRMs, with a\nparticular focus on the unique challenges that arise in this new paradigm. We\nidentify common patterns of inefficiency, examine methods proposed across the\nLRM lifecycle, i.e., from pretraining to inference, and discuss promising\nfuture directions for research. To support ongoing development, we also\nmaintain a real-time GitHub repository tracking recent progress in the field.\nWe hope this survey serves as a foundation for further exploration and inspires\ninnovation in this rapidly evolving area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have\ndemonstrated strong performance gains by scaling up the length of\nChain-of-Thought (CoT) reasoning during inference. However, a growing concern\nlies in their tendency to produce excessively long reasoning traces, which are\noften filled with redundant content (e.g., repeated definitions), over-analysis\nof simple problems, and superficial exploration of multiple reasoning paths for\nharder tasks. This inefficiency introduces significant challenges for training,\ninference, and real-world deployment (e.g., in agent-based systems), where\ntoken economy is critical. In this survey, we provide a comprehensive overview\nof recent efforts aimed at improving reasoning efficiency in LRMs, with a\nparticular focus on the unique challenges that arise in this new paradigm. We\nidentify common patterns of inefficiency, examine methods proposed across the\nLRM lifecycle, i.e., from pretraining to inference, and discuss promising\nfuture directions for research. To support ongoing development, we also\nmaintain a real-time GitHub repository tracking recent progress in the field.\nWe hope this survey serves as a foundation for further exploration and inspires\ninnovation in this rapidly evolving area."
                },
                "authors": [
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Yafu Li"
                    },
                    {
                        "name": "Zhaochen Su"
                    },
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Jianhao Yan"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Daizong Liu"
                    },
                    {
                        "name": "Shuxian Liang"
                    },
                    {
                        "name": "Junxian He"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Jing Shao"
                    },
                    {
                        "name": "Chaochao Lu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Xian-Sheng Hua"
                    },
                    {
                        "name": "Bowen Zhou"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "Survey, 32 pages, Large Reasoning Models, Efficient Reasoning for\n  Language, Multimodality, and Beyond",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21613v1",
                "updated": "2025-03-27T15:36:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    36,
                    24,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T15:36:24Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    36,
                    24,
                    3,
                    86,
                    0
                ],
                "title": "Evaluating book summaries from internal knowledge in Large Language\n  Models: a cross-model and semantic consistency approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating book summaries from internal knowledge in Large Language\n  Models: a cross-model and semantic consistency approach"
                },
                "summary": "We study the ability of large language models (LLMs) to generate\ncomprehensive and accurate book summaries solely from their internal knowledge,\nwithout recourse to the original text. Employing a diverse set of books and\nmultiple LLM architectures, we examine whether these models can synthesize\nmeaningful narratives that align with established human interpretations.\nEvaluation is performed with a LLM-as-a-judge paradigm: each AI-generated\nsummary is compared against a high-quality, human-written summary via a\ncross-model assessment, where all participating LLMs evaluate not only their\nown outputs but also those produced by others. This methodology enables the\nidentification of potential biases, such as the proclivity for models to favor\ntheir own summarization style over others. In addition, alignment between the\nhuman-crafted and LLM-generated summaries is quantified using ROUGE and\nBERTScore metrics, assessing the depth of grammatical and semantic\ncorrespondence. The results reveal nuanced variations in content representation\nand stylistic preferences among the models, highlighting both strengths and\nlimitations inherent in relying on internal knowledge for summarization tasks.\nThese findings contribute to a deeper understanding of LLM internal encodings\nof factual information and the dynamics of cross-model evaluation, with\nimplications for the development of more robust natural language generative\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the ability of large language models (LLMs) to generate\ncomprehensive and accurate book summaries solely from their internal knowledge,\nwithout recourse to the original text. Employing a diverse set of books and\nmultiple LLM architectures, we examine whether these models can synthesize\nmeaningful narratives that align with established human interpretations.\nEvaluation is performed with a LLM-as-a-judge paradigm: each AI-generated\nsummary is compared against a high-quality, human-written summary via a\ncross-model assessment, where all participating LLMs evaluate not only their\nown outputs but also those produced by others. This methodology enables the\nidentification of potential biases, such as the proclivity for models to favor\ntheir own summarization style over others. In addition, alignment between the\nhuman-crafted and LLM-generated summaries is quantified using ROUGE and\nBERTScore metrics, assessing the depth of grammatical and semantic\ncorrespondence. The results reveal nuanced variations in content representation\nand stylistic preferences among the models, highlighting both strengths and\nlimitations inherent in relying on internal knowledge for summarization tasks.\nThese findings contribute to a deeper understanding of LLM internal encodings\nof factual information and the dynamics of cross-model evaluation, with\nimplications for the development of more robust natural language generative\nsystems."
                },
                "authors": [
                    {
                        "name": "Javier Coronado-BlÃ¡zquez"
                    }
                ],
                "author_detail": {
                    "name": "Javier Coronado-BlÃ¡zquez"
                },
                "author": "Javier Coronado-BlÃ¡zquez",
                "arxiv_comment": "22 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01672v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01672v4",
                "updated": "2025-03-27T15:24:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    24,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2024-10-02T15:41:22Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    41,
                    22,
                    2,
                    276,
                    0
                ],
                "title": "Practicing Stress Relief for the Everyday: Designing Social Simulation\n  Using VR, AR, and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practicing Stress Relief for the Everyday: Designing Social Simulation\n  Using VR, AR, and LLMs"
                },
                "summary": "Stress is an inevitable part of day-to-day life yet many find themselves\nunable to manage it themselves, particularly when professional or peer support\nare not always readily available. As self-care becomes increasingly vital for\nmental well-being, this paper explores the potential of social simulation as a\nsafe, virtual environment for practicing stress relief for everyday situations.\nLeveraging the immersive capabilities of VR, AR, and LLMs, we developed eight\ninteractive prototypes for various everyday stressful scenarios (e.g. public\nspeaking) then conducted prototype-driven semi-structured interviews with 19\nparticipants. We reveal that people currently lack effective means to support\nthemselves through everyday stress and found that social simulation fills a gap\nfor simulating real environments for training mental health practices. We\noutline key considerations for future development of simulation for self-care,\nincluding risks of trauma from hyper-realism, distrust of LLM-recommended\ntiming for mental health recommendations, and the value of accessibility for\nself-care interventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stress is an inevitable part of day-to-day life yet many find themselves\nunable to manage it themselves, particularly when professional or peer support\nare not always readily available. As self-care becomes increasingly vital for\nmental well-being, this paper explores the potential of social simulation as a\nsafe, virtual environment for practicing stress relief for everyday situations.\nLeveraging the immersive capabilities of VR, AR, and LLMs, we developed eight\ninteractive prototypes for various everyday stressful scenarios (e.g. public\nspeaking) then conducted prototype-driven semi-structured interviews with 19\nparticipants. We reveal that people currently lack effective means to support\nthemselves through everyday stress and found that social simulation fills a gap\nfor simulating real environments for training mental health practices. We\noutline key considerations for future development of simulation for self-care,\nincluding risks of trauma from hyper-realism, distrust of LLM-recommended\ntiming for mental health recommendations, and the value of accessibility for\nself-care interventions."
                },
                "authors": [
                    {
                        "name": "Anna Fang"
                    },
                    {
                        "name": "Hriday Chhabria"
                    },
                    {
                        "name": "Alekhya Maram"
                    },
                    {
                        "name": "Haiyi Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Haiyi Zhu"
                },
                "author": "Haiyi Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01672v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01672v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21598v1",
                "updated": "2025-03-27T15:19:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    19,
                    55,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T15:19:55Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    19,
                    55,
                    3,
                    86,
                    0
                ],
                "title": "Prompt, Divide, and Conquer: Bypassing Large Language Model Safety\n  Filters via Segmented and Distributed Prompt Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt, Divide, and Conquer: Bypassing Large Language Model Safety\n  Filters via Segmented and Distributed Prompt Processing"
                },
                "summary": "Large Language Models (LLMs) have transformed task automation and content\ngeneration across various domains while incorporating safety filters to prevent\nmisuse. We introduce a novel jailbreaking framework that employs distributed\nprompt processing combined with iterative refinements to bypass these safety\nmeasures, particularly in generating malicious code. Our architecture consists\nof four key modules: prompt segmentation, parallel processing, response\naggregation, and LLM-based jury evaluation. Tested on 500 malicious prompts\nacross 10 cybersecurity categories, the framework achieves a 73.2% Success Rate\n(SR) in generating malicious code. Notably, our comparative analysis reveals\nthat traditional single-LLM judge evaluation overestimates SRs (93.8%) compared\nto our LLM jury system (73.2%), with manual verification confirming that\nsingle-judge assessments often accept incomplete implementations. Moreover, we\ndemonstrate that our distributed architecture improves SRs by 12% over the\nnon-distributed approach in an ablation study, highlighting both the\neffectiveness of distributed prompt processing and the importance of robust\nevaluation methodologies in assessing jailbreak attempts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed task automation and content\ngeneration across various domains while incorporating safety filters to prevent\nmisuse. We introduce a novel jailbreaking framework that employs distributed\nprompt processing combined with iterative refinements to bypass these safety\nmeasures, particularly in generating malicious code. Our architecture consists\nof four key modules: prompt segmentation, parallel processing, response\naggregation, and LLM-based jury evaluation. Tested on 500 malicious prompts\nacross 10 cybersecurity categories, the framework achieves a 73.2% Success Rate\n(SR) in generating malicious code. Notably, our comparative analysis reveals\nthat traditional single-LLM judge evaluation overestimates SRs (93.8%) compared\nto our LLM jury system (73.2%), with manual verification confirming that\nsingle-judge assessments often accept incomplete implementations. Moreover, we\ndemonstrate that our distributed architecture improves SRs by 12% over the\nnon-distributed approach in an ablation study, highlighting both the\neffectiveness of distributed prompt processing and the importance of robust\nevaluation methodologies in assessing jailbreak attempts."
                },
                "authors": [
                    {
                        "name": "Johan WahrÃ©us"
                    },
                    {
                        "name": "Ahmed Hussain"
                    },
                    {
                        "name": "Panos Papadimitratos"
                    }
                ],
                "author_detail": {
                    "name": "Panos Papadimitratos"
                },
                "author": "Panos Papadimitratos",
                "arxiv_comment": "22 pages; 26 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16400v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16400v2",
                "updated": "2025-03-27T15:12:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    12,
                    43,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-20T17:54:37Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    54,
                    37,
                    3,
                    79,
                    0
                ],
                "title": "ScalingNoise: Scaling Inference-Time Search for Generating Infinite\n  Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScalingNoise: Scaling Inference-Time Search for Generating Infinite\n  Videos"
                },
                "summary": "Video diffusion models (VDMs) facilitate the generation of high-quality\nvideos, with current research predominantly concentrated on scaling efforts\nduring training through improvements in data quality, computational resources,\nand model complexity. However, inference-time scaling has received less\nattention, with most approaches restricting models to a single generation\nattempt. Recent studies have uncovered the existence of \"golden noises\" that\ncan enhance video quality during generation. Building on this, we find that\nguiding the scaling inference-time search of VDMs to identify better noise\ncandidates not only evaluates the quality of the frames generated in the\ncurrent step but also preserves the high-level object features by referencing\nthe anchor frame from previous multi-chunks, thereby delivering long-term\nvalue. Our analysis reveals that diffusion models inherently possess flexible\nadjustments of computation by varying denoising steps, and even a one-step\ndenoising approach, when guided by a reward signal, yields significant\nlong-term benefits. Based on the observation, we proposeScalingNoise, a\nplug-and-play inference-time search strategy that identifies golden initial\nnoises for the diffusion sampling process to improve global content consistency\nand visual diversity. Specifically, we perform one-step denoising to convert\ninitial noises into a clip and subsequently evaluate its long-term value,\nleveraging a reward model anchored by previously generated content. Moreover,\nto preserve diversity, we sample candidates from a tilted noise distribution\nthat up-weights promising noises. In this way, ScalingNoise significantly\nreduces noise-induced errors, ensuring more coherent and spatiotemporally\nconsistent video generation. Extensive experiments on benchmark datasets\ndemonstrate that the proposed ScalingNoise effectively improves long video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion models (VDMs) facilitate the generation of high-quality\nvideos, with current research predominantly concentrated on scaling efforts\nduring training through improvements in data quality, computational resources,\nand model complexity. However, inference-time scaling has received less\nattention, with most approaches restricting models to a single generation\nattempt. Recent studies have uncovered the existence of \"golden noises\" that\ncan enhance video quality during generation. Building on this, we find that\nguiding the scaling inference-time search of VDMs to identify better noise\ncandidates not only evaluates the quality of the frames generated in the\ncurrent step but also preserves the high-level object features by referencing\nthe anchor frame from previous multi-chunks, thereby delivering long-term\nvalue. Our analysis reveals that diffusion models inherently possess flexible\nadjustments of computation by varying denoising steps, and even a one-step\ndenoising approach, when guided by a reward signal, yields significant\nlong-term benefits. Based on the observation, we proposeScalingNoise, a\nplug-and-play inference-time search strategy that identifies golden initial\nnoises for the diffusion sampling process to improve global content consistency\nand visual diversity. Specifically, we perform one-step denoising to convert\ninitial noises into a clip and subsequently evaluate its long-term value,\nleveraging a reward model anchored by previously generated content. Moreover,\nto preserve diversity, we sample candidates from a tilted noise distribution\nthat up-weights promising noises. In this way, ScalingNoise significantly\nreduces noise-induced errors, ensuring more coherent and spatiotemporally\nconsistent video generation. Extensive experiments on benchmark datasets\ndemonstrate that the proposed ScalingNoise effectively improves long video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Yulong Li"
                    },
                    {
                        "name": "Yexin Liu"
                    },
                    {
                        "name": "Zelin Peng"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16400v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16400v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21564v1",
                "updated": "2025-03-27T14:47:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    47,
                    43,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T14:47:43Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    47,
                    43,
                    3,
                    86,
                    0
                ],
                "title": "Cooking Task Planning using LLM and Verified by Graph Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooking Task Planning using LLM and Verified by Graph Network"
                },
                "summary": "Cooking tasks remain a challenging problem for robotics due to their\ncomplexity. Videos of people cooking are a valuable source of information for\nsuch task, but introduces a lot of variability in terms of how to translate\nthis data to a robotic environment. This research aims to streamline this\nprocess, focusing on the task plan generation step, by using a Large Language\nModel (LLM)-based Task and Motion Planning (TAMP) framework to autonomously\ngenerate cooking task plans from videos with subtitles, and execute them.\nConventional LLM-based task planning methods are not well-suited for\ninterpreting the cooking video data due to uncertainty in the videos, and the\nrisk of hallucination in its output. To address both of these problems, we\nexplore using LLMs in combination with Functional Object-Oriented Networks\n(FOON), to validate the plan and provide feedback in case of failure. This\ncombination can generate task sequences with manipulation motions that are\nlogically correct and executable by a robot. We compare the execution of the\ngenerated plans for 5 cooking recipes from our approach against the plans\ngenerated by a few-shot LLM-only approach for a dual-arm robot setup. It could\nsuccessfully execute 4 of the plans generated by our approach, whereas only 1\nof the plans generated by solely using the LLM could be executed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooking tasks remain a challenging problem for robotics due to their\ncomplexity. Videos of people cooking are a valuable source of information for\nsuch task, but introduces a lot of variability in terms of how to translate\nthis data to a robotic environment. This research aims to streamline this\nprocess, focusing on the task plan generation step, by using a Large Language\nModel (LLM)-based Task and Motion Planning (TAMP) framework to autonomously\ngenerate cooking task plans from videos with subtitles, and execute them.\nConventional LLM-based task planning methods are not well-suited for\ninterpreting the cooking video data due to uncertainty in the videos, and the\nrisk of hallucination in its output. To address both of these problems, we\nexplore using LLMs in combination with Functional Object-Oriented Networks\n(FOON), to validate the plan and provide feedback in case of failure. This\ncombination can generate task sequences with manipulation motions that are\nlogically correct and executable by a robot. We compare the execution of the\ngenerated plans for 5 cooking recipes from our approach against the plans\ngenerated by a few-shot LLM-only approach for a dual-arm robot setup. It could\nsuccessfully execute 4 of the plans generated by our approach, whereas only 1\nof the plans generated by solely using the LLM could be executed."
                },
                "authors": [
                    {
                        "name": "Ryunosuke Takebayashi"
                    },
                    {
                        "name": "Vitor Hideyo Isume"
                    },
                    {
                        "name": "Takuya Kiyokawa"
                    },
                    {
                        "name": "Weiwei Wan"
                    },
                    {
                        "name": "Kensuke Harada"
                    }
                ],
                "author_detail": {
                    "name": "Kensuke Harada"
                },
                "author": "Kensuke Harada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21557v1",
                "updated": "2025-03-27T14:43:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    43,
                    28,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T14:43:28Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    43,
                    28,
                    3,
                    86,
                    0
                ],
                "title": "debug-gym: A Text-Based Environment for Interactive Debugging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "debug-gym: A Text-Based Environment for Interactive Debugging"
                },
                "summary": "Large Language Models (LLMs) are increasingly relied upon for coding tasks,\nyet in most scenarios it is assumed that all relevant information can be either\naccessed in context or matches their training data. We posit that LLMs can\nbenefit from the ability to interactively explore a codebase to gather the\ninformation relevant to their task. To achieve this, we present a textual\nenvironment, namely debug-gym, for developing LLM-based agents in an\ninteractive coding setting. Our environment is lightweight and provides a\npreset of useful tools, such as a Python debugger (pdb), designed to facilitate\nan LLM-based agent's interactive debugging. Beyond coding and debugging tasks,\nthis approach can be generalized to other tasks that would benefit from\ninformation-seeking behavior by an LLM agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly relied upon for coding tasks,\nyet in most scenarios it is assumed that all relevant information can be either\naccessed in context or matches their training data. We posit that LLMs can\nbenefit from the ability to interactively explore a codebase to gather the\ninformation relevant to their task. To achieve this, we present a textual\nenvironment, namely debug-gym, for developing LLM-based agents in an\ninteractive coding setting. Our environment is lightweight and provides a\npreset of useful tools, such as a Python debugger (pdb), designed to facilitate\nan LLM-based agent's interactive debugging. Beyond coding and debugging tasks,\nthis approach can be generalized to other tasks that would benefit from\ninformation-seeking behavior by an LLM agent."
                },
                "authors": [
                    {
                        "name": "Xingdi Yuan"
                    },
                    {
                        "name": "Morgane M Moss"
                    },
                    {
                        "name": "Charbel El Feghali"
                    },
                    {
                        "name": "Chinmay Singh"
                    },
                    {
                        "name": "Darya Moldavskaya"
                    },
                    {
                        "name": "Drew MacPhee"
                    },
                    {
                        "name": "Lucas Caccia"
                    },
                    {
                        "name": "Matheus Pereira"
                    },
                    {
                        "name": "Minseon Kim"
                    },
                    {
                        "name": "Alessandro Sordoni"
                    },
                    {
                        "name": "Marc-Alexandre CÃ´tÃ©"
                    }
                ],
                "author_detail": {
                    "name": "Marc-Alexandre CÃ´tÃ©"
                },
                "author": "Marc-Alexandre CÃ´tÃ©",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04765v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04765v2",
                "updated": "2025-03-27T14:42:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    42,
                    53,
                    3,
                    86,
                    0
                ],
                "published": "2025-01-08T18:38:25Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    38,
                    25,
                    2,
                    8,
                    0
                ],
                "title": "TREAD: Token Routing for Efficient Architecture-agnostic Diffusion\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TREAD: Token Routing for Efficient Architecture-agnostic Diffusion\n  Training"
                },
                "summary": "Diffusion models have emerged as the mainstream approach for visual\ngeneration. However, these models typically suffer from sample inefficiency and\nhigh training costs. Consequently, methods for efficient finetuning, inference\nand personalization were quickly adopted by the community. However, training\nthese models in the first place remains very costly. While several recent\napproaches - including masking, distillation, and architectural modifications -\nhave been proposed to improve training efficiency, each of these methods comes\nwith a tradeoff: they achieve enhanced performance at the expense of increased\ncomputational cost or vice versa. In contrast, this work aims to improve\ntraining efficiency as well as generative performance at the same time through\nroutes that act as a transport mechanism for randomly selected tokens from\nearly layers to deeper layers of the model. Our method is not limited to the\ncommon transformer-based model - it can also be applied to state-space models\nand achieves this without architectural modifications or additional parameters.\nFinally, we show that TREAD reduces computational cost and simultaneously\nboosts model performance on the standard ImageNet-256 benchmark in\nclass-conditional synthesis. Both of these benefits multiply to a convergence\nspeedup of 14x at 400K training iterations compared to DiT and 37x compared to\nthe best benchmark performance of DiT at 7M training iterations. Furthermore,\nwe achieve a competitive FID of 2.09 in a guided and 3.93 in an unguided\nsetting, which improves upon the DiT, without architectural changes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as the mainstream approach for visual\ngeneration. However, these models typically suffer from sample inefficiency and\nhigh training costs. Consequently, methods for efficient finetuning, inference\nand personalization were quickly adopted by the community. However, training\nthese models in the first place remains very costly. While several recent\napproaches - including masking, distillation, and architectural modifications -\nhave been proposed to improve training efficiency, each of these methods comes\nwith a tradeoff: they achieve enhanced performance at the expense of increased\ncomputational cost or vice versa. In contrast, this work aims to improve\ntraining efficiency as well as generative performance at the same time through\nroutes that act as a transport mechanism for randomly selected tokens from\nearly layers to deeper layers of the model. Our method is not limited to the\ncommon transformer-based model - it can also be applied to state-space models\nand achieves this without architectural modifications or additional parameters.\nFinally, we show that TREAD reduces computational cost and simultaneously\nboosts model performance on the standard ImageNet-256 benchmark in\nclass-conditional synthesis. Both of these benefits multiply to a convergence\nspeedup of 14x at 400K training iterations compared to DiT and 37x compared to\nthe best benchmark performance of DiT at 7M training iterations. Furthermore,\nwe achieve a competitive FID of 2.09 in a guided and 3.93 in an unguided\nsetting, which improves upon the DiT, without architectural changes."
                },
                "authors": [
                    {
                        "name": "Felix Krause"
                    },
                    {
                        "name": "Timy Phan"
                    },
                    {
                        "name": "Ming Gui"
                    },
                    {
                        "name": "Stefan Andreas Baumann"
                    },
                    {
                        "name": "Vincent Tao Hu"
                    },
                    {
                        "name": "BjÃ¶rn Ommer"
                    }
                ],
                "author_detail": {
                    "name": "BjÃ¶rn Ommer"
                },
                "author": "BjÃ¶rn Ommer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04765v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04765v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21544v1",
                "updated": "2025-03-27T14:34:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    34,
                    28,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T14:34:28Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    34,
                    28,
                    3,
                    86,
                    0
                ],
                "title": "SWI: Speaking with Intent in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWI: Speaking with Intent in Large Language Models"
                },
                "summary": "Intent, typically clearly formulated and planned, functions as a cognitive\nframework for reasoning and problem-solving. This paper introduces the concept\nof Speaking with Intent (SWI) in large language models (LLMs), where the\nexplicitly generated intent encapsulates the model's underlying intention and\nprovides high-level planning to guide subsequent analysis and communication. By\nemulating deliberate and purposeful thoughts in the human mind, SWI is\nhypothesized to enhance the reasoning capabilities and generation quality of\nLLMs. Extensive experiments on mathematical reasoning benchmarks consistently\ndemonstrate the superiority of Speaking with Intent over Baseline (i.e.,\ngeneration without explicit intent). Moreover, SWI outperforms answer-trigger\nprompting methods Chain-of-Thought and Plan-and-Solve and maintains competitive\nperformance with the strong method ARR (Analyzing, Retrieving, and Reasoning).\nAdditionally, the effectiveness and generalizability of SWI are solidified on\nreasoning-intensive question answering (QA) and text summarization benchmarks,\nwhere SWI brings consistent improvement to the Baseline generation. In text\nsummarization, SWI-generated summaries exhibit greater accuracy, conciseness,\nand factual correctness, with fewer hallucinations. Furthermore, human\nevaluations verify the coherence, effectiveness, and interpretability of the\nintent produced by SWI. This proof-of-concept study creates a novel avenue for\nenhancing LLMs' reasoning abilities with cognitive notions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent, typically clearly formulated and planned, functions as a cognitive\nframework for reasoning and problem-solving. This paper introduces the concept\nof Speaking with Intent (SWI) in large language models (LLMs), where the\nexplicitly generated intent encapsulates the model's underlying intention and\nprovides high-level planning to guide subsequent analysis and communication. By\nemulating deliberate and purposeful thoughts in the human mind, SWI is\nhypothesized to enhance the reasoning capabilities and generation quality of\nLLMs. Extensive experiments on mathematical reasoning benchmarks consistently\ndemonstrate the superiority of Speaking with Intent over Baseline (i.e.,\ngeneration without explicit intent). Moreover, SWI outperforms answer-trigger\nprompting methods Chain-of-Thought and Plan-and-Solve and maintains competitive\nperformance with the strong method ARR (Analyzing, Retrieving, and Reasoning).\nAdditionally, the effectiveness and generalizability of SWI are solidified on\nreasoning-intensive question answering (QA) and text summarization benchmarks,\nwhere SWI brings consistent improvement to the Baseline generation. In text\nsummarization, SWI-generated summaries exhibit greater accuracy, conciseness,\nand factual correctness, with fewer hallucinations. Furthermore, human\nevaluations verify the coherence, effectiveness, and interpretability of the\nintent produced by SWI. This proof-of-concept study creates a novel avenue for\nenhancing LLMs' reasoning abilities with cognitive notions."
                },
                "authors": [
                    {
                        "name": "Yuwei Yin"
                    },
                    {
                        "name": "EunJeong Hwang"
                    },
                    {
                        "name": "Giuseppe Carenini"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Carenini"
                },
                "author": "Giuseppe Carenini",
                "arxiv_comment": "24 pages. Code: https://github.com/YuweiYin/SWI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16395v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16395v2",
                "updated": "2025-03-27T14:34:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    34,
                    20,
                    3,
                    86,
                    0
                ],
                "published": "2025-01-26T16:29:40Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    16,
                    29,
                    40,
                    6,
                    26,
                    0
                ],
                "title": "Philip G. Wright, directed acyclic graphs, and instrumental variables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Philip G. Wright, directed acyclic graphs, and instrumental variables"
                },
                "summary": "Wright (1928) deals with demand and supply of oils and butter. In Appendix B\nof this book, Philip Wright made several fundamental contributions to causal\ninference. He introduced a structural equation model of supply and demand,\nestablished the identification of supply and demand elasticities via the method\nof moments and directed acyclical graphs, developed empirical methods for\nestimating demand elasticities using weather conditions as instruments, and\nproposed methods for counterfactual analysis of the welfare effect of imposing\ntariffs and taxes. Moreover, he took all of these methods to data. These ideas\nwere far ahead, and much more profound than, any contemporary theoretical and\nempirical developments on causal inference in statistics or econometrics. This\neditorial aims to present P. Wright's work in a more modern framework, in a\nlecture note format that can be useful for teaching and linking to contemporary\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wright (1928) deals with demand and supply of oils and butter. In Appendix B\nof this book, Philip Wright made several fundamental contributions to causal\ninference. He introduced a structural equation model of supply and demand,\nestablished the identification of supply and demand elasticities via the method\nof moments and directed acyclical graphs, developed empirical methods for\nestimating demand elasticities using weather conditions as instruments, and\nproposed methods for counterfactual analysis of the welfare effect of imposing\ntariffs and taxes. Moreover, he took all of these methods to data. These ideas\nwere far ahead, and much more profound than, any contemporary theoretical and\nempirical developments on causal inference in statistics or econometrics. This\neditorial aims to present P. Wright's work in a more modern framework, in a\nlecture note format that can be useful for teaching and linking to contemporary\nresearch."
                },
                "authors": [
                    {
                        "name": "Jaap H. Abbring"
                    },
                    {
                        "name": "Victor Chernozhukov"
                    },
                    {
                        "name": "IvÃ¡n FernÃ¡ndez-Val"
                    }
                ],
                "author_detail": {
                    "name": "IvÃ¡n FernÃ¡ndez-Val"
                },
                "author": "IvÃ¡n FernÃ¡ndez-Val",
                "arxiv_doi": "10.1093/ectj/utaf006",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/ectj/utaf006",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.16395v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16395v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 5 figures, this version fixes a typo in the previous\n  version",
                "arxiv_journal_ref": "The Econometrics Journal 28(1), (2025)",
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62D20, 62P20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21540v1",
                "updated": "2025-03-27T14:31:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    31,
                    17,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T14:31:17Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    31,
                    17,
                    3,
                    86,
                    0
                ],
                "title": "Combining Artificial Users and Psychotherapist Assessment to Evaluate\n  Large Language Model-based Mental Health Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Artificial Users and Psychotherapist Assessment to Evaluate\n  Large Language Model-based Mental Health Chatbots"
                },
                "summary": "Large Language Models (LLMs) promise to overcome limitations of rule-based\nmental health chatbots through more natural conversations. However, evaluating\nLLM-based mental health chatbots presents a significant challenge: Their\nprobabilistic nature requires comprehensive testing to ensure therapeutic\nquality, yet conducting such evaluations with people with depression would\nimpose an additional burden on vulnerable people and risk exposing them to\npotentially harmful content. Our paper presents an evaluation approach for\nLLM-based mental health chatbots that combines dialogue generation with\nartificial users and dialogue evaluation by psychotherapists. We developed\nartificial users based on patient vignettes, systematically varying\ncharacteristics such as depression severity, personality traits, and attitudes\ntoward chatbots, and let them interact with a LLM-based behavioral activation\nchatbot. Ten psychotherapists evaluated 48 randomly selected dialogues using\nstandardized rating scales to assess the quality of behavioral activation and\nits therapeutic capabilities. We found that while artificial users showed\nmoderate authenticity, they enabled comprehensive testing across different\nusers. In addition, the chatbot demonstrated promising capabilities in\ndelivering behavioral activation and maintaining safety. Furthermore, we\nidentified deficits, such as ensuring the appropriateness of the activity plan,\nwhich reveals necessary improvements for the chatbot. Our framework provides an\neffective method for evaluating LLM-based mental health chatbots while\nprotecting vulnerable people during the evaluation process. Future research\nshould improve the authenticity of artificial users and develop LLM-augmented\nevaluation tools to make psychotherapist evaluation more efficient, and thus\nfurther advance the evaluation of LLM-based mental health chatbots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) promise to overcome limitations of rule-based\nmental health chatbots through more natural conversations. However, evaluating\nLLM-based mental health chatbots presents a significant challenge: Their\nprobabilistic nature requires comprehensive testing to ensure therapeutic\nquality, yet conducting such evaluations with people with depression would\nimpose an additional burden on vulnerable people and risk exposing them to\npotentially harmful content. Our paper presents an evaluation approach for\nLLM-based mental health chatbots that combines dialogue generation with\nartificial users and dialogue evaluation by psychotherapists. We developed\nartificial users based on patient vignettes, systematically varying\ncharacteristics such as depression severity, personality traits, and attitudes\ntoward chatbots, and let them interact with a LLM-based behavioral activation\nchatbot. Ten psychotherapists evaluated 48 randomly selected dialogues using\nstandardized rating scales to assess the quality of behavioral activation and\nits therapeutic capabilities. We found that while artificial users showed\nmoderate authenticity, they enabled comprehensive testing across different\nusers. In addition, the chatbot demonstrated promising capabilities in\ndelivering behavioral activation and maintaining safety. Furthermore, we\nidentified deficits, such as ensuring the appropriateness of the activity plan,\nwhich reveals necessary improvements for the chatbot. Our framework provides an\neffective method for evaluating LLM-based mental health chatbots while\nprotecting vulnerable people during the evaluation process. Future research\nshould improve the authenticity of artificial users and develop LLM-augmented\nevaluation tools to make psychotherapist evaluation more efficient, and thus\nfurther advance the evaluation of LLM-based mental health chatbots."
                },
                "authors": [
                    {
                        "name": "Florian Onur Kuhlmeier"
                    },
                    {
                        "name": "Leon Hanschmann"
                    },
                    {
                        "name": "Melina Rabe"
                    },
                    {
                        "name": "Stefan Luettke"
                    },
                    {
                        "name": "Eva-Lotta Brakemeier"
                    },
                    {
                        "name": "Alexander Maedche"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Maedche"
                },
                "author": "Alexander Maedche",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20711v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20711v2",
                "updated": "2025-03-27T14:28:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    28,
                    31,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-26T16:47:14Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    47,
                    14,
                    2,
                    85,
                    0
                ],
                "title": "Demand Estimation with Text and Image Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demand Estimation with Text and Image Data"
                },
                "summary": "We propose a demand estimation method that leverages unstructured text and\nimage data to infer substitution patterns. Using pre-trained deep learning\nmodels, we extract embeddings from product images and textual descriptions and\nincorporate them into a random coefficients logit model. This approach enables\nresearchers to estimate demand even when they lack data on product attributes\nor when consumers value hard-to-quantify attributes, such as visual design or\nfunctional benefits. Using data from a choice experiment, we show that our\napproach outperforms standard attribute-based models in counterfactual\npredictions of consumers' second choices. We also apply it across 40 product\ncategories on Amazon and consistently find that text and image data help\nidentify close substitutes within each category.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a demand estimation method that leverages unstructured text and\nimage data to infer substitution patterns. Using pre-trained deep learning\nmodels, we extract embeddings from product images and textual descriptions and\nincorporate them into a random coefficients logit model. This approach enables\nresearchers to estimate demand even when they lack data on product attributes\nor when consumers value hard-to-quantify attributes, such as visual design or\nfunctional benefits. Using data from a choice experiment, we show that our\napproach outperforms standard attribute-based models in counterfactual\npredictions of consumers' second choices. We also apply it across 40 product\ncategories on Amazon and consistently find that text and image data help\nidentify close substitutes within each category."
                },
                "authors": [
                    {
                        "name": "Giovanni Compiani"
                    },
                    {
                        "name": "Ilya Morozov"
                    },
                    {
                        "name": "Stephan Seiler"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Seiler"
                },
                "author": "Stephan Seiler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20711v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20711v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21534v1",
                "updated": "2025-03-27T14:26:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    26,
                    20,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T14:26:20Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    26,
                    20,
                    3,
                    86,
                    0
                ],
                "title": "Inequality Restricted Minimum Density Power Divergence Estimation in\n  Panel Count Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inequality Restricted Minimum Density Power Divergence Estimation in\n  Panel Count Data"
                },
                "summary": "Analysis of panel count data has garnered a considerable amount of attention\nin the literature, leading to the development of multiple statistical\ntechniques. In inferential analysis, most of the works focus on leveraging\nestimating equations-based techniques or conventional maximum likelihood\nestimation. However, the robustness of these methods is largely questionable.\nIn this paper, we present the robust density power divergence estimation for\npanel count data arising from nonhomogeneous Poisson processes, correlated\nthrough a latent frailty variable. In order to cope with real-world incidents,\nit is often desired to impose certain inequality constraints on the parameter\nspace, giving rise to the restricted minimum density power divergence\nestimator. The significant contribution of this study lies in deriving its\nasymptotic properties. The proposed method ensures high efficiency in the model\nestimation while providing reliable inference despite data contamination.\nMoreover, the density power divergence measure is governed by a tuning\nparameter \\(\\gamma\\), which controls the trade-off between robustness and\nefficiency. To effectively determine the optimal value of \\(\\gamma\\), this\nstudy employs a generalized score-matching technique, marking considerable\nprogress in the data analysis. Simulation studies and real data examples are\nprovided to illustrate the performance of the estimator and to substantiate the\ntheory developed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of panel count data has garnered a considerable amount of attention\nin the literature, leading to the development of multiple statistical\ntechniques. In inferential analysis, most of the works focus on leveraging\nestimating equations-based techniques or conventional maximum likelihood\nestimation. However, the robustness of these methods is largely questionable.\nIn this paper, we present the robust density power divergence estimation for\npanel count data arising from nonhomogeneous Poisson processes, correlated\nthrough a latent frailty variable. In order to cope with real-world incidents,\nit is often desired to impose certain inequality constraints on the parameter\nspace, giving rise to the restricted minimum density power divergence\nestimator. The significant contribution of this study lies in deriving its\nasymptotic properties. The proposed method ensures high efficiency in the model\nestimation while providing reliable inference despite data contamination.\nMoreover, the density power divergence measure is governed by a tuning\nparameter \\(\\gamma\\), which controls the trade-off between robustness and\nefficiency. To effectively determine the optimal value of \\(\\gamma\\), this\nstudy employs a generalized score-matching technique, marking considerable\nprogress in the data analysis. Simulation studies and real data examples are\nprovided to illustrate the performance of the estimator and to substantiate the\ntheory developed."
                },
                "authors": [
                    {
                        "name": "Udita Goswami"
                    },
                    {
                        "name": "Shuvashree Mondal"
                    }
                ],
                "author_detail": {
                    "name": "Shuvashree Mondal"
                },
                "author": "Shuvashree Mondal",
                "arxiv_comment": "35 PAGES, 12 FIGURES, 7 TABLES",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13731v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13731v2",
                "updated": "2025-03-27T14:20:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    20,
                    21,
                    3,
                    86,
                    0
                ],
                "published": "2025-02-19T13:56:20Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    56,
                    20,
                    2,
                    50,
                    0
                ],
                "title": "Robust Counterfactual Inference in Markov Decision Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Counterfactual Inference in Markov Decision Processes"
                },
                "summary": "This paper addresses a key limitation in existing counterfactual inference\nmethods for Markov Decision Processes (MDPs). Current approaches assume a\nspecific causal model to make counterfactuals identifiable. However, there are\nusually many causal models that align with the observational and interventional\ndistributions of an MDP, each yielding different counterfactual distributions,\nso fixing a particular causal model limits the validity (and usefulness) of\ncounterfactual inference. We propose a novel non-parametric approach that\ncomputes tight bounds on counterfactual transition probabilities across all\ncompatible causal models. Unlike previous methods that require solving\nprohibitively large optimisation problems (with variables that grow\nexponentially in the size of the MDP), our approach provides closed-form\nexpressions for these bounds, making computation highly efficient and scalable\nfor non-trivial MDPs. Once such an interval counterfactual MDP is constructed,\nour method identifies robust counterfactual policies that optimise the\nworst-case reward w.r.t. the uncertain interval MDP probabilities. We evaluate\nour method on various case studies, demonstrating improved robustness over\nexisting methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses a key limitation in existing counterfactual inference\nmethods for Markov Decision Processes (MDPs). Current approaches assume a\nspecific causal model to make counterfactuals identifiable. However, there are\nusually many causal models that align with the observational and interventional\ndistributions of an MDP, each yielding different counterfactual distributions,\nso fixing a particular causal model limits the validity (and usefulness) of\ncounterfactual inference. We propose a novel non-parametric approach that\ncomputes tight bounds on counterfactual transition probabilities across all\ncompatible causal models. Unlike previous methods that require solving\nprohibitively large optimisation problems (with variables that grow\nexponentially in the size of the MDP), our approach provides closed-form\nexpressions for these bounds, making computation highly efficient and scalable\nfor non-trivial MDPs. Once such an interval counterfactual MDP is constructed,\nour method identifies robust counterfactual policies that optimise the\nworst-case reward w.r.t. the uncertain interval MDP probabilities. We evaluate\nour method on various case studies, demonstrating improved robustness over\nexisting methods."
                },
                "authors": [
                    {
                        "name": "Jessica Lally"
                    },
                    {
                        "name": "Milad Kazemi"
                    },
                    {
                        "name": "Nicola Paoletti"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Paoletti"
                },
                "author": "Nicola Paoletti",
                "arxiv_comment": "Fixed typo in Equation (5)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13731v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13731v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17922v2",
                "updated": "2025-03-27T14:11:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    11,
                    37,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-23T03:36:52Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    36,
                    52,
                    6,
                    82,
                    0
                ],
                "title": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference"
                },
                "summary": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness."
                },
                "authors": [
                    {
                        "name": "Youhui Zuo"
                    },
                    {
                        "name": "Sibo Wei"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhuorui Liu"
                    },
                    {
                        "name": "Wenpeng Lu"
                    },
                    {
                        "name": "Dawei Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Song"
                },
                "author": "Dawei Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20349v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20349v2",
                "updated": "2025-03-27T13:59:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    13,
                    59,
                    15,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-26T09:20:42Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    20,
                    42,
                    2,
                    85,
                    0
                ],
                "title": "Consistency Trajectory Matching for One-Step Generative Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consistency Trajectory Matching for One-Step Generative Super-Resolution"
                },
                "summary": "Current diffusion-based super-resolution (SR) approaches achieve commendable\nperformance at the cost of high inference overhead. Therefore, distillation\ntechniques are utilized to accelerate the multi-step teacher model into\none-step student model. Nevertheless, these methods significantly raise\ntraining costs and constrain the performance of the student model by the\nteacher model. To overcome these tough challenges, we propose Consistency\nTrajectory Matching for Super-Resolution (CTMSR), a distillation-free strategy\nthat is able to generate photo-realistic SR results in one step. Concretely, we\nfirst formulate a Probability Flow Ordinary Differential Equation (PF-ODE)\ntrajectory to establish a deterministic mapping from low-resolution (LR) images\nwith noise to high-resolution (HR) images. Then we apply the Consistency\nTraining (CT) strategy to directly learn the mapping in one step, eliminating\nthe necessity of pre-trained diffusion model. To further enhance the\nperformance and better leverage the ground-truth during the training process,\nwe aim to align the distribution of SR results more closely with that of the\nnatural images. To this end, we propose to minimize the discrepancy between\ntheir respective PF-ODE trajectories from the LR image distribution by our\nmeticulously designed Distribution Trajectory Matching (DTM) loss, resulting in\nimproved realism of our recovered HR images. Comprehensive experimental results\ndemonstrate that the proposed methods can attain comparable or even superior\ncapabilities on both synthetic and real datasets while maintaining minimal\ninference latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current diffusion-based super-resolution (SR) approaches achieve commendable\nperformance at the cost of high inference overhead. Therefore, distillation\ntechniques are utilized to accelerate the multi-step teacher model into\none-step student model. Nevertheless, these methods significantly raise\ntraining costs and constrain the performance of the student model by the\nteacher model. To overcome these tough challenges, we propose Consistency\nTrajectory Matching for Super-Resolution (CTMSR), a distillation-free strategy\nthat is able to generate photo-realistic SR results in one step. Concretely, we\nfirst formulate a Probability Flow Ordinary Differential Equation (PF-ODE)\ntrajectory to establish a deterministic mapping from low-resolution (LR) images\nwith noise to high-resolution (HR) images. Then we apply the Consistency\nTraining (CT) strategy to directly learn the mapping in one step, eliminating\nthe necessity of pre-trained diffusion model. To further enhance the\nperformance and better leverage the ground-truth during the training process,\nwe aim to align the distribution of SR results more closely with that of the\nnatural images. To this end, we propose to minimize the discrepancy between\ntheir respective PF-ODE trajectories from the LR image distribution by our\nmeticulously designed Distribution Trajectory Matching (DTM) loss, resulting in\nimproved realism of our recovered HR images. Comprehensive experimental results\ndemonstrate that the proposed methods can attain comparable or even superior\ncapabilities on both synthetic and real datasets while maintaining minimal\ninference latency."
                },
                "authors": [
                    {
                        "name": "Weiyi You"
                    },
                    {
                        "name": "Mingyang Zhang"
                    },
                    {
                        "name": "Leheng Zhang"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Kexuan Shi"
                    },
                    {
                        "name": "Shuhang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Shuhang Gu"
                },
                "author": "Shuhang Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20349v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20349v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.08514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.08514v2",
                "updated": "2025-03-27T13:59:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    13,
                    59,
                    5,
                    3,
                    86,
                    0
                ],
                "published": "2024-02-13T15:10:30Z",
                "published_parsed": [
                    2024,
                    2,
                    13,
                    15,
                    10,
                    30,
                    1,
                    44,
                    0
                ],
                "title": "Counterfactual Influence in Markov Decision Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual Influence in Markov Decision Processes"
                },
                "summary": "Our work addresses a fundamental problem in the context of counterfactual\ninference for Markov Decision Processes (MDPs). Given an MDP path $\\tau$, this\nkind of inference allows us to derive counterfactual paths $\\tau'$ describing\nwhat-if versions of $\\tau$ obtained under different action sequences than those\nobserved in $\\tau$. However, as the counterfactual states and actions deviate\nfrom the observed ones over time, the observation $\\tau$ may no longer\ninfluence the counterfactual world, meaning that the analysis is no longer\ntailored to the individual observation, resulting in interventional outcomes\nrather than counterfactual ones. Even though this issue specifically affects\nthe popular Gumbel-max structural causal model used for MDP counterfactuals, it\nhas remained overlooked until now. In this work, we introduce a formal\ncharacterisation of influence based on comparing counterfactual and\ninterventional distributions. We devise an algorithm to construct\ncounterfactual models that automatically satisfy influence constraints.\nLeveraging such models, we derive counterfactual policies that are not just\noptimal for a given reward structure but also remain tailored to the observed\npath. Even though there is an unavoidable trade-off between policy optimality\nand strength of influence constraints, our experiments demonstrate that it is\npossible to derive (near-)optimal policies while remaining under the influence\nof the observation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our work addresses a fundamental problem in the context of counterfactual\ninference for Markov Decision Processes (MDPs). Given an MDP path $\\tau$, this\nkind of inference allows us to derive counterfactual paths $\\tau'$ describing\nwhat-if versions of $\\tau$ obtained under different action sequences than those\nobserved in $\\tau$. However, as the counterfactual states and actions deviate\nfrom the observed ones over time, the observation $\\tau$ may no longer\ninfluence the counterfactual world, meaning that the analysis is no longer\ntailored to the individual observation, resulting in interventional outcomes\nrather than counterfactual ones. Even though this issue specifically affects\nthe popular Gumbel-max structural causal model used for MDP counterfactuals, it\nhas remained overlooked until now. In this work, we introduce a formal\ncharacterisation of influence based on comparing counterfactual and\ninterventional distributions. We devise an algorithm to construct\ncounterfactual models that automatically satisfy influence constraints.\nLeveraging such models, we derive counterfactual policies that are not just\noptimal for a given reward structure but also remain tailored to the observed\npath. Even though there is an unavoidable trade-off between policy optimality\nand strength of influence constraints, our experiments demonstrate that it is\npossible to derive (near-)optimal policies while remaining under the influence\nof the observation."
                },
                "authors": [
                    {
                        "name": "Milad Kazemi"
                    },
                    {
                        "name": "Jessica Lally"
                    },
                    {
                        "name": "Ekaterina Tishchenko"
                    },
                    {
                        "name": "Hana Chockler"
                    },
                    {
                        "name": "Nicola Paoletti"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Paoletti"
                },
                "author": "Nicola Paoletti",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.08514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.08514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21500v1",
                "updated": "2025-03-27T13:40:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    13,
                    40,
                    6,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T13:40:06Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    13,
                    40,
                    6,
                    3,
                    86,
                    0
                ],
                "title": "OpenHuEval: Evaluating Large Language Model on Hungarian Specifics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenHuEval: Evaluating Large Language Model on Hungarian Specifics"
                },
                "summary": "We introduce OpenHuEval, the first benchmark for LLMs focusing on the\nHungarian language and specifics. OpenHuEval is constructed from a vast\ncollection of Hungarian-specific materials sourced from multiple origins. In\nthe construction, we incorporated the latest design principles for evaluating\nLLMs, such as using real user queries from the internet, emphasizing the\nassessment of LLMs' generative capabilities, and employing LLM-as-judge to\nenhance the multidimensionality and accuracy of evaluations. Ultimately,\nOpenHuEval encompasses eight Hungarian-specific dimensions, featuring five\ntasks and 3953 questions. Consequently, OpenHuEval provides the comprehensive,\nin-depth, and scientifically accurate assessment of LLM performance in the\ncontext of the Hungarian language and its specifics. We evaluated current\nmainstream LLMs, including both traditional LLMs and recently developed Large\nReasoning Models. The results demonstrate the significant necessity for\nevaluation and model optimization tailored to the Hungarian language and\nspecifics. We also established the framework for analyzing the thinking\nprocesses of LRMs with OpenHuEval, revealing intrinsic patterns and mechanisms\nof these models in non-English languages, with Hungarian serving as a\nrepresentative example. We will release OpenHuEval at\nhttps://github.com/opendatalab/OpenHuEval .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce OpenHuEval, the first benchmark for LLMs focusing on the\nHungarian language and specifics. OpenHuEval is constructed from a vast\ncollection of Hungarian-specific materials sourced from multiple origins. In\nthe construction, we incorporated the latest design principles for evaluating\nLLMs, such as using real user queries from the internet, emphasizing the\nassessment of LLMs' generative capabilities, and employing LLM-as-judge to\nenhance the multidimensionality and accuracy of evaluations. Ultimately,\nOpenHuEval encompasses eight Hungarian-specific dimensions, featuring five\ntasks and 3953 questions. Consequently, OpenHuEval provides the comprehensive,\nin-depth, and scientifically accurate assessment of LLM performance in the\ncontext of the Hungarian language and its specifics. We evaluated current\nmainstream LLMs, including both traditional LLMs and recently developed Large\nReasoning Models. The results demonstrate the significant necessity for\nevaluation and model optimization tailored to the Hungarian language and\nspecifics. We also established the framework for analyzing the thinking\nprocesses of LRMs with OpenHuEval, revealing intrinsic patterns and mechanisms\nof these models in non-English languages, with Hungarian serving as a\nrepresentative example. We will release OpenHuEval at\nhttps://github.com/opendatalab/OpenHuEval ."
                },
                "authors": [
                    {
                        "name": "Haote Yang"
                    },
                    {
                        "name": "Xingjian Wei"
                    },
                    {
                        "name": "Jiang Wu"
                    },
                    {
                        "name": "NoÃ©mi Ligeti-Nagy"
                    },
                    {
                        "name": "Jiaxing Sun"
                    },
                    {
                        "name": "Yinfan Wang"
                    },
                    {
                        "name": "Zijian GyÅzÅ Yang"
                    },
                    {
                        "name": "Junyuan Gao"
                    },
                    {
                        "name": "Jingchao Wang"
                    },
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Shasha Wang"
                    },
                    {
                        "name": "Nanjun Yu"
                    },
                    {
                        "name": "Zihao Zhang"
                    },
                    {
                        "name": "Shixin Hong"
                    },
                    {
                        "name": "Hongwei Liu"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Lijun Wu"
                    },
                    {
                        "name": "GÃ¡bor PrÃ³szÃ©ky"
                    },
                    {
                        "name": "Conghui He"
                    }
                ],
                "author_detail": {
                    "name": "Conghui He"
                },
                "author": "Conghui He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21483v1",
                "updated": "2025-03-27T13:18:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    13,
                    18,
                    40,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T13:18:40Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    13,
                    18,
                    40,
                    3,
                    86,
                    0
                ],
                "title": "BOLT: Boost Large Vision-Language Model Without Training for Long-form\n  Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOLT: Boost Large Vision-Language Model Without Training for Long-form\n  Video Understanding"
                },
                "summary": "Large video-language models (VLMs) have demonstrated promising progress in\nvarious video understanding tasks. However, their effectiveness in long-form\nvideo analysis is constrained by limited context windows. Traditional\napproaches, such as uniform frame sampling, often inevitably allocate resources\nto irrelevant content, diminishing their effectiveness in real-world scenarios.\nIn this paper, we introduce BOLT, a method to BOost Large VLMs without\nadditional Training through a comprehensive study of frame selection\nstrategies. First, to enable a more realistic evaluation of VLMs in long-form\nvideo understanding, we propose a multi-source retrieval evaluation setting.\nOur findings reveal that uniform sampling performs poorly in noisy contexts,\nunderscoring the importance of selecting the right frames. Second, we explore\nseveral frame selection strategies based on query-frame similarity and analyze\ntheir effectiveness at inference time. Our results show that inverse transform\nsampling yields the most significant performance improvement, increasing\naccuracy on the Video-MME benchmark from 53.8% to 56.1% and MLVU benchmark from\n58.9% to 63.4%. Our code is available at https://github.com/sming256/BOLT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large video-language models (VLMs) have demonstrated promising progress in\nvarious video understanding tasks. However, their effectiveness in long-form\nvideo analysis is constrained by limited context windows. Traditional\napproaches, such as uniform frame sampling, often inevitably allocate resources\nto irrelevant content, diminishing their effectiveness in real-world scenarios.\nIn this paper, we introduce BOLT, a method to BOost Large VLMs without\nadditional Training through a comprehensive study of frame selection\nstrategies. First, to enable a more realistic evaluation of VLMs in long-form\nvideo understanding, we propose a multi-source retrieval evaluation setting.\nOur findings reveal that uniform sampling performs poorly in noisy contexts,\nunderscoring the importance of selecting the right frames. Second, we explore\nseveral frame selection strategies based on query-frame similarity and analyze\ntheir effectiveness at inference time. Our results show that inverse transform\nsampling yields the most significant performance improvement, increasing\naccuracy on the Video-MME benchmark from 53.8% to 56.1% and MLVU benchmark from\n58.9% to 63.4%. Our code is available at https://github.com/sming256/BOLT."
                },
                "authors": [
                    {
                        "name": "Shuming Liu"
                    },
                    {
                        "name": "Chen Zhao"
                    },
                    {
                        "name": "Tianqi Xu"
                    },
                    {
                        "name": "Bernard Ghanem"
                    }
                ],
                "author_detail": {
                    "name": "Bernard Ghanem"
                },
                "author": "Bernard Ghanem",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21480v2",
                "updated": "2025-03-28T12:34:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    12,
                    34,
                    25,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-27T13:12:49Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    13,
                    12,
                    49,
                    3,
                    86,
                    0
                ],
                "title": "OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs"
                },
                "summary": "The use of omni-LLMs (large language models that accept any modality as\ninput), particularly for multimodal cognitive state tasks involving speech, is\nunderstudied. We present OmniVox, the first systematic evaluation of four\nomni-LLMs on the zero-shot emotion recognition task. We evaluate on two widely\nused multimodal emotion benchmarks: IEMOCAP and MELD, and find zero-shot\nomni-LLMs outperform or are competitive with fine-tuned audio models. Alongside\nour audio-only evaluation, we also evaluate omni-LLMs on text only and text and\naudio. We present acoustic prompting, an audio-specific prompting strategy for\nomni-LLMs which focuses on acoustic feature analysis, conversation context\nanalysis, and step-by-step reasoning. We compare our acoustic prompting to\nminimal prompting and full chain-of-thought prompting techniques. We perform a\ncontext window analysis on IEMOCAP and MELD, and find that using context helps,\nespecially on IEMOCAP. We conclude with an error analysis on the generated\nacoustic reasoning outputs from the omni-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of omni-LLMs (large language models that accept any modality as\ninput), particularly for multimodal cognitive state tasks involving speech, is\nunderstudied. We present OmniVox, the first systematic evaluation of four\nomni-LLMs on the zero-shot emotion recognition task. We evaluate on two widely\nused multimodal emotion benchmarks: IEMOCAP and MELD, and find zero-shot\nomni-LLMs outperform or are competitive with fine-tuned audio models. Alongside\nour audio-only evaluation, we also evaluate omni-LLMs on text only and text and\naudio. We present acoustic prompting, an audio-specific prompting strategy for\nomni-LLMs which focuses on acoustic feature analysis, conversation context\nanalysis, and step-by-step reasoning. We compare our acoustic prompting to\nminimal prompting and full chain-of-thought prompting techniques. We perform a\ncontext window analysis on IEMOCAP and MELD, and find that using context helps,\nespecially on IEMOCAP. We conclude with an error analysis on the generated\nacoustic reasoning outputs from the omni-LLMs."
                },
                "authors": [
                    {
                        "name": "John Murzaku"
                    },
                    {
                        "name": "Owen Rambow"
                    }
                ],
                "author_detail": {
                    "name": "Owen Rambow"
                },
                "author": "Owen Rambow",
                "arxiv_comment": "Submitted to COLM 2025. Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21476v1",
                "updated": "2025-03-27T13:06:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    13,
                    6,
                    26,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T13:06:26Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    13,
                    6,
                    26,
                    3,
                    86,
                    0
                ],
                "title": "Robust DNN Partitioning and Resource Allocation Under Uncertain\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust DNN Partitioning and Resource Allocation Under Uncertain\n  Inference Time"
                },
                "summary": "In edge intelligence systems, deep neural network (DNN) partitioning and data\noffloading can provide real-time task inference for resource-constrained mobile\ndevices. However, the inference time of DNNs is typically uncertain and cannot\nbe precisely determined in advance, presenting significant challenges in\nensuring timely task processing within deadlines. To address the uncertain\ninference time, we propose a robust optimization scheme to minimize the total\nenergy consumption of mobile devices while meeting task probabilistic\ndeadlines. The scheme only requires the mean and variance information of the\ninference time, without any prediction methods or distribution functions. The\nproblem is formulated as a mixed-integer nonlinear programming (MINLP) that\ninvolves jointly optimizing the DNN model partitioning and the allocation of\nlocal CPU/GPU frequencies and uplink bandwidth. To tackle the problem, we first\ndecompose the original problem into two subproblems: resource allocation and\nDNN model partitioning. Subsequently, the two subproblems with probability\nconstraints are equivalently transformed into deterministic optimization\nproblems using the chance-constrained programming (CCP) method. Finally, the\nconvex optimization technique and the penalty convex-concave procedure (PCCP)\ntechnique are employed to obtain the optimal solution of the resource\nallocation subproblem and a stationary point of the DNN model partitioning\nsubproblem, respectively. The proposed algorithm leverages real-world data from\npopular hardware platforms and is evaluated on widely used DNN models.\nExtensive simulations show that our proposed algorithm effectively addresses\nthe inference time uncertainty with probabilistic deadline guarantees while\nminimizing the energy consumption of mobile devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In edge intelligence systems, deep neural network (DNN) partitioning and data\noffloading can provide real-time task inference for resource-constrained mobile\ndevices. However, the inference time of DNNs is typically uncertain and cannot\nbe precisely determined in advance, presenting significant challenges in\nensuring timely task processing within deadlines. To address the uncertain\ninference time, we propose a robust optimization scheme to minimize the total\nenergy consumption of mobile devices while meeting task probabilistic\ndeadlines. The scheme only requires the mean and variance information of the\ninference time, without any prediction methods or distribution functions. The\nproblem is formulated as a mixed-integer nonlinear programming (MINLP) that\ninvolves jointly optimizing the DNN model partitioning and the allocation of\nlocal CPU/GPU frequencies and uplink bandwidth. To tackle the problem, we first\ndecompose the original problem into two subproblems: resource allocation and\nDNN model partitioning. Subsequently, the two subproblems with probability\nconstraints are equivalently transformed into deterministic optimization\nproblems using the chance-constrained programming (CCP) method. Finally, the\nconvex optimization technique and the penalty convex-concave procedure (PCCP)\ntechnique are employed to obtain the optimal solution of the resource\nallocation subproblem and a stationary point of the DNN model partitioning\nsubproblem, respectively. The proposed algorithm leverages real-world data from\npopular hardware platforms and is evaluated on widely used DNN models.\nExtensive simulations show that our proposed algorithm effectively addresses\nthe inference time uncertainty with probabilistic deadline guarantees while\nminimizing the energy consumption of mobile devices."
                },
                "authors": [
                    {
                        "name": "Zhaojun Nan"
                    },
                    {
                        "name": "Yunchu Han"
                    },
                    {
                        "name": "Sheng Zhou"
                    },
                    {
                        "name": "Zhisheng Niu"
                    }
                ],
                "author_detail": {
                    "name": "Zhisheng Niu"
                },
                "author": "Zhisheng Niu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18940v2",
                "updated": "2025-03-27T13:05:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    13,
                    5,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-24T17:59:02Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    59,
                    2,
                    0,
                    83,
                    0
                ],
                "title": "Training-free Diffusion Acceleration with Bottleneck Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free Diffusion Acceleration with Bottleneck Sampling"
                },
                "summary": "Diffusion models have demonstrated remarkable capabilities in visual content\ngeneration but remain challenging to deploy due to their high computational\ncost during inference. This computational burden primarily arises from the\nquadratic complexity of self-attention with respect to image or video\nresolution. While existing acceleration methods often compromise output quality\nor necessitate costly retraining, we observe that most diffusion models are\npre-trained at lower resolutions, presenting an opportunity to exploit these\nlow-resolution priors for more efficient inference without degrading\nperformance. In this work, we introduce Bottleneck Sampling, a training-free\nframework that leverages low-resolution priors to reduce computational overhead\nwhile preserving output fidelity. Bottleneck Sampling follows a high-low-high\ndenoising workflow: it performs high-resolution denoising in the initial and\nfinal stages while operating at lower resolutions in intermediate steps. To\nmitigate aliasing and blurring artifacts, we further refine the resolution\ntransition points and adaptively shift the denoising timesteps at each stage.\nWe evaluate Bottleneck Sampling on both image and video generation tasks, where\nextensive experiments demonstrate that it accelerates inference by up to\n3$\\times$ for image generation and 2.5$\\times$ for video generation, all while\nmaintaining output quality comparable to the standard full-resolution sampling\nprocess across multiple evaluation metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated remarkable capabilities in visual content\ngeneration but remain challenging to deploy due to their high computational\ncost during inference. This computational burden primarily arises from the\nquadratic complexity of self-attention with respect to image or video\nresolution. While existing acceleration methods often compromise output quality\nor necessitate costly retraining, we observe that most diffusion models are\npre-trained at lower resolutions, presenting an opportunity to exploit these\nlow-resolution priors for more efficient inference without degrading\nperformance. In this work, we introduce Bottleneck Sampling, a training-free\nframework that leverages low-resolution priors to reduce computational overhead\nwhile preserving output fidelity. Bottleneck Sampling follows a high-low-high\ndenoising workflow: it performs high-resolution denoising in the initial and\nfinal stages while operating at lower resolutions in intermediate steps. To\nmitigate aliasing and blurring artifacts, we further refine the resolution\ntransition points and adaptively shift the denoising timesteps at each stage.\nWe evaluate Bottleneck Sampling on both image and video generation tasks, where\nextensive experiments demonstrate that it accelerates inference by up to\n3$\\times$ for image generation and 2.5$\\times$ for video generation, all while\nmaintaining output quality comparable to the standard full-resolution sampling\nprocess across multiple evaluation metrics."
                },
                "authors": [
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Yuxi Ren"
                    },
                    {
                        "name": "Shanchuan Lin"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Yunhai Tong"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "arxiv_comment": "Project Page: https://tyfeld.github.io/BottleneckSampling.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21473v1",
                "updated": "2025-03-27T13:04:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    13,
                    4,
                    41,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T13:04:41Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    13,
                    4,
                    41,
                    3,
                    86,
                    0
                ],
                "title": "DeepRV: pre-trained spatial priors for accelerated disease mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepRV: pre-trained spatial priors for accelerated disease mapping"
                },
                "summary": "Recently introduced prior-encoding deep generative models (e.g., PriorVAE,\n$\\pi$VAE, and PriorCVAE) have emerged as powerful tools for scalable Bayesian\ninference by emulating complex stochastic processes like Gaussian processes\n(GPs). However, these methods remain largely a proof-of-concept and\ninaccessible to practitioners. We propose DeepRV, a lightweight, decoder-only\napproach that accelerates training, and enhances real-world applicability in\ncomparison to current VAE-based prior encoding approaches. Leveraging\nprobabilistic programming frameworks (e.g., NumPyro) for inference, DeepRV\nachieves significant speedups while also improving the quality of parameter\ninference, closely matching full MCMC sampling. We showcase its effectiveness\nin process emulation and spatial analysis of the UK using simulated data,\ngender-wise cancer mortality rates for individuals under 50, and HIV prevalence\nin Zimbabwe. To bridge the gap between theory and practice, we provide a\nuser-friendly API, enabling scalable and efficient Bayesian inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently introduced prior-encoding deep generative models (e.g., PriorVAE,\n$\\pi$VAE, and PriorCVAE) have emerged as powerful tools for scalable Bayesian\ninference by emulating complex stochastic processes like Gaussian processes\n(GPs). However, these methods remain largely a proof-of-concept and\ninaccessible to practitioners. We propose DeepRV, a lightweight, decoder-only\napproach that accelerates training, and enhances real-world applicability in\ncomparison to current VAE-based prior encoding approaches. Leveraging\nprobabilistic programming frameworks (e.g., NumPyro) for inference, DeepRV\nachieves significant speedups while also improving the quality of parameter\ninference, closely matching full MCMC sampling. We showcase its effectiveness\nin process emulation and spatial analysis of the UK using simulated data,\ngender-wise cancer mortality rates for individuals under 50, and HIV prevalence\nin Zimbabwe. To bridge the gap between theory and practice, we provide a\nuser-friendly API, enabling scalable and efficient Bayesian inference."
                },
                "authors": [
                    {
                        "name": "Jhonathan Navott"
                    },
                    {
                        "name": "Daniel Jenson"
                    },
                    {
                        "name": "Seth Flaxman"
                    },
                    {
                        "name": "Elizaveta Semenova"
                    }
                ],
                "author_detail": {
                    "name": "Elizaveta Semenova"
                },
                "author": "Elizaveta Semenova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21464v1",
                "updated": "2025-03-27T12:54:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    54,
                    0,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T12:54:00Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    54,
                    0,
                    3,
                    86,
                    0
                ],
                "title": "Harnessing Chain-of-Thought Metadata for Task Routing and Adversarial\n  Prompt Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Chain-of-Thought Metadata for Task Routing and Adversarial\n  Prompt Detection"
                },
                "summary": "In this work, we propose a metric called Number of Thoughts (NofT) to\ndetermine the difficulty of tasks pre-prompting and support Large Language\nModels (LLMs) in production contexts. By setting thresholds based on the number\nof thoughts, this metric can discern the difficulty of prompts and support more\neffective prompt routing. A 2% decrease in latency is achieved when routing\nprompts from the MathInstruct dataset through quantized, distilled versions of\nDeepseek with 1.7 billion, 7 billion, and 14 billion parameters. Moreover, this\nmetric can be used to detect adversarial prompts used in prompt injection\nattacks with high efficacy. The Number of Thoughts can inform a classifier that\nachieves 95% accuracy in adversarial prompt detection. Our experiments ad\ndatasets used are available on our GitHub page:\nhttps://github.com/rymarinelli/Number_Of_Thoughts/tree/main.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a metric called Number of Thoughts (NofT) to\ndetermine the difficulty of tasks pre-prompting and support Large Language\nModels (LLMs) in production contexts. By setting thresholds based on the number\nof thoughts, this metric can discern the difficulty of prompts and support more\neffective prompt routing. A 2% decrease in latency is achieved when routing\nprompts from the MathInstruct dataset through quantized, distilled versions of\nDeepseek with 1.7 billion, 7 billion, and 14 billion parameters. Moreover, this\nmetric can be used to detect adversarial prompts used in prompt injection\nattacks with high efficacy. The Number of Thoughts can inform a classifier that\nachieves 95% accuracy in adversarial prompt detection. Our experiments ad\ndatasets used are available on our GitHub page:\nhttps://github.com/rymarinelli/Number_Of_Thoughts/tree/main."
                },
                "authors": [
                    {
                        "name": "Ryan Marinelli"
                    },
                    {
                        "name": "Josef Pichlmeier"
                    },
                    {
                        "name": "Tamas Bisztray"
                    }
                ],
                "author_detail": {
                    "name": "Tamas Bisztray"
                },
                "author": "Tamas Bisztray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21461v1",
                "updated": "2025-03-27T12:50:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    50,
                    49,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T12:50:49Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    50,
                    49,
                    3,
                    86,
                    0
                ],
                "title": "ParaFlow: fast calorimeter simulations parameterized in upstream\n  material configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParaFlow: fast calorimeter simulations parameterized in upstream\n  material configurations"
                },
                "summary": "We study whether machine-learning models for fast calorimeter simulations can\nlearn meaningful representations of calorimeter signatures that account for\nvariations in the full particle detector's configuration. This may open new\nopportunities in high-energy physics measurements, for example in the\nassessment of systematic uncertainties that are related to the detector\ngeometry, in the inference of properties of the detector configuration, or in\nthe automated design of experiments. As a concrete example, we parameterize\nnormalizing-flow-based simulations in configurations of the material upstream\nof a toy calorimeter. We call this model ParaFlow, which is trained to\ninterpolate between different material budgets and positions, as simulated with\nGeant4. We study ParaFlow's performance in terms of photon shower shapes that\nare directly influenced by the properties of the upstream material, in which\nphotons can convert to an electron-positron pair. In general, we find that\nParaFlow is able to reproduce the dependence of the shower shapes on the\nmaterial properties at the few-percent level with larger differences only in\nthe tails of the distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study whether machine-learning models for fast calorimeter simulations can\nlearn meaningful representations of calorimeter signatures that account for\nvariations in the full particle detector's configuration. This may open new\nopportunities in high-energy physics measurements, for example in the\nassessment of systematic uncertainties that are related to the detector\ngeometry, in the inference of properties of the detector configuration, or in\nthe automated design of experiments. As a concrete example, we parameterize\nnormalizing-flow-based simulations in configurations of the material upstream\nof a toy calorimeter. We call this model ParaFlow, which is trained to\ninterpolate between different material budgets and positions, as simulated with\nGeant4. We study ParaFlow's performance in terms of photon shower shapes that\nare directly influenced by the properties of the upstream material, in which\nphotons can convert to an electron-positron pair. In general, we find that\nParaFlow is able to reproduce the dependence of the shower shapes on the\nmaterial properties at the few-percent level with larger differences only in\nthe tails of the distributions."
                },
                "authors": [
                    {
                        "name": "Johannes Erdmann"
                    },
                    {
                        "name": "Jonas Kann"
                    },
                    {
                        "name": "Florian Mausolf"
                    },
                    {
                        "name": "Peter Wissmann"
                    }
                ],
                "author_detail": {
                    "name": "Peter Wissmann"
                },
                "author": "Peter Wissmann",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21460v1",
                "updated": "2025-03-27T12:50:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    50,
                    17,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T12:50:17Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    50,
                    17,
                    3,
                    86,
                    0
                ],
                "title": "Large Language Model Agent: A Survey on Methodology, Applications and\n  Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Agent: A Survey on Methodology, Applications and\n  Challenges"
                },
                "summary": "The era of intelligent agents is upon us, driven by revolutionary\nadvancements in large language models. Large Language Model (LLM) agents, with\ngoal-driven behaviors and dynamic adaptation capabilities, potentially\nrepresent a critical pathway toward artificial general intelligence. This\nsurvey systematically deconstructs LLM agent systems through a\nmethodology-centered taxonomy, linking architectural foundations, collaboration\nmechanisms, and evolutionary pathways. We unify fragmented research threads by\nrevealing fundamental connections between agent design principles and their\nemergent behaviors in complex environments. Our work provides a unified\narchitectural perspective, examining how agents are constructed, how they\ncollaborate, and how they evolve over time, while also addressing evaluation\nmethodologies, tool applications, practical challenges, and diverse application\ndomains. By surveying the latest developments in this rapidly evolving field,\nwe offer researchers a structured taxonomy for understanding LLM agents and\nidentify promising directions for future research. The collection is available\nat https://github.com/luo-junyu/Awesome-Agent-Papers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The era of intelligent agents is upon us, driven by revolutionary\nadvancements in large language models. Large Language Model (LLM) agents, with\ngoal-driven behaviors and dynamic adaptation capabilities, potentially\nrepresent a critical pathway toward artificial general intelligence. This\nsurvey systematically deconstructs LLM agent systems through a\nmethodology-centered taxonomy, linking architectural foundations, collaboration\nmechanisms, and evolutionary pathways. We unify fragmented research threads by\nrevealing fundamental connections between agent design principles and their\nemergent behaviors in complex environments. Our work provides a unified\narchitectural perspective, examining how agents are constructed, how they\ncollaborate, and how they evolve over time, while also addressing evaluation\nmethodologies, tool applications, practical challenges, and diverse application\ndomains. By surveying the latest developments in this rapidly evolving field,\nwe offer researchers a structured taxonomy for understanding LLM agents and\nidentify promising directions for future research. The collection is available\nat https://github.com/luo-junyu/Awesome-Agent-Papers."
                },
                "authors": [
                    {
                        "name": "Junyu Luo"
                    },
                    {
                        "name": "Weizhi Zhang"
                    },
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Yusheng Zhao"
                    },
                    {
                        "name": "Junwei Yang"
                    },
                    {
                        "name": "Yiyang Gu"
                    },
                    {
                        "name": "Bohan Wu"
                    },
                    {
                        "name": "Binqi Chen"
                    },
                    {
                        "name": "Ziyue Qiao"
                    },
                    {
                        "name": "Qingqing Long"
                    },
                    {
                        "name": "Rongcheng Tu"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Wei Ju"
                    },
                    {
                        "name": "Zhiping Xiao"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Meng Xiao"
                    },
                    {
                        "name": "Chenwu Liu"
                    },
                    {
                        "name": "Jingyang Yuan"
                    },
                    {
                        "name": "Shichang Zhang"
                    },
                    {
                        "name": "Yiqiao Jin"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Xian Wu"
                    },
                    {
                        "name": "Hanqing Zhao"
                    },
                    {
                        "name": "Dacheng Tao"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Ming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhang"
                },
                "author": "Ming Zhang",
                "arxiv_comment": "329 papers surveyed, resources are at\n  https://github.com/luo-junyu/Awesome-Agent-Papers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21459v1",
                "updated": "2025-03-27T12:49:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    49,
                    9,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T12:49:09Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    49,
                    9,
                    3,
                    86,
                    0
                ],
                "title": "RoadSocial: A Diverse VideoQA Dataset and Benchmark for Road Event\n  Understanding from Social Video Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoadSocial: A Diverse VideoQA Dataset and Benchmark for Road Event\n  Understanding from Social Video Narratives"
                },
                "summary": "We introduce RoadSocial, a large-scale, diverse VideoQA dataset tailored for\ngeneric road event understanding from social media narratives. Unlike existing\ndatasets limited by regional bias, viewpoint bias and expert-driven\nannotations, RoadSocial captures the global complexity of road events with\nvaried geographies, camera viewpoints (CCTV, handheld, drones) and rich social\ndiscourse. Our scalable semi-automatic annotation framework leverages Text LLMs\nand Video LLMs to generate comprehensive question-answer pairs across 12\nchallenging QA tasks, pushing the boundaries of road event understanding.\nRoadSocial is derived from social media videos spanning 14M frames and 414K\nsocial comments, resulting in a dataset with 13.2K videos, 674 tags and 260K\nhigh-quality QA pairs. We evaluate 18 Video LLMs (open-source and proprietary,\ndriving-specific and general-purpose) on our road event understanding\nbenchmark. We also demonstrate RoadSocial's utility in improving road event\nunderstanding capabilities of general-purpose Video LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RoadSocial, a large-scale, diverse VideoQA dataset tailored for\ngeneric road event understanding from social media narratives. Unlike existing\ndatasets limited by regional bias, viewpoint bias and expert-driven\nannotations, RoadSocial captures the global complexity of road events with\nvaried geographies, camera viewpoints (CCTV, handheld, drones) and rich social\ndiscourse. Our scalable semi-automatic annotation framework leverages Text LLMs\nand Video LLMs to generate comprehensive question-answer pairs across 12\nchallenging QA tasks, pushing the boundaries of road event understanding.\nRoadSocial is derived from social media videos spanning 14M frames and 414K\nsocial comments, resulting in a dataset with 13.2K videos, 674 tags and 260K\nhigh-quality QA pairs. We evaluate 18 Video LLMs (open-source and proprietary,\ndriving-specific and general-purpose) on our road event understanding\nbenchmark. We also demonstrate RoadSocial's utility in improving road event\nunderstanding capabilities of general-purpose Video LLMs."
                },
                "authors": [
                    {
                        "name": "Chirag Parikh"
                    },
                    {
                        "name": "Deepti Rawat"
                    },
                    {
                        "name": "Rakshitha R. T."
                    },
                    {
                        "name": "Tathagata Ghosh"
                    },
                    {
                        "name": "Ravi Kiran Sarvadevabhatla"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Kiran Sarvadevabhatla"
                },
                "author": "Ravi Kiran Sarvadevabhatla",
                "arxiv_comment": "Accepted at CVPR 2025; Project Page: https://roadsocial.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20519v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20519v2",
                "updated": "2025-03-27T12:39:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    39,
                    55,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-26T13:00:51Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    0,
                    51,
                    2,
                    85,
                    0
                ],
                "title": "MAR-3D: Progressive Masked Auto-regressor for High-Resolution 3D\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAR-3D: Progressive Masked Auto-regressor for High-Resolution 3D\n  Generation"
                },
                "summary": "Recent advances in auto-regressive transformers have revolutionized\ngenerative modeling across different domains, from language processing to\nvisual generation, demonstrating remarkable capabilities. However, applying\nthese advances to 3D generation presents three key challenges: the unordered\nnature of 3D data conflicts with sequential next-token prediction paradigm,\nconventional vector quantization approaches incur substantial compression loss\nwhen applied to 3D meshes, and the lack of efficient scaling strategies for\nhigher resolution latent prediction. To address these challenges, we introduce\nMAR-3D, which integrates a pyramid variational autoencoder with a cascaded\nmasked auto-regressive transformer (Cascaded MAR) for progressive latent\nupscaling in the continuous space. Our architecture employs random masking\nduring training and auto-regressive denoising in random order during inference,\nnaturally accommodating the unordered property of 3D latent tokens.\nAdditionally, we propose a cascaded training strategy with condition\naugmentation that enables efficiently up-scale the latent token resolution with\nfast convergence. Extensive experiments demonstrate that MAR-3D not only\nachieves superior performance and generalization capabilities compared to\nexisting methods but also exhibits enhanced scaling capabilities compared to\njoint distribution modeling approaches (e.g., diffusion transformers).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in auto-regressive transformers have revolutionized\ngenerative modeling across different domains, from language processing to\nvisual generation, demonstrating remarkable capabilities. However, applying\nthese advances to 3D generation presents three key challenges: the unordered\nnature of 3D data conflicts with sequential next-token prediction paradigm,\nconventional vector quantization approaches incur substantial compression loss\nwhen applied to 3D meshes, and the lack of efficient scaling strategies for\nhigher resolution latent prediction. To address these challenges, we introduce\nMAR-3D, which integrates a pyramid variational autoencoder with a cascaded\nmasked auto-regressive transformer (Cascaded MAR) for progressive latent\nupscaling in the continuous space. Our architecture employs random masking\nduring training and auto-regressive denoising in random order during inference,\nnaturally accommodating the unordered property of 3D latent tokens.\nAdditionally, we propose a cascaded training strategy with condition\naugmentation that enables efficiently up-scale the latent token resolution with\nfast convergence. Extensive experiments demonstrate that MAR-3D not only\nachieves superior performance and generalization capabilities compared to\nexisting methods but also exhibits enhanced scaling capabilities compared to\njoint distribution modeling approaches (e.g., diffusion transformers)."
                },
                "authors": [
                    {
                        "name": "Jinnan Chen"
                    },
                    {
                        "name": "Lingting Zhu"
                    },
                    {
                        "name": "Zeyu Hu"
                    },
                    {
                        "name": "Shengju Qian"
                    },
                    {
                        "name": "Yugang Chen"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Gim Hee Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gim Hee Lee"
                },
                "author": "Gim Hee Lee",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20519v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20519v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06289v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06289v3",
                "updated": "2025-03-27T12:38:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    38,
                    46,
                    3,
                    86,
                    0
                ],
                "published": "2024-02-09T09:58:35Z",
                "published_parsed": [
                    2024,
                    2,
                    9,
                    9,
                    58,
                    35,
                    4,
                    40,
                    0
                ],
                "title": "FedMIA: An Effective Membership Inference Attack Exploiting \"All for\n  One\" Principle in Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedMIA: An Effective Membership Inference Attack Exploiting \"All for\n  One\" Principle in Federated Learning"
                },
                "summary": "Federated Learning (FL) is a promising approach for training machine learning\nmodels on decentralized data while preserving privacy. However, privacy risks,\nparticularly Membership Inference Attacks (MIAs), which aim to determine\nwhether a specific data point belongs to a target client's training set, remain\na significant concern. Existing methods for implementing MIAs in FL primarily\nanalyze updates from the target client, focusing on metrics such as loss,\ngradient norm, and gradient difference. However, these methods fail to leverage\nupdates from non-target clients, potentially underutilizing available\ninformation. In this paper, we first formulate a one-tailed likelihood-ratio\nhypothesis test based on the likelihood of updates from non-target clients.\nBuilding upon this formulation, we introduce a three-step Membership Inference\nAttack (MIA) method, called FedMIA, which follows the \"all for one\"--leveraging\nupdates from all clients across multiple communication rounds to enhance MIA\neffectiveness. Both theoretical analysis and extensive experimental results\ndemonstrate that FedMIA outperforms existing MIAs in both classification and\ngenerative tasks. Additionally, it can be integrated as an extension to\nexisting methods and is robust against various defense strategies, Non-IID\ndata, and different federated structures. Our code is available in\nhttps://github.com/Liar-Mask/FedMIA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is a promising approach for training machine learning\nmodels on decentralized data while preserving privacy. However, privacy risks,\nparticularly Membership Inference Attacks (MIAs), which aim to determine\nwhether a specific data point belongs to a target client's training set, remain\na significant concern. Existing methods for implementing MIAs in FL primarily\nanalyze updates from the target client, focusing on metrics such as loss,\ngradient norm, and gradient difference. However, these methods fail to leverage\nupdates from non-target clients, potentially underutilizing available\ninformation. In this paper, we first formulate a one-tailed likelihood-ratio\nhypothesis test based on the likelihood of updates from non-target clients.\nBuilding upon this formulation, we introduce a three-step Membership Inference\nAttack (MIA) method, called FedMIA, which follows the \"all for one\"--leveraging\nupdates from all clients across multiple communication rounds to enhance MIA\neffectiveness. Both theoretical analysis and extensive experimental results\ndemonstrate that FedMIA outperforms existing MIAs in both classification and\ngenerative tasks. Additionally, it can be integrated as an extension to\nexisting methods and is robust against various defense strategies, Non-IID\ndata, and different federated structures. Our code is available in\nhttps://github.com/Liar-Mask/FedMIA."
                },
                "authors": [
                    {
                        "name": "Gongxi Zhu"
                    },
                    {
                        "name": "Donghao Li"
                    },
                    {
                        "name": "Hanlin Gu"
                    },
                    {
                        "name": "Yuan Yao"
                    },
                    {
                        "name": "Lixin Fan"
                    },
                    {
                        "name": "Yuxing Han"
                    }
                ],
                "author_detail": {
                    "name": "Yuxing Han"
                },
                "author": "Yuxing Han",
                "arxiv_comment": "14 pages, 6 figures; Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.06289v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06289v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20313v2",
                "updated": "2025-03-27T12:13:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    13,
                    46,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-26T08:25:12Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    8,
                    25,
                    12,
                    2,
                    85,
                    0
                ],
                "title": "TileLink: Generating Efficient Compute-Communication Overlapping Kernels\n  using Tile-Centric Primitives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TileLink: Generating Efficient Compute-Communication Overlapping Kernels\n  using Tile-Centric Primitives"
                },
                "summary": "Large deep learning models have achieved state-of-the-art performance in a\nwide range of tasks. These models often necessitate distributed systems for\nefficient training and inference. The fundamental building blocks for\ndistributed model execution are intra-layer parallel operators. The most\neffective approach to enhancing the performance of intra-layer parallel\noperators involves overlapping computation with communication. The overlapping\ncan be achieved through either operator decomposition or kernel fusion. While\ndecomposing operators is straightforward to implement, it often results in\nsuboptimal performance. On the other hand, fusing communication kernels with\ncompute kernels demands significant expertise and is error-prone.\n  In this paper, we propose TileLink to enable efficient compilation and\ngeneration of overlapped compute-communication kernels. TileLink is composed of\nfrontend and backend. In the frontend, TileLink decouples the design space of\ncommunication and computation, linking these two parts via tile-centric\nprimitives. In the backend, TileLink translates these primitives into low-level\ncommunication instructions, integrating the communication and computation\ncomponents to achieve overlapped execution. In experiments, TileLink achieves\nfrom $1.17\\times$ to $20.76\\times$ speedup to non-overlapping baseline and\nachieves performance comparable to state-of-the-art overlapping libraries on\nGPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large deep learning models have achieved state-of-the-art performance in a\nwide range of tasks. These models often necessitate distributed systems for\nefficient training and inference. The fundamental building blocks for\ndistributed model execution are intra-layer parallel operators. The most\neffective approach to enhancing the performance of intra-layer parallel\noperators involves overlapping computation with communication. The overlapping\ncan be achieved through either operator decomposition or kernel fusion. While\ndecomposing operators is straightforward to implement, it often results in\nsuboptimal performance. On the other hand, fusing communication kernels with\ncompute kernels demands significant expertise and is error-prone.\n  In this paper, we propose TileLink to enable efficient compilation and\ngeneration of overlapped compute-communication kernels. TileLink is composed of\nfrontend and backend. In the frontend, TileLink decouples the design space of\ncommunication and computation, linking these two parts via tile-centric\nprimitives. In the backend, TileLink translates these primitives into low-level\ncommunication instructions, integrating the communication and computation\ncomponents to achieve overlapped execution. In experiments, TileLink achieves\nfrom $1.17\\times$ to $20.76\\times$ speedup to non-overlapping baseline and\nachieves performance comparable to state-of-the-art overlapping libraries on\nGPUs."
                },
                "authors": [
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Jin Fang"
                    },
                    {
                        "name": "Xuegui Zheng"
                    },
                    {
                        "name": "Qi Hou"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Ziheng Jiang"
                    },
                    {
                        "name": "Dongyang Wang"
                    },
                    {
                        "name": "Jianxi Ye"
                    },
                    {
                        "name": "Haibin Lin"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Xin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Liu"
                },
                "author": "Xin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21422v1",
                "updated": "2025-03-27T12:10:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    10,
                    15,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T12:10:15Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    10,
                    15,
                    3,
                    86,
                    0
                ],
                "title": "From Deep Learning to LLMs: A survey of AI in Quantitative Investment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Deep Learning to LLMs: A survey of AI in Quantitative Investment"
                },
                "summary": "Quantitative investment (quant) is an emerging, technology-driven approach in\nasset management, increasingy shaped by advancements in artificial\nintelligence. Recent advances in deep learning and large language models (LLMs)\nfor quant finance have improved predictive modeling and enabled agent-based\nautomation, suggesting a potential paradigm shift in this field. In this\nsurvey, taking alpha strategy as a representative example, we explore how AI\ncontributes to the quantitative investment pipeline. We first examine the early\nstage of quant research, centered on human-crafted features and traditional\nstatistical models with an established alpha pipeline. We then discuss the rise\nof deep learning, which enabled scalable modeling across the entire pipeline\nfrom data processing to order execution. Building on this, we highlight the\nemerging role of LLMs in extending AI beyond prediction, empowering autonomous\nagents to process unstructured data, generate alphas, and support\nself-iterative workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantitative investment (quant) is an emerging, technology-driven approach in\nasset management, increasingy shaped by advancements in artificial\nintelligence. Recent advances in deep learning and large language models (LLMs)\nfor quant finance have improved predictive modeling and enabled agent-based\nautomation, suggesting a potential paradigm shift in this field. In this\nsurvey, taking alpha strategy as a representative example, we explore how AI\ncontributes to the quantitative investment pipeline. We first examine the early\nstage of quant research, centered on human-crafted features and traditional\nstatistical models with an established alpha pipeline. We then discuss the rise\nof deep learning, which enabled scalable modeling across the entire pipeline\nfrom data processing to order execution. Building on this, we highlight the\nemerging role of LLMs in extending AI beyond prediction, empowering autonomous\nagents to process unstructured data, generate alphas, and support\nself-iterative workflows."
                },
                "authors": [
                    {
                        "name": "Bokai Cao"
                    },
                    {
                        "name": "Saizhuo Wang"
                    },
                    {
                        "name": "Xinyi Lin"
                    },
                    {
                        "name": "Xiaojun Wu"
                    },
                    {
                        "name": "Haohan Zhang"
                    },
                    {
                        "name": "Lionel M. Ni"
                    },
                    {
                        "name": "Jian Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jian Guo"
                },
                "author": "Jian Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21419v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21419v2",
                "updated": "2025-03-28T09:44:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    9,
                    44,
                    42,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-27T12:09:04Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    9,
                    4,
                    3,
                    86,
                    0
                ],
                "title": "Neuroplasticity in Artificial Intelligence -- An Overview and\n  Inspirations on Drop In & Out Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuroplasticity in Artificial Intelligence -- An Overview and\n  Inspirations on Drop In & Out Learning"
                },
                "summary": "Artificial Intelligence (AI) has achieved new levels of performance and\nspread in public usage with the rise of deep neural networks (DNNs). Initially\ninspired by human neurons and their connections, NNs have become the foundation\nof AI models for many advanced architectures. However, some of the most\nintegral processes in the human brain, particularly neurogenesis and\nneuroplasticity in addition to the more spread neuroapoptosis have largely been\nignored in DNN architecture design. Instead, contemporary AI development\npredominantly focuses on constructing advanced frameworks, such as large\nlanguage models, which retain a static structure of neural connections during\ntraining and inference. In this light, we explore how neurogenesis,\nneuroapoptosis, and neuroplasticity can inspire future AI advances.\nSpecifically, we examine analogous activities in artificial NNs, introducing\nthe concepts of ``dropin'' for neurogenesis and revisiting ``dropout'' and\nstructural pruning for neuroapoptosis. We additionally suggest neuroplasticity\ncombining the two for future large NNs in ``life-long learning'' settings\nfollowing the biological inspiration. We conclude by advocating for greater\nresearch efforts in this interdisciplinary domain and identifying promising\ndirections for future exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) has achieved new levels of performance and\nspread in public usage with the rise of deep neural networks (DNNs). Initially\ninspired by human neurons and their connections, NNs have become the foundation\nof AI models for many advanced architectures. However, some of the most\nintegral processes in the human brain, particularly neurogenesis and\nneuroplasticity in addition to the more spread neuroapoptosis have largely been\nignored in DNN architecture design. Instead, contemporary AI development\npredominantly focuses on constructing advanced frameworks, such as large\nlanguage models, which retain a static structure of neural connections during\ntraining and inference. In this light, we explore how neurogenesis,\nneuroapoptosis, and neuroplasticity can inspire future AI advances.\nSpecifically, we examine analogous activities in artificial NNs, introducing\nthe concepts of ``dropin'' for neurogenesis and revisiting ``dropout'' and\nstructural pruning for neuroapoptosis. We additionally suggest neuroplasticity\ncombining the two for future large NNs in ``life-long learning'' settings\nfollowing the biological inspiration. We conclude by advocating for greater\nresearch efforts in this interdisciplinary domain and identifying promising\ndirections for future exploration."
                },
                "authors": [
                    {
                        "name": "Yupei Li"
                    },
                    {
                        "name": "Manuel Milling"
                    },
                    {
                        "name": "BjÃ¶rn W. Schuller"
                    }
                ],
                "author_detail": {
                    "name": "BjÃ¶rn W. Schuller"
                },
                "author": "BjÃ¶rn W. Schuller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21419v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21419v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21411v1",
                "updated": "2025-03-27T11:56:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    56,
                    27,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T11:56:27Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    56,
                    27,
                    3,
                    86,
                    0
                ],
                "title": "Exploring the Roles of Large Language Models in Reshaping Transportation\n  Systems: A Survey, Framework, and Roadmap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Roles of Large Language Models in Reshaping Transportation\n  Systems: A Survey, Framework, and Roadmap"
                },
                "summary": "Modern transportation systems face pressing challenges due to increasing\ndemand, dynamic environments, and heterogeneous information integration. The\nrapid evolution of Large Language Models (LLMs) offers transformative potential\nto address these challenges. Extensive knowledge and high-level capabilities\nderived from pretraining evolve the default role of LLMs as text generators to\nbecome versatile, knowledge-driven task solvers for intelligent transportation\nsystems. This survey first presents LLM4TR, a novel conceptual framework that\nsystematically categorizes the roles of LLMs in transportation into four\nsynergetic dimensions: information processors, knowledge encoders, component\ngenerators, and decision facilitators. Through a unified taxonomy, we\nsystematically elucidate how LLMs bridge fragmented data pipelines, enhance\npredictive analytics, simulate human-like reasoning, and enable closed-loop\ninteractions across sensing, learning, modeling, and managing tasks in\ntransportation systems. For each role, our review spans diverse applications,\nfrom traffic prediction and autonomous driving to safety analytics and urban\nmobility optimization, highlighting how emergent capabilities of LLMs such as\nin-context learning and step-by-step reasoning can enhance the operation and\nmanagement of transportation systems. We further curate practical guidance,\nincluding available resources and computational guidelines, to support\nreal-world deployment. By identifying challenges in existing LLM-based\nsolutions, this survey charts a roadmap for advancing LLM-driven transportation\nresearch, positioning LLMs as central actors in the next generation of\ncyber-physical-social mobility ecosystems. Online resources can be found in the\nproject page: https://github.com/tongnie/awesome-llm4tr.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern transportation systems face pressing challenges due to increasing\ndemand, dynamic environments, and heterogeneous information integration. The\nrapid evolution of Large Language Models (LLMs) offers transformative potential\nto address these challenges. Extensive knowledge and high-level capabilities\nderived from pretraining evolve the default role of LLMs as text generators to\nbecome versatile, knowledge-driven task solvers for intelligent transportation\nsystems. This survey first presents LLM4TR, a novel conceptual framework that\nsystematically categorizes the roles of LLMs in transportation into four\nsynergetic dimensions: information processors, knowledge encoders, component\ngenerators, and decision facilitators. Through a unified taxonomy, we\nsystematically elucidate how LLMs bridge fragmented data pipelines, enhance\npredictive analytics, simulate human-like reasoning, and enable closed-loop\ninteractions across sensing, learning, modeling, and managing tasks in\ntransportation systems. For each role, our review spans diverse applications,\nfrom traffic prediction and autonomous driving to safety analytics and urban\nmobility optimization, highlighting how emergent capabilities of LLMs such as\nin-context learning and step-by-step reasoning can enhance the operation and\nmanagement of transportation systems. We further curate practical guidance,\nincluding available resources and computational guidelines, to support\nreal-world deployment. By identifying challenges in existing LLM-based\nsolutions, this survey charts a roadmap for advancing LLM-driven transportation\nresearch, positioning LLMs as central actors in the next generation of\ncyber-physical-social mobility ecosystems. Online resources can be found in the\nproject page: https://github.com/tongnie/awesome-llm4tr."
                },
                "authors": [
                    {
                        "name": "Tong Nie"
                    },
                    {
                        "name": "Jian Sun"
                    },
                    {
                        "name": "Wei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ma"
                },
                "author": "Wei Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06352v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06352v2",
                "updated": "2025-03-27T11:53:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    53,
                    23,
                    3,
                    86,
                    0
                ],
                "published": "2025-02-10T11:05:18Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    11,
                    5,
                    18,
                    0,
                    41,
                    0
                ],
                "title": "LANTERN++: Enhancing Relaxed Speculative Decoding with Static Tree\n  Drafting for Visual Auto-regressive Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LANTERN++: Enhancing Relaxed Speculative Decoding with Static Tree\n  Drafting for Visual Auto-regressive Models"
                },
                "summary": "Speculative decoding has been widely used to accelerate auto-regressive (AR)\ntext generation. However, its effectiveness for visual AR models remains\nlimited due to token selection ambiguity, where multiple tokens share similarly\nlow probabilities and thus reduce acceptance rates. Recently, relaxed\nspeculative decoding with dynamic tree drafting was proposed to mitigate this\nambiguity, demonstrating promising results in accelerating visual AR models.\nHowever, we observe that token selection ambiguity still negatively affects\ndynamic tree drafting, resulting in shallow draft trees and limited\nacceleration. To overcome this issue, we introduce LANTERN++, a refined\nframework that integrates static tree drafting with a tailored relaxed\nacceptance condition, allowing drafts to be selected independently of\nlow-confidence predictions. This enables the acceptance of deeper sequences,\nimproving decoding efficiency while preserving image quality. Extensive\nexperiments on state-of-the-art visual AR models demonstrate that LANTERN++\nsignificantly accelerates inference, achieving up to $\\mathbf{\\times 2.56}$\nspeedup over standard AR decoding while maintaining high image quality. The\ncode is publicly available at https://github.com/jadohu/LANTERN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding has been widely used to accelerate auto-regressive (AR)\ntext generation. However, its effectiveness for visual AR models remains\nlimited due to token selection ambiguity, where multiple tokens share similarly\nlow probabilities and thus reduce acceptance rates. Recently, relaxed\nspeculative decoding with dynamic tree drafting was proposed to mitigate this\nambiguity, demonstrating promising results in accelerating visual AR models.\nHowever, we observe that token selection ambiguity still negatively affects\ndynamic tree drafting, resulting in shallow draft trees and limited\nacceleration. To overcome this issue, we introduce LANTERN++, a refined\nframework that integrates static tree drafting with a tailored relaxed\nacceptance condition, allowing drafts to be selected independently of\nlow-confidence predictions. This enables the acceptance of deeper sequences,\nimproving decoding efficiency while preserving image quality. Extensive\nexperiments on state-of-the-art visual AR models demonstrate that LANTERN++\nsignificantly accelerates inference, achieving up to $\\mathbf{\\times 2.56}$\nspeedup over standard AR decoding while maintaining high image quality. The\ncode is publicly available at https://github.com/jadohu/LANTERN."
                },
                "authors": [
                    {
                        "name": "Sihwan Park"
                    },
                    {
                        "name": "Doohyuk Jang"
                    },
                    {
                        "name": "Sungyub Kim"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Eunho Yang"
                    }
                ],
                "author_detail": {
                    "name": "Eunho Yang"
                },
                "author": "Eunho Yang",
                "arxiv_comment": "ICLR 2025 Workshop at SCOPE (Oral), 16 pages, 5 figures, short paper\n  (6 pages exclude reference and appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06352v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06352v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21408v1",
                "updated": "2025-03-27T11:52:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    52,
                    8,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T11:52:08Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    52,
                    8,
                    3,
                    86,
                    0
                ],
                "title": "VALLR: Visual ASR Language Model for Lip Reading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VALLR: Visual ASR Language Model for Lip Reading"
                },
                "summary": "Lip Reading, or Visual Automatic Speech Recognition (V-ASR), is a complex\ntask requiring the interpretation of spoken language exclusively from visual\ncues, primarily lip movements and facial expressions. This task is especially\nchallenging due to the absence of auditory information and the inherent\nambiguity when visually distinguishing phonemes that have overlapping visemes\nwhere different phonemes appear identical on the lips. Current methods\ntypically attempt to predict words or characters directly from these visual\ncues, but this approach frequently encounters high error rates due to\ncoarticulation effects and viseme ambiguity. We propose a novel two-stage,\nphoneme-centric framework for Visual Automatic Speech Recognition (V-ASR) that\naddresses these longstanding challenges. First, our model predicts a compact\nsequence of phonemes from visual inputs using a Video Transformer with a CTC\nhead, thereby reducing the task complexity and achieving robust speaker\ninvariance. This phoneme output then serves as the input to a fine-tuned Large\nLanguage Model (LLM), which reconstructs coherent words and sentences by\nleveraging broader linguistic context. Unlike existing methods that either\npredict words directly-often faltering on visually similar phonemes-or rely on\nlarge-scale multimodal pre-training, our approach explicitly encodes\nintermediate linguistic structure while remaining highly data efficient. We\ndemonstrate state-of-the-art performance on two challenging datasets, LRS2 and\nLRS3, where our method achieves significant reductions in Word Error Rate (WER)\nachieving a SOTA WER of 18.7 on LRS3 despite using 99.4% less labelled data\nthan the next best approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lip Reading, or Visual Automatic Speech Recognition (V-ASR), is a complex\ntask requiring the interpretation of spoken language exclusively from visual\ncues, primarily lip movements and facial expressions. This task is especially\nchallenging due to the absence of auditory information and the inherent\nambiguity when visually distinguishing phonemes that have overlapping visemes\nwhere different phonemes appear identical on the lips. Current methods\ntypically attempt to predict words or characters directly from these visual\ncues, but this approach frequently encounters high error rates due to\ncoarticulation effects and viseme ambiguity. We propose a novel two-stage,\nphoneme-centric framework for Visual Automatic Speech Recognition (V-ASR) that\naddresses these longstanding challenges. First, our model predicts a compact\nsequence of phonemes from visual inputs using a Video Transformer with a CTC\nhead, thereby reducing the task complexity and achieving robust speaker\ninvariance. This phoneme output then serves as the input to a fine-tuned Large\nLanguage Model (LLM), which reconstructs coherent words and sentences by\nleveraging broader linguistic context. Unlike existing methods that either\npredict words directly-often faltering on visually similar phonemes-or rely on\nlarge-scale multimodal pre-training, our approach explicitly encodes\nintermediate linguistic structure while remaining highly data efficient. We\ndemonstrate state-of-the-art performance on two challenging datasets, LRS2 and\nLRS3, where our method achieves significant reductions in Word Error Rate (WER)\nachieving a SOTA WER of 18.7 on LRS3 despite using 99.4% less labelled data\nthan the next best approach."
                },
                "authors": [
                    {
                        "name": "Marshall Thomas"
                    },
                    {
                        "name": "Edward Fish"
                    },
                    {
                        "name": "Richard Bowden"
                    }
                ],
                "author_detail": {
                    "name": "Richard Bowden"
                },
                "author": "Richard Bowden",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03708v3",
                "updated": "2025-03-27T11:46:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    46,
                    22,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-05T17:59:19Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "title": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach"
                },
                "summary": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights are available at\nhttps://github.com/ali-vilab/CDT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights are available at\nhttps://github.com/ali-vilab/CDT."
                },
                "authors": [
                    {
                        "name": "Nianzu Yang"
                    },
                    {
                        "name": "Pandeng Li"
                    },
                    {
                        "name": "Liming Zhao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chen-Wei Xie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17258v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17258v3",
                "updated": "2025-03-27T11:41:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    41,
                    54,
                    3,
                    86,
                    0
                ],
                "published": "2024-08-30T12:56:17Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    12,
                    56,
                    17,
                    4,
                    243,
                    0
                ],
                "title": "Joint Estimation and Prediction of City-wide Delivery Demand: A Large\n  Language Model Empowered Graph-based Learning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Estimation and Prediction of City-wide Delivery Demand: A Large\n  Language Model Empowered Graph-based Learning Approach"
                },
                "summary": "The proliferation of e-commerce and urbanization has significantly\nintensified delivery operations in urban areas, boosting the volume and\ncomplexity of delivery demand. Data-driven predictive methods, especially those\nutilizing machine learning techniques, have emerged to handle these\ncomplexities in urban delivery demand management problems. One particularly\npressing issue that has yet to be sufficiently addressed is the joint\nestimation and prediction of city-wide delivery demand, as well as the\ngeneralization of the model to new cities. To this end, we formulate this\nproblem as a transferable graph-based spatiotemporal learning task. First, an\nindividual-collective message-passing neural network model is formalized to\ncapture the interaction between demand patterns of associated regions. Second,\nby exploiting recent advances in large language models (LLMs), we extract\ngeneral geospatial knowledge encodings from the unstructured locational data\nusing the embedding generated by LLMs. Last, to encourage the cross-city\ngeneralization of the model, we integrate the encoding into the demand\npredictor in a transferable way. Comprehensive empirical evaluation results on\ntwo real-world delivery datasets, including eight cities in China and the US,\ndemonstrate that our model significantly outperforms state-of-the-art baselines\nin accuracy, efficiency, and transferability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of e-commerce and urbanization has significantly\nintensified delivery operations in urban areas, boosting the volume and\ncomplexity of delivery demand. Data-driven predictive methods, especially those\nutilizing machine learning techniques, have emerged to handle these\ncomplexities in urban delivery demand management problems. One particularly\npressing issue that has yet to be sufficiently addressed is the joint\nestimation and prediction of city-wide delivery demand, as well as the\ngeneralization of the model to new cities. To this end, we formulate this\nproblem as a transferable graph-based spatiotemporal learning task. First, an\nindividual-collective message-passing neural network model is formalized to\ncapture the interaction between demand patterns of associated regions. Second,\nby exploiting recent advances in large language models (LLMs), we extract\ngeneral geospatial knowledge encodings from the unstructured locational data\nusing the embedding generated by LLMs. Last, to encourage the cross-city\ngeneralization of the model, we integrate the encoding into the demand\npredictor in a transferable way. Comprehensive empirical evaluation results on\ntwo real-world delivery datasets, including eight cities in China and the US,\ndemonstrate that our model significantly outperforms state-of-the-art baselines\nin accuracy, efficiency, and transferability."
                },
                "authors": [
                    {
                        "name": "Tong Nie"
                    },
                    {
                        "name": "Junlin He"
                    },
                    {
                        "name": "Yuewen Mei"
                    },
                    {
                        "name": "Guoyang Qin"
                    },
                    {
                        "name": "Guilong Li"
                    },
                    {
                        "name": "Jian Sun"
                    },
                    {
                        "name": "Wei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ma"
                },
                "author": "Wei Ma",
                "arxiv_doi": "10.1016/j.tre.2025.104075",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.tre.2025.104075",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.17258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17258v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Transportation Research Part E: Logistics and Transportation\n  Review, 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21393v1",
                "updated": "2025-03-27T11:35:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    35,
                    40,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T11:35:40Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    35,
                    40,
                    3,
                    86,
                    0
                ],
                "title": "An evaluation of LLMs and Google Translate for translation of selected\n  Indian languages via sentiment and semantic analyses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An evaluation of LLMs and Google Translate for translation of selected\n  Indian languages via sentiment and semantic analyses"
                },
                "summary": "Large Language models (LLMs) have been prominent for language translation,\nincluding low-resource languages. There has been limited study about the\nassessment of the quality of translations generated by LLMs, including Gemini,\nGPT and Google Translate. In this study, we address this limitation by using\nsemantic and sentiment analysis of selected LLMs for Indian languages,\nincluding Sanskrit, Telugu and Hindi. We select prominent texts that have been\nwell translated by experts and use LLMs to generate their translations to\nEnglish, and then we provide a comparison with selected expert (human)\ntranslations. Our findings suggest that while LLMs have made significant\nprogress in translation accuracy, challenges remain in preserving sentiment and\nsemantic integrity, especially in figurative and philosophical contexts. The\nsentiment analysis revealed that GPT-4o and GPT-3.5 are better at preserving\nthe sentiments for the Bhagavad Gita (Sanskrit-English) translations when\ncompared to Google Translate. We observed a similar trend for the case of Tamas\n(Hindi-English) and Maha P (Telugu-English) translations. GPT-4o performs\nsimilarly to GPT-3.5 in the translation in terms of sentiments for the three\nlanguages. We found that LLMs are generally better at translation for capturing\nsentiments when compared to Google Translate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language models (LLMs) have been prominent for language translation,\nincluding low-resource languages. There has been limited study about the\nassessment of the quality of translations generated by LLMs, including Gemini,\nGPT and Google Translate. In this study, we address this limitation by using\nsemantic and sentiment analysis of selected LLMs for Indian languages,\nincluding Sanskrit, Telugu and Hindi. We select prominent texts that have been\nwell translated by experts and use LLMs to generate their translations to\nEnglish, and then we provide a comparison with selected expert (human)\ntranslations. Our findings suggest that while LLMs have made significant\nprogress in translation accuracy, challenges remain in preserving sentiment and\nsemantic integrity, especially in figurative and philosophical contexts. The\nsentiment analysis revealed that GPT-4o and GPT-3.5 are better at preserving\nthe sentiments for the Bhagavad Gita (Sanskrit-English) translations when\ncompared to Google Translate. We observed a similar trend for the case of Tamas\n(Hindi-English) and Maha P (Telugu-English) translations. GPT-4o performs\nsimilarly to GPT-3.5 in the translation in terms of sentiments for the three\nlanguages. We found that LLMs are generally better at translation for capturing\nsentiments when compared to Google Translate."
                },
                "authors": [
                    {
                        "name": "Rohitash Chandra"
                    },
                    {
                        "name": "Aryan Chaudhary"
                    },
                    {
                        "name": "Yeshwanth Rayavarapu"
                    }
                ],
                "author_detail": {
                    "name": "Yeshwanth Rayavarapu"
                },
                "author": "Yeshwanth Rayavarapu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08356v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08356v3",
                "updated": "2025-03-27T11:31:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    31,
                    39,
                    3,
                    86,
                    0
                ],
                "published": "2025-02-12T12:39:51Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    39,
                    51,
                    2,
                    43,
                    0
                ],
                "title": "Systematic Knowledge Injection into Large Language Models via Diverse\n  Augmentation for Domain-Specific RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Knowledge Injection into Large Language Models via Diverse\n  Augmentation for Domain-Specific RAG"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a prominent method for\nincorporating domain knowledge into Large Language Models (LLMs). While RAG\nenhances response relevance by incorporating retrieved domain knowledge in the\ncontext, retrieval errors can still lead to hallucinations and incorrect\nanswers. To recover from retriever failures, domain knowledge is injected by\nfine-tuning the model to generate the correct response, even in the case of\nretrieval errors. However, we observe that without systematic knowledge\naugmentation, fine-tuned LLMs may memorize new information but still fail to\nextract relevant domain knowledge, leading to poor performance. In this work,\nwe present a novel framework that significantly enhances the fine-tuning\nprocess by augmenting the training data in two ways -- context augmentation and\nknowledge paraphrasing. In context augmentation, we create multiple training\nsamples for a given QA pair by varying the relevance of the retrieved\ninformation, teaching the model when to ignore and when to rely on retrieved\ncontent. In knowledge paraphrasing, we fine-tune with multiple answers to the\nsame question, enabling LLMs to better internalize specialized knowledge. To\nmitigate catastrophic forgetting due to fine-tuning, we add a domain-specific\nidentifier to a question and also utilize a replay buffer containing general QA\npairs. Experimental results demonstrate the efficacy of our method over\nexisting techniques, achieving up to 10\\% relative gain in token-level recall\nwhile preserving the LLM's generalization capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a prominent method for\nincorporating domain knowledge into Large Language Models (LLMs). While RAG\nenhances response relevance by incorporating retrieved domain knowledge in the\ncontext, retrieval errors can still lead to hallucinations and incorrect\nanswers. To recover from retriever failures, domain knowledge is injected by\nfine-tuning the model to generate the correct response, even in the case of\nretrieval errors. However, we observe that without systematic knowledge\naugmentation, fine-tuned LLMs may memorize new information but still fail to\nextract relevant domain knowledge, leading to poor performance. In this work,\nwe present a novel framework that significantly enhances the fine-tuning\nprocess by augmenting the training data in two ways -- context augmentation and\nknowledge paraphrasing. In context augmentation, we create multiple training\nsamples for a given QA pair by varying the relevance of the retrieved\ninformation, teaching the model when to ignore and when to rely on retrieved\ncontent. In knowledge paraphrasing, we fine-tune with multiple answers to the\nsame question, enabling LLMs to better internalize specialized knowledge. To\nmitigate catastrophic forgetting due to fine-tuning, we add a domain-specific\nidentifier to a question and also utilize a replay buffer containing general QA\npairs. Experimental results demonstrate the efficacy of our method over\nexisting techniques, achieving up to 10\\% relative gain in token-level recall\nwhile preserving the LLM's generalization capabilities."
                },
                "authors": [
                    {
                        "name": "Kushagra Bhushan"
                    },
                    {
                        "name": "Yatin Nandwani"
                    },
                    {
                        "name": "Dinesh Khandelwal"
                    },
                    {
                        "name": "Sonam Gupta"
                    },
                    {
                        "name": "Gaurav Pandey"
                    },
                    {
                        "name": "Dinesh Raghu"
                    },
                    {
                        "name": "Sachindra Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Sachindra Joshi"
                },
                "author": "Sachindra Joshi",
                "arxiv_comment": "22 pages, 14 tables, to be published in NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08356v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08356v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11441v2",
                "updated": "2025-03-27T11:29:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    29,
                    21,
                    3,
                    86,
                    0
                ],
                "published": "2025-01-20T12:29:09Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    12,
                    29,
                    9,
                    0,
                    20,
                    0
                ],
                "title": "Ontology Matching with Large Language Models and Prioritized Depth-First\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology Matching with Large Language Models and Prioritized Depth-First\n  Search"
                },
                "summary": "Ontology matching (OM) plays a key role in enabling data interoperability and\nknowledge sharing, but it remains challenging due to the need for large\ntraining datasets and limited vocabulary processing in machine learning\napproaches. Recently, methods based on Large Language Model (LLMs) have shown\ngreat promise in OM, particularly through the use of a retrieve-then-prompt\npipeline. In this approach, relevant target entities are first retrieved and\nthen used to prompt the LLM to predict the final matches. Despite their\npotential, these systems still present limited performance and high\ncomputational overhead. To address these issues, we introduce MILA, a novel\napproach that embeds a retrieve-identify-prompt pipeline within a prioritized\ndepth-first search (PDFS) strategy. This approach efficiently identifies a\nlarge number of semantic correspondences with high accuracy, limiting LLM\nrequests to only the most borderline cases. We evaluated MILA using the\nbiomedical challenge proposed in the 2023 and 2024 editions of the Ontology\nAlignment Evaluation Initiative. Our method achieved the highest F-Measure in\nfour of the five unsupervised tasks, outperforming state-of-the-art OM systems\nby up to 17%. It also performed better than or comparable to the leading\nsupervised OM systems. MILA further exhibited task-agnostic performance,\nremaining stable across all tasks and settings, while significantly reducing\nLLM requests. These findings highlight that high-performance LLM-based OM can\nbe achieved through a combination of programmed (PDFS), learned (embedding\nvectors), and prompting-based heuristics, without the need of domain-specific\nheuristics or fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology matching (OM) plays a key role in enabling data interoperability and\nknowledge sharing, but it remains challenging due to the need for large\ntraining datasets and limited vocabulary processing in machine learning\napproaches. Recently, methods based on Large Language Model (LLMs) have shown\ngreat promise in OM, particularly through the use of a retrieve-then-prompt\npipeline. In this approach, relevant target entities are first retrieved and\nthen used to prompt the LLM to predict the final matches. Despite their\npotential, these systems still present limited performance and high\ncomputational overhead. To address these issues, we introduce MILA, a novel\napproach that embeds a retrieve-identify-prompt pipeline within a prioritized\ndepth-first search (PDFS) strategy. This approach efficiently identifies a\nlarge number of semantic correspondences with high accuracy, limiting LLM\nrequests to only the most borderline cases. We evaluated MILA using the\nbiomedical challenge proposed in the 2023 and 2024 editions of the Ontology\nAlignment Evaluation Initiative. Our method achieved the highest F-Measure in\nfour of the five unsupervised tasks, outperforming state-of-the-art OM systems\nby up to 17%. It also performed better than or comparable to the leading\nsupervised OM systems. MILA further exhibited task-agnostic performance,\nremaining stable across all tasks and settings, while significantly reducing\nLLM requests. These findings highlight that high-performance LLM-based OM can\nbe achieved through a combination of programmed (PDFS), learned (embedding\nvectors), and prompting-based heuristics, without the need of domain-specific\nheuristics or fine-tuning."
                },
                "authors": [
                    {
                        "name": "Maria Taboada"
                    },
                    {
                        "name": "Diego Martinez"
                    },
                    {
                        "name": "Mohammed Arideh"
                    },
                    {
                        "name": "Rosa Mosquera"
                    }
                ],
                "author_detail": {
                    "name": "Rosa Mosquera"
                },
                "author": "Rosa Mosquera",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21383v1",
                "updated": "2025-03-27T11:25:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    25,
                    22,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T11:25:22Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    25,
                    22,
                    3,
                    86,
                    0
                ],
                "title": "Controlling Large Language Model with Latent Actions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling Large Language Model with Latent Actions"
                },
                "summary": "Adapting Large Language Models (LLMs) to downstream tasks using Reinforcement\nLearning (RL) has proven to be an effective approach. However, LLMs do not\ninherently define the structure of an agent for RL training, particularly in\nterms of defining the action space. This paper studies learning a compact\nlatent action space to enhance the controllability and exploration of RL for\nLLMs. We propose Controlling Large Language Models with Latent Actions (CoLA),\na framework that integrates a latent action space into pre-trained LLMs. We\napply CoLA to the Llama-3.1-8B model. Our experiments demonstrate that,\ncompared to RL with token-level actions, CoLA's latent action enables greater\nsemantic diversity in text generation. For enhancing downstream tasks, we show\nthat CoLA with RL achieves a score of 42.4 on the math500 benchmark, surpassing\nthe baseline score of 38.2, and reaches 68.2 when augmented with a Monte Carlo\nTree Search variant. Furthermore, CoLA with RL consistently improves\nperformance on agent-based tasks without degrading the pre-trained LLM's\ncapabilities, unlike the baseline. Finally, CoLA reduces computation time by\nhalf in tasks involving enhanced thinking prompts for LLMs by RL. These results\nhighlight CoLA's potential to advance RL-based adaptation of LLMs for\ndownstream applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Large Language Models (LLMs) to downstream tasks using Reinforcement\nLearning (RL) has proven to be an effective approach. However, LLMs do not\ninherently define the structure of an agent for RL training, particularly in\nterms of defining the action space. This paper studies learning a compact\nlatent action space to enhance the controllability and exploration of RL for\nLLMs. We propose Controlling Large Language Models with Latent Actions (CoLA),\na framework that integrates a latent action space into pre-trained LLMs. We\napply CoLA to the Llama-3.1-8B model. Our experiments demonstrate that,\ncompared to RL with token-level actions, CoLA's latent action enables greater\nsemantic diversity in text generation. For enhancing downstream tasks, we show\nthat CoLA with RL achieves a score of 42.4 on the math500 benchmark, surpassing\nthe baseline score of 38.2, and reaches 68.2 when augmented with a Monte Carlo\nTree Search variant. Furthermore, CoLA with RL consistently improves\nperformance on agent-based tasks without degrading the pre-trained LLM's\ncapabilities, unlike the baseline. Finally, CoLA reduces computation time by\nhalf in tasks involving enhanced thinking prompts for LLMs by RL. These results\nhighlight CoLA's potential to advance RL-based adaptation of LLMs for\ndownstream applications."
                },
                "authors": [
                    {
                        "name": "Chengxing Jia"
                    },
                    {
                        "name": "Ziniu Li"
                    },
                    {
                        "name": "Pengyuan Wang"
                    },
                    {
                        "name": "Yi-Chen Li"
                    },
                    {
                        "name": "Zhenyu Hou"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Yang Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Yu"
                },
                "author": "Yang Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21380v1",
                "updated": "2025-03-27T11:20:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    20,
                    17,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T11:20:17Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    20,
                    17,
                    3,
                    86,
                    0
                ],
                "title": "Challenging the Boundaries of Reasoning: An Olympiad-Level Math\n  Benchmark for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenging the Boundaries of Reasoning: An Olympiad-Level Math\n  Benchmark for Large Language Models"
                },
                "summary": "In recent years, the rapid development of large reasoning models has resulted\nin the saturation of existing benchmarks for evaluating mathematical reasoning,\nhighlighting the urgent need for more challenging and rigorous evaluation\nframeworks. To address this gap, we introduce OlymMATH, a novel Olympiad-level\nmathematical benchmark, designed to rigorously test the complex reasoning\ncapabilities of LLMs. OlymMATH features 200 meticulously curated problems, each\nmanually verified and available in parallel English and Chinese versions. The\nproblems are systematically organized into two distinct difficulty tiers: (1)\nAIME-level problems (easy) that establish a baseline for mathematical reasoning\nassessment, and (2) significantly more challenging problems (hard) designed to\npush the boundaries of current state-of-the-art models. In our benchmark, these\nproblems span four core mathematical fields, each including a verifiable\nnumerical solution to enable objective, rule-based evaluation. Empirical\nresults underscore the significant challenge presented by OlymMATH, with\nstate-of-the-art models including DeepSeek-R1 and OpenAI's o3-mini\ndemonstrating notably limited accuracy on the hard subset. Furthermore, the\nbenchmark facilitates comprehensive bilingual assessment of mathematical\nreasoning abilities-a critical dimension that remains largely unaddressed in\nmainstream mathematical reasoning benchmarks. We release the OlymMATH benchmark\nat the STILL project: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the rapid development of large reasoning models has resulted\nin the saturation of existing benchmarks for evaluating mathematical reasoning,\nhighlighting the urgent need for more challenging and rigorous evaluation\nframeworks. To address this gap, we introduce OlymMATH, a novel Olympiad-level\nmathematical benchmark, designed to rigorously test the complex reasoning\ncapabilities of LLMs. OlymMATH features 200 meticulously curated problems, each\nmanually verified and available in parallel English and Chinese versions. The\nproblems are systematically organized into two distinct difficulty tiers: (1)\nAIME-level problems (easy) that establish a baseline for mathematical reasoning\nassessment, and (2) significantly more challenging problems (hard) designed to\npush the boundaries of current state-of-the-art models. In our benchmark, these\nproblems span four core mathematical fields, each including a verifiable\nnumerical solution to enable objective, rule-based evaluation. Empirical\nresults underscore the significant challenge presented by OlymMATH, with\nstate-of-the-art models including DeepSeek-R1 and OpenAI's o3-mini\ndemonstrating notably limited accuracy on the hard subset. Furthermore, the\nbenchmark facilitates comprehensive bilingual assessment of mathematical\nreasoning abilities-a critical dimension that remains largely unaddressed in\nmainstream mathematical reasoning benchmarks. We release the OlymMATH benchmark\nat the STILL project: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs."
                },
                "authors": [
                    {
                        "name": "Haoxiang Sun"
                    },
                    {
                        "name": "Yingqian Min"
                    },
                    {
                        "name": "Zhipeng Chen"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Lei Fang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "Technical Report on Slow Thinking with LLMs: Evaluation Benchmark",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21365v1",
                "updated": "2025-03-27T10:56:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    10,
                    56,
                    53,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T10:56:53Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    10,
                    56,
                    53,
                    3,
                    86,
                    0
                ],
                "title": "CA+: Cognition Augmented Counselor Agent Framework for Long-term Dynamic\n  Client Engagement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CA+: Cognition Augmented Counselor Agent Framework for Long-term Dynamic\n  Client Engagement"
                },
                "summary": "Current AI counseling systems struggle with maintaining effective long-term\nclient engagement. Through formative research with counselors and a systematic\nliterature review, we identified five key design considerations for AI\ncounseling interactions. Based on these insights, we propose CA+, a Cognition\nAugmented counselor framework enhancing contextual understanding through three\ncomponents:\n  (1) Therapy Strategies Module: Implements hierarchical Goals-Session-Action\nplanning with bidirectional adaptation based on client feedback; (2)\nCommunication Form Module: Orchestrates parallel guidance and empathy pathways\nfor balanced therapeutic progress and emotional resonance; (3) Information\nManagement: Utilizes client profile and therapeutic knowledge databases for\ndynamic, context-aware interventions.\n  A three-day longitudinal study with 24 clients demonstrates CA+'s significant\nimprovements in client engagement, perceived empathy, and overall satisfaction\ncompared to a baseline system. Besides, two licensed counselors confirm its\nhigh professionalism. Our research demonstrates the potential for enhancing LLM\nengagement in psychological counseling dialogues through cognitive theory,\nwhich may inspire further innovations in computational interaction in the\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current AI counseling systems struggle with maintaining effective long-term\nclient engagement. Through formative research with counselors and a systematic\nliterature review, we identified five key design considerations for AI\ncounseling interactions. Based on these insights, we propose CA+, a Cognition\nAugmented counselor framework enhancing contextual understanding through three\ncomponents:\n  (1) Therapy Strategies Module: Implements hierarchical Goals-Session-Action\nplanning with bidirectional adaptation based on client feedback; (2)\nCommunication Form Module: Orchestrates parallel guidance and empathy pathways\nfor balanced therapeutic progress and emotional resonance; (3) Information\nManagement: Utilizes client profile and therapeutic knowledge databases for\ndynamic, context-aware interventions.\n  A three-day longitudinal study with 24 clients demonstrates CA+'s significant\nimprovements in client engagement, perceived empathy, and overall satisfaction\ncompared to a baseline system. Besides, two licensed counselors confirm its\nhigh professionalism. Our research demonstrates the potential for enhancing LLM\nengagement in psychological counseling dialogues through cognitive theory,\nwhich may inspire further innovations in computational interaction in the\nfuture."
                },
                "authors": [
                    {
                        "name": "Yuanrong Tang"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Tianhong Wang"
                    },
                    {
                        "name": "Chen Zhong"
                    },
                    {
                        "name": "Jiangtao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jiangtao Gong"
                },
                "author": "Jiangtao Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21364v2",
                "updated": "2025-03-28T04:58:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    4,
                    58,
                    11,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-27T10:55:36Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    10,
                    55,
                    36,
                    3,
                    86,
                    0
                ],
                "title": "LandMarkSystem Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LandMarkSystem Technical Report"
                },
                "summary": "3D reconstruction is vital for applications in autonomous driving, virtual\nreality, augmented reality, and the metaverse. Recent advancements such as\nNeural Radiance Fields(NeRF) and 3D Gaussian Splatting (3DGS) have transformed\nthe field, yet traditional deep learning frameworks struggle to meet the\nincreasing demands for scene quality and scale. This paper introduces\nLandMarkSystem, a novel computing framework designed to enhance multi-scale\nscene reconstruction and rendering. By leveraging a componentized model\nadaptation layer, LandMarkSystem supports various NeRF and 3DGS structures\nwhile optimizing computational efficiency through distributed parallel\ncomputing and model parameter offloading. Our system addresses the limitations\nof existing frameworks, providing dedicated operators for complex 3D sparse\ncomputations, thus facilitating efficient training and rapid inference over\nextensive scenes. Key contributions include a modular architecture, a dynamic\nloading strategy for limited resources, and proven capabilities across multiple\nrepresentative algorithms.This comprehensive solution aims to advance the\nefficiency and effectiveness of 3D reconstruction tasks.To facilitate further\nresearch and collaboration, the source code and documentation for the\nLandMarkSystem project are publicly available in an open-source repository,\naccessing the repository at: https://github.com/InternLandMark/LandMarkSystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D reconstruction is vital for applications in autonomous driving, virtual\nreality, augmented reality, and the metaverse. Recent advancements such as\nNeural Radiance Fields(NeRF) and 3D Gaussian Splatting (3DGS) have transformed\nthe field, yet traditional deep learning frameworks struggle to meet the\nincreasing demands for scene quality and scale. This paper introduces\nLandMarkSystem, a novel computing framework designed to enhance multi-scale\nscene reconstruction and rendering. By leveraging a componentized model\nadaptation layer, LandMarkSystem supports various NeRF and 3DGS structures\nwhile optimizing computational efficiency through distributed parallel\ncomputing and model parameter offloading. Our system addresses the limitations\nof existing frameworks, providing dedicated operators for complex 3D sparse\ncomputations, thus facilitating efficient training and rapid inference over\nextensive scenes. Key contributions include a modular architecture, a dynamic\nloading strategy for limited resources, and proven capabilities across multiple\nrepresentative algorithms.This comprehensive solution aims to advance the\nefficiency and effectiveness of 3D reconstruction tasks.To facilitate further\nresearch and collaboration, the source code and documentation for the\nLandMarkSystem project are publicly available in an open-source repository,\naccessing the repository at: https://github.com/InternLandMark/LandMarkSystem."
                },
                "authors": [
                    {
                        "name": "Zhenxiang Ma"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Miao Tao"
                    },
                    {
                        "name": "Yuanzhen Zhou"
                    },
                    {
                        "name": "Zeyu He"
                    },
                    {
                        "name": "Yuchang Zhang"
                    },
                    {
                        "name": "Rong Fu"
                    },
                    {
                        "name": "Hengjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Hengjie Li"
                },
                "author": "Hengjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21360v1",
                "updated": "2025-03-27T10:52:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    10,
                    52,
                    10,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T10:52:10Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    10,
                    52,
                    10,
                    3,
                    86,
                    0
                ],
                "title": "From User Preferences to Optimization Constraints Using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From User Preferences to Optimization Constraints Using Large Language\n  Models"
                },
                "summary": "This work explores using Large Language Models (LLMs) to translate user\npreferences into energy optimization constraints for home appliances. We\ndescribe a task where natural language user utterances are converted into\nformal constraints for smart appliances, within the broader context of a\nrenewable energy community (REC) and in the Italian scenario. We evaluate the\neffectiveness of various LLMs currently available for Italian in translating\nthese preferences resorting to classical zero-shot, one-shot, and few-shot\nlearning settings, using a pilot dataset of Italian user requests paired with\ncorresponding formal constraint representation. Our contributions include\nestablishing a baseline performance for this task, publicly releasing the\ndataset and code for further research, and providing insights on observed best\npractices and limitations of LLMs in this particular domain",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores using Large Language Models (LLMs) to translate user\npreferences into energy optimization constraints for home appliances. We\ndescribe a task where natural language user utterances are converted into\nformal constraints for smart appliances, within the broader context of a\nrenewable energy community (REC) and in the Italian scenario. We evaluate the\neffectiveness of various LLMs currently available for Italian in translating\nthese preferences resorting to classical zero-shot, one-shot, and few-shot\nlearning settings, using a pilot dataset of Italian user requests paired with\ncorresponding formal constraint representation. Our contributions include\nestablishing a baseline performance for this task, publicly releasing the\ndataset and code for further research, and providing insights on observed best\npractices and limitations of LLMs in this particular domain"
                },
                "authors": [
                    {
                        "name": "Manuela Sanguinetti"
                    },
                    {
                        "name": "Alessandra Perniciano"
                    },
                    {
                        "name": "Luca Zedda"
                    },
                    {
                        "name": "Andrea Loddo"
                    },
                    {
                        "name": "Cecilia Di Ruberto"
                    },
                    {
                        "name": "Maurizio Atzori"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Atzori"
                },
                "author": "Maurizio Atzori",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21358v1",
                "updated": "2025-03-27T10:50:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    10,
                    50,
                    5,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T10:50:05Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    10,
                    50,
                    5,
                    3,
                    86,
                    0
                ],
                "title": "Inference in stochastic differential equations using the Laplace\n  approximation: Demonstration and examples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference in stochastic differential equations using the Laplace\n  approximation: Demonstration and examples"
                },
                "summary": "We consider the problem of estimating states and parameters in a model based\non a system of coupled stochastic differential equations, based on noisy\ndiscrete-time data. Special attention is given to nonlinear dynamics and\nstate-dependent diffusivity, where transition densities are not available in\nclosed form. Our technique adds states between times of observations,\napproximates transition densities using, e.g., the Euler-Maruyama method and\neliminates unobserved states using the Laplace approximation. Using case\nstudies, we demonstrate that transition probabilities are well approximated,\nand that inference is computationally feasible. We discuss limitations and\npotential extensions of the method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of estimating states and parameters in a model based\non a system of coupled stochastic differential equations, based on noisy\ndiscrete-time data. Special attention is given to nonlinear dynamics and\nstate-dependent diffusivity, where transition densities are not available in\nclosed form. Our technique adds states between times of observations,\napproximates transition densities using, e.g., the Euler-Maruyama method and\neliminates unobserved states using the Laplace approximation. Using case\nstudies, we demonstrate that transition probabilities are well approximated,\nand that inference is computationally feasible. We discuss limitations and\npotential extensions of the method."
                },
                "authors": [
                    {
                        "name": "Uffe HÃ¸gsbro Thygesen"
                    },
                    {
                        "name": "Kasper Kristensen"
                    }
                ],
                "author_detail": {
                    "name": "Kasper Kristensen"
                },
                "author": "Kasper Kristensen",
                "arxiv_comment": "25 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "37M10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01877v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01877v2",
                "updated": "2025-03-27T10:38:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    10,
                    38,
                    45,
                    3,
                    86,
                    0
                ],
                "published": "2025-02-26T15:20:01Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    20,
                    1,
                    2,
                    57,
                    0
                ],
                "title": "Starjob: Dataset for LLM-Driven Job Shop Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Starjob: Dataset for LLM-Driven Job Shop Scheduling"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious domains, but their potential for solving combinatorial optimization\nproblems remains largely unexplored. In this paper, we investigate the\napplicability of LLMs to the Job Shop Scheduling Problem (JSSP), a classic\nchallenge in combinatorial optimization that requires efficient job allocation\nto machines to minimize makespan. To this end, we introduce Starjob, the first\nsupervised dataset for JSSP, comprising 130k instances specifically designed\nfor training LLMs. Leveraging this dataset, we fine-tune the LLaMA 8B 4-bit\nquantized model with the LoRA method to develop an end-to-end scheduling\napproach. Our evaluation on standard benchmarks demonstrates that the proposed\nLLM-based method not only surpasses traditional Priority Dispatching Rules\n(PDRs) but also achieves notable improvements over state-of-the-art neural\napproaches like L2D, with an average improvement of 15.36% on DMU and 7.85% on\nTaillard benchmarks. These results highlight the untapped potential of LLMs in\ntackling combinatorial optimization problems, paving the way for future\nadvancements in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious domains, but their potential for solving combinatorial optimization\nproblems remains largely unexplored. In this paper, we investigate the\napplicability of LLMs to the Job Shop Scheduling Problem (JSSP), a classic\nchallenge in combinatorial optimization that requires efficient job allocation\nto machines to minimize makespan. To this end, we introduce Starjob, the first\nsupervised dataset for JSSP, comprising 130k instances specifically designed\nfor training LLMs. Leveraging this dataset, we fine-tune the LLaMA 8B 4-bit\nquantized model with the LoRA method to develop an end-to-end scheduling\napproach. Our evaluation on standard benchmarks demonstrates that the proposed\nLLM-based method not only surpasses traditional Priority Dispatching Rules\n(PDRs) but also achieves notable improvements over state-of-the-art neural\napproaches like L2D, with an average improvement of 15.36% on DMU and 7.85% on\nTaillard benchmarks. These results highlight the untapped potential of LLMs in\ntackling combinatorial optimization problems, paving the way for future\nadvancements in this area."
                },
                "authors": [
                    {
                        "name": "Henrik Abgaryan"
                    },
                    {
                        "name": "Tristan Cazenave"
                    },
                    {
                        "name": "Ararat Harutyunyan"
                    }
                ],
                "author_detail": {
                    "name": "Ararat Harutyunyan"
                },
                "author": "Ararat Harutyunyan",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2408.06993",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01877v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01877v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21349v1",
                "updated": "2025-03-27T10:35:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    10,
                    35,
                    56,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T10:35:56Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    10,
                    35,
                    56,
                    3,
                    86,
                    0
                ],
                "title": "Fine-Tuning LLMs on Small Medical Datasets: Text Classification and\n  Normalization Effectiveness on Cardiology reports and Discharge records",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning LLMs on Small Medical Datasets: Text Classification and\n  Normalization Effectiveness on Cardiology reports and Discharge records"
                },
                "summary": "We investigate the effectiveness of fine-tuning large language models (LLMs)\non small medical datasets for text classification and named entity recognition\ntasks. Using a German cardiology report dataset and the i2b2 Smoking Challenge\ndataset, we demonstrate that fine-tuning small LLMs locally on limited training\ndata can improve performance achieving comparable results to larger models. Our\nexperiments show that fine-tuning improves performance on both tasks, with\nnotable gains observed with as few as 200-300 training examples. Overall, the\nstudy highlights the potential of task-specific fine-tuning of LLMs for\nautomating clinical workflows and efficiently extracting structured data from\nunstructured medical text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the effectiveness of fine-tuning large language models (LLMs)\non small medical datasets for text classification and named entity recognition\ntasks. Using a German cardiology report dataset and the i2b2 Smoking Challenge\ndataset, we demonstrate that fine-tuning small LLMs locally on limited training\ndata can improve performance achieving comparable results to larger models. Our\nexperiments show that fine-tuning improves performance on both tasks, with\nnotable gains observed with as few as 200-300 training examples. Overall, the\nstudy highlights the potential of task-specific fine-tuning of LLMs for\nautomating clinical workflows and efficiently extracting structured data from\nunstructured medical text."
                },
                "authors": [
                    {
                        "name": "Noah Losch"
                    },
                    {
                        "name": "Lucas Plagwitz"
                    },
                    {
                        "name": "Antonius BÃ¼scher"
                    },
                    {
                        "name": "Julian Varghese"
                    }
                ],
                "author_detail": {
                    "name": "Julian Varghese"
                },
                "author": "Julian Varghese",
                "arxiv_comment": "4 pages, 2 tables,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20578v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20578v3",
                "updated": "2025-03-28T02:53:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    2,
                    53,
                    43,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-26T14:25:01Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    25,
                    1,
                    2,
                    85,
                    0
                ],
                "title": "LLPut: Investigating Large Language Models for Bug Report-Based Input\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLPut: Investigating Large Language Models for Bug Report-Based Input\n  Generation"
                },
                "summary": "Failure-inducing inputs play a crucial role in diagnosing and analyzing\nsoftware bugs. Bug reports typically contain these inputs, which developers\nextract to facilitate debugging. Since bug reports are written in natural\nlanguage, prior research has leveraged various Natural Language Processing\n(NLP) techniques for automated input extraction. With the advent of Large\nLanguage Models (LLMs), an important research question arises: how effectively\ncan generative LLMs extract failure-inducing inputs from bug reports? In this\npaper, we propose LLPut, a technique to empirically evaluate the performance of\nthree open-source generative LLMs -- LLaMA, Qwen, and Qwen-Coder -- in\nextracting relevant inputs from bug reports. We conduct an experimental\nevaluation on a dataset of 206 bug reports to assess the accuracy and\neffectiveness of these models. Our findings provide insights into the\ncapabilities and limitations of generative LLMs in automated bug diagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Failure-inducing inputs play a crucial role in diagnosing and analyzing\nsoftware bugs. Bug reports typically contain these inputs, which developers\nextract to facilitate debugging. Since bug reports are written in natural\nlanguage, prior research has leveraged various Natural Language Processing\n(NLP) techniques for automated input extraction. With the advent of Large\nLanguage Models (LLMs), an important research question arises: how effectively\ncan generative LLMs extract failure-inducing inputs from bug reports? In this\npaper, we propose LLPut, a technique to empirically evaluate the performance of\nthree open-source generative LLMs -- LLaMA, Qwen, and Qwen-Coder -- in\nextracting relevant inputs from bug reports. We conduct an experimental\nevaluation on a dataset of 206 bug reports to assess the accuracy and\neffectiveness of these models. Our findings provide insights into the\ncapabilities and limitations of generative LLMs in automated bug diagnosis."
                },
                "authors": [
                    {
                        "name": "Alif Al Hasan"
                    },
                    {
                        "name": "Subarna Saha"
                    },
                    {
                        "name": "Mia Mohammad Imran"
                    },
                    {
                        "name": "Tarannum Shaila Zaman"
                    }
                ],
                "author_detail": {
                    "name": "Tarannum Shaila Zaman"
                },
                "author": "Tarannum Shaila Zaman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20578v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20578v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00493v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00493v2",
                "updated": "2025-03-27T10:30:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    10,
                    30,
                    42,
                    3,
                    86,
                    0
                ],
                "published": "2024-11-30T14:28:53Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    14,
                    28,
                    53,
                    5,
                    335,
                    0
                ],
                "title": "Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene\n  Understanding"
                },
                "summary": "The rapid advancement of Multimodal Large Language Models (MLLMs) has\nsignificantly impacted various multimodal tasks. However, these models face\nchallenges in tasks that require spatial understanding within 3D environments.\nEfforts to enhance MLLMs, such as incorporating point cloud features, have been\nmade, yet a considerable gap remains between the models' learned\nrepresentations and the inherent complexity of 3D scenes. This discrepancy\nlargely stems from the training of MLLMs on predominantly 2D data, which\nrestricts their effectiveness in comprehending 3D spaces. To address this\nissue, in this paper, we propose a novel generalist model, i.e., Video-3D LLM,\nfor 3D scene understanding. By treating 3D scenes as dynamic videos and\nincorporating 3D position encoding into these representations, our Video-3D LLM\naligns video representations with real-world spatial contexts more accurately.\nIn addition, we have implemented a maximum coverage sampling technique to\noptimize the trade-off between computational cost and performance. Extensive\nexperiments demonstrate that our model achieves state-of-the-art performance on\nseveral 3D scene understanding benchmarks, including ScanRefer, Multi3DRefer,\nScan2Cap, ScanQA, and SQA3D.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Multimodal Large Language Models (MLLMs) has\nsignificantly impacted various multimodal tasks. However, these models face\nchallenges in tasks that require spatial understanding within 3D environments.\nEfforts to enhance MLLMs, such as incorporating point cloud features, have been\nmade, yet a considerable gap remains between the models' learned\nrepresentations and the inherent complexity of 3D scenes. This discrepancy\nlargely stems from the training of MLLMs on predominantly 2D data, which\nrestricts their effectiveness in comprehending 3D spaces. To address this\nissue, in this paper, we propose a novel generalist model, i.e., Video-3D LLM,\nfor 3D scene understanding. By treating 3D scenes as dynamic videos and\nincorporating 3D position encoding into these representations, our Video-3D LLM\naligns video representations with real-world spatial contexts more accurately.\nIn addition, we have implemented a maximum coverage sampling technique to\noptimize the trade-off between computational cost and performance. Extensive\nexperiments demonstrate that our model achieves state-of-the-art performance on\nseveral 3D scene understanding benchmarks, including ScanRefer, Multi3DRefer,\nScan2Cap, ScanQA, and SQA3D."
                },
                "authors": [
                    {
                        "name": "Duo Zheng"
                    },
                    {
                        "name": "Shijia Huang"
                    },
                    {
                        "name": "Liwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liwei Wang"
                },
                "author": "Liwei Wang",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00493v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00493v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13744v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13744v3",
                "updated": "2025-03-27T10:27:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    10,
                    27,
                    56,
                    3,
                    86,
                    0
                ],
                "published": "2025-02-19T14:07:37Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    7,
                    37,
                    2,
                    50,
                    0
                ],
                "title": "The Risk-Neutral Equivalent Pricing of Model-Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Risk-Neutral Equivalent Pricing of Model-Uncertainty"
                },
                "summary": "Existing approaches to asset-pricing under model-uncertainty adapt classical\nutility-maximisation frameworks and seek theoretical comprehensiveness. We move\ntoward practice by considering binary model-uncertainties and by switching\nattention from 'preference' to 'constraints'. Economic asset-pricing in this\nsetting is found to decompose into the viable pricing of model-risk and of\nnon-model risk separately such that the former has a unique and intuitive\nrisk-neutral equivalent formulation with convenient properties. Its parameter,\na dynamically conserved constant of model-risk inference, allows an integrated\nrepresentation of ex-ante risk-pricing and bias, such that their ex-post\nprice-effects can be disentangled, through well-known price anomalies such as\nMomentum and Low-Risk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing approaches to asset-pricing under model-uncertainty adapt classical\nutility-maximisation frameworks and seek theoretical comprehensiveness. We move\ntoward practice by considering binary model-uncertainties and by switching\nattention from 'preference' to 'constraints'. Economic asset-pricing in this\nsetting is found to decompose into the viable pricing of model-risk and of\nnon-model risk separately such that the former has a unique and intuitive\nrisk-neutral equivalent formulation with convenient properties. Its parameter,\na dynamically conserved constant of model-risk inference, allows an integrated\nrepresentation of ex-ante risk-pricing and bias, such that their ex-post\nprice-effects can be disentangled, through well-known price anomalies such as\nMomentum and Low-Risk."
                },
                "authors": [
                    {
                        "name": "Ken Kangda Wren"
                    }
                ],
                "author_detail": {
                    "name": "Ken Kangda Wren"
                },
                "author": "Ken Kangda Wren",
                "arxiv_comment": "25 pages of main text, 13 pages of Appendix and Bibliography",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13744v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13744v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.MF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.MF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21335v1",
                "updated": "2025-03-27T10:13:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    10,
                    13,
                    41,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T10:13:41Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    10,
                    13,
                    41,
                    3,
                    86,
                    0
                ],
                "title": "A Low-Power Streaming Speech Enhancement Accelerator For Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Low-Power Streaming Speech Enhancement Accelerator For Edge Devices"
                },
                "summary": "Transformer-based speech enhancement models yield impressive results.\nHowever, their heterogeneous and complex structure restricts model compression\npotential, resulting in greater complexity and reduced hardware efficiency.\nAdditionally, these models are not tailored for streaming and low-power\napplications. Addressing these challenges, this paper proposes a low-power\nstreaming speech enhancement accelerator through model and hardware\noptimization. The proposed high performance model is optimized for hardware\nexecution with the co-design of model compression and target application, which\nreduces 93.9\\% of model size by the proposed domain-aware and streaming-aware\npruning techniques. The required latency is further reduced with batch\nnormalization-based transformers. Additionally, we employed softmax-free\nattention, complemented by an extra batch normalization, facilitating simpler\nhardware design. The tailored hardware accommodates these diverse computing\npatterns by breaking them down into element-wise multiplication and\naccumulation (MAC). This is achieved through a 1-D processing array, utilizing\nconfigurable SRAM addressing, thereby minimizing hardware complexities and\nsimplifying zero skipping. Using the TSMC 40nm CMOS process, the final\nimplementation requires merely 207.8K gates and 53.75KB SRAM. It consumes only\n8.08 mW for real-time inference at a 62.5MHz frequency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based speech enhancement models yield impressive results.\nHowever, their heterogeneous and complex structure restricts model compression\npotential, resulting in greater complexity and reduced hardware efficiency.\nAdditionally, these models are not tailored for streaming and low-power\napplications. Addressing these challenges, this paper proposes a low-power\nstreaming speech enhancement accelerator through model and hardware\noptimization. The proposed high performance model is optimized for hardware\nexecution with the co-design of model compression and target application, which\nreduces 93.9\\% of model size by the proposed domain-aware and streaming-aware\npruning techniques. The required latency is further reduced with batch\nnormalization-based transformers. Additionally, we employed softmax-free\nattention, complemented by an extra batch normalization, facilitating simpler\nhardware design. The tailored hardware accommodates these diverse computing\npatterns by breaking them down into element-wise multiplication and\naccumulation (MAC). This is achieved through a 1-D processing array, utilizing\nconfigurable SRAM addressing, thereby minimizing hardware complexities and\nsimplifying zero skipping. Using the TSMC 40nm CMOS process, the final\nimplementation requires merely 207.8K gates and 53.75KB SRAM. It consumes only\n8.08 mW for real-time inference at a 62.5MHz frequency."
                },
                "authors": [
                    {
                        "name": "Ci-Hao Wu"
                    },
                    {
                        "name": "Tian-Sheuan Chang"
                    }
                ],
                "author_detail": {
                    "name": "Tian-Sheuan Chang"
                },
                "author": "Tian-Sheuan Chang",
                "arxiv_journal_ref": "in IEEE Open Journal of Circuits and Systems, vol. 5, pp. 128-140,\n  2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21330v1",
                "updated": "2025-03-27T10:10:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    10,
                    10,
                    30,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T10:10:30Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    10,
                    10,
                    30,
                    3,
                    86,
                    0
                ],
                "title": "Large Language Models for Traffic and Transportation Research:\n  Methodologies, State of the Art, and Future Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Traffic and Transportation Research:\n  Methodologies, State of the Art, and Future Opportunities"
                },
                "summary": "The rapid rise of Large Language Models (LLMs) is transforming traffic and\ntransportation research, with significant advancements emerging between the\nyears 2023 and 2025 -- a period marked by the inception and swift growth of\nadopting and adapting LLMs for various traffic and transportation applications.\nHowever, despite these significant advancements, a systematic review and\nsynthesis of the existing studies remain lacking. To address this gap, this\npaper provides a comprehensive review of the methodologies and applications of\nLLMs in traffic and transportation, highlighting their ability to process\nunstructured textual data to advance transportation research. We explore key\napplications, including autonomous driving, travel behavior prediction, and\ngeneral transportation-related queries, alongside methodologies such as zero-\nor few-shot learning, prompt engineering, and fine-tuning. Our analysis\nidentifies critical research gaps. From the methodological perspective, many\nresearch gaps can be addressed by integrating LLMs with existing tools and\nrefining LLM architectures. From the application perspective, we identify\nnumerous opportunities for LLMs to tackle a variety of traffic and\ntransportation challenges, building upon existing research. By synthesizing\nthese findings, this review not only clarifies the current state of LLM\nadoption and adaptation in traffic and transportation but also proposes future\nresearch directions, paving the way for smarter and more sustainable\ntransportation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid rise of Large Language Models (LLMs) is transforming traffic and\ntransportation research, with significant advancements emerging between the\nyears 2023 and 2025 -- a period marked by the inception and swift growth of\nadopting and adapting LLMs for various traffic and transportation applications.\nHowever, despite these significant advancements, a systematic review and\nsynthesis of the existing studies remain lacking. To address this gap, this\npaper provides a comprehensive review of the methodologies and applications of\nLLMs in traffic and transportation, highlighting their ability to process\nunstructured textual data to advance transportation research. We explore key\napplications, including autonomous driving, travel behavior prediction, and\ngeneral transportation-related queries, alongside methodologies such as zero-\nor few-shot learning, prompt engineering, and fine-tuning. Our analysis\nidentifies critical research gaps. From the methodological perspective, many\nresearch gaps can be addressed by integrating LLMs with existing tools and\nrefining LLM architectures. From the application perspective, we identify\nnumerous opportunities for LLMs to tackle a variety of traffic and\ntransportation challenges, building upon existing research. By synthesizing\nthese findings, this review not only clarifies the current state of LLM\nadoption and adaptation in traffic and transportation but also proposes future\nresearch directions, paving the way for smarter and more sustainable\ntransportation systems."
                },
                "authors": [
                    {
                        "name": "Yimo Yan"
                    },
                    {
                        "name": "Yejia Liao"
                    },
                    {
                        "name": "Guanhao Xu"
                    },
                    {
                        "name": "Ruili Yao"
                    },
                    {
                        "name": "Huiying Fan"
                    },
                    {
                        "name": "Jingran Sun"
                    },
                    {
                        "name": "Xia Wang"
                    },
                    {
                        "name": "Jonathan Sprinkle"
                    },
                    {
                        "name": "Ziyan An"
                    },
                    {
                        "name": "Meiyi Ma"
                    },
                    {
                        "name": "Xi Cheng"
                    },
                    {
                        "name": "Tong Liu"
                    },
                    {
                        "name": "Zemian Ke"
                    },
                    {
                        "name": "Bo Zou"
                    },
                    {
                        "name": "Matthew Barth"
                    },
                    {
                        "name": "Yong-Hong Kuo"
                    }
                ],
                "author_detail": {
                    "name": "Yong-Hong Kuo"
                },
                "author": "Yong-Hong Kuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21319v1",
                "updated": "2025-03-27T09:58:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    58,
                    7,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T09:58:07Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    58,
                    7,
                    3,
                    86,
                    0
                ],
                "title": "A $B-$anomaly motivated $Z^\\prime$ boson at the energy and precision\n  frontiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A $B-$anomaly motivated $Z^\\prime$ boson at the energy and precision\n  frontiers"
                },
                "summary": "TeV-scale $Z^\\prime$ bosons with family-dependent couplings can explain some\nanomalies inferred from $B-$meson measurements of processes involving the $b\n\\rightarrow s l^+l^-$ transition. A $Z^\\prime$ originating from\nkinetically-mixed spontaneously broken $U(1)_{B_3-L_2}$ gauge symmetry has been\nshown to greatly ameliorate global fits~\\cite{Allanach:2024ozu} in a\n`flavour-preferred' region of parameter space. We provide an exploration of\nthis region at the high luminosity (HL-)LHC with particular attention to which\nsignals could be verified across different discovery modes. Even if the HL-LHC\ndoes not discover the $Z^\\prime$ boson in a resonant di-lepton channel, a\nFCC-ee $Z-$pole run would detect oblique corrections to the electroweak\nprecision observables (EWPOs). Changes due to $Z^\\prime$-induced non-oblique\ncorrections are unlikely to be detected, to within experimental precision. In\nany case, the extended discovery potential offered by a 100 TeV $pp-$collider\nwould afford sensitivity to the entire flavour-preferred region and enable a\nfine-grained and forensic analysis of the~model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeV-scale $Z^\\prime$ bosons with family-dependent couplings can explain some\nanomalies inferred from $B-$meson measurements of processes involving the $b\n\\rightarrow s l^+l^-$ transition. A $Z^\\prime$ originating from\nkinetically-mixed spontaneously broken $U(1)_{B_3-L_2}$ gauge symmetry has been\nshown to greatly ameliorate global fits~\\cite{Allanach:2024ozu} in a\n`flavour-preferred' region of parameter space. We provide an exploration of\nthis region at the high luminosity (HL-)LHC with particular attention to which\nsignals could be verified across different discovery modes. Even if the HL-LHC\ndoes not discover the $Z^\\prime$ boson in a resonant di-lepton channel, a\nFCC-ee $Z-$pole run would detect oblique corrections to the electroweak\nprecision observables (EWPOs). Changes due to $Z^\\prime$-induced non-oblique\ncorrections are unlikely to be detected, to within experimental precision. In\nany case, the extended discovery potential offered by a 100 TeV $pp-$collider\nwould afford sensitivity to the entire flavour-preferred region and enable a\nfine-grained and forensic analysis of the~model."
                },
                "authors": [
                    {
                        "name": "Ben Allanach"
                    },
                    {
                        "name": "Christoph Englert"
                    },
                    {
                        "name": "Wrishik Naskar"
                    }
                ],
                "author_detail": {
                    "name": "Wrishik Naskar"
                },
                "author": "Wrishik Naskar",
                "arxiv_comment": "17 Pages, 8 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21317v1",
                "updated": "2025-03-27T09:56:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    56,
                    1,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T09:56:01Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    56,
                    1,
                    3,
                    86,
                    0
                ],
                "title": "Surface guided analysis of breast changes during post-operative\n  radiotherapy by using a functional map framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surface guided analysis of breast changes during post-operative\n  radiotherapy by using a functional map framework"
                },
                "summary": "The treatment of breast cancer using radiotherapy involves uncertainties\nregarding breast positioning. As the studies progress, more is known about the\nexpected breast positioning errors, which are taken into account in the\nPlanning Target Volume (PTV) in the form of the margin around the clinical\ntarget volume. However, little is known about the non-rigid deformations of the\nbreast in the course of radiotherapy, which is a non-negligible factor to the\ntreatment. Purpose: Taking into account such inter-fractional breast\ndeformations would help develop a promising future direction, such as\npatient-specific adjustable irradiation plannings. Methods: In this study, we\ndevelop a geometric approach to analyze inter-fractional breast deformation\nthroughout the radiotherapy treatment. Our data consists of 3D surface scans of\npatients acquired during radiotherapy sessions using a handheld scanner. We\nadapt functional map framework to compute inter-and intra-patient non-rigid\ncorrespondences, which are then used to analyze intra-patient changes and\ninter-patient variability. Results: The qualitative shape collection analysis\nhighlight deformations in the contralateral breast and armpit areas, along with\npositioning shifts on the head or abdominal regions. We also perform extrinsic\nanalysis, where we align surface acquisitions of the treated breast with the\nCT-derived skin surface to assess displacements and volume changes in the\ntreated area. On average, displacements within the treated breast exhibit\namplitudes of 1-2 mm across sessions, with higher values observed at the time\nof the 25 th irradiation session. Volume changes, inferred from surface\nvariations, reached up to 10%, with values ranging between 2% and 5% over the\ncourse of treatment. Conclusions: We propose a comprehensive workflow for\nanalyzing and modeling breast deformations during radiotherapy using surface\nacquisitions, incorporating a novel inter-collection shape matching approach to\nmodel shape variability within a i shared space across multiple patient shape\ncollections. We validate our method using 3D surface data acquired from\npatients during External Beam Radiotherapy (EBRT) sessions, demonstrating its\neffectiveness. The clinical trial data used in this paper is registered under\nthe ClinicalTrials.gov ID NCT03801850.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The treatment of breast cancer using radiotherapy involves uncertainties\nregarding breast positioning. As the studies progress, more is known about the\nexpected breast positioning errors, which are taken into account in the\nPlanning Target Volume (PTV) in the form of the margin around the clinical\ntarget volume. However, little is known about the non-rigid deformations of the\nbreast in the course of radiotherapy, which is a non-negligible factor to the\ntreatment. Purpose: Taking into account such inter-fractional breast\ndeformations would help develop a promising future direction, such as\npatient-specific adjustable irradiation plannings. Methods: In this study, we\ndevelop a geometric approach to analyze inter-fractional breast deformation\nthroughout the radiotherapy treatment. Our data consists of 3D surface scans of\npatients acquired during radiotherapy sessions using a handheld scanner. We\nadapt functional map framework to compute inter-and intra-patient non-rigid\ncorrespondences, which are then used to analyze intra-patient changes and\ninter-patient variability. Results: The qualitative shape collection analysis\nhighlight deformations in the contralateral breast and armpit areas, along with\npositioning shifts on the head or abdominal regions. We also perform extrinsic\nanalysis, where we align surface acquisitions of the treated breast with the\nCT-derived skin surface to assess displacements and volume changes in the\ntreated area. On average, displacements within the treated breast exhibit\namplitudes of 1-2 mm across sessions, with higher values observed at the time\nof the 25 th irradiation session. Volume changes, inferred from surface\nvariations, reached up to 10%, with values ranging between 2% and 5% over the\ncourse of treatment. Conclusions: We propose a comprehensive workflow for\nanalyzing and modeling breast deformations during radiotherapy using surface\nacquisitions, incorporating a novel inter-collection shape matching approach to\nmodel shape variability within a i shared space across multiple patient shape\ncollections. We validate our method using 3D surface data acquired from\npatients during External Beam Radiotherapy (EBRT) sessions, demonstrating its\neffectiveness. The clinical trial data used in this paper is registered under\nthe ClinicalTrials.gov ID NCT03801850."
                },
                "authors": [
                    {
                        "name": "Pierre Galmiche"
                    },
                    {
                        "name": "Hyewon Seo"
                    },
                    {
                        "name": "Yvan Pin"
                    },
                    {
                        "name": "Philippe Meyer"
                    },
                    {
                        "name": "Georges NoÃ«l"
                    },
                    {
                        "name": "Michel de Mathelin"
                    }
                ],
                "author_detail": {
                    "name": "Michel de Mathelin"
                },
                "arxiv_affiliation": "ICube",
                "author": "Michel de Mathelin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21313v1",
                "updated": "2025-03-27T09:45:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    45,
                    9,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T09:45:09Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    45,
                    9,
                    3,
                    86,
                    0
                ],
                "title": "HORT: Monocular Hand-held Objects Reconstruction with Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HORT: Monocular Hand-held Objects Reconstruction with Transformers"
                },
                "summary": "Reconstructing hand-held objects in 3D from monocular images remains a\nsignificant challenge in computer vision. Most existing approaches rely on\nimplicit 3D representations, which produce overly smooth reconstructions and\nare time-consuming to generate explicit 3D shapes. While more recent methods\ndirectly reconstruct point clouds with diffusion models, the multi-step\ndenoising makes high-resolution reconstruction inefficient. To address these\nlimitations, we propose a transformer-based model to efficiently reconstruct\ndense 3D point clouds of hand-held objects. Our method follows a coarse-to-fine\nstrategy, first generating a sparse point cloud from the image and\nprogressively refining it into a dense representation using pixel-aligned image\nfeatures. To enhance reconstruction accuracy, we integrate image features with\n3D hand geometry to jointly predict the object point cloud and its pose\nrelative to the hand. Our model is trained end-to-end for optimal performance.\nExperimental results on both synthetic and real datasets demonstrate that our\nmethod achieves state-of-the-art accuracy with much faster inference speed,\nwhile generalizing well to in-the-wild images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing hand-held objects in 3D from monocular images remains a\nsignificant challenge in computer vision. Most existing approaches rely on\nimplicit 3D representations, which produce overly smooth reconstructions and\nare time-consuming to generate explicit 3D shapes. While more recent methods\ndirectly reconstruct point clouds with diffusion models, the multi-step\ndenoising makes high-resolution reconstruction inefficient. To address these\nlimitations, we propose a transformer-based model to efficiently reconstruct\ndense 3D point clouds of hand-held objects. Our method follows a coarse-to-fine\nstrategy, first generating a sparse point cloud from the image and\nprogressively refining it into a dense representation using pixel-aligned image\nfeatures. To enhance reconstruction accuracy, we integrate image features with\n3D hand geometry to jointly predict the object point cloud and its pose\nrelative to the hand. Our model is trained end-to-end for optimal performance.\nExperimental results on both synthetic and real datasets demonstrate that our\nmethod achieves state-of-the-art accuracy with much faster inference speed,\nwhile generalizing well to in-the-wild images."
                },
                "authors": [
                    {
                        "name": "Zerui Chen"
                    },
                    {
                        "name": "Rolandos Alexandros Potamias"
                    },
                    {
                        "name": "Shizhe Chen"
                    },
                    {
                        "name": "Cordelia Schmid"
                    }
                ],
                "author_detail": {
                    "name": "Cordelia Schmid"
                },
                "author": "Cordelia Schmid",
                "arxiv_comment": "Project Page: https://zerchen.github.io/projects/hort.html",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15668v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15668v4",
                "updated": "2025-03-27T09:41:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    41,
                    1,
                    3,
                    86,
                    0
                ],
                "published": "2024-05-24T16:05:15Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    16,
                    5,
                    15,
                    4,
                    145,
                    0
                ],
                "title": "What Do You See? Enhancing Zero-Shot Image Classification with\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Do You See? Enhancing Zero-Shot Image Classification with\n  Multimodal Large Language Models"
                },
                "summary": "Large language models (LLMs) have been effectively used for many computer\nvision tasks, including image classification. In this paper, we present a\nsimple yet effective approach for zero-shot image classification using\nmultimodal LLMs. Using multimodal LLMs, we generate comprehensive textual\nrepresentations from input images. These textual representations are then\nutilized to generate fixed-dimensional features in a cross-modal embedding\nspace. Subsequently, these features are fused together to perform zero-shot\nclassification using a linear classifier. Our method does not require prompt\nengineering for each dataset; instead, we use a single, straightforward set of\nprompts across all datasets. We evaluated our method on several datasets and\nour results demonstrate its remarkable effectiveness, surpassing benchmark\naccuracy on multiple datasets. On average, for ten benchmarks, our method\nachieved an accuracy gain of 6.2 percentage points, with an increase of 6.8\npercentage points on the ImageNet dataset, compared to prior methods\nre-evaluated with the same setup. Our findings highlight the potential of\nmultimodal LLMs to enhance computer vision tasks such as zero-shot image\nclassification, offering a significant improvement over traditional methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been effectively used for many computer\nvision tasks, including image classification. In this paper, we present a\nsimple yet effective approach for zero-shot image classification using\nmultimodal LLMs. Using multimodal LLMs, we generate comprehensive textual\nrepresentations from input images. These textual representations are then\nutilized to generate fixed-dimensional features in a cross-modal embedding\nspace. Subsequently, these features are fused together to perform zero-shot\nclassification using a linear classifier. Our method does not require prompt\nengineering for each dataset; instead, we use a single, straightforward set of\nprompts across all datasets. We evaluated our method on several datasets and\nour results demonstrate its remarkable effectiveness, surpassing benchmark\naccuracy on multiple datasets. On average, for ten benchmarks, our method\nachieved an accuracy gain of 6.2 percentage points, with an increase of 6.8\npercentage points on the ImageNet dataset, compared to prior methods\nre-evaluated with the same setup. Our findings highlight the potential of\nmultimodal LLMs to enhance computer vision tasks such as zero-shot image\nclassification, offering a significant improvement over traditional methods."
                },
                "authors": [
                    {
                        "name": "Abdelrahman Abdelhamed"
                    },
                    {
                        "name": "Mahmoud Afifi"
                    },
                    {
                        "name": "Alec Go"
                    }
                ],
                "author_detail": {
                    "name": "Alec Go"
                },
                "author": "Alec Go",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15668v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15668v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21307v1",
                "updated": "2025-03-27T09:31:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    31,
                    35,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T09:31:35Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    31,
                    35,
                    3,
                    86,
                    0
                ],
                "title": "InternVL-X: Advancing and Accelerating InternVL Series with Efficient\n  Visual Token Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InternVL-X: Advancing and Accelerating InternVL Series with Efficient\n  Visual Token Compression"
                },
                "summary": "Most multimodal large language models (MLLMs) treat visual tokens as \"a\nsequence of text\", integrating them with text tokens into a large language\nmodel (LLM). However, a great quantity of visual tokens significantly increases\nthe demand for computational resources and time. In this paper, we propose\nInternVL-X, which outperforms the InternVL model in both performance and\nefficiency by incorporating three visual token compression methods. First, we\npropose a novel vision-language projector, PVTC. This component integrates\nadjacent visual embeddings to form a local query and utilizes the transformed\nCLS token as a global query, then performs point-to-region cross-attention\nthrough these local and global queries to more effectively convert visual\nfeatures. Second, we present a layer-wise visual token compression module,\nLVTC, which compresses tokens in the LLM shallow layers and then expands them\nthrough upsampling and residual connections in the deeper layers. This\nsignificantly enhances the model computational efficiency. Futhermore, we\npropose an efficient high resolution slicing method, RVTC, which dynamically\nadjusts the number of visual tokens based on image area or length filtering.\nRVTC greatly enhances training efficiency with only a slight reduction in\nperformance. By utilizing 20% or fewer visual tokens, InternVL-X achieves\nstate-of-the-art performance on 7 public MLLM benchmarks, and improves the\naverage metric by 2.34% across 12 tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most multimodal large language models (MLLMs) treat visual tokens as \"a\nsequence of text\", integrating them with text tokens into a large language\nmodel (LLM). However, a great quantity of visual tokens significantly increases\nthe demand for computational resources and time. In this paper, we propose\nInternVL-X, which outperforms the InternVL model in both performance and\nefficiency by incorporating three visual token compression methods. First, we\npropose a novel vision-language projector, PVTC. This component integrates\nadjacent visual embeddings to form a local query and utilizes the transformed\nCLS token as a global query, then performs point-to-region cross-attention\nthrough these local and global queries to more effectively convert visual\nfeatures. Second, we present a layer-wise visual token compression module,\nLVTC, which compresses tokens in the LLM shallow layers and then expands them\nthrough upsampling and residual connections in the deeper layers. This\nsignificantly enhances the model computational efficiency. Futhermore, we\npropose an efficient high resolution slicing method, RVTC, which dynamically\nadjusts the number of visual tokens based on image area or length filtering.\nRVTC greatly enhances training efficiency with only a slight reduction in\nperformance. By utilizing 20% or fewer visual tokens, InternVL-X achieves\nstate-of-the-art performance on 7 public MLLM benchmarks, and improves the\naverage metric by 2.34% across 12 tasks."
                },
                "authors": [
                    {
                        "name": "Dongchen Lu"
                    },
                    {
                        "name": "Yuyao Sun"
                    },
                    {
                        "name": "Zilu Zhang"
                    },
                    {
                        "name": "Leping Huang"
                    },
                    {
                        "name": "Jianliang Zeng"
                    },
                    {
                        "name": "Mao Shu"
                    },
                    {
                        "name": "Huo Cao"
                    }
                ],
                "author_detail": {
                    "name": "Huo Cao"
                },
                "author": "Huo Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13088v2",
                "updated": "2025-03-27T09:29:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    29,
                    43,
                    3,
                    86,
                    0
                ],
                "published": "2024-05-21T11:42:15Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    11,
                    42,
                    15,
                    1,
                    142,
                    0
                ],
                "title": "Combining Relevance and Magnitude for Resource-Aware DNN Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Relevance and Magnitude for Resource-Aware DNN Pruning"
                },
                "summary": "Pruning neural networks, i.e., removing some of their parameters whilst\nretaining their accuracy, is one of the main ways to reduce the latency of a\nmachine learning pipeline, especially in resource- and/or bandwidth-constrained\nscenarios. In this context, the pruning technique, i.e., how to choose the\nparameters to remove, is critical to the system performance. In this paper, we\npropose a novel pruning approach, called FlexRel and predicated upon combining\ntraining-time and inference-time information, namely, parameter magnitude and\nrelevance, in order to improve the resulting accuracy whilst saving both\ncomputational resources and bandwidth. Our performance evaluation shows that\nFlexRel is able to achieve higher pruning factors, saving over 35% bandwidth\nfor typical accuracy targets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning neural networks, i.e., removing some of their parameters whilst\nretaining their accuracy, is one of the main ways to reduce the latency of a\nmachine learning pipeline, especially in resource- and/or bandwidth-constrained\nscenarios. In this context, the pruning technique, i.e., how to choose the\nparameters to remove, is critical to the system performance. In this paper, we\npropose a novel pruning approach, called FlexRel and predicated upon combining\ntraining-time and inference-time information, namely, parameter magnitude and\nrelevance, in order to improve the resulting accuracy whilst saving both\ncomputational resources and bandwidth. Our performance evaluation shows that\nFlexRel is able to achieve higher pruning factors, saving over 35% bandwidth\nfor typical accuracy targets."
                },
                "authors": [
                    {
                        "name": "Carla Fabiana Chiasserini"
                    },
                    {
                        "name": "Francesco Malandrino"
                    },
                    {
                        "name": "Nuria Molner"
                    },
                    {
                        "name": "Zhiqiang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Zhao"
                },
                "author": "Zhiqiang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14222v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14222v2",
                "updated": "2025-03-27T09:29:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    29,
                    20,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-18T12:59:50Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    12,
                    59,
                    50,
                    1,
                    77,
                    0
                ],
                "title": "Stacked-Residual PINN for State Reconstruction of Hyperbolic Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stacked-Residual PINN for State Reconstruction of Hyperbolic Systems"
                },
                "summary": "In a more connected world, modeling multi-agent systems with hyperbolic\npartial differential equations (PDEs) offers a potential solution to the curse\nof dimensionality. However, classical control tools need adaptation for these\ncomplex systems. Physics-informed neural networks (PINNs) provide a powerful\nframework to fix this issue by inferring solutions to PDEs by embedding\ngoverning equations into the neural network. A major limitation of original\nPINNs is their inability to capture steep gradients and discontinuities in\nhyperbolic PDEs. This paper proposes a stacked residual PINN method enhanced\nwith a vanishing viscosity mechanism. Initially, a basic PINN with a small\nviscosity coefficient provides a stable, low-fidelity solution. Residual\ncorrection blocks with learnable scaling parameters then iteratively refine\nthis solution, progressively decreasing the viscosity coefficient to transition\nfrom parabolic to hyperbolic PDEs. Applying this method to traffic state\nreconstruction improved results by an order of magnitude in relative\n$\\mathcal{L}^2$ error, demonstrating its potential to accurately estimate\nsolutions where original PINNs struggle with instability and low fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a more connected world, modeling multi-agent systems with hyperbolic\npartial differential equations (PDEs) offers a potential solution to the curse\nof dimensionality. However, classical control tools need adaptation for these\ncomplex systems. Physics-informed neural networks (PINNs) provide a powerful\nframework to fix this issue by inferring solutions to PDEs by embedding\ngoverning equations into the neural network. A major limitation of original\nPINNs is their inability to capture steep gradients and discontinuities in\nhyperbolic PDEs. This paper proposes a stacked residual PINN method enhanced\nwith a vanishing viscosity mechanism. Initially, a basic PINN with a small\nviscosity coefficient provides a stable, low-fidelity solution. Residual\ncorrection blocks with learnable scaling parameters then iteratively refine\nthis solution, progressively decreasing the viscosity coefficient to transition\nfrom parabolic to hyperbolic PDEs. Applying this method to traffic state\nreconstruction improved results by an order of magnitude in relative\n$\\mathcal{L}^2$ error, demonstrating its potential to accurately estimate\nsolutions where original PINNs struggle with instability and low fidelity."
                },
                "authors": [
                    {
                        "name": "Katayoun Eshkofti"
                    },
                    {
                        "name": "Matthieu Barreau"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Barreau"
                },
                "author": "Matthieu Barreau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14222v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14222v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21298v1",
                "updated": "2025-03-27T09:24:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    24,
                    42,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T09:24:42Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    24,
                    42,
                    3,
                    86,
                    0
                ],
                "title": "G{Ã©}n{Ã©}ration de Matrices de Corr{Ã©}lation avec des Structures de\n  Graphe par Optimisation Convexe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "G{Ã©}n{Ã©}ration de Matrices de Corr{Ã©}lation avec des Structures de\n  Graphe par Optimisation Convexe"
                },
                "summary": "This work deals with the generation of theoretical correlation matrices with\nspecific sparsity patterns, associated to graph structures. We present a novel\napproach based on convex optimization, offering greater flexibility compared to\nexisting techniques, notably by controlling the mean of the entry distribution\nin the generated correlation matrices. This allows for the generation of\ncorrelation matrices that better represent realistic data and can be used to\nbenchmark statistical methods for graph inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work deals with the generation of theoretical correlation matrices with\nspecific sparsity patterns, associated to graph structures. We present a novel\napproach based on convex optimization, offering greater flexibility compared to\nexisting techniques, notably by controlling the mean of the entry distribution\nin the generated correlation matrices. This allows for the generation of\ncorrelation matrices that better represent realistic data and can be used to\nbenchmark statistical methods for graph inference."
                },
                "authors": [
                    {
                        "name": "Ali Fahkar"
                    },
                    {
                        "name": "KÃ©vin Polisano"
                    },
                    {
                        "name": "IrÃ¨ne Gannaz"
                    },
                    {
                        "name": "Sophie Achard"
                    }
                ],
                "author_detail": {
                    "name": "Sophie Achard"
                },
                "arxiv_affiliation": "STATIFY, LJK",
                "author": "Sophie Achard",
                "arxiv_comment": "in French language",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21297v1",
                "updated": "2025-03-27T09:24:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    24,
                    18,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T09:24:18Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    24,
                    18,
                    3,
                    86,
                    0
                ],
                "title": "MLDSE: Scaling Design Space Exploration Infrastructure for Multi-Level\n  Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLDSE: Scaling Design Space Exploration Infrastructure for Multi-Level\n  Hardware"
                },
                "summary": "To efficiently support large-scale NNs, multi-level hardware, leveraging\nadvanced integration and interconnection technologies, has emerged as a\npromising solution to counter the slowdown of Moore's law. However, the vast\ndesign space of such hardware, coupled with the complexity of their spatial\nhierarchies and organizations, introduces significant challenges for design\nspace exploration (DSE). Existing DSE tools, which rely on predefined hardware\ntemplates to explore parameters for specific architectures, fall short in\nexploring diverse organizations, spatial hierarchies, and architectural\npolymorphisms inherent in multi-level hardware. To address these limitations,\nwe present Multi-Level Design Space Exploror (MLDSE), a novel infrastructure\nfor domain-specific DSE of multi-level hardware. MLDSE introduces three key\ninnovations from three basic perspectives of DSE: 1) Modeling: MLDSE introduces\na hardware intermediate representation (IR) that can recursively model diverse\nmulti-level hardware with composable elements at various granularities. 2)\nMapping: MLDSE provides a comprehensive spatiotemporal mapping IR and mapping\nprimitives, facilitating the mapping strategy exploration on multi-level\nhardware, especially synchronization and cross-level communication; 3)\nSimulation: MLDSE supports universal simulator generation based on task-level\nevent-driven simulation mechanism. It features a hardware-consistent scheduling\nalgorithm that can handle general task-level resource contention. Through\nexperiments on LLM workloads, we demonstrate MLDSE's unique capability to\nperform three-tier DSE spanning architecture, hardware parameter, and mapping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To efficiently support large-scale NNs, multi-level hardware, leveraging\nadvanced integration and interconnection technologies, has emerged as a\npromising solution to counter the slowdown of Moore's law. However, the vast\ndesign space of such hardware, coupled with the complexity of their spatial\nhierarchies and organizations, introduces significant challenges for design\nspace exploration (DSE). Existing DSE tools, which rely on predefined hardware\ntemplates to explore parameters for specific architectures, fall short in\nexploring diverse organizations, spatial hierarchies, and architectural\npolymorphisms inherent in multi-level hardware. To address these limitations,\nwe present Multi-Level Design Space Exploror (MLDSE), a novel infrastructure\nfor domain-specific DSE of multi-level hardware. MLDSE introduces three key\ninnovations from three basic perspectives of DSE: 1) Modeling: MLDSE introduces\na hardware intermediate representation (IR) that can recursively model diverse\nmulti-level hardware with composable elements at various granularities. 2)\nMapping: MLDSE provides a comprehensive spatiotemporal mapping IR and mapping\nprimitives, facilitating the mapping strategy exploration on multi-level\nhardware, especially synchronization and cross-level communication; 3)\nSimulation: MLDSE supports universal simulator generation based on task-level\nevent-driven simulation mechanism. It features a hardware-consistent scheduling\nalgorithm that can handle general task-level resource contention. Through\nexperiments on LLM workloads, we demonstrate MLDSE's unique capability to\nperform three-tier DSE spanning architecture, hardware parameter, and mapping."
                },
                "authors": [
                    {
                        "name": "Huanyu Qu"
                    },
                    {
                        "name": "Weihao Zhang"
                    },
                    {
                        "name": "Junfeng Lin"
                    },
                    {
                        "name": "Songchen Ma"
                    },
                    {
                        "name": "Hongyi Li"
                    },
                    {
                        "name": "Luping Shi"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2406.15341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15341v2",
                "updated": "2025-03-27T17:59:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    59,
                    22,
                    3,
                    86,
                    0
                ],
                "published": "2024-06-21T17:55:24Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    17,
                    55,
                    24,
                    4,
                    173,
                    0
                ],
                "title": "GenoTEX: A Benchmark for Automated Gene Expression Data Analysis in\n  Alignment with Bioinformaticians",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenoTEX: A Benchmark for Automated Gene Expression Data Analysis in\n  Alignment with Bioinformaticians"
                },
                "summary": "Recent advancements in machine learning have significantly improved the\nidentification of disease-associated genes from gene expression datasets.\nHowever, these processes often require extensive expertise and manual effort,\nlimiting their scalability. Large Language Model (LLM)-based agents have shown\npromise in automating these tasks due to their increasing problem-solving\nabilities. To support the evaluation and development of such methods, we\nintroduce GenoTEX, a benchmark dataset for the automated analysis of gene\nexpression data. GenoTEX provides annotated code and results for solving a wide\nrange of gene identification problems, encompassing dataset selection,\npreprocessing, and statistical analysis, in a pipeline that follows\ncomputational genomics standards. The benchmark includes expert-curated\nannotations from bioinformaticians to ensure accuracy and reliability. To\nprovide baselines for these tasks, we present GenoAgent, a team of LLM-based\nagents that adopt a multi-step programming workflow with flexible\nself-correction, to collaboratively analyze gene expression datasets. Our\nexperiments demonstrate the potential of LLM-based methods in analyzing genomic\ndata, while error analysis highlights the challenges and areas for future\nimprovement. We propose GenoTEX as a promising resource for benchmarking and\nenhancing automated methods for gene expression data analysis. The benchmark is\navailable at https://github.com/Liu-Hy/GenoTex.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in machine learning have significantly improved the\nidentification of disease-associated genes from gene expression datasets.\nHowever, these processes often require extensive expertise and manual effort,\nlimiting their scalability. Large Language Model (LLM)-based agents have shown\npromise in automating these tasks due to their increasing problem-solving\nabilities. To support the evaluation and development of such methods, we\nintroduce GenoTEX, a benchmark dataset for the automated analysis of gene\nexpression data. GenoTEX provides annotated code and results for solving a wide\nrange of gene identification problems, encompassing dataset selection,\npreprocessing, and statistical analysis, in a pipeline that follows\ncomputational genomics standards. The benchmark includes expert-curated\nannotations from bioinformaticians to ensure accuracy and reliability. To\nprovide baselines for these tasks, we present GenoAgent, a team of LLM-based\nagents that adopt a multi-step programming workflow with flexible\nself-correction, to collaboratively analyze gene expression datasets. Our\nexperiments demonstrate the potential of LLM-based methods in analyzing genomic\ndata, while error analysis highlights the challenges and areas for future\nimprovement. We propose GenoTEX as a promising resource for benchmarking and\nenhancing automated methods for gene expression data analysis. The benchmark is\navailable at https://github.com/Liu-Hy/GenoTex."
                },
                "authors": [
                    {
                        "name": "Haoyang Liu"
                    },
                    {
                        "name": "Shuyu Chen"
                    },
                    {
                        "name": "Ye Zhang"
                    },
                    {
                        "name": "Haohan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haohan Wang"
                },
                "author": "Haohan Wang",
                "arxiv_comment": "29 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21760v1",
                "updated": "2025-03-27T17:57:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    57,
                    28,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:57:28Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    57,
                    28,
                    3,
                    86,
                    0
                ],
                "title": "MemInsight: Autonomous Memory Augmentation for LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemInsight: Autonomous Memory Augmentation for LLM Agents"
                },
                "summary": "Large language model (LLM) agents have evolved to intelligently process\ninformation, make decisions, and interact with users or tools. A key capability\nis the integration of long-term memory capabilities, enabling these agents to\ndraw upon historical interactions and knowledge. However, the growing memory\nsize and need for semantic structuring pose significant challenges. In this\nwork, we propose an autonomous memory augmentation approach, MemInsight, to\nenhance semantic data representation and retrieval mechanisms. By leveraging\nautonomous augmentation to historical interactions, LLM agents are shown to\ndeliver more accurate and contextualized responses. We empirically validate the\nefficacy of our proposed approach in three task scenarios; conversational\nrecommendation, question answering and event summarization. On the LLM-REDIAL\ndataset, MemInsight boosts persuasiveness of recommendations by up to 14%.\nMoreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval.\nOur empirical results show the potential of MemInsight to enhance the\ncontextual performance of LLM agents across multiple tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents have evolved to intelligently process\ninformation, make decisions, and interact with users or tools. A key capability\nis the integration of long-term memory capabilities, enabling these agents to\ndraw upon historical interactions and knowledge. However, the growing memory\nsize and need for semantic structuring pose significant challenges. In this\nwork, we propose an autonomous memory augmentation approach, MemInsight, to\nenhance semantic data representation and retrieval mechanisms. By leveraging\nautonomous augmentation to historical interactions, LLM agents are shown to\ndeliver more accurate and contextualized responses. We empirically validate the\nefficacy of our proposed approach in three task scenarios; conversational\nrecommendation, question answering and event summarization. On the LLM-REDIAL\ndataset, MemInsight boosts persuasiveness of recommendations by up to 14%.\nMoreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval.\nOur empirical results show the potential of MemInsight to enhance the\ncontextual performance of LLM agents across multiple tasks."
                },
                "authors": [
                    {
                        "name": "Rana Salama"
                    },
                    {
                        "name": "Jason Cai"
                    },
                    {
                        "name": "Michelle Yuan"
                    },
                    {
                        "name": "Anna Currey"
                    },
                    {
                        "name": "Monica Sunkara"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Yassine Benajiba"
                    }
                ],
                "author_detail": {
                    "name": "Yassine Benajiba"
                },
                "author": "Yassine Benajiba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21757v1",
                "updated": "2025-03-27T17:57:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    57,
                    7,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:57:07Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    57,
                    7,
                    3,
                    86,
                    0
                ],
                "title": "Fwd2Bot: LVLM Visual Token Compression with Double Forward Bottleneck",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fwd2Bot: LVLM Visual Token Compression with Double Forward Bottleneck"
                },
                "summary": "In this work, we aim to compress the vision tokens of a Large Vision Language\nModel (LVLM) into a representation that is simultaneously suitable for (a)\ngenerative and (b) discriminative tasks, (c) is nearly lossless, and (d) is\nstorage-efficient. We propose a novel compression approach, called Fwd2Bot,\nthat uses the LVLM itself to compress the visual information in a task-agnostic\nmanner. At the core of Fwd2bot there exists a \"double-forward pass\" training\nstrategy, whereby, during the first forward pass, the LLM (of the LVLM) creates\na bottleneck by condensing the visual information into a small number of\nsummary tokens. Then, using the same LLM, the second forward pass processes the\nlanguage instruction(s) alongside the summary tokens, used as a direct\nreplacement for the image ones. The training signal is provided by two losses:\nan autoregressive one applied after the second pass that provides a direct\noptimization objective for compression, and a contrastive loss, applied after\nthe first pass, that further boosts the representation strength, especially for\ndiscriminative tasks. The training is further enhanced by stage-specific\nadapters. We accompany the proposed method by an in-depth ablation study.\nOverall, Fwd2Bot results in highly-informative compressed representations\nsuitable for both generative and discriminative tasks. For generative tasks, we\noffer a 2x higher compression rate without compromising the generative\ncapabilities, setting a new state-of-the-art result. For discriminative tasks,\nwe set a new state-of-the-art on image retrieval and compositionality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we aim to compress the vision tokens of a Large Vision Language\nModel (LVLM) into a representation that is simultaneously suitable for (a)\ngenerative and (b) discriminative tasks, (c) is nearly lossless, and (d) is\nstorage-efficient. We propose a novel compression approach, called Fwd2Bot,\nthat uses the LVLM itself to compress the visual information in a task-agnostic\nmanner. At the core of Fwd2bot there exists a \"double-forward pass\" training\nstrategy, whereby, during the first forward pass, the LLM (of the LVLM) creates\na bottleneck by condensing the visual information into a small number of\nsummary tokens. Then, using the same LLM, the second forward pass processes the\nlanguage instruction(s) alongside the summary tokens, used as a direct\nreplacement for the image ones. The training signal is provided by two losses:\nan autoregressive one applied after the second pass that provides a direct\noptimization objective for compression, and a contrastive loss, applied after\nthe first pass, that further boosts the representation strength, especially for\ndiscriminative tasks. The training is further enhanced by stage-specific\nadapters. We accompany the proposed method by an in-depth ablation study.\nOverall, Fwd2Bot results in highly-informative compressed representations\nsuitable for both generative and discriminative tasks. For generative tasks, we\noffer a 2x higher compression rate without compromising the generative\ncapabilities, setting a new state-of-the-art result. For discriminative tasks,\nwe set a new state-of-the-art on image retrieval and compositionality."
                },
                "authors": [
                    {
                        "name": "Adrian Bulat"
                    },
                    {
                        "name": "Yassine Ouali"
                    },
                    {
                        "name": "Georgios Tzimiropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Tzimiropoulos"
                },
                "author": "Georgios Tzimiropoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21755v1",
                "updated": "2025-03-27T17:57:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    57,
                    1,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:57:01Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    57,
                    1,
                    3,
                    86,
                    0
                ],
                "title": "VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic\n  Faithfulness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic\n  Faithfulness"
                },
                "summary": "Video generation has advanced significantly, evolving from producing\nunrealistic outputs to generating videos that appear visually convincing and\ntemporally coherent. To evaluate these video generative models, benchmarks such\nas VBench have been developed to assess their faithfulness, measuring factors\nlike per-frame aesthetics, temporal consistency, and basic prompt adherence.\nHowever, these aspects mainly represent superficial faithfulness, which focus\non whether the video appears visually convincing rather than whether it adheres\nto real-world principles. While recent models perform increasingly well on\nthese metrics, they still struggle to generate videos that are not just\nvisually plausible but fundamentally realistic. To achieve real \"world models\"\nthrough video generation, the next frontier lies in intrinsic faithfulness to\nensure that generated videos adhere to physical laws, commonsense reasoning,\nanatomical correctness, and compositional integrity. Achieving this level of\nrealism is essential for applications such as AI-assisted filmmaking and\nsimulated world modeling. To bridge this gap, we introduce VBench-2.0, a\nnext-generation benchmark designed to automatically evaluate video generative\nmodels for their intrinsic faithfulness. VBench-2.0 assesses five key\ndimensions: Human Fidelity, Controllability, Creativity, Physics, and\nCommonsense, each further broken down into fine-grained capabilities. Tailored\nfor individual dimensions, our evaluation framework integrates generalists such\nas state-of-the-art VLMs and LLMs, and specialists, including anomaly detection\nmethods proposed for video generation. We conduct extensive annotations to\nensure alignment with human judgment. By pushing beyond superficial\nfaithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new\nstandard for the next generation of video generative models in pursuit of\nintrinsic faithfulness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation has advanced significantly, evolving from producing\nunrealistic outputs to generating videos that appear visually convincing and\ntemporally coherent. To evaluate these video generative models, benchmarks such\nas VBench have been developed to assess their faithfulness, measuring factors\nlike per-frame aesthetics, temporal consistency, and basic prompt adherence.\nHowever, these aspects mainly represent superficial faithfulness, which focus\non whether the video appears visually convincing rather than whether it adheres\nto real-world principles. While recent models perform increasingly well on\nthese metrics, they still struggle to generate videos that are not just\nvisually plausible but fundamentally realistic. To achieve real \"world models\"\nthrough video generation, the next frontier lies in intrinsic faithfulness to\nensure that generated videos adhere to physical laws, commonsense reasoning,\nanatomical correctness, and compositional integrity. Achieving this level of\nrealism is essential for applications such as AI-assisted filmmaking and\nsimulated world modeling. To bridge this gap, we introduce VBench-2.0, a\nnext-generation benchmark designed to automatically evaluate video generative\nmodels for their intrinsic faithfulness. VBench-2.0 assesses five key\ndimensions: Human Fidelity, Controllability, Creativity, Physics, and\nCommonsense, each further broken down into fine-grained capabilities. Tailored\nfor individual dimensions, our evaluation framework integrates generalists such\nas state-of-the-art VLMs and LLMs, and specialists, including anomaly detection\nmethods proposed for video generation. We conduct extensive annotations to\nensure alignment with human judgment. By pushing beyond superficial\nfaithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new\nstandard for the next generation of video generative models in pursuit of\nintrinsic faithfulness."
                },
                "authors": [
                    {
                        "name": "Dian Zheng"
                    },
                    {
                        "name": "Ziqi Huang"
                    },
                    {
                        "name": "Hongbo Liu"
                    },
                    {
                        "name": "Kai Zou"
                    },
                    {
                        "name": "Yinan He"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Yuanhan Zhang"
                    },
                    {
                        "name": "Jingwen He"
                    },
                    {
                        "name": "Wei-Shi Zheng"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziwei Liu"
                },
                "author": "Ziwei Liu",
                "arxiv_comment": "Equal contributions from first two authors. Project page:\n  https://vchitect.github.io/VBench-2.0-project/ Code:\n  https://github.com/Vchitect/VBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21735v1",
                "updated": "2025-03-27T17:48:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    48,
                    32,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:48:32Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    48,
                    32,
                    3,
                    86,
                    0
                ],
                "title": "GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release\n  Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release\n  Analytics"
                },
                "summary": "Ensuring the reliability and effectiveness of software release decisions is\ncritical, particularly in safety-critical domains like automotive systems.\nPrecise analysis of release validation data, often presented in tabular form,\nplays a pivotal role in this process. However, traditional methods that rely on\nmanual analysis of extensive test datasets and validation metrics are prone to\ndelays and high costs. Large Language Models (LLMs) offer a promising\nalternative but face challenges in analytical reasoning, contextual\nunderstanding, handling out-of-scope queries, and processing structured test\ndata consistently; limitations that hinder their direct application in\nsafety-critical scenarios. This paper introduces GateLens, an LLM-based tool\nfor analyzing tabular data in the automotive domain. GateLens translates\nnatural language queries into Relational Algebra (RA) expressions and then\ngenerates optimized Python code. It outperforms the baseline system on\nbenchmarking datasets, achieving higher F1 scores and handling complex and\nambiguous queries with greater robustness. Ablation studies confirm the\ncritical role of the RA module, with performance dropping sharply when omitted.\nIndustrial evaluations reveal that GateLens reduces analysis time by over 80%\nwhile maintaining high accuracy and reliability. As demonstrated by presented\nresults, GateLens achieved high performance without relying on few-shot\nexamples, showcasing strong generalization across various query types from\ndiverse company roles. Insights from deploying GateLens with a partner\nautomotive company offer practical guidance for integrating AI into critical\nworkflows such as release validation. Results show that by automating test\nresult analysis, GateLens enables faster, more informed, and dependable release\ndecisions, and can thus advance software scalability and reliability in\nautomotive systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the reliability and effectiveness of software release decisions is\ncritical, particularly in safety-critical domains like automotive systems.\nPrecise analysis of release validation data, often presented in tabular form,\nplays a pivotal role in this process. However, traditional methods that rely on\nmanual analysis of extensive test datasets and validation metrics are prone to\ndelays and high costs. Large Language Models (LLMs) offer a promising\nalternative but face challenges in analytical reasoning, contextual\nunderstanding, handling out-of-scope queries, and processing structured test\ndata consistently; limitations that hinder their direct application in\nsafety-critical scenarios. This paper introduces GateLens, an LLM-based tool\nfor analyzing tabular data in the automotive domain. GateLens translates\nnatural language queries into Relational Algebra (RA) expressions and then\ngenerates optimized Python code. It outperforms the baseline system on\nbenchmarking datasets, achieving higher F1 scores and handling complex and\nambiguous queries with greater robustness. Ablation studies confirm the\ncritical role of the RA module, with performance dropping sharply when omitted.\nIndustrial evaluations reveal that GateLens reduces analysis time by over 80%\nwhile maintaining high accuracy and reliability. As demonstrated by presented\nresults, GateLens achieved high performance without relying on few-shot\nexamples, showcasing strong generalization across various query types from\ndiverse company roles. Insights from deploying GateLens with a partner\nautomotive company offer practical guidance for integrating AI into critical\nworkflows such as release validation. Results show that by automating test\nresult analysis, GateLens enables faster, more informed, and dependable release\ndecisions, and can thus advance software scalability and reliability in\nautomotive systems."
                },
                "authors": [
                    {
                        "name": "Arsham Gholamzadeh Khoee"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Yinan Yu"
                    },
                    {
                        "name": "Robert Feldt"
                    },
                    {
                        "name": "Dhasarathy Parthasarathy"
                    }
                ],
                "author_detail": {
                    "name": "Dhasarathy Parthasarathy"
                },
                "author": "Dhasarathy Parthasarathy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v2",
                "updated": "2025-03-27T17:48:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    48,
                    14,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21730v1",
                "updated": "2025-03-27T17:45:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    45,
                    6,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:45:06Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    45,
                    6,
                    3,
                    86,
                    0
                ],
                "title": "Effective Skill Unlearning through Intervention and Abstention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective Skill Unlearning through Intervention and Abstention"
                },
                "summary": "Large language Models (LLMs) have demonstrated remarkable skills across\nvarious domains. Understanding the mechanisms behind their abilities and\nimplementing controls over them is becoming increasingly important for\ndeveloping better models. In this paper, we focus on skill unlearning in LLMs,\nspecifically unlearning a particular skill while retaining their overall\ncapabilities. We introduce two lightweight, training-free machine skill\nunlearning techniques for LLMs. First, we observe that the pre-activation\ndistribution of neurons in each Feed-Forward Layer (FFL) differs when the model\ndemonstrates different skills. Additionally, we find that queries triggering\nthe same skill cluster within the FFL key space and can be separated from other\nqueries using a hypercube. Based on these observations, we propose two\nlightweight, training-free skill unlearning methods via \\textit{intervention}\nand \\textit{abstention} respectively: \\texttt{Neuron Adjust} and \\texttt{Key\nSpace Detection}. We evaluate our methods on unlearning math-solving,\nPython-coding, and comprehension skills across seven different languages. The\nresults demonstrate their strong unlearning capabilities for the designated\nskills. Specifically, \\texttt{Key Space Detection} achieves over 80\\% relative\nperformance drop on the forgetting skill and less than 10\\% relative\nperformance drop on other skills and the model's general knowledge (MMLU) for\nmost unlearning tasks. Our code is available at\nhttps://github.com/Trustworthy-ML-Lab/effective_skill_unlearning",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language Models (LLMs) have demonstrated remarkable skills across\nvarious domains. Understanding the mechanisms behind their abilities and\nimplementing controls over them is becoming increasingly important for\ndeveloping better models. In this paper, we focus on skill unlearning in LLMs,\nspecifically unlearning a particular skill while retaining their overall\ncapabilities. We introduce two lightweight, training-free machine skill\nunlearning techniques for LLMs. First, we observe that the pre-activation\ndistribution of neurons in each Feed-Forward Layer (FFL) differs when the model\ndemonstrates different skills. Additionally, we find that queries triggering\nthe same skill cluster within the FFL key space and can be separated from other\nqueries using a hypercube. Based on these observations, we propose two\nlightweight, training-free skill unlearning methods via \\textit{intervention}\nand \\textit{abstention} respectively: \\texttt{Neuron Adjust} and \\texttt{Key\nSpace Detection}. We evaluate our methods on unlearning math-solving,\nPython-coding, and comprehension skills across seven different languages. The\nresults demonstrate their strong unlearning capabilities for the designated\nskills. Specifically, \\texttt{Key Space Detection} achieves over 80\\% relative\nperformance drop on the forgetting skill and less than 10\\% relative\nperformance drop on other skills and the model's general knowledge (MMLU) for\nmost unlearning tasks. Our code is available at\nhttps://github.com/Trustworthy-ML-Lab/effective_skill_unlearning"
                },
                "authors": [
                    {
                        "name": "Yongce Li"
                    },
                    {
                        "name": "Chung-En Sun"
                    },
                    {
                        "name": "Tsui-Wei Weng"
                    }
                ],
                "author_detail": {
                    "name": "Tsui-Wei Weng"
                },
                "author": "Tsui-Wei Weng",
                "arxiv_comment": "Accepted to NAACL 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05510v2",
                "updated": "2025-03-27T17:40:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    40,
                    9,
                    3,
                    86,
                    0
                ],
                "published": "2025-01-09T19:00:01Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    19,
                    0,
                    1,
                    3,
                    9,
                    0
                ],
                "title": "OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video\n  Understanding?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video\n  Understanding?"
                },
                "summary": "Temporal Awareness, the ability to reason dynamically based on the timestamp\nwhen a question is raised, is the key distinction between offline and online\nvideo LLMs. Unlike offline models, which rely on complete videos for static,\npost hoc analysis, online models process video streams incrementally and\ndynamically adapt their responses based on the timestamp at which the question\nis posed. Despite its significance, temporal awareness has not been adequately\nevaluated in existing benchmarks. To fill this gap, we present OVO-Bench\n(Online-VideO-Benchmark), a novel video benchmark that emphasizes the\nimportance of timestamps for advanced online video understanding capability\nbenchmarking. OVO-Bench evaluates the ability of video LLMs to reason and\nrespond to events occurring at specific timestamps under three distinct\nscenarios: (1) Backward tracing: trace back to past events to answer the\nquestion. (2) Real-time understanding: understand and respond to events as they\nunfold at the current timestamp. (3) Forward active responding: delay the\nresponse until sufficient future information becomes available to answer the\nquestion accurately. OVO-Bench comprises 12 tasks, featuring 644 unique videos\nand approximately human-curated 2,800 fine-grained meta-annotations with\nprecise timestamps. We combine automated generation pipelines with human\ncuration. With these high-quality samples, we further developed an evaluation\npipeline to systematically query video LLMs along the video timeline.\nEvaluations of nine Video-LLMs reveal that, despite advancements on traditional\nbenchmarks, current models struggle with online video understanding, showing a\nsignificant gap compared to human agents. We hope OVO-Bench will drive progress\nin video LLMs and inspire future research in online video reasoning. Our\nbenchmark and code can be accessed at https://github.com/JoeLeelyf/OVO-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Awareness, the ability to reason dynamically based on the timestamp\nwhen a question is raised, is the key distinction between offline and online\nvideo LLMs. Unlike offline models, which rely on complete videos for static,\npost hoc analysis, online models process video streams incrementally and\ndynamically adapt their responses based on the timestamp at which the question\nis posed. Despite its significance, temporal awareness has not been adequately\nevaluated in existing benchmarks. To fill this gap, we present OVO-Bench\n(Online-VideO-Benchmark), a novel video benchmark that emphasizes the\nimportance of timestamps for advanced online video understanding capability\nbenchmarking. OVO-Bench evaluates the ability of video LLMs to reason and\nrespond to events occurring at specific timestamps under three distinct\nscenarios: (1) Backward tracing: trace back to past events to answer the\nquestion. (2) Real-time understanding: understand and respond to events as they\nunfold at the current timestamp. (3) Forward active responding: delay the\nresponse until sufficient future information becomes available to answer the\nquestion accurately. OVO-Bench comprises 12 tasks, featuring 644 unique videos\nand approximately human-curated 2,800 fine-grained meta-annotations with\nprecise timestamps. We combine automated generation pipelines with human\ncuration. With these high-quality samples, we further developed an evaluation\npipeline to systematically query video LLMs along the video timeline.\nEvaluations of nine Video-LLMs reveal that, despite advancements on traditional\nbenchmarks, current models struggle with online video understanding, showing a\nsignificant gap compared to human agents. We hope OVO-Bench will drive progress\nin video LLMs and inspire future research in online video reasoning. Our\nbenchmark and code can be accessed at https://github.com/JoeLeelyf/OVO-Bench."
                },
                "authors": [
                    {
                        "name": "Yifei Li"
                    },
                    {
                        "name": "Junbo Niu"
                    },
                    {
                        "name": "Ziyang Miao"
                    },
                    {
                        "name": "Chunjiang Ge"
                    },
                    {
                        "name": "Yuanhang Zhou"
                    },
                    {
                        "name": "Qihao He"
                    },
                    {
                        "name": "Xiaoyi Dong"
                    },
                    {
                        "name": "Haodong Duan"
                    },
                    {
                        "name": "Shuangrui Ding"
                    },
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Pan Zhang"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Yuhang Cao"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21720v1",
                "updated": "2025-03-27T17:34:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    34,
                    25,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:34:25Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    34,
                    25,
                    3,
                    86,
                    0
                ],
                "title": "Collab: Controlled Decoding using Mixture of Agents for LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collab: Controlled Decoding using Mixture of Agents for LLM Alignment"
                },
                "summary": "Alignment of Large Language models (LLMs) is crucial for safe and trustworthy\ndeployment in applications. Reinforcement learning from human feedback (RLHF)\nhas emerged as an effective technique to align LLMs to human preferences and\nbroader utilities, but it requires updating billions of model parameters, which\nis computationally expensive. Controlled Decoding, by contrast, provides a\nmechanism for aligning a model at inference time without retraining. However,\nsingle-agent decoding approaches often struggle to adapt to diverse tasks due\nto the complexity and variability inherent in these tasks. To strengthen the\ntest-time performance w.r.t the target task, we propose a mixture of\nagent-based decoding strategies leveraging the existing off-the-shelf aligned\nLLM policies. Treating each prior policy as an agent in the spirit of mixture\nof agent collaboration, we develop a decoding method that allows for\ninference-time alignment through a token-level selection strategy among\nmultiple agents. For each token, the most suitable LLM is dynamically chosen\nfrom a pool of models based on a long-term utility metric. This\npolicy-switching mechanism ensures optimal model selection at each step,\nenabling efficient collaboration and alignment among LLMs during decoding.\nTheoretical analysis of our proposed algorithm establishes optimal performance\nwith respect to the target task represented via a target reward for the given\noff-the-shelf models. We conduct comprehensive empirical evaluations with\nopen-source aligned models on diverse tasks and preferences, which demonstrates\nthe merits of this approach over single-agent decoding baselines. Notably,\nCollab surpasses the current SoTA decoding strategy, achieving an improvement\nof up to 1.56x in average reward and 71.89% in GPT-4 based win-tie rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment of Large Language models (LLMs) is crucial for safe and trustworthy\ndeployment in applications. Reinforcement learning from human feedback (RLHF)\nhas emerged as an effective technique to align LLMs to human preferences and\nbroader utilities, but it requires updating billions of model parameters, which\nis computationally expensive. Controlled Decoding, by contrast, provides a\nmechanism for aligning a model at inference time without retraining. However,\nsingle-agent decoding approaches often struggle to adapt to diverse tasks due\nto the complexity and variability inherent in these tasks. To strengthen the\ntest-time performance w.r.t the target task, we propose a mixture of\nagent-based decoding strategies leveraging the existing off-the-shelf aligned\nLLM policies. Treating each prior policy as an agent in the spirit of mixture\nof agent collaboration, we develop a decoding method that allows for\ninference-time alignment through a token-level selection strategy among\nmultiple agents. For each token, the most suitable LLM is dynamically chosen\nfrom a pool of models based on a long-term utility metric. This\npolicy-switching mechanism ensures optimal model selection at each step,\nenabling efficient collaboration and alignment among LLMs during decoding.\nTheoretical analysis of our proposed algorithm establishes optimal performance\nwith respect to the target task represented via a target reward for the given\noff-the-shelf models. We conduct comprehensive empirical evaluations with\nopen-source aligned models on diverse tasks and preferences, which demonstrates\nthe merits of this approach over single-agent decoding baselines. Notably,\nCollab surpasses the current SoTA decoding strategy, achieving an improvement\nof up to 1.56x in average reward and 71.89% in GPT-4 based win-tie rate."
                },
                "authors": [
                    {
                        "name": "Souradip Chakraborty"
                    },
                    {
                        "name": "Sujay Bhatt"
                    },
                    {
                        "name": "Udari Madhushani Sehwag"
                    },
                    {
                        "name": "Soumya Suvra Ghosal"
                    },
                    {
                        "name": "Jiahao Qiu"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Dinesh Manocha"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Alec Koppel"
                    },
                    {
                        "name": "Sumitra Ganesh"
                    }
                ],
                "author_detail": {
                    "name": "Sumitra Ganesh"
                },
                "author": "Sumitra Ganesh",
                "arxiv_comment": "Accepted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18943v2",
                "updated": "2025-03-27T17:34:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    34,
                    6,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-24T17:59:07Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    59,
                    7,
                    0,
                    83,
                    0
                ],
                "title": "SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language\n  Models for Long-Form Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language\n  Models for Long-Form Video Understanding"
                },
                "summary": "We introduce SlowFast-LLaVA-1.5 (abbreviated as SF-LLaVA-1.5), a family of\nvideo large language models (LLMs) offering a token-efficient solution for\nlong-form video understanding. We incorporate the two-stream SlowFast mechanism\ninto a streamlined training pipeline, and perform joint video-image training on\na carefully curated data mixture of only publicly available datasets. Our\nprimary focus is on highly efficient model scales (1B and 3B), demonstrating\nthat even relatively small Video LLMs can achieve state-of-the-art performance\non video understanding, meeting the demand for mobile-friendly models.\nExperimental results demonstrate that SF-LLaVA-1.5 achieves superior\nperformance on a wide range of video and image tasks, with robust results at\nall model sizes (ranging from 1B to 7B). Notably, SF-LLaVA-1.5 achieves\nstate-of-the-art results in long-form video understanding (e.g., LongVideoBench\nand MLVU) and excels at small scales across various video benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SlowFast-LLaVA-1.5 (abbreviated as SF-LLaVA-1.5), a family of\nvideo large language models (LLMs) offering a token-efficient solution for\nlong-form video understanding. We incorporate the two-stream SlowFast mechanism\ninto a streamlined training pipeline, and perform joint video-image training on\na carefully curated data mixture of only publicly available datasets. Our\nprimary focus is on highly efficient model scales (1B and 3B), demonstrating\nthat even relatively small Video LLMs can achieve state-of-the-art performance\non video understanding, meeting the demand for mobile-friendly models.\nExperimental results demonstrate that SF-LLaVA-1.5 achieves superior\nperformance on a wide range of video and image tasks, with robust results at\nall model sizes (ranging from 1B to 7B). Notably, SF-LLaVA-1.5 achieves\nstate-of-the-art results in long-form video understanding (e.g., LongVideoBench\nand MLVU) and excels at small scales across various video benchmarks."
                },
                "authors": [
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Jiasen Lu"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Zhengfeng Lai"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Kai Kang"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Afshin Dehghan"
                    }
                ],
                "author_detail": {
                    "name": "Afshin Dehghan"
                },
                "author": "Afshin Dehghan",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21717v1",
                "updated": "2025-03-27T17:29:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    29,
                    45,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:29:45Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    29,
                    45,
                    3,
                    86,
                    0
                ],
                "title": "CLAIMCHECK: How Grounded are LLM Critiques of Scientific Papers?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLAIMCHECK: How Grounded are LLM Critiques of Scientific Papers?"
                },
                "summary": "A core part of scientific peer review involves providing expert critiques\nthat directly assess the scientific claims a paper makes. While it is now\npossible to automatically generate plausible (if generic) reviews, ensuring\nthat these reviews are sound and grounded in the papers' claims remains\nchallenging. To facilitate LLM benchmarking on these challenges, we introduce\nCLAIMCHECK, an annotated dataset of NeurIPS 2023 and 2024 submissions and\nreviews mined from OpenReview. CLAIMCHECK is richly annotated by ML experts for\nweakness statements in the reviews and the paper claims that they dispute, as\nwell as fine-grained labels of the validity, objectivity, and type of the\nidentified weaknesses. We benchmark several LLMs on three claim-centric tasks\nsupported by CLAIMCHECK, requiring models to (1) associate weaknesses with the\nclaims they dispute, (2) predict fine-grained labels for weaknesses and rewrite\nthe weaknesses to enhance their specificity, and (3) verify a paper's claims\nwith grounded reasoning. Our experiments reveal that cutting-edge LLMs, while\ncapable of predicting weakness labels in (2), continue to underperform relative\nto human experts on all other tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A core part of scientific peer review involves providing expert critiques\nthat directly assess the scientific claims a paper makes. While it is now\npossible to automatically generate plausible (if generic) reviews, ensuring\nthat these reviews are sound and grounded in the papers' claims remains\nchallenging. To facilitate LLM benchmarking on these challenges, we introduce\nCLAIMCHECK, an annotated dataset of NeurIPS 2023 and 2024 submissions and\nreviews mined from OpenReview. CLAIMCHECK is richly annotated by ML experts for\nweakness statements in the reviews and the paper claims that they dispute, as\nwell as fine-grained labels of the validity, objectivity, and type of the\nidentified weaknesses. We benchmark several LLMs on three claim-centric tasks\nsupported by CLAIMCHECK, requiring models to (1) associate weaknesses with the\nclaims they dispute, (2) predict fine-grained labels for weaknesses and rewrite\nthe weaknesses to enhance their specificity, and (3) verify a paper's claims\nwith grounded reasoning. Our experiments reveal that cutting-edge LLMs, while\ncapable of predicting weakness labels in (2), continue to underperform relative\nto human experts on all other tasks."
                },
                "authors": [
                    {
                        "name": "Jiefu Ou"
                    },
                    {
                        "name": "William Gantt Walden"
                    },
                    {
                        "name": "Kate Sanders"
                    },
                    {
                        "name": "Zhengping Jiang"
                    },
                    {
                        "name": "Kaiser Sun"
                    },
                    {
                        "name": "Jeffrey Cheng"
                    },
                    {
                        "name": "William Jurayj"
                    },
                    {
                        "name": "Miriam Wanner"
                    },
                    {
                        "name": "Shaobo Liang"
                    },
                    {
                        "name": "Candice Morgan"
                    },
                    {
                        "name": "Seunghoon Han"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Chandler May"
                    },
                    {
                        "name": "Hannah Recknor"
                    },
                    {
                        "name": "Daniel Khashabi"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06608v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06608v3",
                "updated": "2025-03-27T17:25:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    25,
                    50,
                    3,
                    86,
                    0
                ],
                "published": "2025-02-10T16:07:54Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    7,
                    54,
                    0,
                    41,
                    0
                ],
                "title": "TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified\n  Flow Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified\n  Flow Models"
                },
                "summary": "Recent advancements in diffusion techniques have propelled image and video\ngeneration to unprecedented levels of quality, significantly accelerating the\ndeployment and application of generative AI. However, 3D shape generation\ntechnology has so far lagged behind, constrained by limitations in 3D data\nscale, complexity of 3D data processing, and insufficient exploration of\nadvanced techniques in the 3D domain. Current approaches to 3D shape generation\nface substantial challenges in terms of output quality, generalization\ncapability, and alignment with input conditions. We present TripoSG, a new\nstreamlined shape diffusion paradigm capable of generating high-fidelity 3D\nmeshes with precise correspondence to input images. Specifically, we propose:\n1) A large-scale rectified flow transformer for 3D shape generation, achieving\nstate-of-the-art fidelity through training on extensive, high-quality data. 2)\nA hybrid supervised training strategy combining SDF, normal, and eikonal losses\nfor 3D VAE, achieving high-quality 3D reconstruction performance. 3) A data\nprocessing pipeline to generate 2 million high-quality 3D samples, highlighting\nthe crucial rules for data quality and quantity in training 3D generative\nmodels. Through comprehensive experiments, we have validated the effectiveness\nof each component in our new framework. The seamless integration of these parts\nhas enabled TripoSG to achieve state-of-the-art performance in 3D shape\ngeneration. The resulting 3D shapes exhibit enhanced detail due to\nhigh-resolution capabilities and demonstrate exceptional fidelity to input\nimages. Moreover, TripoSG demonstrates improved versatility in generating 3D\nmodels from diverse image styles and contents, showcasing strong generalization\ncapabilities. To foster progress and innovation in the field of 3D generation,\nwe will make our model publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in diffusion techniques have propelled image and video\ngeneration to unprecedented levels of quality, significantly accelerating the\ndeployment and application of generative AI. However, 3D shape generation\ntechnology has so far lagged behind, constrained by limitations in 3D data\nscale, complexity of 3D data processing, and insufficient exploration of\nadvanced techniques in the 3D domain. Current approaches to 3D shape generation\nface substantial challenges in terms of output quality, generalization\ncapability, and alignment with input conditions. We present TripoSG, a new\nstreamlined shape diffusion paradigm capable of generating high-fidelity 3D\nmeshes with precise correspondence to input images. Specifically, we propose:\n1) A large-scale rectified flow transformer for 3D shape generation, achieving\nstate-of-the-art fidelity through training on extensive, high-quality data. 2)\nA hybrid supervised training strategy combining SDF, normal, and eikonal losses\nfor 3D VAE, achieving high-quality 3D reconstruction performance. 3) A data\nprocessing pipeline to generate 2 million high-quality 3D samples, highlighting\nthe crucial rules for data quality and quantity in training 3D generative\nmodels. Through comprehensive experiments, we have validated the effectiveness\nof each component in our new framework. The seamless integration of these parts\nhas enabled TripoSG to achieve state-of-the-art performance in 3D shape\ngeneration. The resulting 3D shapes exhibit enhanced detail due to\nhigh-resolution capabilities and demonstrate exceptional fidelity to input\nimages. Moreover, TripoSG demonstrates improved versatility in generating 3D\nmodels from diverse image styles and contents, showcasing strong generalization\ncapabilities. To foster progress and innovation in the field of 3D generation,\nwe will make our model publicly available."
                },
                "authors": [
                    {
                        "name": "Yangguang Li"
                    },
                    {
                        "name": "Zi-Xin Zou"
                    },
                    {
                        "name": "Zexiang Liu"
                    },
                    {
                        "name": "Dehu Wang"
                    },
                    {
                        "name": "Yuan Liang"
                    },
                    {
                        "name": "Zhipeng Yu"
                    },
                    {
                        "name": "Xingchao Liu"
                    },
                    {
                        "name": "Yuan-Chen Guo"
                    },
                    {
                        "name": "Ding Liang"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yan-Pei Cao"
                    }
                ],
                "author_detail": {
                    "name": "Yan-Pei Cao"
                },
                "author": "Yan-Pei Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06608v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06608v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21710v1",
                "updated": "2025-03-27T17:21:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    21,
                    47,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:21:47Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    21,
                    47,
                    3,
                    86,
                    0
                ],
                "title": "Enhancing Repository-Level Software Repair via Repository-Aware\n  Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Repository-Level Software Repair via Repository-Aware\n  Knowledge Graphs"
                },
                "summary": "Repository-level software repair faces challenges in bridging semantic gaps\nbetween issue descriptions and code patches. Existing approaches, which mostly\ndepend on large language models (LLMs), suffer from semantic ambiguities,\nlimited structural context understanding, and insufficient reasoning\ncapability. To address these limitations, we propose KGCompass with two\ninnovations: (1) a novel repository-aware knowledge graph (KG) that accurately\nlinks repository artifacts (issues and pull requests) and codebase entities\n(files, classes, and functions), allowing us to effectively narrow down the\nvast search space to only 20 most relevant functions with accurate candidate\nbug locations and contextual information, and (2) a path-guided repair\nmechanism that leverages KG-mined entity path, tracing through which allows us\nto augment LLMs with relevant contextual information to generate precise\npatches along with their explanations. Experimental results in the\nSWE-Bench-Lite demonstrate that KGCompass achieves state-of-the-art repair\nperformance (45.67%) and function-level localization accuracy (51.33%) across\nopen-source approaches, costing only $0.20 per repair. Our analysis reveals\nthat among successfully localized bugs, 69.7% require multi-hop traversals\nthrough the knowledge graph, without which LLM-based approaches struggle to\naccurately locate bugs. The knowledge graph built in KGCompass is language\nagnostic and can be incrementally updated, making it a practical solution for\nreal-world development environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-level software repair faces challenges in bridging semantic gaps\nbetween issue descriptions and code patches. Existing approaches, which mostly\ndepend on large language models (LLMs), suffer from semantic ambiguities,\nlimited structural context understanding, and insufficient reasoning\ncapability. To address these limitations, we propose KGCompass with two\ninnovations: (1) a novel repository-aware knowledge graph (KG) that accurately\nlinks repository artifacts (issues and pull requests) and codebase entities\n(files, classes, and functions), allowing us to effectively narrow down the\nvast search space to only 20 most relevant functions with accurate candidate\nbug locations and contextual information, and (2) a path-guided repair\nmechanism that leverages KG-mined entity path, tracing through which allows us\nto augment LLMs with relevant contextual information to generate precise\npatches along with their explanations. Experimental results in the\nSWE-Bench-Lite demonstrate that KGCompass achieves state-of-the-art repair\nperformance (45.67%) and function-level localization accuracy (51.33%) across\nopen-source approaches, costing only $0.20 per repair. Our analysis reveals\nthat among successfully localized bugs, 69.7% require multi-hop traversals\nthrough the knowledge graph, without which LLM-based approaches struggle to\naccurately locate bugs. The knowledge graph built in KGCompass is language\nagnostic and can be incrementally updated, making it a practical solution for\nreal-world development environments."
                },
                "authors": [
                    {
                        "name": "Boyang Yang"
                    },
                    {
                        "name": "Haoye Tian"
                    },
                    {
                        "name": "Jiadong Ren"
                    },
                    {
                        "name": "Shunfu Jin"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Bach Le"
                    }
                ],
                "author_detail": {
                    "name": "Bach Le"
                },
                "author": "Bach Le",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20074v2",
                "updated": "2025-03-27T17:16:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    16,
                    44,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-25T21:20:11Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    21,
                    20,
                    11,
                    1,
                    84,
                    0
                ],
                "title": "Adaptive Orchestration for Large-Scale Inference on Heterogeneous\n  Accelerator Systems Balancing Cost, Performance, and Resilience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Orchestration for Large-Scale Inference on Heterogeneous\n  Accelerator Systems Balancing Cost, Performance, and Resilience"
                },
                "summary": "The surge in generative AI workloads has created a need for scalable\ninference systems that can flexibly harness both GPUs and specialized\naccelerators while containing operational costs. This paper proposes a\nhardware-agnostic control loop that adaptively allocates requests across\nheterogeneous accelerators based on real-time cost and capacity signals. The\napproach sustains low latency and high throughput by dynamically shifting\nbetween cost-optimized and capacity-optimized modes, ensuring the most\nefficient use of expensive compute resources under fluctuating availability.\nEvaluated using the Stable Diffusion model, the framework consistently meets\nlatency targets, automatically redirects traffic during capacity shortfalls,\nand capitalizes on lower-cost accelerators when possible. These results\nhighlight how a feedback-driven deployment strategy, spanning the entire\nsoftware and hardware stack, can help organizations efficiently scale\ngenerative AI workloads while maintaining resilience in the face of limited\naccelerator capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The surge in generative AI workloads has created a need for scalable\ninference systems that can flexibly harness both GPUs and specialized\naccelerators while containing operational costs. This paper proposes a\nhardware-agnostic control loop that adaptively allocates requests across\nheterogeneous accelerators based on real-time cost and capacity signals. The\napproach sustains low latency and high throughput by dynamically shifting\nbetween cost-optimized and capacity-optimized modes, ensuring the most\nefficient use of expensive compute resources under fluctuating availability.\nEvaluated using the Stable Diffusion model, the framework consistently meets\nlatency targets, automatically redirects traffic during capacity shortfalls,\nand capitalizes on lower-cost accelerators when possible. These results\nhighlight how a feedback-driven deployment strategy, spanning the entire\nsoftware and hardware stack, can help organizations efficiently scale\ngenerative AI workloads while maintaining resilience in the face of limited\naccelerator capacity."
                },
                "authors": [
                    {
                        "name": "Yahav Biran"
                    },
                    {
                        "name": "Imry Kissos"
                    }
                ],
                "author_detail": {
                    "name": "Imry Kissos"
                },
                "author": "Imry Kissos",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68U01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21683v1",
                "updated": "2025-03-27T16:52:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    52,
                    25,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T16:52:25Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    52,
                    25,
                    3,
                    86,
                    0
                ],
                "title": "LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku\n  with Self-Play and Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku\n  with Self-Play and Reinforcement Learning"
                },
                "summary": "In recent years, large language models (LLMs) have shown significant\nadvancements in natural language processing (NLP), with strong capa-bilities in\ngeneration, comprehension, and rea-soning. These models have found applications\nin education, intelligent decision-making, and gaming. However, effectively\nutilizing LLMs for strategic planning and decision-making in the game of Gomoku\nremains a challenge. This study aims to develop a Gomoku AI system based on\nLLMs, simulating the human learning process of playing chess. The system is\nde-signed to understand and apply Gomoku strat-egies and logic to make rational\ndecisions. The research methods include enabling the model to \"read the board,\"\n\"understand the rules,\" \"select strategies,\" and \"evaluate positions,\" while\nen-hancing its abilities through self-play and rein-forcement learning. The\nresults demonstrate that this approach significantly improves the se-lection of\nmove positions, resolves the issue of generating illegal positions, and reduces\npro-cess time through parallel position evaluation. After extensive self-play\ntraining, the model's Gomoku-playing capabilities have been notably enhanced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have shown significant\nadvancements in natural language processing (NLP), with strong capa-bilities in\ngeneration, comprehension, and rea-soning. These models have found applications\nin education, intelligent decision-making, and gaming. However, effectively\nutilizing LLMs for strategic planning and decision-making in the game of Gomoku\nremains a challenge. This study aims to develop a Gomoku AI system based on\nLLMs, simulating the human learning process of playing chess. The system is\nde-signed to understand and apply Gomoku strat-egies and logic to make rational\ndecisions. The research methods include enabling the model to \"read the board,\"\n\"understand the rules,\" \"select strategies,\" and \"evaluate positions,\" while\nen-hancing its abilities through self-play and rein-forcement learning. The\nresults demonstrate that this approach significantly improves the se-lection of\nmove positions, resolves the issue of generating illegal positions, and reduces\npro-cess time through parallel position evaluation. After extensive self-play\ntraining, the model's Gomoku-playing capabilities have been notably enhanced."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hui Wang"
                },
                "author": "Hui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21670v1",
                "updated": "2025-03-27T16:36:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    36,
                    39,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T16:36:39Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    36,
                    39,
                    3,
                    86,
                    0
                ],
                "title": "COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in\n  Hindi-English Code-Mixing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in\n  Hindi-English Code-Mixing"
                },
                "summary": "The rapid growth of digital communication has driven the widespread use of\ncode-mixing, particularly Hindi-English, in multilingual communities. Existing\ndatasets often focus on romanized text, have limited scope, or rely on\nsynthetic data, which fails to capture realworld language nuances. Human\nannotations are crucial for assessing the naturalness and acceptability of\ncode-mixed text. To address these challenges, We introduce COMI-LINGUA, the\nlargest manually annotated dataset for code-mixed text, comprising 100,970\ninstances evaluated by three expert annotators in both Devanagari and Roman\nscripts. The dataset supports five fundamental NLP tasks: Language\nIdentification, Matrix Language Identification, Part-of-Speech Tagging, Named\nEntity Recognition, and Translation. We evaluate LLMs on these tasks using\nCOMILINGUA, revealing limitations in current multilingual modeling strategies\nand emphasizing the need for improved code-mixed text processing capabilities.\nCOMI-LINGUA is publically availabe at:\nhttps://huggingface.co/datasets/LingoIITGN/COMI-LINGUA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of digital communication has driven the widespread use of\ncode-mixing, particularly Hindi-English, in multilingual communities. Existing\ndatasets often focus on romanized text, have limited scope, or rely on\nsynthetic data, which fails to capture realworld language nuances. Human\nannotations are crucial for assessing the naturalness and acceptability of\ncode-mixed text. To address these challenges, We introduce COMI-LINGUA, the\nlargest manually annotated dataset for code-mixed text, comprising 100,970\ninstances evaluated by three expert annotators in both Devanagari and Roman\nscripts. The dataset supports five fundamental NLP tasks: Language\nIdentification, Matrix Language Identification, Part-of-Speech Tagging, Named\nEntity Recognition, and Translation. We evaluate LLMs on these tasks using\nCOMILINGUA, revealing limitations in current multilingual modeling strategies\nand emphasizing the need for improved code-mixed text processing capabilities.\nCOMI-LINGUA is publically availabe at:\nhttps://huggingface.co/datasets/LingoIITGN/COMI-LINGUA."
                },
                "authors": [
                    {
                        "name": "Rajvee Sheth"
                    },
                    {
                        "name": "Himanshu Beniwal"
                    },
                    {
                        "name": "Mayank Singh"
                    }
                ],
                "author_detail": {
                    "name": "Mayank Singh"
                },
                "author": "Mayank Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16655v2",
                "updated": "2025-03-27T16:26:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    26,
                    55,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-20T19:12:32Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    19,
                    12,
                    32,
                    3,
                    79,
                    0
                ],
                "title": "Accelerating Antibiotic Discovery with Large Language Models and\n  Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Antibiotic Discovery with Large Language Models and\n  Knowledge Graphs"
                },
                "summary": "The discovery of novel antibiotics is critical to address the growing\nantimicrobial resistance (AMR). However, pharmaceutical industries face high\ncosts (over $1 billion), long timelines, and a high failure rate, worsened by\nthe rediscovery of known compounds. We propose an LLM-based pipeline that acts\nas an alarm system, detecting prior evidence of antibiotic activity to prevent\ncostly rediscoveries. The system integrates organism and chemical literature\ninto a Knowledge Graph (KG), ensuring taxonomic resolution, synonym handling,\nand multi-level evidence classification. We tested the pipeline on a private\nlist of 73 potential antibiotic-producing organisms, disclosing 12 negative\nhits for evaluation. The results highlight the effectiveness of the pipeline\nfor evidence reviewing, reducing false negatives, and accelerating\ndecision-making. The KG for negative hits and the user interface for\ninteractive exploration will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The discovery of novel antibiotics is critical to address the growing\nantimicrobial resistance (AMR). However, pharmaceutical industries face high\ncosts (over $1 billion), long timelines, and a high failure rate, worsened by\nthe rediscovery of known compounds. We propose an LLM-based pipeline that acts\nas an alarm system, detecting prior evidence of antibiotic activity to prevent\ncostly rediscoveries. The system integrates organism and chemical literature\ninto a Knowledge Graph (KG), ensuring taxonomic resolution, synonym handling,\nand multi-level evidence classification. We tested the pipeline on a private\nlist of 73 potential antibiotic-producing organisms, disclosing 12 negative\nhits for evaluation. The results highlight the effectiveness of the pipeline\nfor evidence reviewing, reducing false negatives, and accelerating\ndecision-making. The KG for negative hits and the user interface for\ninteractive exploration will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Maxime Delmas"
                    },
                    {
                        "name": "Magdalena Wysocka"
                    },
                    {
                        "name": "Danilo Gusicuma"
                    },
                    {
                        "name": "AndrÃ© Freitas"
                    }
                ],
                "author_detail": {
                    "name": "AndrÃ© Freitas"
                },
                "author": "AndrÃ© Freitas",
                "arxiv_comment": "11 pages, 9 figures, 3 tables fix: table, typos and error analysis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12257v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12257v3",
                "updated": "2025-03-27T16:21:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    21,
                    2,
                    3,
                    86,
                    0
                ],
                "published": "2024-06-18T04:10:38Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    4,
                    10,
                    38,
                    1,
                    170,
                    0
                ],
                "title": "CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large\n  Language Models"
                },
                "summary": "The remarkable performance of large language models (LLMs) in generation\ntasks has enabled practitioners to leverage publicly available models to power\ncustom applications, such as chatbots and virtual assistants. However, the data\nused to train or fine-tune these LLMs is often undisclosed, allowing an\nattacker to compromise the data and inject backdoors into the models. In this\npaper, we develop a novel inference time defense, named CLEANGEN, to mitigate\nbackdoor attacks for generation tasks in LLMs. CLEANGEN is a lightweight and\neffective decoding strategy that is compatible with the state-of-the-art (SOTA)\nLLMs. Our insight behind CLEANGEN is that compared to other LLMs, backdoored\nLLMs assign significantly higher probabilities to tokens representing the\nattacker-desired contents. These discrepancies in token probabilities enable\nCLEANGEN to identify suspicious tokens favored by the attacker and replace them\nwith tokens generated by another LLM that is not compromised by the same\nattacker, thereby avoiding generation of attacker-desired content. We evaluate\nCLEANGEN against five SOTA backdoor attacks. Our results show that CLEANGEN\nachieves lower attack success rates (ASR) compared to five SOTA baseline\ndefenses for all five backdoor attacks. Moreover, LLMs deploying CLEANGEN\nmaintain helpfulness in their responses when serving benign user queries with\nminimal added computational overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable performance of large language models (LLMs) in generation\ntasks has enabled practitioners to leverage publicly available models to power\ncustom applications, such as chatbots and virtual assistants. However, the data\nused to train or fine-tune these LLMs is often undisclosed, allowing an\nattacker to compromise the data and inject backdoors into the models. In this\npaper, we develop a novel inference time defense, named CLEANGEN, to mitigate\nbackdoor attacks for generation tasks in LLMs. CLEANGEN is a lightweight and\neffective decoding strategy that is compatible with the state-of-the-art (SOTA)\nLLMs. Our insight behind CLEANGEN is that compared to other LLMs, backdoored\nLLMs assign significantly higher probabilities to tokens representing the\nattacker-desired contents. These discrepancies in token probabilities enable\nCLEANGEN to identify suspicious tokens favored by the attacker and replace them\nwith tokens generated by another LLM that is not compromised by the same\nattacker, thereby avoiding generation of attacker-desired content. We evaluate\nCLEANGEN against five SOTA backdoor attacks. Our results show that CLEANGEN\nachieves lower attack success rates (ASR) compared to five SOTA baseline\ndefenses for all five backdoor attacks. Moreover, LLMs deploying CLEANGEN\nmaintain helpfulness in their responses when serving benign user queries with\nminimal added computational overhead."
                },
                "authors": [
                    {
                        "name": "Yuetai Li"
                    },
                    {
                        "name": "Zhangchen Xu"
                    },
                    {
                        "name": "Fengqing Jiang"
                    },
                    {
                        "name": "Luyao Niu"
                    },
                    {
                        "name": "Dinuka Sahabandu"
                    },
                    {
                        "name": "Bhaskar Ramasubramanian"
                    },
                    {
                        "name": "Radha Poovendran"
                    }
                ],
                "author_detail": {
                    "name": "Radha Poovendran"
                },
                "author": "Radha Poovendran",
                "arxiv_comment": "This paper is presented at EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12257v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12257v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21651v1",
                "updated": "2025-03-27T16:16:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    16,
                    32,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T16:16:32Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    16,
                    32,
                    3,
                    86,
                    0
                ],
                "title": "Evaluation of Deployable Solar Panels on GRACE-like Satellites by\n  Closed-Loop Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of Deployable Solar Panels on GRACE-like Satellites by\n  Closed-Loop Simulations"
                },
                "summary": "Future satellite gravimetry missions seek to surpass the performance of\nCHAMP, GOCE, GRACE, and GRACE-FO to meet increasing scientific and operational\ndemands. These missions will integrate advanced technologies, including optical\nand quantum accelerometers, high-precision inter-satellite laser ranging, and\nmicro-Newton electric thrusters. However, increased power demands for sensors\nand propulsion systems require larger solar panels, constrained by payload mass\nand launcher limitations. This study assesses the impact of modified satellite\nshapes on gravity field recovery (GFR) using closed-loop simulation. Five\nsatellite configurations were analyzed: a standard shape and variations with\nsingle and double solar panels mounted on the top and bottom of the satellite\nbody, each modeled with distinct finite element models and moments of inertia.\nOrbit simulations accounted for non-spherical static gravity and time-variable\nnon-gravitational forces. Performance of a simplified gravitational reference\nsensor (SGRS) with optical interferometer test mass displacement readout was\nevaluated. The air drag coefficient, a complex parameter influenced by multiple\nfactors, was varied from 2.25 (standard) to 4.5 (double-panel). Time-variable\ngravity background models were excluded to isolate instrument performance\neffects. Gravity models were evaluated in the spectral domain using Degree RMS\nof spherical harmonic coefficient differences. Discrepancies between\nconfigurations stemmed primarily from variations in SGRS actuation noise due to\nsatellite cross-sectional area. Convergence of residuals in the spectral domain\nfor the double-panel configuration under different drag coefficients confirmed\nthe dominant role of SGRS performance in GFR accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Future satellite gravimetry missions seek to surpass the performance of\nCHAMP, GOCE, GRACE, and GRACE-FO to meet increasing scientific and operational\ndemands. These missions will integrate advanced technologies, including optical\nand quantum accelerometers, high-precision inter-satellite laser ranging, and\nmicro-Newton electric thrusters. However, increased power demands for sensors\nand propulsion systems require larger solar panels, constrained by payload mass\nand launcher limitations. This study assesses the impact of modified satellite\nshapes on gravity field recovery (GFR) using closed-loop simulation. Five\nsatellite configurations were analyzed: a standard shape and variations with\nsingle and double solar panels mounted on the top and bottom of the satellite\nbody, each modeled with distinct finite element models and moments of inertia.\nOrbit simulations accounted for non-spherical static gravity and time-variable\nnon-gravitational forces. Performance of a simplified gravitational reference\nsensor (SGRS) with optical interferometer test mass displacement readout was\nevaluated. The air drag coefficient, a complex parameter influenced by multiple\nfactors, was varied from 2.25 (standard) to 4.5 (double-panel). Time-variable\ngravity background models were excluded to isolate instrument performance\neffects. Gravity models were evaluated in the spectral domain using Degree RMS\nof spherical harmonic coefficient differences. Discrepancies between\nconfigurations stemmed primarily from variations in SGRS actuation noise due to\nsatellite cross-sectional area. Convergence of residuals in the spectral domain\nfor the double-panel configuration under different drag coefficients confirmed\nthe dominant role of SGRS performance in GFR accuracy."
                },
                "authors": [
                    {
                        "name": "Andreas Leipner"
                    },
                    {
                        "name": "Alexey Kupriyanov"
                    },
                    {
                        "name": "Arthur Reis"
                    },
                    {
                        "name": "Annike Knabe"
                    },
                    {
                        "name": "Manuel Schilling"
                    },
                    {
                        "name": "Vitali MÃ¼ller"
                    },
                    {
                        "name": "Matthias Weigelt"
                    },
                    {
                        "name": "JÃ¼rgen MÃ¼ller"
                    },
                    {
                        "name": "Meike List"
                    }
                ],
                "author_detail": {
                    "name": "Meike List"
                },
                "author": "Meike List",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08180v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08180v2",
                "updated": "2025-03-27T16:07:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    16,
                    7,
                    18,
                    3,
                    86,
                    0
                ],
                "published": "2025-02-12T07:37:39Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    37,
                    39,
                    2,
                    43,
                    0
                ],
                "title": "Enhancing LLM Character-Level Manipulation via Divide and Conquer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Character-Level Manipulation via Divide and Conquer"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong generalization\ncapabilities across a wide range of natural language processing (NLP) tasks.\nHowever, they exhibit notable weaknesses in character-level string\nmanipulation, struggling with fundamental operations such as character\ndeletion, insertion, and substitution. These challenges stem primarily from\ntokenization constraints, despite the critical role of such operations in data\npreprocessing and code generation. Through systematic analysis, we derive two\nkey insights: (1) LLMs face significant difficulties in leveraging intrinsic\ntoken knowledge for character-level reasoning, and (2) atomized word structures\ncan substantially enhance LLMs' ability to process token-level structural\ninformation. Building on these insights, we propose Character-Level\nManipulation via Divide and Conquer, a novel approach designed to bridge the\ngap between token-level processing and character-level manipulation. Our method\ndecomposes complex operations into explicit character-level subtasks coupled\nwith controlled token reconstruction phases, leading to significant\nimprovements in accuracy. Without additional training, our method significantly\nimproves accuracies on the $\\texttt{Deletion}$, $\\texttt{Insertion}$, and\n$\\texttt{Substitution}$ tasks. To support further research, we open-source our\nimplementation and benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong generalization\ncapabilities across a wide range of natural language processing (NLP) tasks.\nHowever, they exhibit notable weaknesses in character-level string\nmanipulation, struggling with fundamental operations such as character\ndeletion, insertion, and substitution. These challenges stem primarily from\ntokenization constraints, despite the critical role of such operations in data\npreprocessing and code generation. Through systematic analysis, we derive two\nkey insights: (1) LLMs face significant difficulties in leveraging intrinsic\ntoken knowledge for character-level reasoning, and (2) atomized word structures\ncan substantially enhance LLMs' ability to process token-level structural\ninformation. Building on these insights, we propose Character-Level\nManipulation via Divide and Conquer, a novel approach designed to bridge the\ngap between token-level processing and character-level manipulation. Our method\ndecomposes complex operations into explicit character-level subtasks coupled\nwith controlled token reconstruction phases, leading to significant\nimprovements in accuracy. Without additional training, our method significantly\nimproves accuracies on the $\\texttt{Deletion}$, $\\texttt{Insertion}$, and\n$\\texttt{Substitution}$ tasks. To support further research, we open-source our\nimplementation and benchmarks."
                },
                "authors": [
                    {
                        "name": "Zhen Xiong"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Zhecheng Li"
                    },
                    {
                        "name": "Yiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yiwei Wang"
                },
                "author": "Yiwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08180v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08180v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15260v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15260v2",
                "updated": "2025-03-27T15:59:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    59,
                    24,
                    3,
                    86,
                    0
                ],
                "published": "2024-07-21T20:24:21Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    20,
                    24,
                    21,
                    6,
                    203,
                    0
                ],
                "title": "On the Viability of Semi-Supervised Segmentation Methods for Statistical\n  Shape Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Viability of Semi-Supervised Segmentation Methods for Statistical\n  Shape Modeling"
                },
                "summary": "Statistical Shape Models (SSMs) excel at identifying population level\nanatomical variations, which is at the core of various clinical and biomedical\napplications, including morphology-based diagnostics and surgical planning.\nHowever, the effectiveness of SSM is often constrained by the necessity for\nexpert-driven manual segmentation, a process that is both time-intensive and\nexpensive, thereby restricting their broader application and utility. Recent\ndeep learning approaches enable the direct estimation of Statistical Shape\nModels (SSMs) from unsegmented images. While these models can predict SSMs\nwithout segmentation during deployment, they do not address the challenge of\nacquiring the manual annotations needed for training, particularly in\nresource-limited settings. Semi-supervised models for anatomy segmentation can\nmitigate the annotation burden. Yet, despite the abundance of available\napproaches, there are no established guidelines to inform end-users on their\neffectiveness for the downstream task of constructing SSMs. In this study, we\nsystematically evaluate the potential of semi-supervised methods as viable\nalternatives to manual segmentations for building SSMs. We establish a new\nperformance benchmark by employing various semi-supervised methods for anatomy\nsegmentation under low annotation settings, utilizing the predicted\nsegmentations for the task of SSM. Our results indicate that some methods\nproduce noisy segmentation, which is very unfavorable for SSM tasks, while\nothers can capture the correct modes of variations in the population cohort\nwith 60-80% reduction in required manual annotation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Shape Models (SSMs) excel at identifying population level\nanatomical variations, which is at the core of various clinical and biomedical\napplications, including morphology-based diagnostics and surgical planning.\nHowever, the effectiveness of SSM is often constrained by the necessity for\nexpert-driven manual segmentation, a process that is both time-intensive and\nexpensive, thereby restricting their broader application and utility. Recent\ndeep learning approaches enable the direct estimation of Statistical Shape\nModels (SSMs) from unsegmented images. While these models can predict SSMs\nwithout segmentation during deployment, they do not address the challenge of\nacquiring the manual annotations needed for training, particularly in\nresource-limited settings. Semi-supervised models for anatomy segmentation can\nmitigate the annotation burden. Yet, despite the abundance of available\napproaches, there are no established guidelines to inform end-users on their\neffectiveness for the downstream task of constructing SSMs. In this study, we\nsystematically evaluate the potential of semi-supervised methods as viable\nalternatives to manual segmentations for building SSMs. We establish a new\nperformance benchmark by employing various semi-supervised methods for anatomy\nsegmentation under low annotation settings, utilizing the predicted\nsegmentations for the task of SSM. Our results indicate that some methods\nproduce noisy segmentation, which is very unfavorable for SSM tasks, while\nothers can capture the correct modes of variations in the population cohort\nwith 60-80% reduction in required manual annotation"
                },
                "authors": [
                    {
                        "name": "Asma Khan"
                    },
                    {
                        "name": "Tushar Kataria"
                    },
                    {
                        "name": "Janmesh Ukey"
                    },
                    {
                        "name": "Shireen Y. Elhabian"
                    }
                ],
                "author_detail": {
                    "name": "Shireen Y. Elhabian"
                },
                "author": "Shireen Y. Elhabian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15260v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15260v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19838v2",
                "updated": "2025-03-27T15:41:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    41,
                    28,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-25T16:59:38Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    59,
                    38,
                    1,
                    84,
                    0
                ],
                "title": "Compact and stable source of polarization-entangled photon-pairs based\n  on a folded linear displacement interferometer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compact and stable source of polarization-entangled photon-pairs based\n  on a folded linear displacement interferometer"
                },
                "summary": "The realization of quantum networks requires the development of robust low\nsize, weight and power (SWaP) systems suitable for operation under harsh\nenvironments in remote and mobile nodes such as satellites. We present a source\nof polarization-entangled photon-pairs in a folded linear displacement\ninterferometer based on spontaneous parametric down conversion using a Type-0\nperiodically poled potassium titanyl phosphate crystal. Featuring a compact and\nstable double-pass geometry using a corner-cube retroreflector, the source has\na detected pair rate of 2.5 M pairs/s/mW with a Bell state fidelity of 94.1%\n+/- 2.1%. The qualities and demonstrated performance of the source make it\nsuitable for deployment in entanglement-based quantum networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The realization of quantum networks requires the development of robust low\nsize, weight and power (SWaP) systems suitable for operation under harsh\nenvironments in remote and mobile nodes such as satellites. We present a source\nof polarization-entangled photon-pairs in a folded linear displacement\ninterferometer based on spontaneous parametric down conversion using a Type-0\nperiodically poled potassium titanyl phosphate crystal. Featuring a compact and\nstable double-pass geometry using a corner-cube retroreflector, the source has\na detected pair rate of 2.5 M pairs/s/mW with a Bell state fidelity of 94.1%\n+/- 2.1%. The qualities and demonstrated performance of the source make it\nsuitable for deployment in entanglement-based quantum networks."
                },
                "authors": [
                    {
                        "name": "Sarah E. McCarthy"
                    },
                    {
                        "name": "Ali Anwar"
                    },
                    {
                        "name": "Daniel K. L. Oi"
                    },
                    {
                        "name": "Loyd J. McKnight"
                    }
                ],
                "author_detail": {
                    "name": "Loyd J. McKnight"
                },
                "author": "Loyd J. McKnight",
                "arxiv_comment": "11 pages, 8 figures, submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21620v1",
                "updated": "2025-03-27T15:39:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    39,
                    30,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T15:39:30Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    39,
                    30,
                    3,
                    86,
                    0
                ],
                "title": "UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement\n  Learning"
                },
                "summary": "The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities\nin LLMs through reinforcement learning (RL) with rule-based rewards. Building\non this idea, we are the first to explore how rule-based RL can enhance the\nreasoning capabilities of multimodal large language models (MLLMs) for graphic\nuser interface (GUI) action prediction tasks. To this end, we curate a small\nyet high-quality dataset of 136 challenging tasks, encompassing five common\naction types on mobile devices. We also introduce a unified rule-based action\nreward, enabling model optimization via policy-based algorithms such as Group\nRelative Policy Optimization (GRPO). Experimental results demonstrate that our\nproposed data-efficient model, UI-R1-3B, achieves substantial improvements on\nboth in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the ID\nbenchmark AndroidControl, the action type accuracy improves by 15%, while\ngrounding accuracy increases by 10.3%, compared with the base model (i.e.\nQwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our model\nsurpasses the base model by 6.0% and achieves competitive performance with\nlarger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning\n(SFT) on 76K data. These results underscore the potential of rule-based\nreinforcement learning to advance GUI understanding and control, paving the way\nfor future research in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities\nin LLMs through reinforcement learning (RL) with rule-based rewards. Building\non this idea, we are the first to explore how rule-based RL can enhance the\nreasoning capabilities of multimodal large language models (MLLMs) for graphic\nuser interface (GUI) action prediction tasks. To this end, we curate a small\nyet high-quality dataset of 136 challenging tasks, encompassing five common\naction types on mobile devices. We also introduce a unified rule-based action\nreward, enabling model optimization via policy-based algorithms such as Group\nRelative Policy Optimization (GRPO). Experimental results demonstrate that our\nproposed data-efficient model, UI-R1-3B, achieves substantial improvements on\nboth in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the ID\nbenchmark AndroidControl, the action type accuracy improves by 15%, while\ngrounding accuracy increases by 10.3%, compared with the base model (i.e.\nQwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our model\nsurpasses the base model by 6.0% and achieves competitive performance with\nlarger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning\n(SFT) on 76K data. These results underscore the potential of rule-based\nreinforcement learning to advance GUI understanding and control, paving the way\nfor future research in this domain."
                },
                "authors": [
                    {
                        "name": "Zhengxi Lu"
                    },
                    {
                        "name": "Yuxiang Chai"
                    },
                    {
                        "name": "Yaxuan Guo"
                    },
                    {
                        "name": "Xi Yin"
                    },
                    {
                        "name": "Liang Liu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Guanjing Xiong"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21615v1",
                "updated": "2025-03-27T15:36:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    36,
                    49,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T15:36:49Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    36,
                    49,
                    3,
                    86,
                    0
                ],
                "title": "A Measure Based Generalizable Approach to Understandability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Measure Based Generalizable Approach to Understandability"
                },
                "summary": "Successful agent-human partnerships require that any agent generated\ninformation is understandable to the human, and that the human can easily steer\nthe agent towards a goal. Such effective communication requires the agent to\ndevelop a finer-level notion of what is understandable to the human.\nState-of-the-art agents, including LLMs, lack this detailed notion of\nunderstandability because they only capture average human sensibilities from\nthe training data, and therefore afford limited steerability (e.g., requiring\nnon-trivial prompt engineering).\n  In this paper, instead of only relying on data, we argue for developing\ngeneralizable, domain-agnostic measures of understandability that can be used\nas directives for these agents. Existing research on understandability measures\nis fragmented, we survey various such efforts across domains, and lay a\ncognitive-science-rooted groundwork for more coherent and domain-agnostic\nresearch investigations in future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Successful agent-human partnerships require that any agent generated\ninformation is understandable to the human, and that the human can easily steer\nthe agent towards a goal. Such effective communication requires the agent to\ndevelop a finer-level notion of what is understandable to the human.\nState-of-the-art agents, including LLMs, lack this detailed notion of\nunderstandability because they only capture average human sensibilities from\nthe training data, and therefore afford limited steerability (e.g., requiring\nnon-trivial prompt engineering).\n  In this paper, instead of only relying on data, we argue for developing\ngeneralizable, domain-agnostic measures of understandability that can be used\nas directives for these agents. Existing research on understandability measures\nis fragmented, we survey various such efforts across domains, and lay a\ncognitive-science-rooted groundwork for more coherent and domain-agnostic\nresearch investigations in future."
                },
                "authors": [
                    {
                        "name": "Vikas Kushwaha"
                    },
                    {
                        "name": "Sruti Srinivasa Ragavan"
                    },
                    {
                        "name": "Subhajit Roy"
                    }
                ],
                "author_detail": {
                    "name": "Subhajit Roy"
                },
                "author": "Subhajit Roy",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21614v1",
                "updated": "2025-03-27T15:36:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    36,
                    30,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T15:36:30Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    36,
                    30,
                    3,
                    86,
                    0
                ],
                "title": "A Survey of Efficient Reasoning for Large Reasoning Models: Language,\n  Multimodality, and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Efficient Reasoning for Large Reasoning Models: Language,\n  Multimodality, and Beyond"
                },
                "summary": "Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have\ndemonstrated strong performance gains by scaling up the length of\nChain-of-Thought (CoT) reasoning during inference. However, a growing concern\nlies in their tendency to produce excessively long reasoning traces, which are\noften filled with redundant content (e.g., repeated definitions), over-analysis\nof simple problems, and superficial exploration of multiple reasoning paths for\nharder tasks. This inefficiency introduces significant challenges for training,\ninference, and real-world deployment (e.g., in agent-based systems), where\ntoken economy is critical. In this survey, we provide a comprehensive overview\nof recent efforts aimed at improving reasoning efficiency in LRMs, with a\nparticular focus on the unique challenges that arise in this new paradigm. We\nidentify common patterns of inefficiency, examine methods proposed across the\nLRM lifecycle, i.e., from pretraining to inference, and discuss promising\nfuture directions for research. To support ongoing development, we also\nmaintain a real-time GitHub repository tracking recent progress in the field.\nWe hope this survey serves as a foundation for further exploration and inspires\ninnovation in this rapidly evolving area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have\ndemonstrated strong performance gains by scaling up the length of\nChain-of-Thought (CoT) reasoning during inference. However, a growing concern\nlies in their tendency to produce excessively long reasoning traces, which are\noften filled with redundant content (e.g., repeated definitions), over-analysis\nof simple problems, and superficial exploration of multiple reasoning paths for\nharder tasks. This inefficiency introduces significant challenges for training,\ninference, and real-world deployment (e.g., in agent-based systems), where\ntoken economy is critical. In this survey, we provide a comprehensive overview\nof recent efforts aimed at improving reasoning efficiency in LRMs, with a\nparticular focus on the unique challenges that arise in this new paradigm. We\nidentify common patterns of inefficiency, examine methods proposed across the\nLRM lifecycle, i.e., from pretraining to inference, and discuss promising\nfuture directions for research. To support ongoing development, we also\nmaintain a real-time GitHub repository tracking recent progress in the field.\nWe hope this survey serves as a foundation for further exploration and inspires\ninnovation in this rapidly evolving area."
                },
                "authors": [
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Yafu Li"
                    },
                    {
                        "name": "Zhaochen Su"
                    },
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Jianhao Yan"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Daizong Liu"
                    },
                    {
                        "name": "Shuxian Liang"
                    },
                    {
                        "name": "Junxian He"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Jing Shao"
                    },
                    {
                        "name": "Chaochao Lu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Xian-Sheng Hua"
                    },
                    {
                        "name": "Bowen Zhou"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "Survey, 32 pages, Large Reasoning Models, Efficient Reasoning for\n  Language, Multimodality, and Beyond",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21613v1",
                "updated": "2025-03-27T15:36:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    36,
                    24,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T15:36:24Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    36,
                    24,
                    3,
                    86,
                    0
                ],
                "title": "Evaluating book summaries from internal knowledge in Large Language\n  Models: a cross-model and semantic consistency approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating book summaries from internal knowledge in Large Language\n  Models: a cross-model and semantic consistency approach"
                },
                "summary": "We study the ability of large language models (LLMs) to generate\ncomprehensive and accurate book summaries solely from their internal knowledge,\nwithout recourse to the original text. Employing a diverse set of books and\nmultiple LLM architectures, we examine whether these models can synthesize\nmeaningful narratives that align with established human interpretations.\nEvaluation is performed with a LLM-as-a-judge paradigm: each AI-generated\nsummary is compared against a high-quality, human-written summary via a\ncross-model assessment, where all participating LLMs evaluate not only their\nown outputs but also those produced by others. This methodology enables the\nidentification of potential biases, such as the proclivity for models to favor\ntheir own summarization style over others. In addition, alignment between the\nhuman-crafted and LLM-generated summaries is quantified using ROUGE and\nBERTScore metrics, assessing the depth of grammatical and semantic\ncorrespondence. The results reveal nuanced variations in content representation\nand stylistic preferences among the models, highlighting both strengths and\nlimitations inherent in relying on internal knowledge for summarization tasks.\nThese findings contribute to a deeper understanding of LLM internal encodings\nof factual information and the dynamics of cross-model evaluation, with\nimplications for the development of more robust natural language generative\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the ability of large language models (LLMs) to generate\ncomprehensive and accurate book summaries solely from their internal knowledge,\nwithout recourse to the original text. Employing a diverse set of books and\nmultiple LLM architectures, we examine whether these models can synthesize\nmeaningful narratives that align with established human interpretations.\nEvaluation is performed with a LLM-as-a-judge paradigm: each AI-generated\nsummary is compared against a high-quality, human-written summary via a\ncross-model assessment, where all participating LLMs evaluate not only their\nown outputs but also those produced by others. This methodology enables the\nidentification of potential biases, such as the proclivity for models to favor\ntheir own summarization style over others. In addition, alignment between the\nhuman-crafted and LLM-generated summaries is quantified using ROUGE and\nBERTScore metrics, assessing the depth of grammatical and semantic\ncorrespondence. The results reveal nuanced variations in content representation\nand stylistic preferences among the models, highlighting both strengths and\nlimitations inherent in relying on internal knowledge for summarization tasks.\nThese findings contribute to a deeper understanding of LLM internal encodings\nof factual information and the dynamics of cross-model evaluation, with\nimplications for the development of more robust natural language generative\nsystems."
                },
                "authors": [
                    {
                        "name": "Javier Coronado-BlÃ¡zquez"
                    }
                ],
                "author_detail": {
                    "name": "Javier Coronado-BlÃ¡zquez"
                },
                "author": "Javier Coronado-BlÃ¡zquez",
                "arxiv_comment": "22 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01672v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01672v4",
                "updated": "2025-03-27T15:24:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    24,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2024-10-02T15:41:22Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    41,
                    22,
                    2,
                    276,
                    0
                ],
                "title": "Practicing Stress Relief for the Everyday: Designing Social Simulation\n  Using VR, AR, and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practicing Stress Relief for the Everyday: Designing Social Simulation\n  Using VR, AR, and LLMs"
                },
                "summary": "Stress is an inevitable part of day-to-day life yet many find themselves\nunable to manage it themselves, particularly when professional or peer support\nare not always readily available. As self-care becomes increasingly vital for\nmental well-being, this paper explores the potential of social simulation as a\nsafe, virtual environment for practicing stress relief for everyday situations.\nLeveraging the immersive capabilities of VR, AR, and LLMs, we developed eight\ninteractive prototypes for various everyday stressful scenarios (e.g. public\nspeaking) then conducted prototype-driven semi-structured interviews with 19\nparticipants. We reveal that people currently lack effective means to support\nthemselves through everyday stress and found that social simulation fills a gap\nfor simulating real environments for training mental health practices. We\noutline key considerations for future development of simulation for self-care,\nincluding risks of trauma from hyper-realism, distrust of LLM-recommended\ntiming for mental health recommendations, and the value of accessibility for\nself-care interventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stress is an inevitable part of day-to-day life yet many find themselves\nunable to manage it themselves, particularly when professional or peer support\nare not always readily available. As self-care becomes increasingly vital for\nmental well-being, this paper explores the potential of social simulation as a\nsafe, virtual environment for practicing stress relief for everyday situations.\nLeveraging the immersive capabilities of VR, AR, and LLMs, we developed eight\ninteractive prototypes for various everyday stressful scenarios (e.g. public\nspeaking) then conducted prototype-driven semi-structured interviews with 19\nparticipants. We reveal that people currently lack effective means to support\nthemselves through everyday stress and found that social simulation fills a gap\nfor simulating real environments for training mental health practices. We\noutline key considerations for future development of simulation for self-care,\nincluding risks of trauma from hyper-realism, distrust of LLM-recommended\ntiming for mental health recommendations, and the value of accessibility for\nself-care interventions."
                },
                "authors": [
                    {
                        "name": "Anna Fang"
                    },
                    {
                        "name": "Hriday Chhabria"
                    },
                    {
                        "name": "Alekhya Maram"
                    },
                    {
                        "name": "Haiyi Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Haiyi Zhu"
                },
                "author": "Haiyi Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01672v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01672v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21602v1",
                "updated": "2025-03-27T15:22:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    22,
                    2,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T15:22:02Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    22,
                    2,
                    3,
                    86,
                    0
                ],
                "title": "GenEdit: Compounding Operators and Continuous Improvement to Tackle\n  Text-to-SQL in the Enterprise",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenEdit: Compounding Operators and Continuous Improvement to Tackle\n  Text-to-SQL in the Enterprise"
                },
                "summary": "Recent advancements in Text-to-SQL, driven by large language models, are\ndemocratizing data access. Despite these advancements, enterprise deployments\nremain challenging due to the need to capture business-specific knowledge,\nhandle complex queries, and meet expectations of continuous improvements. To\naddress these issues, we designed and implemented GenEdit: our Text-to-SQL\ngeneration system that improves with user feedback. GenEdit builds and\nmaintains a company-specific knowledge set, employs a pipeline of operators\ndecomposing SQL generation, and uses feedback to update its knowledge set to\nimprove future SQL generations.\n  We describe GenEdit's architecture made of two core modules: (i) decomposed\nSQL generation; and (ii) knowledge set edits based on user feedback. For\ngeneration, GenEdit leverages compounding operators to improve knowledge\nretrieval and to create a plan as chain-of-thought steps that guides\ngeneration. GenEdit first retrieves relevant examples in an initial retrieval\nstage where original SQL queries are decomposed into sub-statements, clauses or\nsub-queries. It then also retrieves instructions and schema elements. Using the\nretrieved contextual information, GenEdit then generates step-by-step plan in\nnatural language on how to produce the query. Finally, GenEdit uses the plan to\ngenerate SQL, minimizing the need for model reasoning, which enhances complex\nSQL generation. If necessary, GenEdit regenerates the query based on syntactic\nand semantic errors. The knowledge set edits are recommended through an\ninteractive copilot, allowing users to iterate on their feedback and to\nregenerate SQL queries as needed. Each generation uses staged edits which\nupdate the generation prompt. Once the feedback is submitted, it gets merged\nafter passing regression testing and obtaining an approval, improving future\ngenerations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Text-to-SQL, driven by large language models, are\ndemocratizing data access. Despite these advancements, enterprise deployments\nremain challenging due to the need to capture business-specific knowledge,\nhandle complex queries, and meet expectations of continuous improvements. To\naddress these issues, we designed and implemented GenEdit: our Text-to-SQL\ngeneration system that improves with user feedback. GenEdit builds and\nmaintains a company-specific knowledge set, employs a pipeline of operators\ndecomposing SQL generation, and uses feedback to update its knowledge set to\nimprove future SQL generations.\n  We describe GenEdit's architecture made of two core modules: (i) decomposed\nSQL generation; and (ii) knowledge set edits based on user feedback. For\ngeneration, GenEdit leverages compounding operators to improve knowledge\nretrieval and to create a plan as chain-of-thought steps that guides\ngeneration. GenEdit first retrieves relevant examples in an initial retrieval\nstage where original SQL queries are decomposed into sub-statements, clauses or\nsub-queries. It then also retrieves instructions and schema elements. Using the\nretrieved contextual information, GenEdit then generates step-by-step plan in\nnatural language on how to produce the query. Finally, GenEdit uses the plan to\ngenerate SQL, minimizing the need for model reasoning, which enhances complex\nSQL generation. If necessary, GenEdit regenerates the query based on syntactic\nand semantic errors. The knowledge set edits are recommended through an\ninteractive copilot, allowing users to iterate on their feedback and to\nregenerate SQL queries as needed. Each generation uses staged edits which\nupdate the generation prompt. Once the feedback is submitted, it gets merged\nafter passing regression testing and obtaining an approval, improving future\ngenerations."
                },
                "authors": [
                    {
                        "name": "Karime Maamari"
                    },
                    {
                        "name": "Connor Landy"
                    },
                    {
                        "name": "Amine Mhedhbi"
                    }
                ],
                "author_detail": {
                    "name": "Amine Mhedhbi"
                },
                "author": "Amine Mhedhbi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21601v1",
                "updated": "2025-03-27T15:20:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    20,
                    59,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T15:20:59Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    20,
                    59,
                    3,
                    86,
                    0
                ],
                "title": "A Deep Reinforcement Learning-based Approach for Adaptive Handover\n  Protocols",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Deep Reinforcement Learning-based Approach for Adaptive Handover\n  Protocols"
                },
                "summary": "The use of higher frequencies in mobile communication systems leads to\nsmaller cell sizes, resulting in the deployment of more base stations and an\nincrease in handovers to support user mobility. This can lead to frequent radio\nlink failures and reduced data rates. In this work, we propose a handover\noptimization method using proximal policy optimization (PPO) to develop an\nadaptive handover protocol. Our PPO-based agent, implemented in the base\nstations, is highly adaptive to varying user equipment speeds and outperforms\nthe 3GPP-standardized 5G NR handover procedure in terms of average data rate\nand radio link failure rate. Additionally, our simulation environment is\ncarefully designed to ensure high accuracy, realistic user movements, and fair\nbenchmarking against the 3GPP handover method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of higher frequencies in mobile communication systems leads to\nsmaller cell sizes, resulting in the deployment of more base stations and an\nincrease in handovers to support user mobility. This can lead to frequent radio\nlink failures and reduced data rates. In this work, we propose a handover\noptimization method using proximal policy optimization (PPO) to develop an\nadaptive handover protocol. Our PPO-based agent, implemented in the base\nstations, is highly adaptive to varying user equipment speeds and outperforms\nthe 3GPP-standardized 5G NR handover procedure in terms of average data rate\nand radio link failure rate. Additionally, our simulation environment is\ncarefully designed to ensure high accuracy, realistic user movements, and fair\nbenchmarking against the 3GPP handover method."
                },
                "authors": [
                    {
                        "name": "Johannes Voigt"
                    },
                    {
                        "name": "Peter Jiacheng Gu"
                    },
                    {
                        "name": "Peter Rost"
                    }
                ],
                "author_detail": {
                    "name": "Peter Rost"
                },
                "author": "Peter Rost",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21599v1",
                "updated": "2025-03-27T15:20:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    20,
                    32,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T15:20:32Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    20,
                    32,
                    3,
                    86,
                    0
                ],
                "title": "Leveraging Line-of-Sight Propagation for Near-Field Beamfocusing in\n  Cell-Free Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Line-of-Sight Propagation for Near-Field Beamfocusing in\n  Cell-Free Networks"
                },
                "summary": "Cell-free (CF) massive multiple-input multiple-output (MIMO) is a promising\napproach for next-generation wireless networks, enabling scalable deployments\nof multiple small access points (APs) to enhance coverage and service for\nmultiple user equipments (UEs). While most existing research focuses on\nlow-frequency bands with Rayleigh fading models, emerging 5G trends are\nshifting toward higher frequencies, where geometric channel models and\nline-of-sight (LoS) propagation become more relevant. In this work, we explore\nhow distributed massive MIMO in the LoS regime can achieve near-field-like\nconditions by forming artificially large arrays through coordinated AP\ndeployments. We investigate centralized and decentralized CF architectures,\nleveraging structured channel estimation (SCE) techniques that exploit the\nline-of-sight properties of geometric channels. Our results demonstrate that\ndense distributed AP deployments significantly improve system performance\nw.r.t. the case of a co-located array, even in highly populated UE scenarios,\nwhile SCE approaches the performance of perfect CSI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-free (CF) massive multiple-input multiple-output (MIMO) is a promising\napproach for next-generation wireless networks, enabling scalable deployments\nof multiple small access points (APs) to enhance coverage and service for\nmultiple user equipments (UEs). While most existing research focuses on\nlow-frequency bands with Rayleigh fading models, emerging 5G trends are\nshifting toward higher frequencies, where geometric channel models and\nline-of-sight (LoS) propagation become more relevant. In this work, we explore\nhow distributed massive MIMO in the LoS regime can achieve near-field-like\nconditions by forming artificially large arrays through coordinated AP\ndeployments. We investigate centralized and decentralized CF architectures,\nleveraging structured channel estimation (SCE) techniques that exploit the\nline-of-sight properties of geometric channels. Our results demonstrate that\ndense distributed AP deployments significantly improve system performance\nw.r.t. the case of a co-located array, even in highly populated UE scenarios,\nwhile SCE approaches the performance of perfect CSI."
                },
                "authors": [
                    {
                        "name": "Georgios Mylonopoulos"
                    },
                    {
                        "name": "Giovanni Interdonato"
                    },
                    {
                        "name": "Stefano Buzzi"
                    },
                    {
                        "name": "Pei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pei Liu"
                },
                "author": "Pei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21598v1",
                "updated": "2025-03-27T15:19:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    19,
                    55,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T15:19:55Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    19,
                    55,
                    3,
                    86,
                    0
                ],
                "title": "Prompt, Divide, and Conquer: Bypassing Large Language Model Safety\n  Filters via Segmented and Distributed Prompt Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt, Divide, and Conquer: Bypassing Large Language Model Safety\n  Filters via Segmented and Distributed Prompt Processing"
                },
                "summary": "Large Language Models (LLMs) have transformed task automation and content\ngeneration across various domains while incorporating safety filters to prevent\nmisuse. We introduce a novel jailbreaking framework that employs distributed\nprompt processing combined with iterative refinements to bypass these safety\nmeasures, particularly in generating malicious code. Our architecture consists\nof four key modules: prompt segmentation, parallel processing, response\naggregation, and LLM-based jury evaluation. Tested on 500 malicious prompts\nacross 10 cybersecurity categories, the framework achieves a 73.2% Success Rate\n(SR) in generating malicious code. Notably, our comparative analysis reveals\nthat traditional single-LLM judge evaluation overestimates SRs (93.8%) compared\nto our LLM jury system (73.2%), with manual verification confirming that\nsingle-judge assessments often accept incomplete implementations. Moreover, we\ndemonstrate that our distributed architecture improves SRs by 12% over the\nnon-distributed approach in an ablation study, highlighting both the\neffectiveness of distributed prompt processing and the importance of robust\nevaluation methodologies in assessing jailbreak attempts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed task automation and content\ngeneration across various domains while incorporating safety filters to prevent\nmisuse. We introduce a novel jailbreaking framework that employs distributed\nprompt processing combined with iterative refinements to bypass these safety\nmeasures, particularly in generating malicious code. Our architecture consists\nof four key modules: prompt segmentation, parallel processing, response\naggregation, and LLM-based jury evaluation. Tested on 500 malicious prompts\nacross 10 cybersecurity categories, the framework achieves a 73.2% Success Rate\n(SR) in generating malicious code. Notably, our comparative analysis reveals\nthat traditional single-LLM judge evaluation overestimates SRs (93.8%) compared\nto our LLM jury system (73.2%), with manual verification confirming that\nsingle-judge assessments often accept incomplete implementations. Moreover, we\ndemonstrate that our distributed architecture improves SRs by 12% over the\nnon-distributed approach in an ablation study, highlighting both the\neffectiveness of distributed prompt processing and the importance of robust\nevaluation methodologies in assessing jailbreak attempts."
                },
                "authors": [
                    {
                        "name": "Johan WahrÃ©us"
                    },
                    {
                        "name": "Ahmed Hussain"
                    },
                    {
                        "name": "Panos Papadimitratos"
                    }
                ],
                "author_detail": {
                    "name": "Panos Papadimitratos"
                },
                "author": "Panos Papadimitratos",
                "arxiv_comment": "22 pages; 26 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21564v1",
                "updated": "2025-03-27T14:47:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    47,
                    43,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T14:47:43Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    47,
                    43,
                    3,
                    86,
                    0
                ],
                "title": "Cooking Task Planning using LLM and Verified by Graph Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooking Task Planning using LLM and Verified by Graph Network"
                },
                "summary": "Cooking tasks remain a challenging problem for robotics due to their\ncomplexity. Videos of people cooking are a valuable source of information for\nsuch task, but introduces a lot of variability in terms of how to translate\nthis data to a robotic environment. This research aims to streamline this\nprocess, focusing on the task plan generation step, by using a Large Language\nModel (LLM)-based Task and Motion Planning (TAMP) framework to autonomously\ngenerate cooking task plans from videos with subtitles, and execute them.\nConventional LLM-based task planning methods are not well-suited for\ninterpreting the cooking video data due to uncertainty in the videos, and the\nrisk of hallucination in its output. To address both of these problems, we\nexplore using LLMs in combination with Functional Object-Oriented Networks\n(FOON), to validate the plan and provide feedback in case of failure. This\ncombination can generate task sequences with manipulation motions that are\nlogically correct and executable by a robot. We compare the execution of the\ngenerated plans for 5 cooking recipes from our approach against the plans\ngenerated by a few-shot LLM-only approach for a dual-arm robot setup. It could\nsuccessfully execute 4 of the plans generated by our approach, whereas only 1\nof the plans generated by solely using the LLM could be executed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooking tasks remain a challenging problem for robotics due to their\ncomplexity. Videos of people cooking are a valuable source of information for\nsuch task, but introduces a lot of variability in terms of how to translate\nthis data to a robotic environment. This research aims to streamline this\nprocess, focusing on the task plan generation step, by using a Large Language\nModel (LLM)-based Task and Motion Planning (TAMP) framework to autonomously\ngenerate cooking task plans from videos with subtitles, and execute them.\nConventional LLM-based task planning methods are not well-suited for\ninterpreting the cooking video data due to uncertainty in the videos, and the\nrisk of hallucination in its output. To address both of these problems, we\nexplore using LLMs in combination with Functional Object-Oriented Networks\n(FOON), to validate the plan and provide feedback in case of failure. This\ncombination can generate task sequences with manipulation motions that are\nlogically correct and executable by a robot. We compare the execution of the\ngenerated plans for 5 cooking recipes from our approach against the plans\ngenerated by a few-shot LLM-only approach for a dual-arm robot setup. It could\nsuccessfully execute 4 of the plans generated by our approach, whereas only 1\nof the plans generated by solely using the LLM could be executed."
                },
                "authors": [
                    {
                        "name": "Ryunosuke Takebayashi"
                    },
                    {
                        "name": "Vitor Hideyo Isume"
                    },
                    {
                        "name": "Takuya Kiyokawa"
                    },
                    {
                        "name": "Weiwei Wan"
                    },
                    {
                        "name": "Kensuke Harada"
                    }
                ],
                "author_detail": {
                    "name": "Kensuke Harada"
                },
                "author": "Kensuke Harada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21557v1",
                "updated": "2025-03-27T14:43:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    43,
                    28,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T14:43:28Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    43,
                    28,
                    3,
                    86,
                    0
                ],
                "title": "debug-gym: A Text-Based Environment for Interactive Debugging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "debug-gym: A Text-Based Environment for Interactive Debugging"
                },
                "summary": "Large Language Models (LLMs) are increasingly relied upon for coding tasks,\nyet in most scenarios it is assumed that all relevant information can be either\naccessed in context or matches their training data. We posit that LLMs can\nbenefit from the ability to interactively explore a codebase to gather the\ninformation relevant to their task. To achieve this, we present a textual\nenvironment, namely debug-gym, for developing LLM-based agents in an\ninteractive coding setting. Our environment is lightweight and provides a\npreset of useful tools, such as a Python debugger (pdb), designed to facilitate\nan LLM-based agent's interactive debugging. Beyond coding and debugging tasks,\nthis approach can be generalized to other tasks that would benefit from\ninformation-seeking behavior by an LLM agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly relied upon for coding tasks,\nyet in most scenarios it is assumed that all relevant information can be either\naccessed in context or matches their training data. We posit that LLMs can\nbenefit from the ability to interactively explore a codebase to gather the\ninformation relevant to their task. To achieve this, we present a textual\nenvironment, namely debug-gym, for developing LLM-based agents in an\ninteractive coding setting. Our environment is lightweight and provides a\npreset of useful tools, such as a Python debugger (pdb), designed to facilitate\nan LLM-based agent's interactive debugging. Beyond coding and debugging tasks,\nthis approach can be generalized to other tasks that would benefit from\ninformation-seeking behavior by an LLM agent."
                },
                "authors": [
                    {
                        "name": "Xingdi Yuan"
                    },
                    {
                        "name": "Morgane M Moss"
                    },
                    {
                        "name": "Charbel El Feghali"
                    },
                    {
                        "name": "Chinmay Singh"
                    },
                    {
                        "name": "Darya Moldavskaya"
                    },
                    {
                        "name": "Drew MacPhee"
                    },
                    {
                        "name": "Lucas Caccia"
                    },
                    {
                        "name": "Matheus Pereira"
                    },
                    {
                        "name": "Minseon Kim"
                    },
                    {
                        "name": "Alessandro Sordoni"
                    },
                    {
                        "name": "Marc-Alexandre CÃ´tÃ©"
                    }
                ],
                "author_detail": {
                    "name": "Marc-Alexandre CÃ´tÃ©"
                },
                "author": "Marc-Alexandre CÃ´tÃ©",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21544v1",
                "updated": "2025-03-27T14:34:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    34,
                    28,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T14:34:28Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    34,
                    28,
                    3,
                    86,
                    0
                ],
                "title": "SWI: Speaking with Intent in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWI: Speaking with Intent in Large Language Models"
                },
                "summary": "Intent, typically clearly formulated and planned, functions as a cognitive\nframework for reasoning and problem-solving. This paper introduces the concept\nof Speaking with Intent (SWI) in large language models (LLMs), where the\nexplicitly generated intent encapsulates the model's underlying intention and\nprovides high-level planning to guide subsequent analysis and communication. By\nemulating deliberate and purposeful thoughts in the human mind, SWI is\nhypothesized to enhance the reasoning capabilities and generation quality of\nLLMs. Extensive experiments on mathematical reasoning benchmarks consistently\ndemonstrate the superiority of Speaking with Intent over Baseline (i.e.,\ngeneration without explicit intent). Moreover, SWI outperforms answer-trigger\nprompting methods Chain-of-Thought and Plan-and-Solve and maintains competitive\nperformance with the strong method ARR (Analyzing, Retrieving, and Reasoning).\nAdditionally, the effectiveness and generalizability of SWI are solidified on\nreasoning-intensive question answering (QA) and text summarization benchmarks,\nwhere SWI brings consistent improvement to the Baseline generation. In text\nsummarization, SWI-generated summaries exhibit greater accuracy, conciseness,\nand factual correctness, with fewer hallucinations. Furthermore, human\nevaluations verify the coherence, effectiveness, and interpretability of the\nintent produced by SWI. This proof-of-concept study creates a novel avenue for\nenhancing LLMs' reasoning abilities with cognitive notions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent, typically clearly formulated and planned, functions as a cognitive\nframework for reasoning and problem-solving. This paper introduces the concept\nof Speaking with Intent (SWI) in large language models (LLMs), where the\nexplicitly generated intent encapsulates the model's underlying intention and\nprovides high-level planning to guide subsequent analysis and communication. By\nemulating deliberate and purposeful thoughts in the human mind, SWI is\nhypothesized to enhance the reasoning capabilities and generation quality of\nLLMs. Extensive experiments on mathematical reasoning benchmarks consistently\ndemonstrate the superiority of Speaking with Intent over Baseline (i.e.,\ngeneration without explicit intent). Moreover, SWI outperforms answer-trigger\nprompting methods Chain-of-Thought and Plan-and-Solve and maintains competitive\nperformance with the strong method ARR (Analyzing, Retrieving, and Reasoning).\nAdditionally, the effectiveness and generalizability of SWI are solidified on\nreasoning-intensive question answering (QA) and text summarization benchmarks,\nwhere SWI brings consistent improvement to the Baseline generation. In text\nsummarization, SWI-generated summaries exhibit greater accuracy, conciseness,\nand factual correctness, with fewer hallucinations. Furthermore, human\nevaluations verify the coherence, effectiveness, and interpretability of the\nintent produced by SWI. This proof-of-concept study creates a novel avenue for\nenhancing LLMs' reasoning abilities with cognitive notions."
                },
                "authors": [
                    {
                        "name": "Yuwei Yin"
                    },
                    {
                        "name": "EunJeong Hwang"
                    },
                    {
                        "name": "Giuseppe Carenini"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Carenini"
                },
                "author": "Giuseppe Carenini",
                "arxiv_comment": "24 pages. Code: https://github.com/YuweiYin/SWI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21542v1",
                "updated": "2025-03-27T14:33:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    33,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T14:33:19Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    33,
                    19,
                    3,
                    86,
                    0
                ],
                "title": "Shape Adaptive Reconfigurable Holographic Surfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shape Adaptive Reconfigurable Holographic Surfaces"
                },
                "summary": "Reconfigurable Intelligent Surfaces (RIS) have emerged as a key solution to\ndynamically adjust wireless propagation by tuning the reflection coefficients\nof large arrays of passive elements. Reconfigurable Holographic Surfaces (RHS)\nbuild on the same foundation as RIS but extend it by employing holographic\nprinciples for finer-grained wave manipulation | that is, applying higher\nspatial control over the reflected signals for more precise beam steering. In\nthis paper, we investigate shape-adaptive RHS deployments in a multi-user\nnetwork. Rather than treating each RHS as a uniform reflecting surface, we\npropose a selective element activation strategy that dynamically adapts the\nspatial arrangement of deployed RHS regions to a subset of predefined shapes.\nIn particular, we formulate a system throughput maximization problem that\noptimizes the shape of the selected RHS elements, active beamforming at the\naccess point (AP), and passive beamforming at the RHS to enhance coverage and\nmitigate signal blockage. The resulting problem is non-convex and becomes even\nmore challenging to solve as the number of RHS and users increases; to tackle\nthis, we introduce an alternating optimization (AO) approach that efficiently\nfinds near-optimal solutions irrespective of the number or spatial\nconfiguration of RHS. Numerical results demonstrate that shape adaptation\nenables more efficient resource distribution, enhancing the effectiveness of\nmulti-RHS deployments as the network scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable Intelligent Surfaces (RIS) have emerged as a key solution to\ndynamically adjust wireless propagation by tuning the reflection coefficients\nof large arrays of passive elements. Reconfigurable Holographic Surfaces (RHS)\nbuild on the same foundation as RIS but extend it by employing holographic\nprinciples for finer-grained wave manipulation | that is, applying higher\nspatial control over the reflected signals for more precise beam steering. In\nthis paper, we investigate shape-adaptive RHS deployments in a multi-user\nnetwork. Rather than treating each RHS as a uniform reflecting surface, we\npropose a selective element activation strategy that dynamically adapts the\nspatial arrangement of deployed RHS regions to a subset of predefined shapes.\nIn particular, we formulate a system throughput maximization problem that\noptimizes the shape of the selected RHS elements, active beamforming at the\naccess point (AP), and passive beamforming at the RHS to enhance coverage and\nmitigate signal blockage. The resulting problem is non-convex and becomes even\nmore challenging to solve as the number of RHS and users increases; to tackle\nthis, we introduce an alternating optimization (AO) approach that efficiently\nfinds near-optimal solutions irrespective of the number or spatial\nconfiguration of RHS. Numerical results demonstrate that shape adaptation\nenables more efficient resource distribution, enhancing the effectiveness of\nmulti-RHS deployments as the network scales."
                },
                "authors": [
                    {
                        "name": "Jalal Jalali"
                    },
                    {
                        "name": "Mostafa Darabi"
                    },
                    {
                        "name": "Rodrigo C. de Lamare"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo C. de Lamare"
                },
                "author": "Rodrigo C. de Lamare",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21540v1",
                "updated": "2025-03-27T14:31:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    31,
                    17,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T14:31:17Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    31,
                    17,
                    3,
                    86,
                    0
                ],
                "title": "Combining Artificial Users and Psychotherapist Assessment to Evaluate\n  Large Language Model-based Mental Health Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Artificial Users and Psychotherapist Assessment to Evaluate\n  Large Language Model-based Mental Health Chatbots"
                },
                "summary": "Large Language Models (LLMs) promise to overcome limitations of rule-based\nmental health chatbots through more natural conversations. However, evaluating\nLLM-based mental health chatbots presents a significant challenge: Their\nprobabilistic nature requires comprehensive testing to ensure therapeutic\nquality, yet conducting such evaluations with people with depression would\nimpose an additional burden on vulnerable people and risk exposing them to\npotentially harmful content. Our paper presents an evaluation approach for\nLLM-based mental health chatbots that combines dialogue generation with\nartificial users and dialogue evaluation by psychotherapists. We developed\nartificial users based on patient vignettes, systematically varying\ncharacteristics such as depression severity, personality traits, and attitudes\ntoward chatbots, and let them interact with a LLM-based behavioral activation\nchatbot. Ten psychotherapists evaluated 48 randomly selected dialogues using\nstandardized rating scales to assess the quality of behavioral activation and\nits therapeutic capabilities. We found that while artificial users showed\nmoderate authenticity, they enabled comprehensive testing across different\nusers. In addition, the chatbot demonstrated promising capabilities in\ndelivering behavioral activation and maintaining safety. Furthermore, we\nidentified deficits, such as ensuring the appropriateness of the activity plan,\nwhich reveals necessary improvements for the chatbot. Our framework provides an\neffective method for evaluating LLM-based mental health chatbots while\nprotecting vulnerable people during the evaluation process. Future research\nshould improve the authenticity of artificial users and develop LLM-augmented\nevaluation tools to make psychotherapist evaluation more efficient, and thus\nfurther advance the evaluation of LLM-based mental health chatbots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) promise to overcome limitations of rule-based\nmental health chatbots through more natural conversations. However, evaluating\nLLM-based mental health chatbots presents a significant challenge: Their\nprobabilistic nature requires comprehensive testing to ensure therapeutic\nquality, yet conducting such evaluations with people with depression would\nimpose an additional burden on vulnerable people and risk exposing them to\npotentially harmful content. Our paper presents an evaluation approach for\nLLM-based mental health chatbots that combines dialogue generation with\nartificial users and dialogue evaluation by psychotherapists. We developed\nartificial users based on patient vignettes, systematically varying\ncharacteristics such as depression severity, personality traits, and attitudes\ntoward chatbots, and let them interact with a LLM-based behavioral activation\nchatbot. Ten psychotherapists evaluated 48 randomly selected dialogues using\nstandardized rating scales to assess the quality of behavioral activation and\nits therapeutic capabilities. We found that while artificial users showed\nmoderate authenticity, they enabled comprehensive testing across different\nusers. In addition, the chatbot demonstrated promising capabilities in\ndelivering behavioral activation and maintaining safety. Furthermore, we\nidentified deficits, such as ensuring the appropriateness of the activity plan,\nwhich reveals necessary improvements for the chatbot. Our framework provides an\neffective method for evaluating LLM-based mental health chatbots while\nprotecting vulnerable people during the evaluation process. Future research\nshould improve the authenticity of artificial users and develop LLM-augmented\nevaluation tools to make psychotherapist evaluation more efficient, and thus\nfurther advance the evaluation of LLM-based mental health chatbots."
                },
                "authors": [
                    {
                        "name": "Florian Onur Kuhlmeier"
                    },
                    {
                        "name": "Leon Hanschmann"
                    },
                    {
                        "name": "Melina Rabe"
                    },
                    {
                        "name": "Stefan Luettke"
                    },
                    {
                        "name": "Eva-Lotta Brakemeier"
                    },
                    {
                        "name": "Alexander Maedche"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Maedche"
                },
                "author": "Alexander Maedche",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17922v2",
                "updated": "2025-03-27T14:11:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    11,
                    37,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-23T03:36:52Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    36,
                    52,
                    6,
                    82,
                    0
                ],
                "title": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference"
                },
                "summary": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness."
                },
                "authors": [
                    {
                        "name": "Youhui Zuo"
                    },
                    {
                        "name": "Sibo Wei"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhuorui Liu"
                    },
                    {
                        "name": "Wenpeng Lu"
                    },
                    {
                        "name": "Dawei Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Song"
                },
                "author": "Dawei Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21522v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21522v1",
                "updated": "2025-03-27T14:10:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    10,
                    33,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T14:10:33Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    10,
                    33,
                    3,
                    86,
                    0
                ],
                "title": "MONO2REST: Identifying and Exposing Microservices: a Reusable\n  RESTification Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MONO2REST: Identifying and Exposing Microservices: a Reusable\n  RESTification Approach"
                },
                "summary": "The microservices architectural style has become the de facto standard for\nlarge-scale cloud applications, offering numerous benefits in scalability,\nmaintainability, and deployment flexibility. Many organizations are pursuing\nthe migration of legacy monolithic systems to a microservices architecture.\nHowever, this process is challenging, risky, time-intensive, and\nprone-to-failure while several organizations lack necessary financial\nresources, time, or expertise to set up this migration process. So, rather than\ntrying to migrate a legacy system where migration is risky or not feasible, we\nsuggest exposing it as a microservice application without without having to\nmigrate it. In this paper, we present a reusable, automated, two-phase approach\nthat combines evolutionary algorithms with machine learning techniques. In the\nfirst phase, we identify microservices at the method level using a\nmulti-objective genetic algorithm that considers both structural and semantic\ndependencies between methods. In the second phase, we generate REST APIs for\neach identified microservice using a classification algorithm to assign HTTP\nmethods and endpoints. We evaluated our approach with a case study on the\nSpring PetClinic application, which has both monolithic and microservices\nimplementations that serve as ground truth for comparison. Results demonstrate\nthat our approach successfully aligns identified microservices with those in\nthe reference microservices implementation, highlighting its effectiveness in\nservice identification and API generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The microservices architectural style has become the de facto standard for\nlarge-scale cloud applications, offering numerous benefits in scalability,\nmaintainability, and deployment flexibility. Many organizations are pursuing\nthe migration of legacy monolithic systems to a microservices architecture.\nHowever, this process is challenging, risky, time-intensive, and\nprone-to-failure while several organizations lack necessary financial\nresources, time, or expertise to set up this migration process. So, rather than\ntrying to migrate a legacy system where migration is risky or not feasible, we\nsuggest exposing it as a microservice application without without having to\nmigrate it. In this paper, we present a reusable, automated, two-phase approach\nthat combines evolutionary algorithms with machine learning techniques. In the\nfirst phase, we identify microservices at the method level using a\nmulti-objective genetic algorithm that considers both structural and semantic\ndependencies between methods. In the second phase, we generate REST APIs for\neach identified microservice using a classification algorithm to assign HTTP\nmethods and endpoints. We evaluated our approach with a case study on the\nSpring PetClinic application, which has both monolithic and microservices\nimplementations that serve as ground truth for comparison. Results demonstrate\nthat our approach successfully aligns identified microservices with those in\nthe reference microservices implementation, highlighting its effectiveness in\nservice identification and API generation."
                },
                "authors": [
                    {
                        "name": "MatthÃ©o Lecrivain"
                    },
                    {
                        "name": "Hanifa Barry"
                    },
                    {
                        "name": "Dalila Tamzalit"
                    },
                    {
                        "name": "Houari Sahraoui"
                    }
                ],
                "author_detail": {
                    "name": "Houari Sahraoui"
                },
                "author": "Houari Sahraoui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21522v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21522v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21500v1",
                "updated": "2025-03-27T13:40:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    13,
                    40,
                    6,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T13:40:06Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    13,
                    40,
                    6,
                    3,
                    86,
                    0
                ],
                "title": "OpenHuEval: Evaluating Large Language Model on Hungarian Specifics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenHuEval: Evaluating Large Language Model on Hungarian Specifics"
                },
                "summary": "We introduce OpenHuEval, the first benchmark for LLMs focusing on the\nHungarian language and specifics. OpenHuEval is constructed from a vast\ncollection of Hungarian-specific materials sourced from multiple origins. In\nthe construction, we incorporated the latest design principles for evaluating\nLLMs, such as using real user queries from the internet, emphasizing the\nassessment of LLMs' generative capabilities, and employing LLM-as-judge to\nenhance the multidimensionality and accuracy of evaluations. Ultimately,\nOpenHuEval encompasses eight Hungarian-specific dimensions, featuring five\ntasks and 3953 questions. Consequently, OpenHuEval provides the comprehensive,\nin-depth, and scientifically accurate assessment of LLM performance in the\ncontext of the Hungarian language and its specifics. We evaluated current\nmainstream LLMs, including both traditional LLMs and recently developed Large\nReasoning Models. The results demonstrate the significant necessity for\nevaluation and model optimization tailored to the Hungarian language and\nspecifics. We also established the framework for analyzing the thinking\nprocesses of LRMs with OpenHuEval, revealing intrinsic patterns and mechanisms\nof these models in non-English languages, with Hungarian serving as a\nrepresentative example. We will release OpenHuEval at\nhttps://github.com/opendatalab/OpenHuEval .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce OpenHuEval, the first benchmark for LLMs focusing on the\nHungarian language and specifics. OpenHuEval is constructed from a vast\ncollection of Hungarian-specific materials sourced from multiple origins. In\nthe construction, we incorporated the latest design principles for evaluating\nLLMs, such as using real user queries from the internet, emphasizing the\nassessment of LLMs' generative capabilities, and employing LLM-as-judge to\nenhance the multidimensionality and accuracy of evaluations. Ultimately,\nOpenHuEval encompasses eight Hungarian-specific dimensions, featuring five\ntasks and 3953 questions. Consequently, OpenHuEval provides the comprehensive,\nin-depth, and scientifically accurate assessment of LLM performance in the\ncontext of the Hungarian language and its specifics. We evaluated current\nmainstream LLMs, including both traditional LLMs and recently developed Large\nReasoning Models. The results demonstrate the significant necessity for\nevaluation and model optimization tailored to the Hungarian language and\nspecifics. We also established the framework for analyzing the thinking\nprocesses of LRMs with OpenHuEval, revealing intrinsic patterns and mechanisms\nof these models in non-English languages, with Hungarian serving as a\nrepresentative example. We will release OpenHuEval at\nhttps://github.com/opendatalab/OpenHuEval ."
                },
                "authors": [
                    {
                        "name": "Haote Yang"
                    },
                    {
                        "name": "Xingjian Wei"
                    },
                    {
                        "name": "Jiang Wu"
                    },
                    {
                        "name": "NoÃ©mi Ligeti-Nagy"
                    },
                    {
                        "name": "Jiaxing Sun"
                    },
                    {
                        "name": "Yinfan Wang"
                    },
                    {
                        "name": "Zijian GyÅzÅ Yang"
                    },
                    {
                        "name": "Junyuan Gao"
                    },
                    {
                        "name": "Jingchao Wang"
                    },
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Shasha Wang"
                    },
                    {
                        "name": "Nanjun Yu"
                    },
                    {
                        "name": "Zihao Zhang"
                    },
                    {
                        "name": "Shixin Hong"
                    },
                    {
                        "name": "Hongwei Liu"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Lijun Wu"
                    },
                    {
                        "name": "GÃ¡bor PrÃ³szÃ©ky"
                    },
                    {
                        "name": "Conghui He"
                    }
                ],
                "author_detail": {
                    "name": "Conghui He"
                },
                "author": "Conghui He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21480v2",
                "updated": "2025-03-28T12:34:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    12,
                    34,
                    25,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-27T13:12:49Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    13,
                    12,
                    49,
                    3,
                    86,
                    0
                ],
                "title": "OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs"
                },
                "summary": "The use of omni-LLMs (large language models that accept any modality as\ninput), particularly for multimodal cognitive state tasks involving speech, is\nunderstudied. We present OmniVox, the first systematic evaluation of four\nomni-LLMs on the zero-shot emotion recognition task. We evaluate on two widely\nused multimodal emotion benchmarks: IEMOCAP and MELD, and find zero-shot\nomni-LLMs outperform or are competitive with fine-tuned audio models. Alongside\nour audio-only evaluation, we also evaluate omni-LLMs on text only and text and\naudio. We present acoustic prompting, an audio-specific prompting strategy for\nomni-LLMs which focuses on acoustic feature analysis, conversation context\nanalysis, and step-by-step reasoning. We compare our acoustic prompting to\nminimal prompting and full chain-of-thought prompting techniques. We perform a\ncontext window analysis on IEMOCAP and MELD, and find that using context helps,\nespecially on IEMOCAP. We conclude with an error analysis on the generated\nacoustic reasoning outputs from the omni-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of omni-LLMs (large language models that accept any modality as\ninput), particularly for multimodal cognitive state tasks involving speech, is\nunderstudied. We present OmniVox, the first systematic evaluation of four\nomni-LLMs on the zero-shot emotion recognition task. We evaluate on two widely\nused multimodal emotion benchmarks: IEMOCAP and MELD, and find zero-shot\nomni-LLMs outperform or are competitive with fine-tuned audio models. Alongside\nour audio-only evaluation, we also evaluate omni-LLMs on text only and text and\naudio. We present acoustic prompting, an audio-specific prompting strategy for\nomni-LLMs which focuses on acoustic feature analysis, conversation context\nanalysis, and step-by-step reasoning. We compare our acoustic prompting to\nminimal prompting and full chain-of-thought prompting techniques. We perform a\ncontext window analysis on IEMOCAP and MELD, and find that using context helps,\nespecially on IEMOCAP. We conclude with an error analysis on the generated\nacoustic reasoning outputs from the omni-LLMs."
                },
                "authors": [
                    {
                        "name": "John Murzaku"
                    },
                    {
                        "name": "Owen Rambow"
                    }
                ],
                "author_detail": {
                    "name": "Owen Rambow"
                },
                "author": "Owen Rambow",
                "arxiv_comment": "Submitted to COLM 2025. Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21464v1",
                "updated": "2025-03-27T12:54:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    54,
                    0,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T12:54:00Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    54,
                    0,
                    3,
                    86,
                    0
                ],
                "title": "Harnessing Chain-of-Thought Metadata for Task Routing and Adversarial\n  Prompt Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Chain-of-Thought Metadata for Task Routing and Adversarial\n  Prompt Detection"
                },
                "summary": "In this work, we propose a metric called Number of Thoughts (NofT) to\ndetermine the difficulty of tasks pre-prompting and support Large Language\nModels (LLMs) in production contexts. By setting thresholds based on the number\nof thoughts, this metric can discern the difficulty of prompts and support more\neffective prompt routing. A 2% decrease in latency is achieved when routing\nprompts from the MathInstruct dataset through quantized, distilled versions of\nDeepseek with 1.7 billion, 7 billion, and 14 billion parameters. Moreover, this\nmetric can be used to detect adversarial prompts used in prompt injection\nattacks with high efficacy. The Number of Thoughts can inform a classifier that\nachieves 95% accuracy in adversarial prompt detection. Our experiments ad\ndatasets used are available on our GitHub page:\nhttps://github.com/rymarinelli/Number_Of_Thoughts/tree/main.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a metric called Number of Thoughts (NofT) to\ndetermine the difficulty of tasks pre-prompting and support Large Language\nModels (LLMs) in production contexts. By setting thresholds based on the number\nof thoughts, this metric can discern the difficulty of prompts and support more\neffective prompt routing. A 2% decrease in latency is achieved when routing\nprompts from the MathInstruct dataset through quantized, distilled versions of\nDeepseek with 1.7 billion, 7 billion, and 14 billion parameters. Moreover, this\nmetric can be used to detect adversarial prompts used in prompt injection\nattacks with high efficacy. The Number of Thoughts can inform a classifier that\nachieves 95% accuracy in adversarial prompt detection. Our experiments ad\ndatasets used are available on our GitHub page:\nhttps://github.com/rymarinelli/Number_Of_Thoughts/tree/main."
                },
                "authors": [
                    {
                        "name": "Ryan Marinelli"
                    },
                    {
                        "name": "Josef Pichlmeier"
                    },
                    {
                        "name": "Tamas Bisztray"
                    }
                ],
                "author_detail": {
                    "name": "Tamas Bisztray"
                },
                "author": "Tamas Bisztray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21460v1",
                "updated": "2025-03-27T12:50:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    50,
                    17,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T12:50:17Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    50,
                    17,
                    3,
                    86,
                    0
                ],
                "title": "Large Language Model Agent: A Survey on Methodology, Applications and\n  Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Agent: A Survey on Methodology, Applications and\n  Challenges"
                },
                "summary": "The era of intelligent agents is upon us, driven by revolutionary\nadvancements in large language models. Large Language Model (LLM) agents, with\ngoal-driven behaviors and dynamic adaptation capabilities, potentially\nrepresent a critical pathway toward artificial general intelligence. This\nsurvey systematically deconstructs LLM agent systems through a\nmethodology-centered taxonomy, linking architectural foundations, collaboration\nmechanisms, and evolutionary pathways. We unify fragmented research threads by\nrevealing fundamental connections between agent design principles and their\nemergent behaviors in complex environments. Our work provides a unified\narchitectural perspective, examining how agents are constructed, how they\ncollaborate, and how they evolve over time, while also addressing evaluation\nmethodologies, tool applications, practical challenges, and diverse application\ndomains. By surveying the latest developments in this rapidly evolving field,\nwe offer researchers a structured taxonomy for understanding LLM agents and\nidentify promising directions for future research. The collection is available\nat https://github.com/luo-junyu/Awesome-Agent-Papers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The era of intelligent agents is upon us, driven by revolutionary\nadvancements in large language models. Large Language Model (LLM) agents, with\ngoal-driven behaviors and dynamic adaptation capabilities, potentially\nrepresent a critical pathway toward artificial general intelligence. This\nsurvey systematically deconstructs LLM agent systems through a\nmethodology-centered taxonomy, linking architectural foundations, collaboration\nmechanisms, and evolutionary pathways. We unify fragmented research threads by\nrevealing fundamental connections between agent design principles and their\nemergent behaviors in complex environments. Our work provides a unified\narchitectural perspective, examining how agents are constructed, how they\ncollaborate, and how they evolve over time, while also addressing evaluation\nmethodologies, tool applications, practical challenges, and diverse application\ndomains. By surveying the latest developments in this rapidly evolving field,\nwe offer researchers a structured taxonomy for understanding LLM agents and\nidentify promising directions for future research. The collection is available\nat https://github.com/luo-junyu/Awesome-Agent-Papers."
                },
                "authors": [
                    {
                        "name": "Junyu Luo"
                    },
                    {
                        "name": "Weizhi Zhang"
                    },
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Yusheng Zhao"
                    },
                    {
                        "name": "Junwei Yang"
                    },
                    {
                        "name": "Yiyang Gu"
                    },
                    {
                        "name": "Bohan Wu"
                    },
                    {
                        "name": "Binqi Chen"
                    },
                    {
                        "name": "Ziyue Qiao"
                    },
                    {
                        "name": "Qingqing Long"
                    },
                    {
                        "name": "Rongcheng Tu"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Wei Ju"
                    },
                    {
                        "name": "Zhiping Xiao"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Meng Xiao"
                    },
                    {
                        "name": "Chenwu Liu"
                    },
                    {
                        "name": "Jingyang Yuan"
                    },
                    {
                        "name": "Shichang Zhang"
                    },
                    {
                        "name": "Yiqiao Jin"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Xian Wu"
                    },
                    {
                        "name": "Hanqing Zhao"
                    },
                    {
                        "name": "Dacheng Tao"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Ming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhang"
                },
                "author": "Ming Zhang",
                "arxiv_comment": "329 papers surveyed, resources are at\n  https://github.com/luo-junyu/Awesome-Agent-Papers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21459v1",
                "updated": "2025-03-27T12:49:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    49,
                    9,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T12:49:09Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    49,
                    9,
                    3,
                    86,
                    0
                ],
                "title": "RoadSocial: A Diverse VideoQA Dataset and Benchmark for Road Event\n  Understanding from Social Video Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoadSocial: A Diverse VideoQA Dataset and Benchmark for Road Event\n  Understanding from Social Video Narratives"
                },
                "summary": "We introduce RoadSocial, a large-scale, diverse VideoQA dataset tailored for\ngeneric road event understanding from social media narratives. Unlike existing\ndatasets limited by regional bias, viewpoint bias and expert-driven\nannotations, RoadSocial captures the global complexity of road events with\nvaried geographies, camera viewpoints (CCTV, handheld, drones) and rich social\ndiscourse. Our scalable semi-automatic annotation framework leverages Text LLMs\nand Video LLMs to generate comprehensive question-answer pairs across 12\nchallenging QA tasks, pushing the boundaries of road event understanding.\nRoadSocial is derived from social media videos spanning 14M frames and 414K\nsocial comments, resulting in a dataset with 13.2K videos, 674 tags and 260K\nhigh-quality QA pairs. We evaluate 18 Video LLMs (open-source and proprietary,\ndriving-specific and general-purpose) on our road event understanding\nbenchmark. We also demonstrate RoadSocial's utility in improving road event\nunderstanding capabilities of general-purpose Video LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RoadSocial, a large-scale, diverse VideoQA dataset tailored for\ngeneric road event understanding from social media narratives. Unlike existing\ndatasets limited by regional bias, viewpoint bias and expert-driven\nannotations, RoadSocial captures the global complexity of road events with\nvaried geographies, camera viewpoints (CCTV, handheld, drones) and rich social\ndiscourse. Our scalable semi-automatic annotation framework leverages Text LLMs\nand Video LLMs to generate comprehensive question-answer pairs across 12\nchallenging QA tasks, pushing the boundaries of road event understanding.\nRoadSocial is derived from social media videos spanning 14M frames and 414K\nsocial comments, resulting in a dataset with 13.2K videos, 674 tags and 260K\nhigh-quality QA pairs. We evaluate 18 Video LLMs (open-source and proprietary,\ndriving-specific and general-purpose) on our road event understanding\nbenchmark. We also demonstrate RoadSocial's utility in improving road event\nunderstanding capabilities of general-purpose Video LLMs."
                },
                "authors": [
                    {
                        "name": "Chirag Parikh"
                    },
                    {
                        "name": "Deepti Rawat"
                    },
                    {
                        "name": "Rakshitha R. T."
                    },
                    {
                        "name": "Tathagata Ghosh"
                    },
                    {
                        "name": "Ravi Kiran Sarvadevabhatla"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Kiran Sarvadevabhatla"
                },
                "author": "Ravi Kiran Sarvadevabhatla",
                "arxiv_comment": "Accepted at CVPR 2025; Project Page: https://roadsocial.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21433v1",
                "updated": "2025-03-27T12:18:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    18,
                    49,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T12:18:49Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    18,
                    49,
                    3,
                    86,
                    0
                ],
                "title": "On Learning-Based Traffic Monitoring With a Swarm of Drones",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Learning-Based Traffic Monitoring With a Swarm of Drones"
                },
                "summary": "Efficient traffic monitoring is crucial for managing urban transportation\nnetworks, especially under congested and dynamically changing traffic\nconditions. Drones offer a scalable and cost-effective alternative to fixed\nsensor networks. However, deploying fleets of low-cost drones for traffic\nmonitoring poses challenges in adaptability, scalability, and real-time\noperation. To address these issues, we propose a learning-based framework for\ndecentralized traffic monitoring with drone swarms, targeting the uneven and\nunpredictable distribution of monitoring needs across urban areas. Our approach\nintroduces a semi-decentralized reinforcement learning model, which trains a\nsingle Q-function using the collective experience of the swarm. This model\nsupports full scalability, flexible deployment, and, when hardware allows, the\nonline adaptation of each drone's action-selection mechanism. We first train\nand evaluate the model in a synthetic traffic environment, followed by a case\nstudy using real traffic data from Shenzhen, China, to validate its performance\nand demonstrate its potential for real-world applications in complex urban\nmonitoring tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient traffic monitoring is crucial for managing urban transportation\nnetworks, especially under congested and dynamically changing traffic\nconditions. Drones offer a scalable and cost-effective alternative to fixed\nsensor networks. However, deploying fleets of low-cost drones for traffic\nmonitoring poses challenges in adaptability, scalability, and real-time\noperation. To address these issues, we propose a learning-based framework for\ndecentralized traffic monitoring with drone swarms, targeting the uneven and\nunpredictable distribution of monitoring needs across urban areas. Our approach\nintroduces a semi-decentralized reinforcement learning model, which trains a\nsingle Q-function using the collective experience of the swarm. This model\nsupports full scalability, flexible deployment, and, when hardware allows, the\nonline adaptation of each drone's action-selection mechanism. We first train\nand evaluate the model in a synthetic traffic environment, followed by a case\nstudy using real traffic data from Shenzhen, China, to validate its performance\nand demonstrate its potential for real-world applications in complex urban\nmonitoring tasks."
                },
                "authors": [
                    {
                        "name": "Marko Maljkovic"
                    },
                    {
                        "name": "Nikolas Geroliminis"
                    }
                ],
                "author_detail": {
                    "name": "Nikolas Geroliminis"
                },
                "author": "Nikolas Geroliminis",
                "arxiv_comment": "Extended version of the paper accepted for presentation at the 23rd\n  IEEE European Control Conference (ECC 2025), Thessaloniki, Greece",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17038v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17038v3",
                "updated": "2025-03-27T12:14:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    14,
                    56,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-21T10:48:35Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    48,
                    35,
                    4,
                    80,
                    0
                ],
                "title": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation"
                },
                "summary": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference."
                },
                "authors": [
                    {
                        "name": "Ashutosh Pradhan"
                    },
                    {
                        "name": "Daniele Ottaviano"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Haozheng Huang"
                    },
                    {
                        "name": "Alexander Zuepke"
                    },
                    {
                        "name": "Andrea Bastoni"
                    },
                    {
                        "name": "Marco Caccamo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Caccamo"
                },
                "author": "Marco Caccamo",
                "arxiv_comment": "Accepted for publication in the Proceedings of the 31st IEEE\n  Real-Time and Embedded Technology and Applications Symposium (RTAS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17038v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17038v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.3; C.4; D.4.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21422v1",
                "updated": "2025-03-27T12:10:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    10,
                    15,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T12:10:15Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    10,
                    15,
                    3,
                    86,
                    0
                ],
                "title": "From Deep Learning to LLMs: A survey of AI in Quantitative Investment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Deep Learning to LLMs: A survey of AI in Quantitative Investment"
                },
                "summary": "Quantitative investment (quant) is an emerging, technology-driven approach in\nasset management, increasingy shaped by advancements in artificial\nintelligence. Recent advances in deep learning and large language models (LLMs)\nfor quant finance have improved predictive modeling and enabled agent-based\nautomation, suggesting a potential paradigm shift in this field. In this\nsurvey, taking alpha strategy as a representative example, we explore how AI\ncontributes to the quantitative investment pipeline. We first examine the early\nstage of quant research, centered on human-crafted features and traditional\nstatistical models with an established alpha pipeline. We then discuss the rise\nof deep learning, which enabled scalable modeling across the entire pipeline\nfrom data processing to order execution. Building on this, we highlight the\nemerging role of LLMs in extending AI beyond prediction, empowering autonomous\nagents to process unstructured data, generate alphas, and support\nself-iterative workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantitative investment (quant) is an emerging, technology-driven approach in\nasset management, increasingy shaped by advancements in artificial\nintelligence. Recent advances in deep learning and large language models (LLMs)\nfor quant finance have improved predictive modeling and enabled agent-based\nautomation, suggesting a potential paradigm shift in this field. In this\nsurvey, taking alpha strategy as a representative example, we explore how AI\ncontributes to the quantitative investment pipeline. We first examine the early\nstage of quant research, centered on human-crafted features and traditional\nstatistical models with an established alpha pipeline. We then discuss the rise\nof deep learning, which enabled scalable modeling across the entire pipeline\nfrom data processing to order execution. Building on this, we highlight the\nemerging role of LLMs in extending AI beyond prediction, empowering autonomous\nagents to process unstructured data, generate alphas, and support\nself-iterative workflows."
                },
                "authors": [
                    {
                        "name": "Bokai Cao"
                    },
                    {
                        "name": "Saizhuo Wang"
                    },
                    {
                        "name": "Xinyi Lin"
                    },
                    {
                        "name": "Xiaojun Wu"
                    },
                    {
                        "name": "Haohan Zhang"
                    },
                    {
                        "name": "Lionel M. Ni"
                    },
                    {
                        "name": "Jian Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jian Guo"
                },
                "author": "Jian Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21411v1",
                "updated": "2025-03-27T11:56:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    56,
                    27,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T11:56:27Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    56,
                    27,
                    3,
                    86,
                    0
                ],
                "title": "Exploring the Roles of Large Language Models in Reshaping Transportation\n  Systems: A Survey, Framework, and Roadmap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Roles of Large Language Models in Reshaping Transportation\n  Systems: A Survey, Framework, and Roadmap"
                },
                "summary": "Modern transportation systems face pressing challenges due to increasing\ndemand, dynamic environments, and heterogeneous information integration. The\nrapid evolution of Large Language Models (LLMs) offers transformative potential\nto address these challenges. Extensive knowledge and high-level capabilities\nderived from pretraining evolve the default role of LLMs as text generators to\nbecome versatile, knowledge-driven task solvers for intelligent transportation\nsystems. This survey first presents LLM4TR, a novel conceptual framework that\nsystematically categorizes the roles of LLMs in transportation into four\nsynergetic dimensions: information processors, knowledge encoders, component\ngenerators, and decision facilitators. Through a unified taxonomy, we\nsystematically elucidate how LLMs bridge fragmented data pipelines, enhance\npredictive analytics, simulate human-like reasoning, and enable closed-loop\ninteractions across sensing, learning, modeling, and managing tasks in\ntransportation systems. For each role, our review spans diverse applications,\nfrom traffic prediction and autonomous driving to safety analytics and urban\nmobility optimization, highlighting how emergent capabilities of LLMs such as\nin-context learning and step-by-step reasoning can enhance the operation and\nmanagement of transportation systems. We further curate practical guidance,\nincluding available resources and computational guidelines, to support\nreal-world deployment. By identifying challenges in existing LLM-based\nsolutions, this survey charts a roadmap for advancing LLM-driven transportation\nresearch, positioning LLMs as central actors in the next generation of\ncyber-physical-social mobility ecosystems. Online resources can be found in the\nproject page: https://github.com/tongnie/awesome-llm4tr.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern transportation systems face pressing challenges due to increasing\ndemand, dynamic environments, and heterogeneous information integration. The\nrapid evolution of Large Language Models (LLMs) offers transformative potential\nto address these challenges. Extensive knowledge and high-level capabilities\nderived from pretraining evolve the default role of LLMs as text generators to\nbecome versatile, knowledge-driven task solvers for intelligent transportation\nsystems. This survey first presents LLM4TR, a novel conceptual framework that\nsystematically categorizes the roles of LLMs in transportation into four\nsynergetic dimensions: information processors, knowledge encoders, component\ngenerators, and decision facilitators. Through a unified taxonomy, we\nsystematically elucidate how LLMs bridge fragmented data pipelines, enhance\npredictive analytics, simulate human-like reasoning, and enable closed-loop\ninteractions across sensing, learning, modeling, and managing tasks in\ntransportation systems. For each role, our review spans diverse applications,\nfrom traffic prediction and autonomous driving to safety analytics and urban\nmobility optimization, highlighting how emergent capabilities of LLMs such as\nin-context learning and step-by-step reasoning can enhance the operation and\nmanagement of transportation systems. We further curate practical guidance,\nincluding available resources and computational guidelines, to support\nreal-world deployment. By identifying challenges in existing LLM-based\nsolutions, this survey charts a roadmap for advancing LLM-driven transportation\nresearch, positioning LLMs as central actors in the next generation of\ncyber-physical-social mobility ecosystems. Online resources can be found in the\nproject page: https://github.com/tongnie/awesome-llm4tr."
                },
                "authors": [
                    {
                        "name": "Tong Nie"
                    },
                    {
                        "name": "Jian Sun"
                    },
                    {
                        "name": "Wei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ma"
                },
                "author": "Wei Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21408v1",
                "updated": "2025-03-27T11:52:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    52,
                    8,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T11:52:08Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    52,
                    8,
                    3,
                    86,
                    0
                ],
                "title": "VALLR: Visual ASR Language Model for Lip Reading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VALLR: Visual ASR Language Model for Lip Reading"
                },
                "summary": "Lip Reading, or Visual Automatic Speech Recognition (V-ASR), is a complex\ntask requiring the interpretation of spoken language exclusively from visual\ncues, primarily lip movements and facial expressions. This task is especially\nchallenging due to the absence of auditory information and the inherent\nambiguity when visually distinguishing phonemes that have overlapping visemes\nwhere different phonemes appear identical on the lips. Current methods\ntypically attempt to predict words or characters directly from these visual\ncues, but this approach frequently encounters high error rates due to\ncoarticulation effects and viseme ambiguity. We propose a novel two-stage,\nphoneme-centric framework for Visual Automatic Speech Recognition (V-ASR) that\naddresses these longstanding challenges. First, our model predicts a compact\nsequence of phonemes from visual inputs using a Video Transformer with a CTC\nhead, thereby reducing the task complexity and achieving robust speaker\ninvariance. This phoneme output then serves as the input to a fine-tuned Large\nLanguage Model (LLM), which reconstructs coherent words and sentences by\nleveraging broader linguistic context. Unlike existing methods that either\npredict words directly-often faltering on visually similar phonemes-or rely on\nlarge-scale multimodal pre-training, our approach explicitly encodes\nintermediate linguistic structure while remaining highly data efficient. We\ndemonstrate state-of-the-art performance on two challenging datasets, LRS2 and\nLRS3, where our method achieves significant reductions in Word Error Rate (WER)\nachieving a SOTA WER of 18.7 on LRS3 despite using 99.4% less labelled data\nthan the next best approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lip Reading, or Visual Automatic Speech Recognition (V-ASR), is a complex\ntask requiring the interpretation of spoken language exclusively from visual\ncues, primarily lip movements and facial expressions. This task is especially\nchallenging due to the absence of auditory information and the inherent\nambiguity when visually distinguishing phonemes that have overlapping visemes\nwhere different phonemes appear identical on the lips. Current methods\ntypically attempt to predict words or characters directly from these visual\ncues, but this approach frequently encounters high error rates due to\ncoarticulation effects and viseme ambiguity. We propose a novel two-stage,\nphoneme-centric framework for Visual Automatic Speech Recognition (V-ASR) that\naddresses these longstanding challenges. First, our model predicts a compact\nsequence of phonemes from visual inputs using a Video Transformer with a CTC\nhead, thereby reducing the task complexity and achieving robust speaker\ninvariance. This phoneme output then serves as the input to a fine-tuned Large\nLanguage Model (LLM), which reconstructs coherent words and sentences by\nleveraging broader linguistic context. Unlike existing methods that either\npredict words directly-often faltering on visually similar phonemes-or rely on\nlarge-scale multimodal pre-training, our approach explicitly encodes\nintermediate linguistic structure while remaining highly data efficient. We\ndemonstrate state-of-the-art performance on two challenging datasets, LRS2 and\nLRS3, where our method achieves significant reductions in Word Error Rate (WER)\nachieving a SOTA WER of 18.7 on LRS3 despite using 99.4% less labelled data\nthan the next best approach."
                },
                "authors": [
                    {
                        "name": "Marshall Thomas"
                    },
                    {
                        "name": "Edward Fish"
                    },
                    {
                        "name": "Richard Bowden"
                    }
                ],
                "author_detail": {
                    "name": "Richard Bowden"
                },
                "author": "Richard Bowden",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17258v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17258v3",
                "updated": "2025-03-27T11:41:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    41,
                    54,
                    3,
                    86,
                    0
                ],
                "published": "2024-08-30T12:56:17Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    12,
                    56,
                    17,
                    4,
                    243,
                    0
                ],
                "title": "Joint Estimation and Prediction of City-wide Delivery Demand: A Large\n  Language Model Empowered Graph-based Learning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Estimation and Prediction of City-wide Delivery Demand: A Large\n  Language Model Empowered Graph-based Learning Approach"
                },
                "summary": "The proliferation of e-commerce and urbanization has significantly\nintensified delivery operations in urban areas, boosting the volume and\ncomplexity of delivery demand. Data-driven predictive methods, especially those\nutilizing machine learning techniques, have emerged to handle these\ncomplexities in urban delivery demand management problems. One particularly\npressing issue that has yet to be sufficiently addressed is the joint\nestimation and prediction of city-wide delivery demand, as well as the\ngeneralization of the model to new cities. To this end, we formulate this\nproblem as a transferable graph-based spatiotemporal learning task. First, an\nindividual-collective message-passing neural network model is formalized to\ncapture the interaction between demand patterns of associated regions. Second,\nby exploiting recent advances in large language models (LLMs), we extract\ngeneral geospatial knowledge encodings from the unstructured locational data\nusing the embedding generated by LLMs. Last, to encourage the cross-city\ngeneralization of the model, we integrate the encoding into the demand\npredictor in a transferable way. Comprehensive empirical evaluation results on\ntwo real-world delivery datasets, including eight cities in China and the US,\ndemonstrate that our model significantly outperforms state-of-the-art baselines\nin accuracy, efficiency, and transferability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of e-commerce and urbanization has significantly\nintensified delivery operations in urban areas, boosting the volume and\ncomplexity of delivery demand. Data-driven predictive methods, especially those\nutilizing machine learning techniques, have emerged to handle these\ncomplexities in urban delivery demand management problems. One particularly\npressing issue that has yet to be sufficiently addressed is the joint\nestimation and prediction of city-wide delivery demand, as well as the\ngeneralization of the model to new cities. To this end, we formulate this\nproblem as a transferable graph-based spatiotemporal learning task. First, an\nindividual-collective message-passing neural network model is formalized to\ncapture the interaction between demand patterns of associated regions. Second,\nby exploiting recent advances in large language models (LLMs), we extract\ngeneral geospatial knowledge encodings from the unstructured locational data\nusing the embedding generated by LLMs. Last, to encourage the cross-city\ngeneralization of the model, we integrate the encoding into the demand\npredictor in a transferable way. Comprehensive empirical evaluation results on\ntwo real-world delivery datasets, including eight cities in China and the US,\ndemonstrate that our model significantly outperforms state-of-the-art baselines\nin accuracy, efficiency, and transferability."
                },
                "authors": [
                    {
                        "name": "Tong Nie"
                    },
                    {
                        "name": "Junlin He"
                    },
                    {
                        "name": "Yuewen Mei"
                    },
                    {
                        "name": "Guoyang Qin"
                    },
                    {
                        "name": "Guilong Li"
                    },
                    {
                        "name": "Jian Sun"
                    },
                    {
                        "name": "Wei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ma"
                },
                "author": "Wei Ma",
                "arxiv_doi": "10.1016/j.tre.2025.104075",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.tre.2025.104075",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.17258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17258v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Transportation Research Part E: Logistics and Transportation\n  Review, 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21393v1",
                "updated": "2025-03-27T11:35:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    35,
                    40,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T11:35:40Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    35,
                    40,
                    3,
                    86,
                    0
                ],
                "title": "An evaluation of LLMs and Google Translate for translation of selected\n  Indian languages via sentiment and semantic analyses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An evaluation of LLMs and Google Translate for translation of selected\n  Indian languages via sentiment and semantic analyses"
                },
                "summary": "Large Language models (LLMs) have been prominent for language translation,\nincluding low-resource languages. There has been limited study about the\nassessment of the quality of translations generated by LLMs, including Gemini,\nGPT and Google Translate. In this study, we address this limitation by using\nsemantic and sentiment analysis of selected LLMs for Indian languages,\nincluding Sanskrit, Telugu and Hindi. We select prominent texts that have been\nwell translated by experts and use LLMs to generate their translations to\nEnglish, and then we provide a comparison with selected expert (human)\ntranslations. Our findings suggest that while LLMs have made significant\nprogress in translation accuracy, challenges remain in preserving sentiment and\nsemantic integrity, especially in figurative and philosophical contexts. The\nsentiment analysis revealed that GPT-4o and GPT-3.5 are better at preserving\nthe sentiments for the Bhagavad Gita (Sanskrit-English) translations when\ncompared to Google Translate. We observed a similar trend for the case of Tamas\n(Hindi-English) and Maha P (Telugu-English) translations. GPT-4o performs\nsimilarly to GPT-3.5 in the translation in terms of sentiments for the three\nlanguages. We found that LLMs are generally better at translation for capturing\nsentiments when compared to Google Translate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language models (LLMs) have been prominent for language translation,\nincluding low-resource languages. There has been limited study about the\nassessment of the quality of translations generated by LLMs, including Gemini,\nGPT and Google Translate. In this study, we address this limitation by using\nsemantic and sentiment analysis of selected LLMs for Indian languages,\nincluding Sanskrit, Telugu and Hindi. We select prominent texts that have been\nwell translated by experts and use LLMs to generate their translations to\nEnglish, and then we provide a comparison with selected expert (human)\ntranslations. Our findings suggest that while LLMs have made significant\nprogress in translation accuracy, challenges remain in preserving sentiment and\nsemantic integrity, especially in figurative and philosophical contexts. The\nsentiment analysis revealed that GPT-4o and GPT-3.5 are better at preserving\nthe sentiments for the Bhagavad Gita (Sanskrit-English) translations when\ncompared to Google Translate. We observed a similar trend for the case of Tamas\n(Hindi-English) and Maha P (Telugu-English) translations. GPT-4o performs\nsimilarly to GPT-3.5 in the translation in terms of sentiments for the three\nlanguages. We found that LLMs are generally better at translation for capturing\nsentiments when compared to Google Translate."
                },
                "authors": [
                    {
                        "name": "Rohitash Chandra"
                    },
                    {
                        "name": "Aryan Chaudhary"
                    },
                    {
                        "name": "Yeshwanth Rayavarapu"
                    }
                ],
                "author_detail": {
                    "name": "Yeshwanth Rayavarapu"
                },
                "author": "Yeshwanth Rayavarapu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08356v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08356v3",
                "updated": "2025-03-27T11:31:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    31,
                    39,
                    3,
                    86,
                    0
                ],
                "published": "2025-02-12T12:39:51Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    39,
                    51,
                    2,
                    43,
                    0
                ],
                "title": "Systematic Knowledge Injection into Large Language Models via Diverse\n  Augmentation for Domain-Specific RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Knowledge Injection into Large Language Models via Diverse\n  Augmentation for Domain-Specific RAG"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a prominent method for\nincorporating domain knowledge into Large Language Models (LLMs). While RAG\nenhances response relevance by incorporating retrieved domain knowledge in the\ncontext, retrieval errors can still lead to hallucinations and incorrect\nanswers. To recover from retriever failures, domain knowledge is injected by\nfine-tuning the model to generate the correct response, even in the case of\nretrieval errors. However, we observe that without systematic knowledge\naugmentation, fine-tuned LLMs may memorize new information but still fail to\nextract relevant domain knowledge, leading to poor performance. In this work,\nwe present a novel framework that significantly enhances the fine-tuning\nprocess by augmenting the training data in two ways -- context augmentation and\nknowledge paraphrasing. In context augmentation, we create multiple training\nsamples for a given QA pair by varying the relevance of the retrieved\ninformation, teaching the model when to ignore and when to rely on retrieved\ncontent. In knowledge paraphrasing, we fine-tune with multiple answers to the\nsame question, enabling LLMs to better internalize specialized knowledge. To\nmitigate catastrophic forgetting due to fine-tuning, we add a domain-specific\nidentifier to a question and also utilize a replay buffer containing general QA\npairs. Experimental results demonstrate the efficacy of our method over\nexisting techniques, achieving up to 10\\% relative gain in token-level recall\nwhile preserving the LLM's generalization capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a prominent method for\nincorporating domain knowledge into Large Language Models (LLMs). While RAG\nenhances response relevance by incorporating retrieved domain knowledge in the\ncontext, retrieval errors can still lead to hallucinations and incorrect\nanswers. To recover from retriever failures, domain knowledge is injected by\nfine-tuning the model to generate the correct response, even in the case of\nretrieval errors. However, we observe that without systematic knowledge\naugmentation, fine-tuned LLMs may memorize new information but still fail to\nextract relevant domain knowledge, leading to poor performance. In this work,\nwe present a novel framework that significantly enhances the fine-tuning\nprocess by augmenting the training data in two ways -- context augmentation and\nknowledge paraphrasing. In context augmentation, we create multiple training\nsamples for a given QA pair by varying the relevance of the retrieved\ninformation, teaching the model when to ignore and when to rely on retrieved\ncontent. In knowledge paraphrasing, we fine-tune with multiple answers to the\nsame question, enabling LLMs to better internalize specialized knowledge. To\nmitigate catastrophic forgetting due to fine-tuning, we add a domain-specific\nidentifier to a question and also utilize a replay buffer containing general QA\npairs. Experimental results demonstrate the efficacy of our method over\nexisting techniques, achieving up to 10\\% relative gain in token-level recall\nwhile preserving the LLM's generalization capabilities."
                },
                "authors": [
                    {
                        "name": "Kushagra Bhushan"
                    },
                    {
                        "name": "Yatin Nandwani"
                    },
                    {
                        "name": "Dinesh Khandelwal"
                    },
                    {
                        "name": "Sonam Gupta"
                    },
                    {
                        "name": "Gaurav Pandey"
                    },
                    {
                        "name": "Dinesh Raghu"
                    },
                    {
                        "name": "Sachindra Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Sachindra Joshi"
                },
                "author": "Sachindra Joshi",
                "arxiv_comment": "22 pages, 14 tables, to be published in NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08356v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08356v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11441v2",
                "updated": "2025-03-27T11:29:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    29,
                    21,
                    3,
                    86,
                    0
                ],
                "published": "2025-01-20T12:29:09Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    12,
                    29,
                    9,
                    0,
                    20,
                    0
                ],
                "title": "Ontology Matching with Large Language Models and Prioritized Depth-First\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology Matching with Large Language Models and Prioritized Depth-First\n  Search"
                },
                "summary": "Ontology matching (OM) plays a key role in enabling data interoperability and\nknowledge sharing, but it remains challenging due to the need for large\ntraining datasets and limited vocabulary processing in machine learning\napproaches. Recently, methods based on Large Language Model (LLMs) have shown\ngreat promise in OM, particularly through the use of a retrieve-then-prompt\npipeline. In this approach, relevant target entities are first retrieved and\nthen used to prompt the LLM to predict the final matches. Despite their\npotential, these systems still present limited performance and high\ncomputational overhead. To address these issues, we introduce MILA, a novel\napproach that embeds a retrieve-identify-prompt pipeline within a prioritized\ndepth-first search (PDFS) strategy. This approach efficiently identifies a\nlarge number of semantic correspondences with high accuracy, limiting LLM\nrequests to only the most borderline cases. We evaluated MILA using the\nbiomedical challenge proposed in the 2023 and 2024 editions of the Ontology\nAlignment Evaluation Initiative. Our method achieved the highest F-Measure in\nfour of the five unsupervised tasks, outperforming state-of-the-art OM systems\nby up to 17%. It also performed better than or comparable to the leading\nsupervised OM systems. MILA further exhibited task-agnostic performance,\nremaining stable across all tasks and settings, while significantly reducing\nLLM requests. These findings highlight that high-performance LLM-based OM can\nbe achieved through a combination of programmed (PDFS), learned (embedding\nvectors), and prompting-based heuristics, without the need of domain-specific\nheuristics or fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology matching (OM) plays a key role in enabling data interoperability and\nknowledge sharing, but it remains challenging due to the need for large\ntraining datasets and limited vocabulary processing in machine learning\napproaches. Recently, methods based on Large Language Model (LLMs) have shown\ngreat promise in OM, particularly through the use of a retrieve-then-prompt\npipeline. In this approach, relevant target entities are first retrieved and\nthen used to prompt the LLM to predict the final matches. Despite their\npotential, these systems still present limited performance and high\ncomputational overhead. To address these issues, we introduce MILA, a novel\napproach that embeds a retrieve-identify-prompt pipeline within a prioritized\ndepth-first search (PDFS) strategy. This approach efficiently identifies a\nlarge number of semantic correspondences with high accuracy, limiting LLM\nrequests to only the most borderline cases. We evaluated MILA using the\nbiomedical challenge proposed in the 2023 and 2024 editions of the Ontology\nAlignment Evaluation Initiative. Our method achieved the highest F-Measure in\nfour of the five unsupervised tasks, outperforming state-of-the-art OM systems\nby up to 17%. It also performed better than or comparable to the leading\nsupervised OM systems. MILA further exhibited task-agnostic performance,\nremaining stable across all tasks and settings, while significantly reducing\nLLM requests. These findings highlight that high-performance LLM-based OM can\nbe achieved through a combination of programmed (PDFS), learned (embedding\nvectors), and prompting-based heuristics, without the need of domain-specific\nheuristics or fine-tuning."
                },
                "authors": [
                    {
                        "name": "Maria Taboada"
                    },
                    {
                        "name": "Diego Martinez"
                    },
                    {
                        "name": "Mohammed Arideh"
                    },
                    {
                        "name": "Rosa Mosquera"
                    }
                ],
                "author_detail": {
                    "name": "Rosa Mosquera"
                },
                "author": "Rosa Mosquera",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21383v1",
                "updated": "2025-03-27T11:25:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    25,
                    22,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T11:25:22Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    25,
                    22,
                    3,
                    86,
                    0
                ],
                "title": "Controlling Large Language Model with Latent Actions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling Large Language Model with Latent Actions"
                },
                "summary": "Adapting Large Language Models (LLMs) to downstream tasks using Reinforcement\nLearning (RL) has proven to be an effective approach. However, LLMs do not\ninherently define the structure of an agent for RL training, particularly in\nterms of defining the action space. This paper studies learning a compact\nlatent action space to enhance the controllability and exploration of RL for\nLLMs. We propose Controlling Large Language Models with Latent Actions (CoLA),\na framework that integrates a latent action space into pre-trained LLMs. We\napply CoLA to the Llama-3.1-8B model. Our experiments demonstrate that,\ncompared to RL with token-level actions, CoLA's latent action enables greater\nsemantic diversity in text generation. For enhancing downstream tasks, we show\nthat CoLA with RL achieves a score of 42.4 on the math500 benchmark, surpassing\nthe baseline score of 38.2, and reaches 68.2 when augmented with a Monte Carlo\nTree Search variant. Furthermore, CoLA with RL consistently improves\nperformance on agent-based tasks without degrading the pre-trained LLM's\ncapabilities, unlike the baseline. Finally, CoLA reduces computation time by\nhalf in tasks involving enhanced thinking prompts for LLMs by RL. These results\nhighlight CoLA's potential to advance RL-based adaptation of LLMs for\ndownstream applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Large Language Models (LLMs) to downstream tasks using Reinforcement\nLearning (RL) has proven to be an effective approach. However, LLMs do not\ninherently define the structure of an agent for RL training, particularly in\nterms of defining the action space. This paper studies learning a compact\nlatent action space to enhance the controllability and exploration of RL for\nLLMs. We propose Controlling Large Language Models with Latent Actions (CoLA),\na framework that integrates a latent action space into pre-trained LLMs. We\napply CoLA to the Llama-3.1-8B model. Our experiments demonstrate that,\ncompared to RL with token-level actions, CoLA's latent action enables greater\nsemantic diversity in text generation. For enhancing downstream tasks, we show\nthat CoLA with RL achieves a score of 42.4 on the math500 benchmark, surpassing\nthe baseline score of 38.2, and reaches 68.2 when augmented with a Monte Carlo\nTree Search variant. Furthermore, CoLA with RL consistently improves\nperformance on agent-based tasks without degrading the pre-trained LLM's\ncapabilities, unlike the baseline. Finally, CoLA reduces computation time by\nhalf in tasks involving enhanced thinking prompts for LLMs by RL. These results\nhighlight CoLA's potential to advance RL-based adaptation of LLMs for\ndownstream applications."
                },
                "authors": [
                    {
                        "name": "Chengxing Jia"
                    },
                    {
                        "name": "Ziniu Li"
                    },
                    {
                        "name": "Pengyuan Wang"
                    },
                    {
                        "name": "Yi-Chen Li"
                    },
                    {
                        "name": "Zhenyu Hou"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Yang Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Yu"
                },
                "author": "Yang Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21380v1",
                "updated": "2025-03-27T11:20:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    20,
                    17,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T11:20:17Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    20,
                    17,
                    3,
                    86,
                    0
                ],
                "title": "Challenging the Boundaries of Reasoning: An Olympiad-Level Math\n  Benchmark for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenging the Boundaries of Reasoning: An Olympiad-Level Math\n  Benchmark for Large Language Models"
                },
                "summary": "In recent years, the rapid development of large reasoning models has resulted\nin the saturation of existing benchmarks for evaluating mathematical reasoning,\nhighlighting the urgent need for more challenging and rigorous evaluation\nframeworks. To address this gap, we introduce OlymMATH, a novel Olympiad-level\nmathematical benchmark, designed to rigorously test the complex reasoning\ncapabilities of LLMs. OlymMATH features 200 meticulously curated problems, each\nmanually verified and available in parallel English and Chinese versions. The\nproblems are systematically organized into two distinct difficulty tiers: (1)\nAIME-level problems (easy) that establish a baseline for mathematical reasoning\nassessment, and (2) significantly more challenging problems (hard) designed to\npush the boundaries of current state-of-the-art models. In our benchmark, these\nproblems span four core mathematical fields, each including a verifiable\nnumerical solution to enable objective, rule-based evaluation. Empirical\nresults underscore the significant challenge presented by OlymMATH, with\nstate-of-the-art models including DeepSeek-R1 and OpenAI's o3-mini\ndemonstrating notably limited accuracy on the hard subset. Furthermore, the\nbenchmark facilitates comprehensive bilingual assessment of mathematical\nreasoning abilities-a critical dimension that remains largely unaddressed in\nmainstream mathematical reasoning benchmarks. We release the OlymMATH benchmark\nat the STILL project: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the rapid development of large reasoning models has resulted\nin the saturation of existing benchmarks for evaluating mathematical reasoning,\nhighlighting the urgent need for more challenging and rigorous evaluation\nframeworks. To address this gap, we introduce OlymMATH, a novel Olympiad-level\nmathematical benchmark, designed to rigorously test the complex reasoning\ncapabilities of LLMs. OlymMATH features 200 meticulously curated problems, each\nmanually verified and available in parallel English and Chinese versions. The\nproblems are systematically organized into two distinct difficulty tiers: (1)\nAIME-level problems (easy) that establish a baseline for mathematical reasoning\nassessment, and (2) significantly more challenging problems (hard) designed to\npush the boundaries of current state-of-the-art models. In our benchmark, these\nproblems span four core mathematical fields, each including a verifiable\nnumerical solution to enable objective, rule-based evaluation. Empirical\nresults underscore the significant challenge presented by OlymMATH, with\nstate-of-the-art models including DeepSeek-R1 and OpenAI's o3-mini\ndemonstrating notably limited accuracy on the hard subset. Furthermore, the\nbenchmark facilitates comprehensive bilingual assessment of mathematical\nreasoning abilities-a critical dimension that remains largely unaddressed in\nmainstream mathematical reasoning benchmarks. We release the OlymMATH benchmark\nat the STILL project: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs."
                },
                "authors": [
                    {
                        "name": "Haoxiang Sun"
                    },
                    {
                        "name": "Yingqian Min"
                    },
                    {
                        "name": "Zhipeng Chen"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Lei Fang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "Technical Report on Slow Thinking with LLMs: Evaluation Benchmark",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21365v1",
                "updated": "2025-03-27T10:56:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    10,
                    56,
                    53,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T10:56:53Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    10,
                    56,
                    53,
                    3,
                    86,
                    0
                ],
                "title": "CA+: Cognition Augmented Counselor Agent Framework for Long-term Dynamic\n  Client Engagement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CA+: Cognition Augmented Counselor Agent Framework for Long-term Dynamic\n  Client Engagement"
                },
                "summary": "Current AI counseling systems struggle with maintaining effective long-term\nclient engagement. Through formative research with counselors and a systematic\nliterature review, we identified five key design considerations for AI\ncounseling interactions. Based on these insights, we propose CA+, a Cognition\nAugmented counselor framework enhancing contextual understanding through three\ncomponents:\n  (1) Therapy Strategies Module: Implements hierarchical Goals-Session-Action\nplanning with bidirectional adaptation based on client feedback; (2)\nCommunication Form Module: Orchestrates parallel guidance and empathy pathways\nfor balanced therapeutic progress and emotional resonance; (3) Information\nManagement: Utilizes client profile and therapeutic knowledge databases for\ndynamic, context-aware interventions.\n  A three-day longitudinal study with 24 clients demonstrates CA+'s significant\nimprovements in client engagement, perceived empathy, and overall satisfaction\ncompared to a baseline system. Besides, two licensed counselors confirm its\nhigh professionalism. Our research demonstrates the potential for enhancing LLM\nengagement in psychological counseling dialogues through cognitive theory,\nwhich may inspire further innovations in computational interaction in the\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current AI counseling systems struggle with maintaining effective long-term\nclient engagement. Through formative research with counselors and a systematic\nliterature review, we identified five key design considerations for AI\ncounseling interactions. Based on these insights, we propose CA+, a Cognition\nAugmented counselor framework enhancing contextual understanding through three\ncomponents:\n  (1) Therapy Strategies Module: Implements hierarchical Goals-Session-Action\nplanning with bidirectional adaptation based on client feedback; (2)\nCommunication Form Module: Orchestrates parallel guidance and empathy pathways\nfor balanced therapeutic progress and emotional resonance; (3) Information\nManagement: Utilizes client profile and therapeutic knowledge databases for\ndynamic, context-aware interventions.\n  A three-day longitudinal study with 24 clients demonstrates CA+'s significant\nimprovements in client engagement, perceived empathy, and overall satisfaction\ncompared to a baseline system. Besides, two licensed counselors confirm its\nhigh professionalism. Our research demonstrates the potential for enhancing LLM\nengagement in psychological counseling dialogues through cognitive theory,\nwhich may inspire further innovations in computational interaction in the\nfuture."
                },
                "authors": [
                    {
                        "name": "Yuanrong Tang"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Tianhong Wang"
                    },
                    {
                        "name": "Chen Zhong"
                    },
                    {
                        "name": "Jiangtao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jiangtao Gong"
                },
                "author": "Jiangtao Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21360v1",
                "updated": "2025-03-27T10:52:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    10,
                    52,
                    10,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T10:52:10Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    10,
                    52,
                    10,
                    3,
                    86,
                    0
                ],
                "title": "From User Preferences to Optimization Constraints Using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From User Preferences to Optimization Constraints Using Large Language\n  Models"
                },
                "summary": "This work explores using Large Language Models (LLMs) to translate user\npreferences into energy optimization constraints for home appliances. We\ndescribe a task where natural language user utterances are converted into\nformal constraints for smart appliances, within the broader context of a\nrenewable energy community (REC) and in the Italian scenario. We evaluate the\neffectiveness of various LLMs currently available for Italian in translating\nthese preferences resorting to classical zero-shot, one-shot, and few-shot\nlearning settings, using a pilot dataset of Italian user requests paired with\ncorresponding formal constraint representation. Our contributions include\nestablishing a baseline performance for this task, publicly releasing the\ndataset and code for further research, and providing insights on observed best\npractices and limitations of LLMs in this particular domain",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores using Large Language Models (LLMs) to translate user\npreferences into energy optimization constraints for home appliances. We\ndescribe a task where natural language user utterances are converted into\nformal constraints for smart appliances, within the broader context of a\nrenewable energy community (REC) and in the Italian scenario. We evaluate the\neffectiveness of various LLMs currently available for Italian in translating\nthese preferences resorting to classical zero-shot, one-shot, and few-shot\nlearning settings, using a pilot dataset of Italian user requests paired with\ncorresponding formal constraint representation. Our contributions include\nestablishing a baseline performance for this task, publicly releasing the\ndataset and code for further research, and providing insights on observed best\npractices and limitations of LLMs in this particular domain"
                },
                "authors": [
                    {
                        "name": "Manuela Sanguinetti"
                    },
                    {
                        "name": "Alessandra Perniciano"
                    },
                    {
                        "name": "Luca Zedda"
                    },
                    {
                        "name": "Andrea Loddo"
                    },
                    {
                        "name": "Cecilia Di Ruberto"
                    },
                    {
                        "name": "Maurizio Atzori"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Atzori"
                },
                "author": "Maurizio Atzori",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01877v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01877v2",
                "updated": "2025-03-27T10:38:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    10,
                    38,
                    45,
                    3,
                    86,
                    0
                ],
                "published": "2025-02-26T15:20:01Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    20,
                    1,
                    2,
                    57,
                    0
                ],
                "title": "Starjob: Dataset for LLM-Driven Job Shop Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Starjob: Dataset for LLM-Driven Job Shop Scheduling"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious domains, but their potential for solving combinatorial optimization\nproblems remains largely unexplored. In this paper, we investigate the\napplicability of LLMs to the Job Shop Scheduling Problem (JSSP), a classic\nchallenge in combinatorial optimization that requires efficient job allocation\nto machines to minimize makespan. To this end, we introduce Starjob, the first\nsupervised dataset for JSSP, comprising 130k instances specifically designed\nfor training LLMs. Leveraging this dataset, we fine-tune the LLaMA 8B 4-bit\nquantized model with the LoRA method to develop an end-to-end scheduling\napproach. Our evaluation on standard benchmarks demonstrates that the proposed\nLLM-based method not only surpasses traditional Priority Dispatching Rules\n(PDRs) but also achieves notable improvements over state-of-the-art neural\napproaches like L2D, with an average improvement of 15.36% on DMU and 7.85% on\nTaillard benchmarks. These results highlight the untapped potential of LLMs in\ntackling combinatorial optimization problems, paving the way for future\nadvancements in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious domains, but their potential for solving combinatorial optimization\nproblems remains largely unexplored. In this paper, we investigate the\napplicability of LLMs to the Job Shop Scheduling Problem (JSSP), a classic\nchallenge in combinatorial optimization that requires efficient job allocation\nto machines to minimize makespan. To this end, we introduce Starjob, the first\nsupervised dataset for JSSP, comprising 130k instances specifically designed\nfor training LLMs. Leveraging this dataset, we fine-tune the LLaMA 8B 4-bit\nquantized model with the LoRA method to develop an end-to-end scheduling\napproach. Our evaluation on standard benchmarks demonstrates that the proposed\nLLM-based method not only surpasses traditional Priority Dispatching Rules\n(PDRs) but also achieves notable improvements over state-of-the-art neural\napproaches like L2D, with an average improvement of 15.36% on DMU and 7.85% on\nTaillard benchmarks. These results highlight the untapped potential of LLMs in\ntackling combinatorial optimization problems, paving the way for future\nadvancements in this area."
                },
                "authors": [
                    {
                        "name": "Henrik Abgaryan"
                    },
                    {
                        "name": "Tristan Cazenave"
                    },
                    {
                        "name": "Ararat Harutyunyan"
                    }
                ],
                "author_detail": {
                    "name": "Ararat Harutyunyan"
                },
                "author": "Ararat Harutyunyan",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2408.06993",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01877v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01877v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21349v1",
                "updated": "2025-03-27T10:35:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    10,
                    35,
                    56,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T10:35:56Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    10,
                    35,
                    56,
                    3,
                    86,
                    0
                ],
                "title": "Fine-Tuning LLMs on Small Medical Datasets: Text Classification and\n  Normalization Effectiveness on Cardiology reports and Discharge records",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning LLMs on Small Medical Datasets: Text Classification and\n  Normalization Effectiveness on Cardiology reports and Discharge records"
                },
                "summary": "We investigate the effectiveness of fine-tuning large language models (LLMs)\non small medical datasets for text classification and named entity recognition\ntasks. Using a German cardiology report dataset and the i2b2 Smoking Challenge\ndataset, we demonstrate that fine-tuning small LLMs locally on limited training\ndata can improve performance achieving comparable results to larger models. Our\nexperiments show that fine-tuning improves performance on both tasks, with\nnotable gains observed with as few as 200-300 training examples. Overall, the\nstudy highlights the potential of task-specific fine-tuning of LLMs for\nautomating clinical workflows and efficiently extracting structured data from\nunstructured medical text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the effectiveness of fine-tuning large language models (LLMs)\non small medical datasets for text classification and named entity recognition\ntasks. Using a German cardiology report dataset and the i2b2 Smoking Challenge\ndataset, we demonstrate that fine-tuning small LLMs locally on limited training\ndata can improve performance achieving comparable results to larger models. Our\nexperiments show that fine-tuning improves performance on both tasks, with\nnotable gains observed with as few as 200-300 training examples. Overall, the\nstudy highlights the potential of task-specific fine-tuning of LLMs for\nautomating clinical workflows and efficiently extracting structured data from\nunstructured medical text."
                },
                "authors": [
                    {
                        "name": "Noah Losch"
                    },
                    {
                        "name": "Lucas Plagwitz"
                    },
                    {
                        "name": "Antonius BÃ¼scher"
                    },
                    {
                        "name": "Julian Varghese"
                    }
                ],
                "author_detail": {
                    "name": "Julian Varghese"
                },
                "author": "Julian Varghese",
                "arxiv_comment": "4 pages, 2 tables,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20578v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20578v3",
                "updated": "2025-03-28T02:53:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    2,
                    53,
                    43,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-26T14:25:01Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    25,
                    1,
                    2,
                    85,
                    0
                ],
                "title": "LLPut: Investigating Large Language Models for Bug Report-Based Input\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLPut: Investigating Large Language Models for Bug Report-Based Input\n  Generation"
                },
                "summary": "Failure-inducing inputs play a crucial role in diagnosing and analyzing\nsoftware bugs. Bug reports typically contain these inputs, which developers\nextract to facilitate debugging. Since bug reports are written in natural\nlanguage, prior research has leveraged various Natural Language Processing\n(NLP) techniques for automated input extraction. With the advent of Large\nLanguage Models (LLMs), an important research question arises: how effectively\ncan generative LLMs extract failure-inducing inputs from bug reports? In this\npaper, we propose LLPut, a technique to empirically evaluate the performance of\nthree open-source generative LLMs -- LLaMA, Qwen, and Qwen-Coder -- in\nextracting relevant inputs from bug reports. We conduct an experimental\nevaluation on a dataset of 206 bug reports to assess the accuracy and\neffectiveness of these models. Our findings provide insights into the\ncapabilities and limitations of generative LLMs in automated bug diagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Failure-inducing inputs play a crucial role in diagnosing and analyzing\nsoftware bugs. Bug reports typically contain these inputs, which developers\nextract to facilitate debugging. Since bug reports are written in natural\nlanguage, prior research has leveraged various Natural Language Processing\n(NLP) techniques for automated input extraction. With the advent of Large\nLanguage Models (LLMs), an important research question arises: how effectively\ncan generative LLMs extract failure-inducing inputs from bug reports? In this\npaper, we propose LLPut, a technique to empirically evaluate the performance of\nthree open-source generative LLMs -- LLaMA, Qwen, and Qwen-Coder -- in\nextracting relevant inputs from bug reports. We conduct an experimental\nevaluation on a dataset of 206 bug reports to assess the accuracy and\neffectiveness of these models. Our findings provide insights into the\ncapabilities and limitations of generative LLMs in automated bug diagnosis."
                },
                "authors": [
                    {
                        "name": "Alif Al Hasan"
                    },
                    {
                        "name": "Subarna Saha"
                    },
                    {
                        "name": "Mia Mohammad Imran"
                    },
                    {
                        "name": "Tarannum Shaila Zaman"
                    }
                ],
                "author_detail": {
                    "name": "Tarannum Shaila Zaman"
                },
                "author": "Tarannum Shaila Zaman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20578v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20578v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00493v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00493v2",
                "updated": "2025-03-27T10:30:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    10,
                    30,
                    42,
                    3,
                    86,
                    0
                ],
                "published": "2024-11-30T14:28:53Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    14,
                    28,
                    53,
                    5,
                    335,
                    0
                ],
                "title": "Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene\n  Understanding"
                },
                "summary": "The rapid advancement of Multimodal Large Language Models (MLLMs) has\nsignificantly impacted various multimodal tasks. However, these models face\nchallenges in tasks that require spatial understanding within 3D environments.\nEfforts to enhance MLLMs, such as incorporating point cloud features, have been\nmade, yet a considerable gap remains between the models' learned\nrepresentations and the inherent complexity of 3D scenes. This discrepancy\nlargely stems from the training of MLLMs on predominantly 2D data, which\nrestricts their effectiveness in comprehending 3D spaces. To address this\nissue, in this paper, we propose a novel generalist model, i.e., Video-3D LLM,\nfor 3D scene understanding. By treating 3D scenes as dynamic videos and\nincorporating 3D position encoding into these representations, our Video-3D LLM\naligns video representations with real-world spatial contexts more accurately.\nIn addition, we have implemented a maximum coverage sampling technique to\noptimize the trade-off between computational cost and performance. Extensive\nexperiments demonstrate that our model achieves state-of-the-art performance on\nseveral 3D scene understanding benchmarks, including ScanRefer, Multi3DRefer,\nScan2Cap, ScanQA, and SQA3D.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Multimodal Large Language Models (MLLMs) has\nsignificantly impacted various multimodal tasks. However, these models face\nchallenges in tasks that require spatial understanding within 3D environments.\nEfforts to enhance MLLMs, such as incorporating point cloud features, have been\nmade, yet a considerable gap remains between the models' learned\nrepresentations and the inherent complexity of 3D scenes. This discrepancy\nlargely stems from the training of MLLMs on predominantly 2D data, which\nrestricts their effectiveness in comprehending 3D spaces. To address this\nissue, in this paper, we propose a novel generalist model, i.e., Video-3D LLM,\nfor 3D scene understanding. By treating 3D scenes as dynamic videos and\nincorporating 3D position encoding into these representations, our Video-3D LLM\naligns video representations with real-world spatial contexts more accurately.\nIn addition, we have implemented a maximum coverage sampling technique to\noptimize the trade-off between computational cost and performance. Extensive\nexperiments demonstrate that our model achieves state-of-the-art performance on\nseveral 3D scene understanding benchmarks, including ScanRefer, Multi3DRefer,\nScan2Cap, ScanQA, and SQA3D."
                },
                "authors": [
                    {
                        "name": "Duo Zheng"
                    },
                    {
                        "name": "Shijia Huang"
                    },
                    {
                        "name": "Liwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liwei Wang"
                },
                "author": "Liwei Wang",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00493v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00493v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21330v1",
                "updated": "2025-03-27T10:10:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    10,
                    10,
                    30,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T10:10:30Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    10,
                    10,
                    30,
                    3,
                    86,
                    0
                ],
                "title": "Large Language Models for Traffic and Transportation Research:\n  Methodologies, State of the Art, and Future Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Traffic and Transportation Research:\n  Methodologies, State of the Art, and Future Opportunities"
                },
                "summary": "The rapid rise of Large Language Models (LLMs) is transforming traffic and\ntransportation research, with significant advancements emerging between the\nyears 2023 and 2025 -- a period marked by the inception and swift growth of\nadopting and adapting LLMs for various traffic and transportation applications.\nHowever, despite these significant advancements, a systematic review and\nsynthesis of the existing studies remain lacking. To address this gap, this\npaper provides a comprehensive review of the methodologies and applications of\nLLMs in traffic and transportation, highlighting their ability to process\nunstructured textual data to advance transportation research. We explore key\napplications, including autonomous driving, travel behavior prediction, and\ngeneral transportation-related queries, alongside methodologies such as zero-\nor few-shot learning, prompt engineering, and fine-tuning. Our analysis\nidentifies critical research gaps. From the methodological perspective, many\nresearch gaps can be addressed by integrating LLMs with existing tools and\nrefining LLM architectures. From the application perspective, we identify\nnumerous opportunities for LLMs to tackle a variety of traffic and\ntransportation challenges, building upon existing research. By synthesizing\nthese findings, this review not only clarifies the current state of LLM\nadoption and adaptation in traffic and transportation but also proposes future\nresearch directions, paving the way for smarter and more sustainable\ntransportation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid rise of Large Language Models (LLMs) is transforming traffic and\ntransportation research, with significant advancements emerging between the\nyears 2023 and 2025 -- a period marked by the inception and swift growth of\nadopting and adapting LLMs for various traffic and transportation applications.\nHowever, despite these significant advancements, a systematic review and\nsynthesis of the existing studies remain lacking. To address this gap, this\npaper provides a comprehensive review of the methodologies and applications of\nLLMs in traffic and transportation, highlighting their ability to process\nunstructured textual data to advance transportation research. We explore key\napplications, including autonomous driving, travel behavior prediction, and\ngeneral transportation-related queries, alongside methodologies such as zero-\nor few-shot learning, prompt engineering, and fine-tuning. Our analysis\nidentifies critical research gaps. From the methodological perspective, many\nresearch gaps can be addressed by integrating LLMs with existing tools and\nrefining LLM architectures. From the application perspective, we identify\nnumerous opportunities for LLMs to tackle a variety of traffic and\ntransportation challenges, building upon existing research. By synthesizing\nthese findings, this review not only clarifies the current state of LLM\nadoption and adaptation in traffic and transportation but also proposes future\nresearch directions, paving the way for smarter and more sustainable\ntransportation systems."
                },
                "authors": [
                    {
                        "name": "Yimo Yan"
                    },
                    {
                        "name": "Yejia Liao"
                    },
                    {
                        "name": "Guanhao Xu"
                    },
                    {
                        "name": "Ruili Yao"
                    },
                    {
                        "name": "Huiying Fan"
                    },
                    {
                        "name": "Jingran Sun"
                    },
                    {
                        "name": "Xia Wang"
                    },
                    {
                        "name": "Jonathan Sprinkle"
                    },
                    {
                        "name": "Ziyan An"
                    },
                    {
                        "name": "Meiyi Ma"
                    },
                    {
                        "name": "Xi Cheng"
                    },
                    {
                        "name": "Tong Liu"
                    },
                    {
                        "name": "Zemian Ke"
                    },
                    {
                        "name": "Bo Zou"
                    },
                    {
                        "name": "Matthew Barth"
                    },
                    {
                        "name": "Yong-Hong Kuo"
                    }
                ],
                "author_detail": {
                    "name": "Yong-Hong Kuo"
                },
                "author": "Yong-Hong Kuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15668v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15668v4",
                "updated": "2025-03-27T09:41:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    41,
                    1,
                    3,
                    86,
                    0
                ],
                "published": "2024-05-24T16:05:15Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    16,
                    5,
                    15,
                    4,
                    145,
                    0
                ],
                "title": "What Do You See? Enhancing Zero-Shot Image Classification with\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Do You See? Enhancing Zero-Shot Image Classification with\n  Multimodal Large Language Models"
                },
                "summary": "Large language models (LLMs) have been effectively used for many computer\nvision tasks, including image classification. In this paper, we present a\nsimple yet effective approach for zero-shot image classification using\nmultimodal LLMs. Using multimodal LLMs, we generate comprehensive textual\nrepresentations from input images. These textual representations are then\nutilized to generate fixed-dimensional features in a cross-modal embedding\nspace. Subsequently, these features are fused together to perform zero-shot\nclassification using a linear classifier. Our method does not require prompt\nengineering for each dataset; instead, we use a single, straightforward set of\nprompts across all datasets. We evaluated our method on several datasets and\nour results demonstrate its remarkable effectiveness, surpassing benchmark\naccuracy on multiple datasets. On average, for ten benchmarks, our method\nachieved an accuracy gain of 6.2 percentage points, with an increase of 6.8\npercentage points on the ImageNet dataset, compared to prior methods\nre-evaluated with the same setup. Our findings highlight the potential of\nmultimodal LLMs to enhance computer vision tasks such as zero-shot image\nclassification, offering a significant improvement over traditional methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been effectively used for many computer\nvision tasks, including image classification. In this paper, we present a\nsimple yet effective approach for zero-shot image classification using\nmultimodal LLMs. Using multimodal LLMs, we generate comprehensive textual\nrepresentations from input images. These textual representations are then\nutilized to generate fixed-dimensional features in a cross-modal embedding\nspace. Subsequently, these features are fused together to perform zero-shot\nclassification using a linear classifier. Our method does not require prompt\nengineering for each dataset; instead, we use a single, straightforward set of\nprompts across all datasets. We evaluated our method on several datasets and\nour results demonstrate its remarkable effectiveness, surpassing benchmark\naccuracy on multiple datasets. On average, for ten benchmarks, our method\nachieved an accuracy gain of 6.2 percentage points, with an increase of 6.8\npercentage points on the ImageNet dataset, compared to prior methods\nre-evaluated with the same setup. Our findings highlight the potential of\nmultimodal LLMs to enhance computer vision tasks such as zero-shot image\nclassification, offering a significant improvement over traditional methods."
                },
                "authors": [
                    {
                        "name": "Abdelrahman Abdelhamed"
                    },
                    {
                        "name": "Mahmoud Afifi"
                    },
                    {
                        "name": "Alec Go"
                    }
                ],
                "author_detail": {
                    "name": "Alec Go"
                },
                "author": "Alec Go",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15668v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15668v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21307v1",
                "updated": "2025-03-27T09:31:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    31,
                    35,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T09:31:35Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    31,
                    35,
                    3,
                    86,
                    0
                ],
                "title": "InternVL-X: Advancing and Accelerating InternVL Series with Efficient\n  Visual Token Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InternVL-X: Advancing and Accelerating InternVL Series with Efficient\n  Visual Token Compression"
                },
                "summary": "Most multimodal large language models (MLLMs) treat visual tokens as \"a\nsequence of text\", integrating them with text tokens into a large language\nmodel (LLM). However, a great quantity of visual tokens significantly increases\nthe demand for computational resources and time. In this paper, we propose\nInternVL-X, which outperforms the InternVL model in both performance and\nefficiency by incorporating three visual token compression methods. First, we\npropose a novel vision-language projector, PVTC. This component integrates\nadjacent visual embeddings to form a local query and utilizes the transformed\nCLS token as a global query, then performs point-to-region cross-attention\nthrough these local and global queries to more effectively convert visual\nfeatures. Second, we present a layer-wise visual token compression module,\nLVTC, which compresses tokens in the LLM shallow layers and then expands them\nthrough upsampling and residual connections in the deeper layers. This\nsignificantly enhances the model computational efficiency. Futhermore, we\npropose an efficient high resolution slicing method, RVTC, which dynamically\nadjusts the number of visual tokens based on image area or length filtering.\nRVTC greatly enhances training efficiency with only a slight reduction in\nperformance. By utilizing 20% or fewer visual tokens, InternVL-X achieves\nstate-of-the-art performance on 7 public MLLM benchmarks, and improves the\naverage metric by 2.34% across 12 tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most multimodal large language models (MLLMs) treat visual tokens as \"a\nsequence of text\", integrating them with text tokens into a large language\nmodel (LLM). However, a great quantity of visual tokens significantly increases\nthe demand for computational resources and time. In this paper, we propose\nInternVL-X, which outperforms the InternVL model in both performance and\nefficiency by incorporating three visual token compression methods. First, we\npropose a novel vision-language projector, PVTC. This component integrates\nadjacent visual embeddings to form a local query and utilizes the transformed\nCLS token as a global query, then performs point-to-region cross-attention\nthrough these local and global queries to more effectively convert visual\nfeatures. Second, we present a layer-wise visual token compression module,\nLVTC, which compresses tokens in the LLM shallow layers and then expands them\nthrough upsampling and residual connections in the deeper layers. This\nsignificantly enhances the model computational efficiency. Futhermore, we\npropose an efficient high resolution slicing method, RVTC, which dynamically\nadjusts the number of visual tokens based on image area or length filtering.\nRVTC greatly enhances training efficiency with only a slight reduction in\nperformance. By utilizing 20% or fewer visual tokens, InternVL-X achieves\nstate-of-the-art performance on 7 public MLLM benchmarks, and improves the\naverage metric by 2.34% across 12 tasks."
                },
                "authors": [
                    {
                        "name": "Dongchen Lu"
                    },
                    {
                        "name": "Yuyao Sun"
                    },
                    {
                        "name": "Zilu Zhang"
                    },
                    {
                        "name": "Leping Huang"
                    },
                    {
                        "name": "Jianliang Zeng"
                    },
                    {
                        "name": "Mao Shu"
                    },
                    {
                        "name": "Huo Cao"
                    }
                ],
                "author_detail": {
                    "name": "Huo Cao"
                },
                "author": "Huo Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21305v1",
                "updated": "2025-03-27T09:31:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    31,
                    10,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T09:31:10Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    31,
                    10,
                    3,
                    86,
                    0
                ],
                "title": "DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep\n  Models with Limited Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep\n  Models with Limited Data"
                },
                "summary": "Backdoor attacks are among the most effective, practical, and stealthy\nattacks in deep learning. In this paper, we consider a practical scenario where\na developer obtains a deep model from a third party and uses it as part of a\nsafety-critical system. The developer wants to inspect the model for potential\nbackdoors prior to system deployment. We find that most existing detection\ntechniques make assumptions that are not applicable to this scenario. In this\npaper, we present a novel framework for detecting backdoors under realistic\nrestrictions. We generate candidate triggers by deductively searching over the\nspace of possible triggers. We construct and optimize a smoothed version of\nAttack Success Rate as our search objective. Starting from a broad class of\ntemplate attacks and just using the forward pass of a deep model, we reverse\nengineer the backdoor attack. We conduct extensive evaluation on a wide range\nof attacks, models, and datasets, with our technique performing almost\nperfectly across these settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backdoor attacks are among the most effective, practical, and stealthy\nattacks in deep learning. In this paper, we consider a practical scenario where\na developer obtains a deep model from a third party and uses it as part of a\nsafety-critical system. The developer wants to inspect the model for potential\nbackdoors prior to system deployment. We find that most existing detection\ntechniques make assumptions that are not applicable to this scenario. In this\npaper, we present a novel framework for detecting backdoors under realistic\nrestrictions. We generate candidate triggers by deductively searching over the\nspace of possible triggers. We construct and optimize a smoothed version of\nAttack Success Rate as our search objective. Starting from a broad class of\ntemplate attacks and just using the forward pass of a deep model, we reverse\nengineer the backdoor attack. We conduct extensive evaluation on a wide range\nof attacks, models, and datasets, with our technique performing almost\nperfectly across these settings."
                },
                "authors": [
                    {
                        "name": "Dorde Popovic"
                    },
                    {
                        "name": "Amin Sadeghi"
                    },
                    {
                        "name": "Ting Yu"
                    },
                    {
                        "name": "Sanjay Chawla"
                    },
                    {
                        "name": "Issa Khalil"
                    }
                ],
                "author_detail": {
                    "name": "Issa Khalil"
                },
                "author": "Issa Khalil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21297v1",
                "updated": "2025-03-27T09:24:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    24,
                    18,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T09:24:18Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    24,
                    18,
                    3,
                    86,
                    0
                ],
                "title": "MLDSE: Scaling Design Space Exploration Infrastructure for Multi-Level\n  Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLDSE: Scaling Design Space Exploration Infrastructure for Multi-Level\n  Hardware"
                },
                "summary": "To efficiently support large-scale NNs, multi-level hardware, leveraging\nadvanced integration and interconnection technologies, has emerged as a\npromising solution to counter the slowdown of Moore's law. However, the vast\ndesign space of such hardware, coupled with the complexity of their spatial\nhierarchies and organizations, introduces significant challenges for design\nspace exploration (DSE). Existing DSE tools, which rely on predefined hardware\ntemplates to explore parameters for specific architectures, fall short in\nexploring diverse organizations, spatial hierarchies, and architectural\npolymorphisms inherent in multi-level hardware. To address these limitations,\nwe present Multi-Level Design Space Exploror (MLDSE), a novel infrastructure\nfor domain-specific DSE of multi-level hardware. MLDSE introduces three key\ninnovations from three basic perspectives of DSE: 1) Modeling: MLDSE introduces\na hardware intermediate representation (IR) that can recursively model diverse\nmulti-level hardware with composable elements at various granularities. 2)\nMapping: MLDSE provides a comprehensive spatiotemporal mapping IR and mapping\nprimitives, facilitating the mapping strategy exploration on multi-level\nhardware, especially synchronization and cross-level communication; 3)\nSimulation: MLDSE supports universal simulator generation based on task-level\nevent-driven simulation mechanism. It features a hardware-consistent scheduling\nalgorithm that can handle general task-level resource contention. Through\nexperiments on LLM workloads, we demonstrate MLDSE's unique capability to\nperform three-tier DSE spanning architecture, hardware parameter, and mapping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To efficiently support large-scale NNs, multi-level hardware, leveraging\nadvanced integration and interconnection technologies, has emerged as a\npromising solution to counter the slowdown of Moore's law. However, the vast\ndesign space of such hardware, coupled with the complexity of their spatial\nhierarchies and organizations, introduces significant challenges for design\nspace exploration (DSE). Existing DSE tools, which rely on predefined hardware\ntemplates to explore parameters for specific architectures, fall short in\nexploring diverse organizations, spatial hierarchies, and architectural\npolymorphisms inherent in multi-level hardware. To address these limitations,\nwe present Multi-Level Design Space Exploror (MLDSE), a novel infrastructure\nfor domain-specific DSE of multi-level hardware. MLDSE introduces three key\ninnovations from three basic perspectives of DSE: 1) Modeling: MLDSE introduces\na hardware intermediate representation (IR) that can recursively model diverse\nmulti-level hardware with composable elements at various granularities. 2)\nMapping: MLDSE provides a comprehensive spatiotemporal mapping IR and mapping\nprimitives, facilitating the mapping strategy exploration on multi-level\nhardware, especially synchronization and cross-level communication; 3)\nSimulation: MLDSE supports universal simulator generation based on task-level\nevent-driven simulation mechanism. It features a hardware-consistent scheduling\nalgorithm that can handle general task-level resource contention. Through\nexperiments on LLM workloads, we demonstrate MLDSE's unique capability to\nperform three-tier DSE spanning architecture, hardware parameter, and mapping."
                },
                "authors": [
                    {
                        "name": "Huanyu Qu"
                    },
                    {
                        "name": "Weihao Zhang"
                    },
                    {
                        "name": "Junfeng Lin"
                    },
                    {
                        "name": "Songchen Ma"
                    },
                    {
                        "name": "Hongyi Li"
                    },
                    {
                        "name": "Luping Shi"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21295v1",
                "updated": "2025-03-27T09:23:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    23,
                    8,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T09:23:08Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    23,
                    8,
                    3,
                    86,
                    0
                ],
                "title": "R-PRM: Reasoning-Driven Process Reward Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-PRM: Reasoning-Driven Process Reward Modeling"
                },
                "summary": "Large language models (LLMs) inevitably make mistakes when performing\nstep-by-step mathematical reasoning. Process Reward Models (PRMs) have emerged\nas a promising solution by evaluating each reasoning step. However, existing\nPRMs typically output evaluation scores directly, limiting both learning\nefficiency and evaluation accuracy, which is further exacerbated by the\nscarcity of annotated data. To address these issues, we propose\nReasoning-Driven Process Reward Modeling (R-PRM). First, we leverage stronger\nLLMs to generate seed data from limited annotations, effectively bootstrapping\nour model's reasoning capabilities and enabling comprehensive step-by-step\nevaluation. Second, we further enhance performance through preference\noptimization, without requiring additional annotated data. Third, we introduce\ninference-time scaling to fully harness the model's reasoning potential.\nExtensive experiments demonstrate R-PRM's effectiveness: on ProcessBench and\nPRMBench, it surpasses strong baselines by 11.9 and 8.5 points in F1 scores,\nrespectively. When applied to guide mathematical reasoning, R-PRM achieves\nconsistent accuracy improvements of over 8.5 points across six challenging\ndatasets. Further analysis reveals that R-PRM exhibits more comprehensive\nevaluation and stronger generalization capabilities, thereby highlighting its\nsignificant potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) inevitably make mistakes when performing\nstep-by-step mathematical reasoning. Process Reward Models (PRMs) have emerged\nas a promising solution by evaluating each reasoning step. However, existing\nPRMs typically output evaluation scores directly, limiting both learning\nefficiency and evaluation accuracy, which is further exacerbated by the\nscarcity of annotated data. To address these issues, we propose\nReasoning-Driven Process Reward Modeling (R-PRM). First, we leverage stronger\nLLMs to generate seed data from limited annotations, effectively bootstrapping\nour model's reasoning capabilities and enabling comprehensive step-by-step\nevaluation. Second, we further enhance performance through preference\noptimization, without requiring additional annotated data. Third, we introduce\ninference-time scaling to fully harness the model's reasoning potential.\nExtensive experiments demonstrate R-PRM's effectiveness: on ProcessBench and\nPRMBench, it surpasses strong baselines by 11.9 and 8.5 points in F1 scores,\nrespectively. When applied to guide mathematical reasoning, R-PRM achieves\nconsistent accuracy improvements of over 8.5 points across six challenging\ndatasets. Further analysis reveals that R-PRM exhibits more comprehensive\nevaluation and stronger generalization capabilities, thereby highlighting its\nsignificant potential."
                },
                "authors": [
                    {
                        "name": "Shuaijie She"
                    },
                    {
                        "name": "Junxiao Liu"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Jiajun Chen"
                    },
                    {
                        "name": "Xin Huang"
                    },
                    {
                        "name": "Shujian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Shujian Huang"
                },
                "author": "Shujian Huang",
                "arxiv_comment": "The project is available at https://github.com/NJUNLP/R-PRM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21279v1",
                "updated": "2025-03-27T08:59:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    8,
                    59,
                    30,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T08:59:30Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    8,
                    59,
                    30,
                    3,
                    86,
                    0
                ],
                "title": "Asynchronous BFT Consensus Made Wireless",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asynchronous BFT Consensus Made Wireless"
                },
                "summary": "Asynchronous Byzantine fault-tolerant (BFT) consensus protocols, known for\ntheir robustness in unpredictable environments without relying on timing\nassumptions, are becoming increasingly vital for wireless applications. While\nthese protocols have proven effective in wired networks, their adaptation to\nwireless environments presents significant challenges. Asynchronous BFT\nconsensus, characterized by its N parallel consensus components (e.g.,\nasynchronous Byzantine agreement, reliable broadcast), suffers from high\nmessage complexity, leading to network congestion and inefficiency, especially\nin resource-constrained wireless networks. Asynchronous Byzantine agreement\n(ABA) protocols, a foundational component of asynchronous BFT, require careful\nbalancing of message complexity and cryptographic overhead to achieve efficient\nimplementation in wireless settings. Additionally, the absence of dedicated\ntestbeds for asynchronous wireless BFT consensus protocols hinders development\nand performance evaluation. To address these challenges, we propose a consensus\nbatching protocol (ConsensusBatcher), which supports both vertical and\nhorizontal batching of multiple parallel consensus components. We leverage\nConsensusBatcher to adapt three asynchronous BFT consensus protocols\n(HoneyBadgerBFT, BEAT, and Dumbo) from wired networks to resource-constrained\nwireless networks. To evaluate the performance of ConsensusBatcher-enabled\nconsensus protocols in wireless environments, we develop and open-source a\ntestbed for deployment and performance assessment of these protocols. Using\nthis testbed, we demonstrate that ConsensusBatcher-based consensus reduces\nlatency by 48% to 59% and increases throughput by 48% to 62% compared to\nbaseline consensus protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asynchronous Byzantine fault-tolerant (BFT) consensus protocols, known for\ntheir robustness in unpredictable environments without relying on timing\nassumptions, are becoming increasingly vital for wireless applications. While\nthese protocols have proven effective in wired networks, their adaptation to\nwireless environments presents significant challenges. Asynchronous BFT\nconsensus, characterized by its N parallel consensus components (e.g.,\nasynchronous Byzantine agreement, reliable broadcast), suffers from high\nmessage complexity, leading to network congestion and inefficiency, especially\nin resource-constrained wireless networks. Asynchronous Byzantine agreement\n(ABA) protocols, a foundational component of asynchronous BFT, require careful\nbalancing of message complexity and cryptographic overhead to achieve efficient\nimplementation in wireless settings. Additionally, the absence of dedicated\ntestbeds for asynchronous wireless BFT consensus protocols hinders development\nand performance evaluation. To address these challenges, we propose a consensus\nbatching protocol (ConsensusBatcher), which supports both vertical and\nhorizontal batching of multiple parallel consensus components. We leverage\nConsensusBatcher to adapt three asynchronous BFT consensus protocols\n(HoneyBadgerBFT, BEAT, and Dumbo) from wired networks to resource-constrained\nwireless networks. To evaluate the performance of ConsensusBatcher-enabled\nconsensus protocols in wireless environments, we develop and open-source a\ntestbed for deployment and performance assessment of these protocols. Using\nthis testbed, we demonstrate that ConsensusBatcher-based consensus reduces\nlatency by 48% to 59% and increases throughput by 48% to 62% compared to\nbaseline consensus protocols."
                },
                "authors": [
                    {
                        "name": "Shuo Liu"
                    },
                    {
                        "name": "Minghui Xu"
                    },
                    {
                        "name": "Tianyi Sun"
                    },
                    {
                        "name": "Xiuzhen Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xiuzhen Cheng"
                },
                "author": "Xiuzhen Cheng",
                "arxiv_comment": "Accepted to IEEE ICDCS 2025, 11 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20083v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20083v2",
                "updated": "2025-03-27T08:54:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    8,
                    54,
                    4,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-25T21:44:10Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    21,
                    44,
                    10,
                    1,
                    84,
                    0
                ],
                "title": "Cross-Tokenizer Distillation via Approximate Likelihood Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Tokenizer Distillation via Approximate Likelihood Matching"
                },
                "summary": "Distillation has shown remarkable success in transferring knowledge from a\nLarge Language Model (LLM) teacher to a student LLM. However, current\ndistillation methods predominantly require the same tokenizer between the\nteacher and the student, restricting their applicability to only a small subset\nof teacher-student pairs. In this work, we develop a cross-tokenizer\ndistillation method to solve this crucial deficiency. Our method is the first\nto enable cross-tokenizer distillation without a next-token prediction loss as\nthe main objective, instead purely maximizing the student predictions'\nsimilarity to the teacher's predictions (known as pure distillation), while\nalso being robust to large mismatches between the teacher and the student\ntokenizer function and vocabulary. Empirically, our method enables\nsubstantially improved performance as tested on two use cases. First, we show\nthat viewing tokenizer transfer as self-distillation enables unprecedently\neffective transfer across tokenizers. We transfer (subword-level) Llama and\nGemma models to byte-level tokenization more effectively than prior methods\ntransfer to a similar subword tokenizer under a comparable training budget.\nTransferring different base models to the same tokenizer also enables\nensembling them (e.g., via averaging their predicted probabilities) which\nboosts performance. Second, we use our cross-tokenizer distillation method to\ndistil a large maths-specialized LLM into a smaller model, achieving\ncompetitive maths problem-solving performance. Overall, our results make\nsubstantial strides toward better adaptability and enhanced interaction between\ndifferent LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distillation has shown remarkable success in transferring knowledge from a\nLarge Language Model (LLM) teacher to a student LLM. However, current\ndistillation methods predominantly require the same tokenizer between the\nteacher and the student, restricting their applicability to only a small subset\nof teacher-student pairs. In this work, we develop a cross-tokenizer\ndistillation method to solve this crucial deficiency. Our method is the first\nto enable cross-tokenizer distillation without a next-token prediction loss as\nthe main objective, instead purely maximizing the student predictions'\nsimilarity to the teacher's predictions (known as pure distillation), while\nalso being robust to large mismatches between the teacher and the student\ntokenizer function and vocabulary. Empirically, our method enables\nsubstantially improved performance as tested on two use cases. First, we show\nthat viewing tokenizer transfer as self-distillation enables unprecedently\neffective transfer across tokenizers. We transfer (subword-level) Llama and\nGemma models to byte-level tokenization more effectively than prior methods\ntransfer to a similar subword tokenizer under a comparable training budget.\nTransferring different base models to the same tokenizer also enables\nensembling them (e.g., via averaging their predicted probabilities) which\nboosts performance. Second, we use our cross-tokenizer distillation method to\ndistil a large maths-specialized LLM into a smaller model, achieving\ncompetitive maths problem-solving performance. Overall, our results make\nsubstantial strides toward better adaptability and enhanced interaction between\ndifferent LLMs."
                },
                "authors": [
                    {
                        "name": "Benjamin Minixhofer"
                    },
                    {
                        "name": "Ivan VuliÄ"
                    },
                    {
                        "name": "Edoardo Maria Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo Maria Ponti"
                },
                "author": "Edoardo Maria Ponti",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20083v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20083v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21248v1",
                "updated": "2025-03-27T08:09:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    8,
                    9,
                    15,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T08:09:15Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    8,
                    9,
                    15,
                    3,
                    86,
                    0
                ],
                "title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via\n  Inspiration-Based Task Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResearchBench: Benchmarking LLMs in Scientific Discovery via\n  Inspiration-Based Task Decomposition"
                },
                "summary": "Large language models (LLMs) have demonstrated potential in assisting\nscientific research, yet their ability to discover high-quality research\nhypotheses remains unexamined due to the lack of a dedicated benchmark. To\naddress this gap, we introduce the first large-scale benchmark for evaluating\nLLMs with a near-sufficient set of sub-tasks of scientific discovery:\ninspiration retrieval, hypothesis composition, and hypothesis ranking. We\ndevelop an automated framework that extracts critical components - research\nquestions, background surveys, inspirations, and hypotheses - from scientific\npapers across 12 disciplines, with expert validation confirming its accuracy.\nTo prevent data contamination, we focus exclusively on papers published in\n2024, ensuring minimal overlap with LLM pretraining data. Our evaluation\nreveals that LLMs perform well in retrieving inspirations, an\nout-of-distribution task, suggesting their ability to surface novel knowledge\nassociations. This positions LLMs as \"research hypothesis mines\", capable of\nfacilitating automated scientific discovery by generating innovative hypotheses\nat scale with minimal human intervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated potential in assisting\nscientific research, yet their ability to discover high-quality research\nhypotheses remains unexamined due to the lack of a dedicated benchmark. To\naddress this gap, we introduce the first large-scale benchmark for evaluating\nLLMs with a near-sufficient set of sub-tasks of scientific discovery:\ninspiration retrieval, hypothesis composition, and hypothesis ranking. We\ndevelop an automated framework that extracts critical components - research\nquestions, background surveys, inspirations, and hypotheses - from scientific\npapers across 12 disciplines, with expert validation confirming its accuracy.\nTo prevent data contamination, we focus exclusively on papers published in\n2024, ensuring minimal overlap with LLM pretraining data. Our evaluation\nreveals that LLMs perform well in retrieving inspirations, an\nout-of-distribution task, suggesting their ability to surface novel knowledge\nassociations. This positions LLMs as \"research hypothesis mines\", capable of\nfacilitating automated scientific discovery by generating innovative hypotheses\nat scale with minimal human intervention."
                },
                "authors": [
                    {
                        "name": "Yujie Liu"
                    },
                    {
                        "name": "Zonglin Yang"
                    },
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Ben Gao"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Shixiang Tang"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Erik Cambria"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dongzhan Zhou"
                },
                "author": "Dongzhan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14138v2",
                "updated": "2025-03-27T08:07:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    8,
                    7,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2024-10-18T03:22:06Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    22,
                    6,
                    4,
                    292,
                    0
                ],
                "title": "ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and\n  Wisdom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and\n  Wisdom"
                },
                "summary": "Large vision-language models (LVLMs) have witnessed significant progress on\nvisual understanding tasks. However, they often prioritize language knowledge\nover image information on visual reasoning tasks, incurring performance\ndegradation. To tackle this issue, we first identify the drawbacks of existing\nsolutions (i.e., insufficient and irrelevant visual descriptions, and limited\nmulti-modal capacities). We then decompose visual reasoning process into two\nstages: visual perception (i.e., eyesight) and textual reasoning (i.e.,\nwisdom), and introduce a novel visual reasoning framework named ProReason. This\nframework features multi-run proactive perception and decoupled\nvision-reasoning capabilities. Briefly, given a multi-modal question, ProReason\niterates proactive information collection and reasoning until the answer can be\nconcluded with necessary and sufficient visual descriptions. Notably, the\ndisassociation of capabilities allows seamless integration of existing large\nlanguage models (LLMs) to compensate for the reasoning deficits of LVLMs. Our\nextensive experiments demonstrate that ProReason outperforms both existing\nmulti-step reasoning frameworks and passive peer methods on a wide range of\nbenchmarks for both open-source and closed-source models. In addition, with the\nassistance of LLMs, ProReason achieves a performance improvement of up to 15%\non MMMU benchmark. Our insights into existing solutions and the decoupled\nperspective for feasible integration of LLMs illuminate future research on\nvisual reasoning techniques, especially LLM-assisted ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (LVLMs) have witnessed significant progress on\nvisual understanding tasks. However, they often prioritize language knowledge\nover image information on visual reasoning tasks, incurring performance\ndegradation. To tackle this issue, we first identify the drawbacks of existing\nsolutions (i.e., insufficient and irrelevant visual descriptions, and limited\nmulti-modal capacities). We then decompose visual reasoning process into two\nstages: visual perception (i.e., eyesight) and textual reasoning (i.e.,\nwisdom), and introduce a novel visual reasoning framework named ProReason. This\nframework features multi-run proactive perception and decoupled\nvision-reasoning capabilities. Briefly, given a multi-modal question, ProReason\niterates proactive information collection and reasoning until the answer can be\nconcluded with necessary and sufficient visual descriptions. Notably, the\ndisassociation of capabilities allows seamless integration of existing large\nlanguage models (LLMs) to compensate for the reasoning deficits of LVLMs. Our\nextensive experiments demonstrate that ProReason outperforms both existing\nmulti-step reasoning frameworks and passive peer methods on a wide range of\nbenchmarks for both open-source and closed-source models. In addition, with the\nassistance of LLMs, ProReason achieves a performance improvement of up to 15%\non MMMU benchmark. Our insights into existing solutions and the decoupled\nperspective for feasible integration of LLMs illuminate future research on\nvisual reasoning techniques, especially LLM-assisted ones."
                },
                "authors": [
                    {
                        "name": "Jingqi Zhou"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Jingwei Dong"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Jiahui Gao"
                    },
                    {
                        "name": "Jiyue Jiang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21237v1",
                "updated": "2025-03-27T07:54:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    54,
                    39,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T07:54:39Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    54,
                    39,
                    3,
                    86,
                    0
                ],
                "title": "Bias-Aware Agent: Enhancing Fairness in AI-Driven Knowledge Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias-Aware Agent: Enhancing Fairness in AI-Driven Knowledge Retrieval"
                },
                "summary": "Advancements in retrieving accessible information have evolved faster in the\nlast few years compared to the decades since the internet's creation. Search\nengines, like Google, have been the number one way to find relevant data. They\nhave always relied on the user's abilities to find the best information in its\nbillions of links and sources at everybody's fingertips. The advent of large\nlanguage models (LLMs) has completely transformed the field of information\nretrieval. The LLMs excel not only at retrieving relevant knowledge but also at\nsummarizing it effectively, making information more accessible and consumable\nfor users. On top of it, the rise of AI Agents has introduced another aspect to\ninformation retrieval i.e. dynamic information retrieval which enables the\nintegration of real-time data such as weather forecasts, and financial data\nwith the knowledge base to curate context-aware knowledge. However, despite\nthese advancements the agents remain susceptible to issues of bias and\nfairness, challenges deeply rooted within the knowledge base and training of\nLLMs. This study introduces a novel approach to bias-aware knowledge retrieval\nby leveraging agentic framework and the innovative use of bias detectors as\ntools to identify and highlight inherent biases in the retrieved content. By\nempowering users with transparency and awareness, this approach aims to foster\nmore equitable information systems and promote the development of responsible\nAI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in retrieving accessible information have evolved faster in the\nlast few years compared to the decades since the internet's creation. Search\nengines, like Google, have been the number one way to find relevant data. They\nhave always relied on the user's abilities to find the best information in its\nbillions of links and sources at everybody's fingertips. The advent of large\nlanguage models (LLMs) has completely transformed the field of information\nretrieval. The LLMs excel not only at retrieving relevant knowledge but also at\nsummarizing it effectively, making information more accessible and consumable\nfor users. On top of it, the rise of AI Agents has introduced another aspect to\ninformation retrieval i.e. dynamic information retrieval which enables the\nintegration of real-time data such as weather forecasts, and financial data\nwith the knowledge base to curate context-aware knowledge. However, despite\nthese advancements the agents remain susceptible to issues of bias and\nfairness, challenges deeply rooted within the knowledge base and training of\nLLMs. This study introduces a novel approach to bias-aware knowledge retrieval\nby leveraging agentic framework and the innovative use of bias detectors as\ntools to identify and highlight inherent biases in the retrieved content. By\nempowering users with transparency and awareness, this approach aims to foster\nmore equitable information systems and promote the development of responsible\nAI."
                },
                "authors": [
                    {
                        "name": "Karanbir Singh"
                    },
                    {
                        "name": "William Ngu"
                    }
                ],
                "author_detail": {
                    "name": "William Ngu"
                },
                "author": "William Ngu",
                "arxiv_doi": "10.1145/3701716.3716885",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3716885",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.21237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12767v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12767v4",
                "updated": "2025-03-27T07:49:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    49,
                    51,
                    3,
                    86,
                    0
                ],
                "published": "2025-02-18T11:31:52Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    11,
                    31,
                    52,
                    1,
                    49,
                    0
                ],
                "title": "R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on\n  Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on\n  Knowledge Graphs"
                },
                "summary": "Recent studies have combined Large Language Models (LLMs) with Knowledge\nGraphs (KGs) to enhance reasoning, improving inference accuracy without\nadditional training while mitigating hallucination. However, existing\nframeworks are often rigid, struggling to adapt to KG or task changes. They\nalso rely heavily on powerful LLMs for reliable (i.e., trustworthy) reasoning.\nTo address this, We introduce R2-KG, a plug-and-play, dual-agent framework that\nseparates reasoning into two roles: an Operator (a low-capacity LLM) that\ngathers evidence and a Supervisor (a high-capacity LLM) that makes final\njudgments. This design is cost-efficient for LLM inference while still\nmaintaining strong reasoning accuracy. Additionally, R2-KG employs an\nAbstention mechanism, generating answers only when sufficient evidence is\ncollected from KG, which significantly enhances reliability. Experiments across\nmultiple KG-based reasoning tasks show that R2-KG consistently outperforms\nbaselines in both accuracy and reliability, regardless of the inherent\ncapability of LLMs used as the Operator. Further experiments reveal that the\nsingle-agent version of R2-KG, equipped with a strict self-consistency\nstrategy, achieves significantly higher-than-baseline reliability while\nreducing inference cost. However, it also leads to a higher abstention rate in\ncomplex KGs. Our findings establish R2-KG as a flexible and cost-effective\nsolution for KG-based reasoning. It reduces reliance on high-capacity LLMs\nwhile ensuring trustworthy inference. The code is available at\nhttps://github.com/ekrxjwh2009/R2-KG/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have combined Large Language Models (LLMs) with Knowledge\nGraphs (KGs) to enhance reasoning, improving inference accuracy without\nadditional training while mitigating hallucination. However, existing\nframeworks are often rigid, struggling to adapt to KG or task changes. They\nalso rely heavily on powerful LLMs for reliable (i.e., trustworthy) reasoning.\nTo address this, We introduce R2-KG, a plug-and-play, dual-agent framework that\nseparates reasoning into two roles: an Operator (a low-capacity LLM) that\ngathers evidence and a Supervisor (a high-capacity LLM) that makes final\njudgments. This design is cost-efficient for LLM inference while still\nmaintaining strong reasoning accuracy. Additionally, R2-KG employs an\nAbstention mechanism, generating answers only when sufficient evidence is\ncollected from KG, which significantly enhances reliability. Experiments across\nmultiple KG-based reasoning tasks show that R2-KG consistently outperforms\nbaselines in both accuracy and reliability, regardless of the inherent\ncapability of LLMs used as the Operator. Further experiments reveal that the\nsingle-agent version of R2-KG, equipped with a strict self-consistency\nstrategy, achieves significantly higher-than-baseline reliability while\nreducing inference cost. However, it also leads to a higher abstention rate in\ncomplex KGs. Our findings establish R2-KG as a flexible and cost-effective\nsolution for KG-based reasoning. It reduces reliance on high-capacity LLMs\nwhile ensuring trustworthy inference. The code is available at\nhttps://github.com/ekrxjwh2009/R2-KG/."
                },
                "authors": [
                    {
                        "name": "Sumin Jo"
                    },
                    {
                        "name": "Junseong Choi"
                    },
                    {
                        "name": "Jiho Kim"
                    },
                    {
                        "name": "Edward Choi"
                    }
                ],
                "author_detail": {
                    "name": "Edward Choi"
                },
                "author": "Edward Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12767v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12767v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21223v1",
                "updated": "2025-03-27T07:28:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    28,
                    30,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T07:28:30Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    28,
                    30,
                    3,
                    86,
                    0
                ],
                "title": "Rethinking Graph Structure Learning in the Era of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Graph Structure Learning in the Era of LLMs"
                },
                "summary": "Recently, the emergence of large language models (LLMs) has prompted\nresearchers to explore the integration of language descriptions into graphs,\naiming to enhance model encoding capabilities from a data-centric perspective.\nThis graph representation is called text-attributed graphs (TAGs). A review of\nprior advancements highlights that graph structure learning (GSL) is a pivotal\ntechnique for improving data utility, making it highly relevant to efficient\nTAG learning. However, most GSL methods are tailored for traditional graphs\nwithout textual information, underscoring the necessity of developing a new GSL\nparadigm. Despite clear motivations, it remains challenging: (1) How can we\ndefine a reasonable optimization objective for GSL in the era of LLMs,\nconsidering the massive parameters in LLM? (2) How can we design an efficient\nmodel architecture that enables seamless integration of LLM for this\noptimization objective? For Question 1, we reformulate existing GSL\noptimization objectives as a tree optimization framework, shifting the focus\nfrom obtaining a well-trained edge predictor to a language-aware tree sampler.\nFor Question 2, we propose decoupled and training-free model design principles\nfor LLM integration, shifting the focus from computation-intensive fine-tuning\nto more efficient inference. Based on this, we propose Large Language and Tree\nAssistant (LLaTA), which leverages tree-based LLM in-context learning to\nenhance the understanding of topology and text, enabling reliable inference and\ngenerating improved graph structure. Extensive experiments on 10 TAG datasets\ndemonstrate that LLaTA enjoys flexibility - incorporated with any backbone;\nscalability - outperforms other LLM-based GSL methods in terms of running\nefficiency; effectiveness - achieves SOTA performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the emergence of large language models (LLMs) has prompted\nresearchers to explore the integration of language descriptions into graphs,\naiming to enhance model encoding capabilities from a data-centric perspective.\nThis graph representation is called text-attributed graphs (TAGs). A review of\nprior advancements highlights that graph structure learning (GSL) is a pivotal\ntechnique for improving data utility, making it highly relevant to efficient\nTAG learning. However, most GSL methods are tailored for traditional graphs\nwithout textual information, underscoring the necessity of developing a new GSL\nparadigm. Despite clear motivations, it remains challenging: (1) How can we\ndefine a reasonable optimization objective for GSL in the era of LLMs,\nconsidering the massive parameters in LLM? (2) How can we design an efficient\nmodel architecture that enables seamless integration of LLM for this\noptimization objective? For Question 1, we reformulate existing GSL\noptimization objectives as a tree optimization framework, shifting the focus\nfrom obtaining a well-trained edge predictor to a language-aware tree sampler.\nFor Question 2, we propose decoupled and training-free model design principles\nfor LLM integration, shifting the focus from computation-intensive fine-tuning\nto more efficient inference. Based on this, we propose Large Language and Tree\nAssistant (LLaTA), which leverages tree-based LLM in-context learning to\nenhance the understanding of topology and text, enabling reliable inference and\ngenerating improved graph structure. Extensive experiments on 10 TAG datasets\ndemonstrate that LLaTA enjoys flexibility - incorporated with any backbone;\nscalability - outperforms other LLM-based GSL methods in terms of running\nefficiency; effectiveness - achieves SOTA performance."
                },
                "authors": [
                    {
                        "name": "Zhihan Zhang"
                    },
                    {
                        "name": "Xunkai Li"
                    },
                    {
                        "name": "Guang Zeng"
                    },
                    {
                        "name": "Hongchao Qin"
                    },
                    {
                        "name": "Ronghua Li"
                    },
                    {
                        "name": "Guoren Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoren Wang"
                },
                "author": "Guoren Wang",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08972v2",
                "updated": "2025-03-27T07:21:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    21,
                    27,
                    3,
                    86,
                    0
                ],
                "published": "2025-02-13T05:20:21Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    5,
                    20,
                    21,
                    3,
                    44,
                    0
                ],
                "title": "Tuning-Free Personalized Alignment via Trial-Error-Explain In-Context\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuning-Free Personalized Alignment via Trial-Error-Explain In-Context\n  Learning"
                },
                "summary": "Language models are aligned to the collective voice of many, resulting in\ngeneric outputs that do not align with specific users' styles. In this work, we\npresent Trial-Error-Explain In-Context Learning} (ITCL), a tuning-free method\nthat personalizes language models for text generation tasks with fewer than 10\nexamples per user. TICL iteratively expands an in-context learning prompt via a\ntrial-error-explain process, adding model-generated negative samples and\nexplanations that provide fine-grained guidance towards a specific user's\nstyle. TICL achieves favorable win rates on pairwise comparisons with\nLLM-as-a-judge up to 91.5% against the previous state-of-the-art and\noutperforms competitive tuning-free baselines for personalized alignment tasks\nof writing emails, essays and news articles. Both lexical and qualitative\nanalyses show that the negative samples and explanations enable language models\nto learn stylistic context more effectively and overcome the bias towards\nstructural and formal phrases observed in their zero-shot outputs. By\nfront-loading inference compute to create a user-specific in-context learning\nprompt that does not require extra generation steps at test time, TICL presents\na novel yet simple approach for personalized alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models are aligned to the collective voice of many, resulting in\ngeneric outputs that do not align with specific users' styles. In this work, we\npresent Trial-Error-Explain In-Context Learning} (ITCL), a tuning-free method\nthat personalizes language models for text generation tasks with fewer than 10\nexamples per user. TICL iteratively expands an in-context learning prompt via a\ntrial-error-explain process, adding model-generated negative samples and\nexplanations that provide fine-grained guidance towards a specific user's\nstyle. TICL achieves favorable win rates on pairwise comparisons with\nLLM-as-a-judge up to 91.5% against the previous state-of-the-art and\noutperforms competitive tuning-free baselines for personalized alignment tasks\nof writing emails, essays and news articles. Both lexical and qualitative\nanalyses show that the negative samples and explanations enable language models\nto learn stylistic context more effectively and overcome the bias towards\nstructural and formal phrases observed in their zero-shot outputs. By\nfront-loading inference compute to create a user-specific in-context learning\nprompt that does not require extra generation steps at test time, TICL presents\na novel yet simple approach for personalized alignment."
                },
                "authors": [
                    {
                        "name": "Hyundong Cho"
                    },
                    {
                        "name": "Karishma Sharma"
                    },
                    {
                        "name": "Nicolaas Jedema"
                    },
                    {
                        "name": "Leonardo F. R. Ribeiro"
                    },
                    {
                        "name": "Alessandro Moschitti"
                    },
                    {
                        "name": "Ravi Krishnan"
                    },
                    {
                        "name": "Jonathan May"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan May"
                },
                "author": "Jonathan May",
                "arxiv_comment": "NAACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18305v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18305v2",
                "updated": "2025-03-27T07:16:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    16,
                    23,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-24T03:10:34Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    10,
                    34,
                    0,
                    83,
                    0
                ],
                "title": "Enhancing LLM-based Code Translation in Repository Context via Triple\n  Knowledge-Augmented",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM-based Code Translation in Repository Context via Triple\n  Knowledge-Augmented"
                },
                "summary": "Large language models (LLMs) have behaved well in function-level code\ntranslation without repository-level context. However, the performance of LLMs\nin repository-level context code translation remains suboptimal due to complex\ndependencies and context, hindering their adoption in industrial settings. In\nthis work, we propose a novel LLM-based code translation technique K-Trans,\nwhich leverages triple knowledge augmentation to enhance LLM's translation\nquality under repository context in real-world software development. First,\nK-Trans constructs a translation knowledge base by extracting relevant\ninformation from target-language codebases, the repository being translated,\nand prior translation results. Second, for each function to be translated,\nK-Trans retrieves relevant triple knowledge, including target-language code\nsamples, dependency usage examples, and successful translation function pairs,\nserving as references to enhance LLM for translation. Third, K-Trans constructs\na knowledge-augmented translation prompt using the retrieved triple knowledge\nand employs LLMs to generate the translated code while preserving repository\ncontext. It further leverages LLMs for self-debugging, enhancing translation\ncorrectness.\n  The experiments show that K-Trans substantially outperforms the baseline\nadapted from previous work by 19.4%/40.2% relative improvement in pass@1 and\n0.138 in CodeBLEU. It is important to note that the results also demonstrate\nthat each knowledge significantly contributes to K-Trans's effectiveness in\nhandling repository-level context code translation, with dependency usage\nexamples making the most notable contribution. Moreover, as the self-evolution\nprocess progresses, the knowledge base continuously enhances the LLM's\nperformance across various aspects of the repository-level code translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have behaved well in function-level code\ntranslation without repository-level context. However, the performance of LLMs\nin repository-level context code translation remains suboptimal due to complex\ndependencies and context, hindering their adoption in industrial settings. In\nthis work, we propose a novel LLM-based code translation technique K-Trans,\nwhich leverages triple knowledge augmentation to enhance LLM's translation\nquality under repository context in real-world software development. First,\nK-Trans constructs a translation knowledge base by extracting relevant\ninformation from target-language codebases, the repository being translated,\nand prior translation results. Second, for each function to be translated,\nK-Trans retrieves relevant triple knowledge, including target-language code\nsamples, dependency usage examples, and successful translation function pairs,\nserving as references to enhance LLM for translation. Third, K-Trans constructs\na knowledge-augmented translation prompt using the retrieved triple knowledge\nand employs LLMs to generate the translated code while preserving repository\ncontext. It further leverages LLMs for self-debugging, enhancing translation\ncorrectness.\n  The experiments show that K-Trans substantially outperforms the baseline\nadapted from previous work by 19.4%/40.2% relative improvement in pass@1 and\n0.138 in CodeBLEU. It is important to note that the results also demonstrate\nthat each knowledge significantly contributes to K-Trans's effectiveness in\nhandling repository-level context code translation, with dependency usage\nexamples making the most notable contribution. Moreover, as the self-evolution\nprocess progresses, the knowledge base continuously enhances the LLM's\nperformance across various aspects of the repository-level code translation."
                },
                "authors": [
                    {
                        "name": "Guangsheng Ou"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Xueying Du"
                    },
                    {
                        "name": "Shengbo Wang"
                    },
                    {
                        "name": "Zekai Zhang"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18305v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18305v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10095v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10095v2",
                "updated": "2025-03-27T07:14:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    14,
                    15,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-13T06:42:37Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    6,
                    42,
                    37,
                    3,
                    72,
                    0
                ],
                "title": "Cognitive-Mental-LLM: Evaluating Reasoning in Large Language Models for\n  Mental Health Prediction via Online Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive-Mental-LLM: Evaluating Reasoning in Large Language Models for\n  Mental Health Prediction via Online Text"
                },
                "summary": "Large Language Models (LLMs) have demonstrated potential in predicting mental\nhealth outcomes from online text, yet traditional classification methods often\nlack interpretability and robustness. This study evaluates structured reasoning\ntechniques-Chain-of-Thought (CoT), Self-Consistency (SC-CoT), and\nTree-of-Thought (ToT)-to improve classification accuracy across multiple mental\nhealth datasets sourced from Reddit. We analyze reasoning-driven prompting\nstrategies, including Zero-shot CoT and Few-shot CoT, using key performance\nmetrics such as Balanced Accuracy, F1 score, and Sensitivity/Specificity. Our\nfindings indicate that reasoning-enhanced techniques improve classification\nperformance over direct prediction, particularly in complex cases. Compared to\nbaselines such as Zero Shot non-CoT Prompting, and fine-tuned pre-trained\ntransformers such as BERT and Mental-RoBerta, and fine-tuned Open Source LLMs\nsuch as Mental Alpaca and Mental-Flan-T5, reasoning-driven LLMs yield notable\ngains on datasets like Dreaddit (+0.52\\% over M-LLM, +0.82\\% over BERT) and\nSDCNL (+4.67\\% over M-LLM, +2.17\\% over BERT). However, performance declines in\nDepression Severity, and CSSRS predictions suggest dataset-specific\nlimitations, likely due to our using a more extensive test set. Among prompting\nstrategies, Few-shot CoT consistently outperforms others, reinforcing the\neffectiveness of reasoning-driven LLMs. Nonetheless, dataset variability\nhighlights challenges in model reliability and interpretability. This study\nprovides a comprehensive benchmark of reasoning-based LLM techniques for mental\nhealth text classification. It offers insights into their potential for\nscalable clinical applications while identifying key challenges for future\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated potential in predicting mental\nhealth outcomes from online text, yet traditional classification methods often\nlack interpretability and robustness. This study evaluates structured reasoning\ntechniques-Chain-of-Thought (CoT), Self-Consistency (SC-CoT), and\nTree-of-Thought (ToT)-to improve classification accuracy across multiple mental\nhealth datasets sourced from Reddit. We analyze reasoning-driven prompting\nstrategies, including Zero-shot CoT and Few-shot CoT, using key performance\nmetrics such as Balanced Accuracy, F1 score, and Sensitivity/Specificity. Our\nfindings indicate that reasoning-enhanced techniques improve classification\nperformance over direct prediction, particularly in complex cases. Compared to\nbaselines such as Zero Shot non-CoT Prompting, and fine-tuned pre-trained\ntransformers such as BERT and Mental-RoBerta, and fine-tuned Open Source LLMs\nsuch as Mental Alpaca and Mental-Flan-T5, reasoning-driven LLMs yield notable\ngains on datasets like Dreaddit (+0.52\\% over M-LLM, +0.82\\% over BERT) and\nSDCNL (+4.67\\% over M-LLM, +2.17\\% over BERT). However, performance declines in\nDepression Severity, and CSSRS predictions suggest dataset-specific\nlimitations, likely due to our using a more extensive test set. Among prompting\nstrategies, Few-shot CoT consistently outperforms others, reinforcing the\neffectiveness of reasoning-driven LLMs. Nonetheless, dataset variability\nhighlights challenges in model reliability and interpretability. This study\nprovides a comprehensive benchmark of reasoning-based LLM techniques for mental\nhealth text classification. It offers insights into their potential for\nscalable clinical applications while identifying key challenges for future\nimprovements."
                },
                "authors": [
                    {
                        "name": "Avinash Patil"
                    },
                    {
                        "name": "Amardeep Kour Gedhu"
                    }
                ],
                "author_detail": {
                    "name": "Amardeep Kour Gedhu"
                },
                "author": "Amardeep Kour Gedhu",
                "arxiv_comment": "8 pages, 4 Figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10095v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10095v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13990v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13990v5",
                "updated": "2025-03-27T07:12:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    12,
                    39,
                    3,
                    86,
                    0
                ],
                "published": "2024-11-21T10:00:52Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    0,
                    52,
                    3,
                    326,
                    0
                ],
                "title": "Repository-level Code Translation Benchmark Targeting Rust",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-level Code Translation Benchmark Targeting Rust"
                },
                "summary": "Recent advancements in large language models (LLMs) have demonstrated\nimpressive capabilities in code translation, typically evaluated using\nbenchmarks like CodeTransOcean. However, these benchmarks fail to capture\nreal-world complexities by focusing primarily on simple function-level\ntranslations and overlooking repository-level context (e.g., dependencies).\nMoreover, LLMs' effectiveness in translating to newer, low-resource languages\nlike Rust remains largely underexplored. To address this gap, we introduce\nRustRepoTrans, the first repository-level code translation benchmark,\ncomprising 375 tasks translating into Rust from C++, Java, and Python. Using\nthis benchmark, we evaluate four state-of-the-art LLMs, analyzing their errors\nto assess limitations in complex translation scenarios. Among them, Claude-3.5\nperforms best with 43.5% Pass@1, excelling in both basic functionality and\nadditional translation abilities, such as noise robustness and syntactical\ndifference identification. However, even Claude-3.5 experiences a 30.8%\nperformance drop (Pass@1 from 74.3% to 43.5%) when handling repository-level\ncontext compared to previous benchmarks without such context. We also find that\nLLMs struggle with language differences in complex tasks, and dependencies\nfurther increase translation difficulty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have demonstrated\nimpressive capabilities in code translation, typically evaluated using\nbenchmarks like CodeTransOcean. However, these benchmarks fail to capture\nreal-world complexities by focusing primarily on simple function-level\ntranslations and overlooking repository-level context (e.g., dependencies).\nMoreover, LLMs' effectiveness in translating to newer, low-resource languages\nlike Rust remains largely underexplored. To address this gap, we introduce\nRustRepoTrans, the first repository-level code translation benchmark,\ncomprising 375 tasks translating into Rust from C++, Java, and Python. Using\nthis benchmark, we evaluate four state-of-the-art LLMs, analyzing their errors\nto assess limitations in complex translation scenarios. Among them, Claude-3.5\nperforms best with 43.5% Pass@1, excelling in both basic functionality and\nadditional translation abilities, such as noise robustness and syntactical\ndifference identification. However, even Claude-3.5 experiences a 30.8%\nperformance drop (Pass@1 from 74.3% to 43.5%) when handling repository-level\ncontext compared to previous benchmarks without such context. We also find that\nLLMs struggle with language differences in complex tasks, and dependencies\nfurther increase translation difficulty."
                },
                "authors": [
                    {
                        "name": "Guangsheng Ou"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13990v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13990v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14963v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14963v5",
                "updated": "2025-03-27T07:08:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    8,
                    33,
                    3,
                    86,
                    0
                ],
                "published": "2024-04-23T12:16:05Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    12,
                    16,
                    5,
                    1,
                    114,
                    0
                ],
                "title": "Achieving >97% on GSM8K: Deeply Understanding the Problems Makes LLMs\n  Better Solvers for Math Word Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving >97% on GSM8K: Deeply Understanding the Problems Makes LLMs\n  Better Solvers for Math Word Problems"
                },
                "summary": "Chain-of-Thought (CoT) prompting has enhanced the performance of Large\nLanguage Models (LLMs) across various reasoning tasks. However, CoT still falls\nshort in dealing with complex math word problems, as it usually suffers from\nthree pitfalls: semantic misunderstanding errors, calculation errors, and\nstep-missing errors. Prior studies involve addressing the calculation errors\nand step-missing errors, but neglect the semantic misunderstanding errors,\nwhich is the major factor limiting the reasoning performance of LLMs. To this\nend, we propose a simple-yet-effective method, namely Deeply Understanding the\nProblems (DUP), to improve the LLMs' math problem-solving ability by addressing\nsemantic misunderstanding errors. The core of our method is to encourage the\nLLMs to deeply understand the problems and extract the key problem-solving\ninformation used for better reasoning. Extensive experiments on 10 diverse\nreasoning benchmarks show that our DUP method consistently outperforms the\nother counterparts by a large margin. More encouragingly, DUP achieves a new\nSOTA result on the GSM8K benchmark, with an accuracy of 97.1% under the\nzero-shot setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting has enhanced the performance of Large\nLanguage Models (LLMs) across various reasoning tasks. However, CoT still falls\nshort in dealing with complex math word problems, as it usually suffers from\nthree pitfalls: semantic misunderstanding errors, calculation errors, and\nstep-missing errors. Prior studies involve addressing the calculation errors\nand step-missing errors, but neglect the semantic misunderstanding errors,\nwhich is the major factor limiting the reasoning performance of LLMs. To this\nend, we propose a simple-yet-effective method, namely Deeply Understanding the\nProblems (DUP), to improve the LLMs' math problem-solving ability by addressing\nsemantic misunderstanding errors. The core of our method is to encourage the\nLLMs to deeply understand the problems and extract the key problem-solving\ninformation used for better reasoning. Extensive experiments on 10 diverse\nreasoning benchmarks show that our DUP method consistently outperforms the\nother counterparts by a large margin. More encouragingly, DUP achieves a new\nSOTA result on the GSM8K benchmark, with an accuracy of 97.1% under the\nzero-shot setting."
                },
                "authors": [
                    {
                        "name": "Qihuang Zhong"
                    },
                    {
                        "name": "Kang Wang"
                    },
                    {
                        "name": "Ziyang Xu"
                    },
                    {
                        "name": "Juhua Liu"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Bo Du"
                    }
                ],
                "author_detail": {
                    "name": "Bo Du"
                },
                "author": "Bo Du",
                "arxiv_comment": "The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: { 10.1007/s11704-025-41102-z }",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14963v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14963v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21213v1",
                "updated": "2025-03-27T07:05:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    5,
                    22,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T07:05:22Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    5,
                    22,
                    3,
                    86,
                    0
                ],
                "title": "Resource-Efficient Federated Fine-Tuning Large Language Models for\n  Heterogeneous Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Efficient Federated Fine-Tuning Large Language Models for\n  Heterogeneous Data"
                },
                "summary": "Fine-tuning large language models (LLMs) via federated learning, i.e.,\nFedLLM, has been proposed to adapt LLMs for various downstream applications in\na privacy-preserving way. To reduce the fine-tuning costs on\nresource-constrained devices, FedLoRA is proposed to fine-tune only a small\nsubset of model parameters by integrating low-rank adaptation (LoRA) into\nFedLLM. However, apart from resource constraints, there is still another\ncritical challenge, i.e., data heterogeneity, severely hindering the\nimplementation of FedLoRA in practical applications. Herein, inspired by the\nprevious group-based federated learning paradigm, we propose a hierarchical\nFedLoRA framework, termed HierFedLoRA, to address these challenges.\nSpecifically, HierFedLoRA partitions all devices into multiple near-IID groups\nand adjusts the intra-group aggregation frequency for each group to eliminate\nthe negative effects of non-IID data. Meanwhile, to reduce the computation and\ncommunication cost, HierFedLoRA dynamically assigns diverse and suitable\nfine-tuning depth (i.e., the number of continuous fine-tuning layers from the\noutput) for each group. HierFedLoRA explores jointly optimizing aggregation\nfrequency and depth upon their coupled relationship to better enhance the\nperformance of FedLoRA. Extensive experiments are conducted on a physical\nplatform with 80 commercial devices. The results show that HierFedLoRA improves\nthe final model accuracy by 1.6% to 4.2%, speeding up the fine-tuning process\nby at least 2.1$\\times$, compared to the strong baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) via federated learning, i.e.,\nFedLLM, has been proposed to adapt LLMs for various downstream applications in\na privacy-preserving way. To reduce the fine-tuning costs on\nresource-constrained devices, FedLoRA is proposed to fine-tune only a small\nsubset of model parameters by integrating low-rank adaptation (LoRA) into\nFedLLM. However, apart from resource constraints, there is still another\ncritical challenge, i.e., data heterogeneity, severely hindering the\nimplementation of FedLoRA in practical applications. Herein, inspired by the\nprevious group-based federated learning paradigm, we propose a hierarchical\nFedLoRA framework, termed HierFedLoRA, to address these challenges.\nSpecifically, HierFedLoRA partitions all devices into multiple near-IID groups\nand adjusts the intra-group aggregation frequency for each group to eliminate\nthe negative effects of non-IID data. Meanwhile, to reduce the computation and\ncommunication cost, HierFedLoRA dynamically assigns diverse and suitable\nfine-tuning depth (i.e., the number of continuous fine-tuning layers from the\noutput) for each group. HierFedLoRA explores jointly optimizing aggregation\nfrequency and depth upon their coupled relationship to better enhance the\nperformance of FedLoRA. Extensive experiments are conducted on a physical\nplatform with 80 commercial devices. The results show that HierFedLoRA improves\nthe final model accuracy by 1.6% to 4.2%, speeding up the fine-tuning process\nby at least 2.1$\\times$, compared to the strong baselines."
                },
                "authors": [
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Yunming Liao"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Yang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Xu"
                },
                "author": "Yang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09056v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09056v3",
                "updated": "2025-03-27T06:45:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    6,
                    45,
                    16,
                    3,
                    86,
                    0
                ],
                "published": "2025-02-13T08:10:45Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    8,
                    10,
                    45,
                    3,
                    44,
                    0
                ],
                "title": "Adapting Language-Specific LLMs to a Reasoning Model in One Day via\n  Model Merging -- An Open Recipe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Language-Specific LLMs to a Reasoning Model in One Day via\n  Model Merging -- An Open Recipe"
                },
                "summary": "This paper investigates data selection and model merging methodologies aimed\nat incorporating advanced reasoning capabilities such as those of DeepSeek R1\ninto language-specific large language models (LLMs), with a particular focus on\nthe Thai LLM. Our goal is to enhance the reasoning capabilities of\nlanguage-specific LLMs while maintaining their target language abilities.\nDeepSeek R1 excels in reasoning but primarily benefits high-resource languages\nsuch as English and Chinese. However, low-resource languages remain underserved\ndue to the dominance of English-centric training data and model optimizations,\nwhich limit performance in these languages. This limitation results in\nunreliable code-switching and diminished effectiveness on tasks in low-resource\nlanguages. Meanwhile, local and regional LLM initiatives have attempted to\nbridge this gap by developing language-specific LLMs that focus on improving\nlocal linguistic fidelity. We demonstrate that, with only publicly available\ndatasets and a computational budget of $120, it is possible to enhance the\nreasoning capabilities of language-specific LLMs to match the level of DeepSeek\nR1, without compromising their performance on target language tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates data selection and model merging methodologies aimed\nat incorporating advanced reasoning capabilities such as those of DeepSeek R1\ninto language-specific large language models (LLMs), with a particular focus on\nthe Thai LLM. Our goal is to enhance the reasoning capabilities of\nlanguage-specific LLMs while maintaining their target language abilities.\nDeepSeek R1 excels in reasoning but primarily benefits high-resource languages\nsuch as English and Chinese. However, low-resource languages remain underserved\ndue to the dominance of English-centric training data and model optimizations,\nwhich limit performance in these languages. This limitation results in\nunreliable code-switching and diminished effectiveness on tasks in low-resource\nlanguages. Meanwhile, local and regional LLM initiatives have attempted to\nbridge this gap by developing language-specific LLMs that focus on improving\nlocal linguistic fidelity. We demonstrate that, with only publicly available\ndatasets and a computational budget of $120, it is possible to enhance the\nreasoning capabilities of language-specific LLMs to match the level of DeepSeek\nR1, without compromising their performance on target language tasks."
                },
                "authors": [
                    {
                        "name": "Kunat Pipatanakul"
                    },
                    {
                        "name": "Pittawat Taveekitworachai"
                    },
                    {
                        "name": "Potsawee Manakul"
                    },
                    {
                        "name": "Kasima Tharnpipitchai"
                    }
                ],
                "author_detail": {
                    "name": "Kasima Tharnpipitchai"
                },
                "author": "Kasima Tharnpipitchai",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09056v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09056v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09042v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09042v2",
                "updated": "2025-03-27T06:45:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    6,
                    45,
                    15,
                    3,
                    86,
                    0
                ],
                "published": "2025-02-13T07:55:54Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    7,
                    55,
                    54,
                    3,
                    44,
                    0
                ],
                "title": "Typhoon T1: An Open Thai Reasoning Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Typhoon T1: An Open Thai Reasoning Model"
                },
                "summary": "This paper introduces Typhoon T1, an open effort to develop an open Thai\nreasoning model. A reasoning model is a relatively new type of generative model\nbuilt on top of large language models (LLMs). A reasoning model generates a\nlong chain of thought before arriving at a final answer, an approach found to\nimprove performance on complex tasks. However, details on developing such a\nmodel are limited, especially for reasoning models that can generate traces in\na low-resource language. Typhoon T1 presents an open effort that dives into the\ndetails of developing a reasoning model in a more cost-effective way by\nleveraging supervised fine-tuning using open datasets, instead of reinforcement\nlearning. This paper shares the details about synthetic data generation and\ntraining, as well as our dataset and model weights. Additionally, we provide\ninsights gained from developing a reasoning model that generalizes across\ndomains and is capable of generating reasoning traces in a low-resource\nlanguage, using Thai as an example. We hope this open effort provides a\nfoundation for further research in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Typhoon T1, an open effort to develop an open Thai\nreasoning model. A reasoning model is a relatively new type of generative model\nbuilt on top of large language models (LLMs). A reasoning model generates a\nlong chain of thought before arriving at a final answer, an approach found to\nimprove performance on complex tasks. However, details on developing such a\nmodel are limited, especially for reasoning models that can generate traces in\na low-resource language. Typhoon T1 presents an open effort that dives into the\ndetails of developing a reasoning model in a more cost-effective way by\nleveraging supervised fine-tuning using open datasets, instead of reinforcement\nlearning. This paper shares the details about synthetic data generation and\ntraining, as well as our dataset and model weights. Additionally, we provide\ninsights gained from developing a reasoning model that generalizes across\ndomains and is capable of generating reasoning traces in a low-resource\nlanguage, using Thai as an example. We hope this open effort provides a\nfoundation for further research in this field."
                },
                "authors": [
                    {
                        "name": "Pittawat Taveekitworachai"
                    },
                    {
                        "name": "Potsawee Manakul"
                    },
                    {
                        "name": "Kasima Tharnpipitchai"
                    },
                    {
                        "name": "Kunat Pipatanakul"
                    }
                ],
                "author_detail": {
                    "name": "Kunat Pipatanakul"
                },
                "author": "Kunat Pipatanakul",
                "arxiv_comment": "25 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09042v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09042v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18769v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18769v2",
                "updated": "2025-03-27T06:39:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    6,
                    39,
                    47,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-24T15:16:51Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    16,
                    51,
                    0,
                    83,
                    0
                ],
                "title": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and\n  Symbolic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and\n  Symbolic Reasoning"
                },
                "summary": "This paper presents AlphaSpace, a novel methodology designed to enhance the\nspatial reasoning capabilities of language models for robotic manipulation in\n3D Cartesian space. AlphaSpace employs a hierarchical semantics-based\ntokenization strategy that encodes spatial information at both coarse and\nfine-grained levels. Our approach represents objects with their attributes,\npositions, and height information through structured tokens, enabling precise\nspatial reasoning without relying on traditional vision-based embeddings. This\napproach enables LLMs to accurately manipulate objects by positioning them at\nspecific (x, y, z) coordinates. Experimental results suggest that AlphaSpace\ndemonstrates promising potential for improving manipulation tasks, achieving a\ntotal accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude\n3.5 Sonnet. These results demonstrate the potential of structured spatial\nencoding for manipulation tasks and warrant further exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents AlphaSpace, a novel methodology designed to enhance the\nspatial reasoning capabilities of language models for robotic manipulation in\n3D Cartesian space. AlphaSpace employs a hierarchical semantics-based\ntokenization strategy that encodes spatial information at both coarse and\nfine-grained levels. Our approach represents objects with their attributes,\npositions, and height information through structured tokens, enabling precise\nspatial reasoning without relying on traditional vision-based embeddings. This\napproach enables LLMs to accurately manipulate objects by positioning them at\nspecific (x, y, z) coordinates. Experimental results suggest that AlphaSpace\ndemonstrates promising potential for improving manipulation tasks, achieving a\ntotal accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude\n3.5 Sonnet. These results demonstrate the potential of structured spatial\nencoding for manipulation tasks and warrant further exploration."
                },
                "authors": [
                    {
                        "name": "Alan Dao"
                    },
                    {
                        "name": "Dinh Bach Vu"
                    },
                    {
                        "name": "Bui Quang Huy"
                    }
                ],
                "author_detail": {
                    "name": "Bui Quang Huy"
                },
                "arxiv_affiliation": "Gia Tuan Dao",
                "author": "Bui Quang Huy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18769v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18769v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02471v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02471v2",
                "updated": "2025-03-27T06:39:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    6,
                    39,
                    45,
                    3,
                    86,
                    0
                ],
                "published": "2025-01-05T07:46:51Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    46,
                    51,
                    6,
                    5,
                    0
                ],
                "title": "Hengqin-RA-v1: Advanced Large Language Model for Diagnosis and Treatment\n  of Rheumatoid Arthritis with Dataset based Traditional Chinese Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hengqin-RA-v1: Advanced Large Language Model for Diagnosis and Treatment\n  of Rheumatoid Arthritis with Dataset based Traditional Chinese Medicine"
                },
                "summary": "Large language models (LLMs) primarily trained on English texts, often face\nbiases and inaccuracies in Chinese contexts. Their limitations are pronounced\nin fields like Traditional Chinese Medicine (TCM), where cultural and clinical\nsubtleties are vital, further hindered by a lack of domain-specific data, such\nas rheumatoid arthritis (RA). To address these issues, this paper introduces\nHengqin-RA-v1, the first large language model specifically tailored for TCM\nwith a focus on diagnosing and treating RA. We also present HQ-GCM-RA-C1, a\ncomprehensive RA-specific dataset curated from ancient Chinese medical\nliterature, classical texts, and modern clinical studies. This dataset empowers\nHengqin-RA-v1 to deliver accurate and culturally informed responses,\neffectively bridging the gaps left by general-purpose models. Extensive\nexperiments demonstrate that Hengqin-RA-v1 outperforms state-of-the-art models,\neven surpassing the diagnostic accuracy of TCM practitioners in certain cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) primarily trained on English texts, often face\nbiases and inaccuracies in Chinese contexts. Their limitations are pronounced\nin fields like Traditional Chinese Medicine (TCM), where cultural and clinical\nsubtleties are vital, further hindered by a lack of domain-specific data, such\nas rheumatoid arthritis (RA). To address these issues, this paper introduces\nHengqin-RA-v1, the first large language model specifically tailored for TCM\nwith a focus on diagnosing and treating RA. We also present HQ-GCM-RA-C1, a\ncomprehensive RA-specific dataset curated from ancient Chinese medical\nliterature, classical texts, and modern clinical studies. This dataset empowers\nHengqin-RA-v1 to deliver accurate and culturally informed responses,\neffectively bridging the gaps left by general-purpose models. Extensive\nexperiments demonstrate that Hengqin-RA-v1 outperforms state-of-the-art models,\neven surpassing the diagnostic accuracy of TCM practitioners in certain cases."
                },
                "authors": [
                    {
                        "name": "Yishen Liu"
                    },
                    {
                        "name": "Shengda Luo"
                    },
                    {
                        "name": "Zishao Zhong"
                    },
                    {
                        "name": "Tongtong Wu"
                    },
                    {
                        "name": "Jianguo Zhang"
                    },
                    {
                        "name": "Peiyao Ou"
                    },
                    {
                        "name": "Yong Liang"
                    },
                    {
                        "name": "Liang Liu"
                    },
                    {
                        "name": "Hudan Pan"
                    }
                ],
                "author_detail": {
                    "name": "Hudan Pan"
                },
                "author": "Hudan Pan",
                "arxiv_comment": "8 pages, 5 figures, AAAI-2025 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02471v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02471v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06874v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06874v2",
                "updated": "2025-03-27T06:37:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    6,
                    37,
                    40,
                    3,
                    86,
                    0
                ],
                "published": "2025-02-08T09:02:43Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    9,
                    2,
                    43,
                    5,
                    39,
                    0
                ],
                "title": "Group Reasoning Emission Estimation Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group Reasoning Emission Estimation Networks"
                },
                "summary": "Accurate greenhouse gas (GHG) emission reporting is critical for governments,\nbusinesses, and investors. However, adoption remains limited particularly among\nsmall and medium enterprises due to high implementation costs, fragmented\nemission factor databases, and a lack of robust sector classification methods.\nTo address these challenges, we introduce Group Reasoning Emission Estimation\nNetworks (GREEN), an AI-driven carbon accounting framework that standardizes\nenterprise-level emission estimation, constructs a large-scale benchmark\ndataset, and leverages a novel reasoning approach with large language models\n(LLMs). Specifically, we compile textual descriptions for 20,850 companies with\nvalidated North American Industry Classification System (NAICS) labels and\nalign these with an economic model of carbon intensity factors. By reframing\nsector classification as an information retrieval task, we fine-tune\nSentence-BERT models using a contrastive learning loss. To overcome the\nlimitations of single-stage models in handling thousands of hierarchical\ncategories, we propose a Group Reasoning method that ensembles LLM classifiers\nbased on the natural NAICS ontology, decomposing the task into multiple\nsub-classification steps. We theoretically prove that this approach reduces\nclassification uncertainty and computational complexity. Experiments on 1,114\nNAICS categories yield state-of-the-art performance (83.68% Top-1, 91.47%\nTop-10 accuracy), and case studies on 20 companies report a mean absolute\npercentage error (MAPE) of 45.88%. The project is available at:\nhttps://huggingface.co/datasets/Yvnminc/ExioNAICS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate greenhouse gas (GHG) emission reporting is critical for governments,\nbusinesses, and investors. However, adoption remains limited particularly among\nsmall and medium enterprises due to high implementation costs, fragmented\nemission factor databases, and a lack of robust sector classification methods.\nTo address these challenges, we introduce Group Reasoning Emission Estimation\nNetworks (GREEN), an AI-driven carbon accounting framework that standardizes\nenterprise-level emission estimation, constructs a large-scale benchmark\ndataset, and leverages a novel reasoning approach with large language models\n(LLMs). Specifically, we compile textual descriptions for 20,850 companies with\nvalidated North American Industry Classification System (NAICS) labels and\nalign these with an economic model of carbon intensity factors. By reframing\nsector classification as an information retrieval task, we fine-tune\nSentence-BERT models using a contrastive learning loss. To overcome the\nlimitations of single-stage models in handling thousands of hierarchical\ncategories, we propose a Group Reasoning method that ensembles LLM classifiers\nbased on the natural NAICS ontology, decomposing the task into multiple\nsub-classification steps. We theoretically prove that this approach reduces\nclassification uncertainty and computational complexity. Experiments on 1,114\nNAICS categories yield state-of-the-art performance (83.68% Top-1, 91.47%\nTop-10 accuracy), and case studies on 20 companies report a mean absolute\npercentage error (MAPE) of 45.88%. The project is available at:\nhttps://huggingface.co/datasets/Yvnminc/ExioNAICS."
                },
                "authors": [
                    {
                        "name": "Yanming Guo"
                    },
                    {
                        "name": "Xiao Qian"
                    },
                    {
                        "name": "Kevin Credit"
                    },
                    {
                        "name": "Jin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jin Ma"
                },
                "author": "Jin Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06874v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06874v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21190v1",
                "updated": "2025-03-27T06:14:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    6,
                    14,
                    21,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T06:14:21Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    6,
                    14,
                    21,
                    3,
                    86,
                    0
                ],
                "title": "Leveraging LLMs with Iterative Loop Structure for Enhanced Social\n  Intelligence in Video Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs with Iterative Loop Structure for Enhanced Social\n  Intelligence in Video Question Answering"
                },
                "summary": "Social intelligence, the ability to interpret emotions, intentions, and\nbehaviors, is essential for effective communication and adaptive responses. As\nrobots and AI systems become more prevalent in caregiving, healthcare, and\neducation, the demand for AI that can interact naturally with humans grows.\nHowever, creating AI that seamlessly integrates multiple modalities, such as\nvision and speech, remains a challenge. Current video-based methods for social\nintelligence rely on general video recognition or emotion recognition\ntechniques, often overlook the unique elements inherent in human interactions.\nTo address this, we propose the Looped Video Debating (LVD) framework, which\nintegrates Large Language Models (LLMs) with visual information, such as facial\nexpressions and body movements, to enhance the transparency and reliability of\nquestion-answering tasks involving human interaction videos. Our results on the\nSocial-IQ 2.0 benchmark show that LVD achieves state-of-the-art performance\nwithout fine-tuning. Furthermore, supplementary human annotations on existing\ndatasets provide insights into the model's accuracy, guiding future\nimprovements in AI-driven social intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social intelligence, the ability to interpret emotions, intentions, and\nbehaviors, is essential for effective communication and adaptive responses. As\nrobots and AI systems become more prevalent in caregiving, healthcare, and\neducation, the demand for AI that can interact naturally with humans grows.\nHowever, creating AI that seamlessly integrates multiple modalities, such as\nvision and speech, remains a challenge. Current video-based methods for social\nintelligence rely on general video recognition or emotion recognition\ntechniques, often overlook the unique elements inherent in human interactions.\nTo address this, we propose the Looped Video Debating (LVD) framework, which\nintegrates Large Language Models (LLMs) with visual information, such as facial\nexpressions and body movements, to enhance the transparency and reliability of\nquestion-answering tasks involving human interaction videos. Our results on the\nSocial-IQ 2.0 benchmark show that LVD achieves state-of-the-art performance\nwithout fine-tuning. Furthermore, supplementary human annotations on existing\ndatasets provide insights into the model's accuracy, guiding future\nimprovements in AI-driven social intelligence."
                },
                "authors": [
                    {
                        "name": "Erika Mori"
                    },
                    {
                        "name": "Yue Qiu"
                    },
                    {
                        "name": "Hirokatsu Kataoka"
                    },
                    {
                        "name": "Yoshimitsu Aoki"
                    }
                ],
                "author_detail": {
                    "name": "Yoshimitsu Aoki"
                },
                "author": "Yoshimitsu Aoki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21188v1",
                "updated": "2025-03-27T06:10:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    6,
                    10,
                    22,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T06:10:22Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    6,
                    10,
                    22,
                    3,
                    86,
                    0
                ],
                "title": "Are We Solving a Well-Defined Problem? A Task-Centric Perspective on\n  Recommendation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are We Solving a Well-Defined Problem? A Task-Centric Perspective on\n  Recommendation Tasks"
                },
                "summary": "Recommender systems (RecSys) leverage user interaction history to predict and\nsuggest relevant items, shaping user experiences across various domains. While\nmany studies adopt a general problem definition, i.e., to recommend preferred\nitems to users based on past interactions, such abstraction often lacks the\ndomain-specific nuances necessary for practical deployment. However, models are\nfrequently evaluated using datasets from online recommender platforms, which\ninherently reflect these specificities. In this paper, we analyze RecSys task\nformulations, emphasizing key components such as input-output structures,\ntemporal dynamics, and candidate item selection. All these factors directly\nimpact offline evaluation. We further examine the complexities of user-item\ninteractions, including decision-making costs, multi-step engagements, and\nunobservable interactions, which may influence model design and loss functions.\nAdditionally, we explore the balance between task specificity and model\ngeneralizability, highlighting how well-defined task formulations serve as the\nfoundation for robust evaluation and effective solution development. By\nclarifying task definitions and their implications, this work provides a\nstructured perspective on RecSys research. The goal is to help researchers\nbetter navigate the field, particularly in understanding specificities of the\nRecSys tasks and ensuring fair and meaningful evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems (RecSys) leverage user interaction history to predict and\nsuggest relevant items, shaping user experiences across various domains. While\nmany studies adopt a general problem definition, i.e., to recommend preferred\nitems to users based on past interactions, such abstraction often lacks the\ndomain-specific nuances necessary for practical deployment. However, models are\nfrequently evaluated using datasets from online recommender platforms, which\ninherently reflect these specificities. In this paper, we analyze RecSys task\nformulations, emphasizing key components such as input-output structures,\ntemporal dynamics, and candidate item selection. All these factors directly\nimpact offline evaluation. We further examine the complexities of user-item\ninteractions, including decision-making costs, multi-step engagements, and\nunobservable interactions, which may influence model design and loss functions.\nAdditionally, we explore the balance between task specificity and model\ngeneralizability, highlighting how well-defined task formulations serve as the\nfoundation for robust evaluation and effective solution development. By\nclarifying task definitions and their implications, this work provides a\nstructured perspective on RecSys research. The goal is to help researchers\nbetter navigate the field, particularly in understanding specificities of the\nRecSys tasks and ensuring fair and meaningful evaluations."
                },
                "authors": [
                    {
                        "name": "Aixin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Aixin Sun"
                },
                "author": "Aixin Sun",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21187v1",
                "updated": "2025-03-27T06:08:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    6,
                    8,
                    24,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T06:08:24Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    6,
                    8,
                    24,
                    3,
                    86,
                    0
                ],
                "title": "DGSUnet: An Improved Unet Model with DINO-Guided SAM2 for Multi-Scale\n  Feature Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DGSUnet: An Improved Unet Model with DINO-Guided SAM2 for Multi-Scale\n  Feature Collaboration"
                },
                "summary": "Despite the significant advancements in general image segmentation achieved\nby large-scale pre-trained foundation models (such as Meta's Segment Any-thing\nModel (SAM) series and DINOv2), their performance in specialized fields remains\nlimited by two critical issues: the excessive training costs due to large model\nparameters, and the insufficient ability to represent specific domain\ncharacteristics. This paper proposes a multi-scale feature collabora-tion\nframework guided by DINOv2 for SAM2, with core innovations in three aspects:\n(1) Establishing a feature collaboration mechanism between DINOv2 and SAM2\nbackbones, where high-dimensional semantic features extracted by the\nself-supervised model guide multi-scale feature fusion; (2) Designing\nlightweight adapter modules and cross-modal, cross-layer feature fusion units\nto inject cross-domain knowledge while freezing the base model parameters; (3)\nConstructing a U-shaped network structure based on U-net, which utilizes\nattention mechanisms to achieve adaptive aggregation decoding of\nmulti-granularity features. This framework surpasses existing state-of-the-art\nmeth-ods in downstream tasks such as camouflage target detection and salient\nob-ject detection, without requiring costly training processes. It provides a\ntech-nical pathway for efficient deployment of visual image segmentation,\ndemon-strating significant application value in a wide range of downstream\ntasks and specialized fields within image segmentation.Project page:\nhttps://github.com/CheneyXuYiMin/SAM2DINO-Seg",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the significant advancements in general image segmentation achieved\nby large-scale pre-trained foundation models (such as Meta's Segment Any-thing\nModel (SAM) series and DINOv2), their performance in specialized fields remains\nlimited by two critical issues: the excessive training costs due to large model\nparameters, and the insufficient ability to represent specific domain\ncharacteristics. This paper proposes a multi-scale feature collabora-tion\nframework guided by DINOv2 for SAM2, with core innovations in three aspects:\n(1) Establishing a feature collaboration mechanism between DINOv2 and SAM2\nbackbones, where high-dimensional semantic features extracted by the\nself-supervised model guide multi-scale feature fusion; (2) Designing\nlightweight adapter modules and cross-modal, cross-layer feature fusion units\nto inject cross-domain knowledge while freezing the base model parameters; (3)\nConstructing a U-shaped network structure based on U-net, which utilizes\nattention mechanisms to achieve adaptive aggregation decoding of\nmulti-granularity features. This framework surpasses existing state-of-the-art\nmeth-ods in downstream tasks such as camouflage target detection and salient\nob-ject detection, without requiring costly training processes. It provides a\ntech-nical pathway for efficient deployment of visual image segmentation,\ndemon-strating significant application value in a wide range of downstream\ntasks and specialized fields within image segmentation.Project page:\nhttps://github.com/CheneyXuYiMin/SAM2DINO-Seg"
                },
                "authors": [
                    {
                        "name": "Yimin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yimin Xu"
                },
                "author": "Yimin Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21186v1",
                "updated": "2025-03-27T06:07:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    6,
                    7,
                    51,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T06:07:51Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    6,
                    7,
                    51,
                    3,
                    86,
                    0
                ],
                "title": "DemoQuanDT: A Carrier-Grade QKD Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DemoQuanDT: A Carrier-Grade QKD Network"
                },
                "summary": "Quantum Key Distribution Networks (QKDN) enable secure communication even in\nthe age of powerful quantum computers. In the hands of a network operator,\nwhich can offer its service to many users, the economic viability of a QKDN\nincreases significantly. The highly challenging operator-user relationship in a\nlarge-scale network setting demands additional requirements to ensure\ncarrier-grade operation. Addressing this challenge, this work presents a\ncarrier-grade QKDN architecture, which combines the functional QKDN\narchitecture with the operational perspective of a network operator, ultimately\nenhancing the economic viability of QKDN. The focus is on the network and key\nmanagement aspects of a QKDN while assuming state-of-the-art commercial\nQKD-Modules. The presented architecture was rolled out within an in-field\ndemonstrator, connecting the cities of Berlin and Bonn over a link distance of\n923 km across Germany. We could show, that the proposed network architecture is\nfeasible, integrable, and scalable making it suitable for deployment in\nreal-world networks. Overall, the presented carrier-grade QKDN architecture\npromises to serve as a blueprint for network operators providing QKD-based\nservices to their customers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Key Distribution Networks (QKDN) enable secure communication even in\nthe age of powerful quantum computers. In the hands of a network operator,\nwhich can offer its service to many users, the economic viability of a QKDN\nincreases significantly. The highly challenging operator-user relationship in a\nlarge-scale network setting demands additional requirements to ensure\ncarrier-grade operation. Addressing this challenge, this work presents a\ncarrier-grade QKDN architecture, which combines the functional QKDN\narchitecture with the operational perspective of a network operator, ultimately\nenhancing the economic viability of QKDN. The focus is on the network and key\nmanagement aspects of a QKDN while assuming state-of-the-art commercial\nQKD-Modules. The presented architecture was rolled out within an in-field\ndemonstrator, connecting the cities of Berlin and Bonn over a link distance of\n923 km across Germany. We could show, that the proposed network architecture is\nfeasible, integrable, and scalable making it suitable for deployment in\nreal-world networks. Overall, the presented carrier-grade QKDN architecture\npromises to serve as a blueprint for network operators providing QKD-based\nservices to their customers."
                },
                "authors": [
                    {
                        "name": "P. Horoschenkoff"
                    },
                    {
                        "name": "J. Henrich"
                    },
                    {
                        "name": "R. BÃ¶hn"
                    },
                    {
                        "name": "I. Khan"
                    },
                    {
                        "name": "J. RÃ¶diger"
                    },
                    {
                        "name": "M. Gunkel"
                    },
                    {
                        "name": "M. Bauch"
                    },
                    {
                        "name": "J. Benda"
                    },
                    {
                        "name": "P. BlÃ¤cker"
                    },
                    {
                        "name": "E. Eichhammer"
                    },
                    {
                        "name": "U. Eismann"
                    },
                    {
                        "name": "G. Frenck"
                    },
                    {
                        "name": "H. Griesser"
                    },
                    {
                        "name": "W. Jontofsohn"
                    },
                    {
                        "name": "N. Kopshoff"
                    },
                    {
                        "name": "S. RÃ¶hrich"
                    },
                    {
                        "name": "F. Seidl"
                    },
                    {
                        "name": "N. Schark"
                    },
                    {
                        "name": "E. Sollner"
                    },
                    {
                        "name": "D. von Blanckenburg"
                    },
                    {
                        "name": "A. Heinemann"
                    },
                    {
                        "name": "M. Stiemerling"
                    },
                    {
                        "name": "M. GÃ¤rtner"
                    }
                ],
                "author_detail": {
                    "name": "M. GÃ¤rtner"
                },
                "author": "M. GÃ¤rtner",
                "arxiv_comment": "All rights, including for text and data mining (TDM), Artificial\n  Intelligence (AI) training, and similar technologies, are reserved. This\n  project has received funding from the German research ministry\n  \"Bundesministerium fuer Bildung, Wissenschaft, Forschung und Technologie\"\n  (BMBF) as part of the DemoQuanDT research and innovation programm under grand\n  agreement No. 16KISQ074",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19470v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19470v2",
                "updated": "2025-03-27T05:56:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    5,
                    56,
                    31,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-25T09:00:58Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    9,
                    0,
                    58,
                    1,
                    84,
                    0
                ],
                "title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement\n  Learning"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning,\nexemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating\nreasoning with external search processes remains challenging, especially for\ncomplex multi-hop questions requiring multiple retrieval steps. We propose\nReSearch, a novel framework that trains LLMs to Reason with Search via\nreinforcement learning without using any supervised data on reasoning steps.\nOur approach treats search operations as integral components of the reasoning\nchain, where when and how to perform searches is guided by text-based thinking,\nand search results subsequently influence further reasoning. We train ReSearch\non Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct\nextensive experiments. Despite being trained on only one dataset, our models\ndemonstrate strong generalizability across various benchmarks. Analysis reveals\nthat ReSearch naturally elicits advanced reasoning capabilities such as\nreflection and self-correction during the reinforcement learning process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning,\nexemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating\nreasoning with external search processes remains challenging, especially for\ncomplex multi-hop questions requiring multiple retrieval steps. We propose\nReSearch, a novel framework that trains LLMs to Reason with Search via\nreinforcement learning without using any supervised data on reasoning steps.\nOur approach treats search operations as integral components of the reasoning\nchain, where when and how to perform searches is guided by text-based thinking,\nand search results subsequently influence further reasoning. We train ReSearch\non Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct\nextensive experiments. Despite being trained on only one dataset, our models\ndemonstrate strong generalizability across various benchmarks. Analysis reveals\nthat ReSearch naturally elicits advanced reasoning capabilities such as\nreflection and self-correction during the reinforcement learning process."
                },
                "authors": [
                    {
                        "name": "Mingyang Chen"
                    },
                    {
                        "name": "Tianpeng Li"
                    },
                    {
                        "name": "Haoze Sun"
                    },
                    {
                        "name": "Yijie Zhou"
                    },
                    {
                        "name": "Chenzheng Zhu"
                    },
                    {
                        "name": "Haofen Wang"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    },
                    {
                        "name": "Wen Zhang"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Zenan Zhou"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19470v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19470v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21172v1",
                "updated": "2025-03-27T05:46:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    5,
                    46,
                    15,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T05:46:15Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    5,
                    46,
                    15,
                    3,
                    86,
                    0
                ],
                "title": "Model as a Game: On Numerical and Spatial Consistency for Generative\n  Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model as a Game: On Numerical and Spatial Consistency for Generative\n  Games"
                },
                "summary": "Recent advances in generative models have significantly impacted game\ngeneration. However, despite producing high-quality graphics and adequately\nreceiving player input, existing models often fail to maintain fundamental game\nproperties such as numerical and spatial consistency. Numerical consistency\nensures gameplay mechanics correctly reflect score changes and other\nquantitative elements, while spatial consistency prevents jarring scene\ntransitions, providing seamless player experiences. In this paper, we revisit\nthe paradigm of generative games to explore what truly constitutes a Model as a\nGame (MaaG) with a well-developed mechanism. We begin with an empirical study\non ``Traveler'', a 2D game created by an LLM featuring minimalist rules yet\nchallenging generative models in maintaining consistency. Based on the DiT\narchitecture, we design two specialized modules: (1) a numerical module that\nintegrates a LogicNet to determine event triggers, with calculations processed\nexternally as conditions for image generation; and (2) a spatial module that\nmaintains a map of explored areas, retrieving location-specific information\nduring generation and linking new observations to ensure continuity.\nExperiments across three games demonstrate that our integrated modules\nsignificantly enhance performance on consistency metrics compared to baselines,\nwhile incurring minimal time overhead during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative models have significantly impacted game\ngeneration. However, despite producing high-quality graphics and adequately\nreceiving player input, existing models often fail to maintain fundamental game\nproperties such as numerical and spatial consistency. Numerical consistency\nensures gameplay mechanics correctly reflect score changes and other\nquantitative elements, while spatial consistency prevents jarring scene\ntransitions, providing seamless player experiences. In this paper, we revisit\nthe paradigm of generative games to explore what truly constitutes a Model as a\nGame (MaaG) with a well-developed mechanism. We begin with an empirical study\non ``Traveler'', a 2D game created by an LLM featuring minimalist rules yet\nchallenging generative models in maintaining consistency. Based on the DiT\narchitecture, we design two specialized modules: (1) a numerical module that\nintegrates a LogicNet to determine event triggers, with calculations processed\nexternally as conditions for image generation; and (2) a spatial module that\nmaintains a map of explored areas, retrieving location-specific information\nduring generation and linking new observations to ensure continuity.\nExperiments across three games demonstrate that our integrated modules\nsignificantly enhance performance on consistency metrics compared to baselines,\nwhile incurring minimal time overhead during inference."
                },
                "authors": [
                    {
                        "name": "Jingye Chen"
                    },
                    {
                        "name": "Yuzhong Zhao"
                    },
                    {
                        "name": "Yupan Huang"
                    },
                    {
                        "name": "Lei Cui"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Tengchao Lv"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05759v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05759v3",
                "updated": "2025-03-27T05:46:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    5,
                    46,
                    13,
                    3,
                    86,
                    0
                ],
                "published": "2025-02-09T03:37:06Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    3,
                    37,
                    6,
                    6,
                    40,
                    0
                ],
                "title": "Reinforced Lifelong Editing for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforced Lifelong Editing for Language Models"
                },
                "summary": "Large language models (LLMs) acquire information from pre-training corpora,\nbut their stored knowledge can become inaccurate or outdated over time. Model\nediting addresses this challenge by modifying model parameters without\nretraining, and prevalent approaches leverage hypernetworks to generate these\nparameter updates. However, they face significant challenges in lifelong\nediting due to their incompatibility with LLM parameters that dynamically\nchange during the editing process. To address this, we observed that\nhypernetwork-based lifelong editing aligns with reinforcement learning modeling\nand proposed RLEdit, an RL-based editing method. By treating editing losses as\nrewards and optimizing hypernetwork parameters at the full knowledge sequence\nlevel, we enable it to precisely capture LLM changes and generate appropriate\nparameter updates. Our extensive empirical evaluation across several LLMs\ndemonstrates that RLEdit outperforms existing methods in lifelong editing with\nsuperior effectiveness and efficiency, achieving a 59.24% improvement while\nrequiring only 2.11% of the time compared to most approaches. Our code is\navailable at: https://github.com/zhrli324/RLEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) acquire information from pre-training corpora,\nbut their stored knowledge can become inaccurate or outdated over time. Model\nediting addresses this challenge by modifying model parameters without\nretraining, and prevalent approaches leverage hypernetworks to generate these\nparameter updates. However, they face significant challenges in lifelong\nediting due to their incompatibility with LLM parameters that dynamically\nchange during the editing process. To address this, we observed that\nhypernetwork-based lifelong editing aligns with reinforcement learning modeling\nand proposed RLEdit, an RL-based editing method. By treating editing losses as\nrewards and optimizing hypernetwork parameters at the full knowledge sequence\nlevel, we enable it to precisely capture LLM changes and generate appropriate\nparameter updates. Our extensive empirical evaluation across several LLMs\ndemonstrates that RLEdit outperforms existing methods in lifelong editing with\nsuperior effectiveness and efficiency, achieving a 59.24% improvement while\nrequiring only 2.11% of the time compared to most approaches. Our code is\navailable at: https://github.com/zhrli324/RLEdit."
                },
                "authors": [
                    {
                        "name": "Zherui Li"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Zhenhong Zhou"
                    },
                    {
                        "name": "Fei Sun"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Xiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Wang"
                },
                "author": "Xiang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05759v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05759v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12051v2",
                "updated": "2025-03-27T05:38:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    5,
                    38,
                    57,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-15T08:54:25Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    8,
                    54,
                    25,
                    5,
                    74,
                    0
                ],
                "title": "TLUE: A Tibetan Language Understanding Evaluation Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TLUE: A Tibetan Language Understanding Evaluation Benchmark"
                },
                "summary": "Large language models (LLMs) have made tremendous progress in recent years,\nbut low-resource languages, such as Tibetan, remain significantly\nunderrepresented in their evaluation. Despite Tibetan being spoken by over\nseven million people, it has largely been neglected in the development and\nassessment of LLMs. To address this gap, we present TLUE (A Tibetan Language\nUnderstanding Evaluation Benchmark), the first large-scale benchmark for\nassessing LLMs' capabilities in Tibetan. TLUE comprises two major components:\n(1) a comprehensive multi-task understanding benchmark spanning 5 domains and\n67 subdomains, and (2) a safety benchmark covering 7 subdomains. We evaluate a\ndiverse set of state-of-the-art LLMs. Experimental results demonstrate that\nmost LLMs perform below the random baseline, highlighting the considerable\nchallenges LLMs face in processing Tibetan, a low-resource language. TLUE\nprovides an essential foundation for driving future research and progress in\nTibetan language understanding and underscores the need for greater inclusivity\nin LLM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made tremendous progress in recent years,\nbut low-resource languages, such as Tibetan, remain significantly\nunderrepresented in their evaluation. Despite Tibetan being spoken by over\nseven million people, it has largely been neglected in the development and\nassessment of LLMs. To address this gap, we present TLUE (A Tibetan Language\nUnderstanding Evaluation Benchmark), the first large-scale benchmark for\nassessing LLMs' capabilities in Tibetan. TLUE comprises two major components:\n(1) a comprehensive multi-task understanding benchmark spanning 5 domains and\n67 subdomains, and (2) a safety benchmark covering 7 subdomains. We evaluate a\ndiverse set of state-of-the-art LLMs. Experimental results demonstrate that\nmost LLMs perform below the random baseline, highlighting the considerable\nchallenges LLMs face in processing Tibetan, a low-resource language. TLUE\nprovides an essential foundation for driving future research and progress in\nTibetan language understanding and underscores the need for greater inclusivity\nin LLM development."
                },
                "authors": [
                    {
                        "name": "Fan Gao"
                    },
                    {
                        "name": "Cheng Huang"
                    },
                    {
                        "name": "Nyima Tashi"
                    },
                    {
                        "name": "Xiangxiang Wang"
                    },
                    {
                        "name": "Thupten Tsering"
                    },
                    {
                        "name": "Ban Ma-bao"
                    },
                    {
                        "name": "Renzeg Duojie"
                    },
                    {
                        "name": "Gadeng Luosang"
                    },
                    {
                        "name": "Rinchen Dongrub"
                    },
                    {
                        "name": "Dorje Tashi"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yongbin Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Yu"
                },
                "author": "Yongbin Yu",
                "arxiv_comment": "6 figures, 21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21164v1",
                "updated": "2025-03-27T05:19:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    5,
                    19,
                    41,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T05:19:41Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    5,
                    19,
                    41,
                    3,
                    86,
                    0
                ],
                "title": "Adversarial Wear and Tear: Exploiting Natural Damage for Generating\n  Physical-World Adversarial Examples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Wear and Tear: Exploiting Natural Damage for Generating\n  Physical-World Adversarial Examples"
                },
                "summary": "The presence of adversarial examples in the physical world poses significant\nchallenges to the deployment of Deep Neural Networks in safety-critical\napplications such as autonomous driving. Most existing methods for crafting\nphysical-world adversarial examples are ad-hoc, relying on temporary\nmodifications like shadows, laser beams, or stickers that are tailored to\nspecific scenarios. In this paper, we introduce a new class of physical-world\nadversarial examples, AdvWT, which draws inspiration from the naturally\noccurring phenomenon of `wear and tear', an inherent property of physical\nobjects. Unlike manually crafted perturbations, `wear and tear' emerges\norganically over time due to environmental degradation, as seen in the gradual\ndeterioration of outdoor signboards. To achieve this, AdvWT follows a two-step\napproach. First, a GAN-based, unsupervised image-to-image translation network\nis employed to model these naturally occurring damages, particularly in the\ncontext of outdoor signboards. The translation network encodes the\ncharacteristics of damaged signs into a latent `damage style code'. In the\nsecond step, we introduce adversarial perturbations into the style code,\nstrategically optimizing its transformation process. This manipulation subtly\nalters the damage style representation, guiding the network to generate\nadversarial images where the appearance of damages remains perceptually\nrealistic, while simultaneously ensuring their effectiveness in misleading\nneural networks. Through comprehensive experiments on two traffic sign\ndatasets, we show that AdvWT effectively misleads DNNs in both digital and\nphysical domains. AdvWT achieves an effective attack success rate, greater\nrobustness, and a more natural appearance compared to existing physical-world\nadversarial examples. Additionally, integrating AdvWT into training enhances a\nmodel's generalizability to real-world damaged signs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The presence of adversarial examples in the physical world poses significant\nchallenges to the deployment of Deep Neural Networks in safety-critical\napplications such as autonomous driving. Most existing methods for crafting\nphysical-world adversarial examples are ad-hoc, relying on temporary\nmodifications like shadows, laser beams, or stickers that are tailored to\nspecific scenarios. In this paper, we introduce a new class of physical-world\nadversarial examples, AdvWT, which draws inspiration from the naturally\noccurring phenomenon of `wear and tear', an inherent property of physical\nobjects. Unlike manually crafted perturbations, `wear and tear' emerges\norganically over time due to environmental degradation, as seen in the gradual\ndeterioration of outdoor signboards. To achieve this, AdvWT follows a two-step\napproach. First, a GAN-based, unsupervised image-to-image translation network\nis employed to model these naturally occurring damages, particularly in the\ncontext of outdoor signboards. The translation network encodes the\ncharacteristics of damaged signs into a latent `damage style code'. In the\nsecond step, we introduce adversarial perturbations into the style code,\nstrategically optimizing its transformation process. This manipulation subtly\nalters the damage style representation, guiding the network to generate\nadversarial images where the appearance of damages remains perceptually\nrealistic, while simultaneously ensuring their effectiveness in misleading\nneural networks. Through comprehensive experiments on two traffic sign\ndatasets, we show that AdvWT effectively misleads DNNs in both digital and\nphysical domains. AdvWT achieves an effective attack success rate, greater\nrobustness, and a more natural appearance compared to existing physical-world\nadversarial examples. Additionally, integrating AdvWT into training enhances a\nmodel's generalizability to real-world damaged signs."
                },
                "authors": [
                    {
                        "name": "Samra Irshad"
                    },
                    {
                        "name": "Seungkyu Lee"
                    },
                    {
                        "name": "Nassir Navab"
                    },
                    {
                        "name": "Hong Joo Lee"
                    },
                    {
                        "name": "Seong Tae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seong Tae Kim"
                },
                "author": "Seong Tae Kim",
                "arxiv_comment": "11 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09766v3",
                "updated": "2025-03-27T05:05:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    5,
                    5,
                    3,
                    3,
                    86,
                    0
                ],
                "published": "2025-01-15T04:52:34Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    4,
                    52,
                    34,
                    2,
                    15,
                    0
                ],
                "title": "iTool: Boosting Tool Use of Large Language Models via Iterative\n  Reinforced Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iTool: Boosting Tool Use of Large Language Models via Iterative\n  Reinforced Fine-Tuning"
                },
                "summary": "Augmenting large language models (LLMs) with external tools is known as a\npromising approach to enhancing their capabilities, especially for complex\ntasks. Synthesizing tool-use data through real-world simulations is an\neffective way to achieve it. Nevertheless, our investigation reveals that (1)\ntraining gains significantly decay as synthetic data increases. The model\nstruggles to benefit from more synthetic data due to potential data diversity\nissues, resulting in poor performance in complex scenarios. Moreover, we find\nthat (2) this challenge primarily manifests as minor discrepancies between the\nmodel's output and the ground truth response (termed as deficiency), such as\nerrors in parameter values that require complex reasoning from the context to\nresolve. To this end, we propose an iterative reinforced fine-tuning strategy\ndesigned to alleviate these challenges. This strategy involves: (1) enhancing\nthe diversity of synthetic data through path exploration of Monte Carlo Tree\nSearch. (2) iteratively identifying deficiency-related data, constructing\nfine-grained preference pairs to pinpoint deficiencies, and then applying\npreference optimization to optimize these deficiencies. Our experiments show\nthat models trained using our method achieve about 12\\% better performance than\nbaseline models, outperforming larger open-source and closed-source models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting large language models (LLMs) with external tools is known as a\npromising approach to enhancing their capabilities, especially for complex\ntasks. Synthesizing tool-use data through real-world simulations is an\neffective way to achieve it. Nevertheless, our investigation reveals that (1)\ntraining gains significantly decay as synthetic data increases. The model\nstruggles to benefit from more synthetic data due to potential data diversity\nissues, resulting in poor performance in complex scenarios. Moreover, we find\nthat (2) this challenge primarily manifests as minor discrepancies between the\nmodel's output and the ground truth response (termed as deficiency), such as\nerrors in parameter values that require complex reasoning from the context to\nresolve. To this end, we propose an iterative reinforced fine-tuning strategy\ndesigned to alleviate these challenges. This strategy involves: (1) enhancing\nthe diversity of synthetic data through path exploration of Monte Carlo Tree\nSearch. (2) iteratively identifying deficiency-related data, constructing\nfine-grained preference pairs to pinpoint deficiencies, and then applying\npreference optimization to optimize these deficiencies. Our experiments show\nthat models trained using our method achieve about 12\\% better performance than\nbaseline models, outperforming larger open-source and closed-source models."
                },
                "authors": [
                    {
                        "name": "Yirong Zeng"
                    },
                    {
                        "name": "Xiao Ding"
                    },
                    {
                        "name": "Yuxian Wang"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Wu Ning"
                    },
                    {
                        "name": "Yutai Hou"
                    },
                    {
                        "name": "Xu Huang"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_comment": "under review ACL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21157v1",
                "updated": "2025-03-27T04:50:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    4,
                    50,
                    14,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T04:50:14Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    4,
                    50,
                    14,
                    3,
                    86,
                    0
                ],
                "title": "Real-Time Evaluation Models for RAG: Who Detects Hallucinations Best?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Evaluation Models for RAG: Who Detects Hallucinations Best?"
                },
                "summary": "This article surveys Evaluation models to automatically detect hallucinations\nin Retrieval-Augmented Generation (RAG), and presents a comprehensive benchmark\nof their performance across six RAG applications. Methods included in our study\ninclude: LLM-as-a-Judge, Prometheus, Lynx, the Hughes Hallucination Evaluation\nModel (HHEM), and the Trustworthy Language Model (TLM). These approaches are\nall reference-free, requiring no ground-truth answers/labels to catch incorrect\nLLM responses. Our study reveals that, across diverse RAG applications, some of\nthese approaches consistently detect incorrect RAG responses with high\nprecision/recall.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article surveys Evaluation models to automatically detect hallucinations\nin Retrieval-Augmented Generation (RAG), and presents a comprehensive benchmark\nof their performance across six RAG applications. Methods included in our study\ninclude: LLM-as-a-Judge, Prometheus, Lynx, the Hughes Hallucination Evaluation\nModel (HHEM), and the Trustworthy Language Model (TLM). These approaches are\nall reference-free, requiring no ground-truth answers/labels to catch incorrect\nLLM responses. Our study reveals that, across diverse RAG applications, some of\nthese approaches consistently detect incorrect RAG responses with high\nprecision/recall."
                },
                "authors": [
                    {
                        "name": "Ashish Sardana"
                    }
                ],
                "author_detail": {
                    "name": "Ashish Sardana"
                },
                "author": "Ashish Sardana",
                "arxiv_comment": "11 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21155v1",
                "updated": "2025-03-27T04:48:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    4,
                    48,
                    58,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T04:48:58Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    4,
                    48,
                    58,
                    3,
                    86,
                    0
                ],
                "title": "Embedding Domain-Specific Knowledge from LLMs into the Feature\n  Engineering Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding Domain-Specific Knowledge from LLMs into the Feature\n  Engineering Pipeline"
                },
                "summary": "Feature engineering is mandatory in the machine learning pipeline to obtain\nrobust models. While evolutionary computation is well-known for its great\nresults both in feature selection and feature construction, its methods are\ncomputationally expensive due to the large number of evaluations required to\ninduce the final model. Part of the reason why these algorithms require a large\nnumber of evaluations is their lack of domain-specific knowledge, resulting in\na lot of random guessing during evolution. In this work, we propose using Large\nLanguage Models (LLMs) as an initial feature construction step to add knowledge\nto the dataset. By doing so, our results show that the evolution can converge\nfaster, saving us computational resources. The proposed approach only provides\nthe names of the features in the dataset and the target objective to the LLM,\nmaking it usable even when working with datasets containing private data. While\nconsistent improvements to test performance were only observed for one-third of\nthe datasets (CSS, PM, and IM10), possibly due to problems being easily\nexplored by LLMs, this approach only decreased the model performance in 1/77\ntest cases. Additionally, this work introduces the M6GP feature engineering\nalgorithm to symbolic regression, showing it can improve the results of the\nrandom forest regressor and produce competitive results with its predecessor,\nM3GP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature engineering is mandatory in the machine learning pipeline to obtain\nrobust models. While evolutionary computation is well-known for its great\nresults both in feature selection and feature construction, its methods are\ncomputationally expensive due to the large number of evaluations required to\ninduce the final model. Part of the reason why these algorithms require a large\nnumber of evaluations is their lack of domain-specific knowledge, resulting in\na lot of random guessing during evolution. In this work, we propose using Large\nLanguage Models (LLMs) as an initial feature construction step to add knowledge\nto the dataset. By doing so, our results show that the evolution can converge\nfaster, saving us computational resources. The proposed approach only provides\nthe names of the features in the dataset and the target objective to the LLM,\nmaking it usable even when working with datasets containing private data. While\nconsistent improvements to test performance were only observed for one-third of\nthe datasets (CSS, PM, and IM10), possibly due to problems being easily\nexplored by LLMs, this approach only decreased the model performance in 1/77\ntest cases. Additionally, this work introduces the M6GP feature engineering\nalgorithm to symbolic regression, showing it can improve the results of the\nrandom forest regressor and produce competitive results with its predecessor,\nM3GP."
                },
                "authors": [
                    {
                        "name": "JoÃ£o Eduardo Batista"
                    }
                ],
                "author_detail": {
                    "name": "JoÃ£o Eduardo Batista"
                },
                "author": "JoÃ£o Eduardo Batista",
                "arxiv_comment": "9 pages, 4 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19804v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19804v2",
                "updated": "2025-03-27T04:36:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    4,
                    36,
                    46,
                    3,
                    86,
                    0
                ],
                "published": "2024-09-29T22:04:26Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    22,
                    4,
                    26,
                    6,
                    273,
                    0
                ],
                "title": "Does RAG Introduce Unfairness in LLMs? Evaluating Fairness in\n  Retrieval-Augmented Generation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does RAG Introduce Unfairness in LLMs? Evaluating Fairness in\n  Retrieval-Augmented Generation Systems"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has recently gained significant\nattention for its enhanced ability to integrate external knowledge sources into\nopen-domain question answering (QA) tasks. However, it remains unclear how\nthese models address fairness concerns, particularly with respect to sensitive\nattributes such as gender, geographic location, and other demographic factors.\nFirst, as language models evolve to prioritize utility, like improving exact\nmatch accuracy, fairness considerations may have been largely overlooked.\nSecond, the complex, multi-component architecture of RAG methods poses\nchallenges in identifying and mitigating biases, as each component is optimized\nfor distinct objectives. In this paper, we aim to empirically evaluate fairness\nin several RAG methods. We propose a fairness evaluation framework tailored to\nRAG, using scenario-based questions and analyzing disparities across\ndemographic attributes. Our experimental results indicate that, despite recent\nadvances in utility-driven optimization, fairness issues persist in both the\nretrieval and generation stages. These findings underscore the need for\ntargeted interventions to address fairness concerns throughout the RAG\npipeline. The dataset and code used in this study are publicly available at\nthis GitHub Repository https://github.com/elviswxy/RAG_fairness .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has recently gained significant\nattention for its enhanced ability to integrate external knowledge sources into\nopen-domain question answering (QA) tasks. However, it remains unclear how\nthese models address fairness concerns, particularly with respect to sensitive\nattributes such as gender, geographic location, and other demographic factors.\nFirst, as language models evolve to prioritize utility, like improving exact\nmatch accuracy, fairness considerations may have been largely overlooked.\nSecond, the complex, multi-component architecture of RAG methods poses\nchallenges in identifying and mitigating biases, as each component is optimized\nfor distinct objectives. In this paper, we aim to empirically evaluate fairness\nin several RAG methods. We propose a fairness evaluation framework tailored to\nRAG, using scenario-based questions and analyzing disparities across\ndemographic attributes. Our experimental results indicate that, despite recent\nadvances in utility-driven optimization, fairness issues persist in both the\nretrieval and generation stages. These findings underscore the need for\ntargeted interventions to address fairness concerns throughout the RAG\npipeline. The dataset and code used in this study are publicly available at\nthis GitHub Repository https://github.com/elviswxy/RAG_fairness ."
                },
                "authors": [
                    {
                        "name": "Xuyang Wu"
                    },
                    {
                        "name": "Shuowei Li"
                    },
                    {
                        "name": "Hsin-Tai Wu"
                    },
                    {
                        "name": "Zhiqiang Tao"
                    },
                    {
                        "name": "Yi Fang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Fang"
                },
                "author": "Yi Fang",
                "arxiv_comment": "Published at COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19804v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19804v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13012v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13012v3",
                "updated": "2025-03-27T04:07:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    4,
                    7,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2025-02-18T16:33:33Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    33,
                    33,
                    1,
                    49,
                    0
                ],
                "title": "Towards a Design Guideline for RPA Evaluation: A Survey of Large\n  Language Model-Based Role-Playing Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Design Guideline for RPA Evaluation: A Survey of Large\n  Language Model-Based Role-Playing Agents"
                },
                "summary": "Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that\nsimulates human-like behaviors in a variety of tasks. However, evaluating RPAs\nis challenging due to diverse task requirements and agent designs. This paper\nproposes an evidence-based, actionable, and generalizable evaluation design\nguideline for LLM-based RPA by systematically reviewing 1,676 papers published\nbetween Jan. 2021 and Dec. 2024. Our analysis identifies six agent attributes,\nseven task attributes, and seven evaluation metrics from existing literature.\nBased on these findings, we present an RPA evaluation design guideline to help\nresearchers develop more systematic and consistent evaluation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that\nsimulates human-like behaviors in a variety of tasks. However, evaluating RPAs\nis challenging due to diverse task requirements and agent designs. This paper\nproposes an evidence-based, actionable, and generalizable evaluation design\nguideline for LLM-based RPA by systematically reviewing 1,676 papers published\nbetween Jan. 2021 and Dec. 2024. Our analysis identifies six agent attributes,\nseven task attributes, and seven evaluation metrics from existing literature.\nBased on these findings, we present an RPA evaluation design guideline to help\nresearchers develop more systematic and consistent evaluation methods."
                },
                "authors": [
                    {
                        "name": "Chaoran Chen"
                    },
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Ruishi Zou"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Weimin Lyu"
                    },
                    {
                        "name": "Yanfang Ye"
                    },
                    {
                        "name": "Toby Jia-Jun Li"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13012v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13012v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00590v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00590v2",
                "updated": "2025-03-27T03:55:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    3,
                    55,
                    13,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-01T19:02:28Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    19,
                    2,
                    28,
                    5,
                    60,
                    0
                ],
                "title": "Characterizing LLM-Empowered Personalized Story-Reading and Interaction\n  for Children: Insights from Multi-Stakeholder Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing LLM-Empowered Personalized Story-Reading and Interaction\n  for Children: Insights from Multi-Stakeholder Perspectives"
                },
                "summary": "Personalized interaction is highly valued by parents in their story-reading\nactivities with children. While AI-empowered story-reading tools have been\nincreasingly used, their abilities to support personalized interaction with\nchildren are still limited. Recent advances in large language models (LLMs)\nshow promise in facilitating personalized interactions, but little is known\nabout how to effectively and appropriately use LLMs to enhance children's\npersonalized story-reading experiences. This work explores this question\nthrough a design-based study. Drawing on a formative study, we designed and\ndeveloped StoryMate, an LLM-empowered personalized interactive story-reading\ntool for children, following an empirical study with children, parents, and\neducation experts. Our participants valued the personalized features in\nStoryMate, and also highlighted the need to support personalized content,\nguiding mechanisms, reading context variations, and interactive interfaces.\nBased on these findings, we propose a series of design recommendations for\nbetter using LLMs to empower children's personalized story reading and\ninteraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized interaction is highly valued by parents in their story-reading\nactivities with children. While AI-empowered story-reading tools have been\nincreasingly used, their abilities to support personalized interaction with\nchildren are still limited. Recent advances in large language models (LLMs)\nshow promise in facilitating personalized interactions, but little is known\nabout how to effectively and appropriately use LLMs to enhance children's\npersonalized story-reading experiences. This work explores this question\nthrough a design-based study. Drawing on a formative study, we designed and\ndeveloped StoryMate, an LLM-empowered personalized interactive story-reading\ntool for children, following an empirical study with children, parents, and\neducation experts. Our participants valued the personalized features in\nStoryMate, and also highlighted the need to support personalized content,\nguiding mechanisms, reading context variations, and interactive interfaces.\nBased on these findings, we propose a series of design recommendations for\nbetter using LLMs to empower children's personalized story reading and\ninteraction."
                },
                "authors": [
                    {
                        "name": "Jiaju Chen"
                    },
                    {
                        "name": "Minglong Tang"
                    },
                    {
                        "name": "Yuxuan Lu"
                    },
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Elissa Fan"
                    },
                    {
                        "name": "Xiaojuan Ma"
                    },
                    {
                        "name": "Ying Xu"
                    },
                    {
                        "name": "Dakuo Wang"
                    },
                    {
                        "name": "Yuling Sun"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "arxiv_doi": "10.1145/3706598.3713275",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713275",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.00590v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00590v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at CHI 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21135v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21135v1",
                "updated": "2025-03-27T03:52:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    3,
                    52,
                    25,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T03:52:25Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    3,
                    52,
                    25,
                    3,
                    86,
                    0
                ],
                "title": "MoQa: Rethinking MoE Quantization with Multi-stage Data-model\n  Distribution Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoQa: Rethinking MoE Quantization with Multi-stage Data-model\n  Distribution Awareness"
                },
                "summary": "With the advances in artificial intelligence, Mix-of-Experts (MoE) has become\nthe main form of Large Language Models (LLMs), and its demand for model\ncompression is increasing. Quantization is an effective method that not only\ncompresses the models but also significantly accelerates their performance.\nExisting quantization methods have gradually shifted the focus from parameter\nscaling to the analysis of data distributions. However, their analysis is\ndesigned for dense LLMs and relies on the simple one-model-all-data mapping,\nwhich is unsuitable for MoEs. This paper proposes a new quantization framework\ncalled MoQa. MoQa decouples the data-model distribution complexity of MoEs in\nmultiple analysis stages, quantitively revealing the dynamics during sparse\ndata activation, data-parameter mapping, and inter-expert correlations. Based\non these, MoQa identifies particular experts' and parameters' significance with\noptimal data-model distribution awareness and proposes a series of fine-grained\nmix-quantization strategies adaptive to various data activation and expert\ncombination scenarios. Moreover, MoQa discusses the limitations of existing\nquantization and analyzes the impact of each stage analysis, showing novel\ninsights for MoE quantization. Experiments show that MoQa achieves a 1.69~2.18\nperplexity decrease in language modeling tasks and a 1.58%~8.91% accuracy\nimprovement in zero-shot inference tasks. We believe MoQa will play a role in\nfuture MoE construction, optimization, and compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advances in artificial intelligence, Mix-of-Experts (MoE) has become\nthe main form of Large Language Models (LLMs), and its demand for model\ncompression is increasing. Quantization is an effective method that not only\ncompresses the models but also significantly accelerates their performance.\nExisting quantization methods have gradually shifted the focus from parameter\nscaling to the analysis of data distributions. However, their analysis is\ndesigned for dense LLMs and relies on the simple one-model-all-data mapping,\nwhich is unsuitable for MoEs. This paper proposes a new quantization framework\ncalled MoQa. MoQa decouples the data-model distribution complexity of MoEs in\nmultiple analysis stages, quantitively revealing the dynamics during sparse\ndata activation, data-parameter mapping, and inter-expert correlations. Based\non these, MoQa identifies particular experts' and parameters' significance with\noptimal data-model distribution awareness and proposes a series of fine-grained\nmix-quantization strategies adaptive to various data activation and expert\ncombination scenarios. Moreover, MoQa discusses the limitations of existing\nquantization and analyzes the impact of each stage analysis, showing novel\ninsights for MoE quantization. Experiments show that MoQa achieves a 1.69~2.18\nperplexity decrease in language modeling tasks and a 1.58%~8.91% accuracy\nimprovement in zero-shot inference tasks. We believe MoQa will play a role in\nfuture MoE construction, optimization, and compression."
                },
                "authors": [
                    {
                        "name": "Zihao Zheng"
                    },
                    {
                        "name": "Xiuping Cui"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Maoliang Li"
                    },
                    {
                        "name": "Jiayu Chen"
                    },
                    {
                        "name": "Yun"
                    },
                    {
                        "name": "Liang"
                    },
                    {
                        "name": "Xiang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Chen"
                },
                "arxiv_affiliation": "Eric",
                "author": "Xiang Chen",
                "arxiv_comment": "6 pages, 6 figures and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21135v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]