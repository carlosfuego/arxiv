[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.08523v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v3",
                "updated": "2025-09-16T10:33:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    33,
                    29,
                    1,
                    259,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_doi": "10.1145/3744916.3764523",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3744916.3764523",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08523v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ICSE '26 (The 48th IEEE/ACM International Conference on\n  Software Engineering)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12900v1",
                "updated": "2025-09-16T09:54:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    54,
                    58,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:54:58Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    54,
                    58,
                    1,
                    259,
                    0
                ],
                "title": "Topology and Fragility of European High-Voltage Networks: A\n  Cross-Country Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topology and Fragility of European High-Voltage Networks: A\n  Cross-Country Comparative Analysis"
                },
                "summary": "Reliable electricity supply depends on the seamless operation of high-voltage\ngrid infrastructure spanning both transmission and sub-transmission levels.\nBeneath this apparent uniformity lies a striking structural diversity, which\nleaves a clear imprint on system vulnerability. In this paper, we present\nharmonized topological models of the high-voltage grids of 15 European\ncountries, integrating all elements at voltage levels above 110 kV. Topological\nanalysis of these networks reveals a simple yet robust pattern: node degree\ndistributions consistently follow an exponential decay, but the rate of decay\nvaries significantly across countries. Through a detailed and systematic\nevaluation of network tolerance to node and edge removals, we show that the\ndecay rate delineates the boundary between systems that are more resilient to\nfailures and those that are prone to large-scale disruptions. Furthermore, we\ndemonstrate that this numerical boundary is highly sensitive to which layers of\nthe infrastructure are included in the models. To our knowledge, this study\nprovides the first quantitative cross-country comparison of 15 European\nhigh-voltage networks, linking topological properties with vulnerability\ncharacteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable electricity supply depends on the seamless operation of high-voltage\ngrid infrastructure spanning both transmission and sub-transmission levels.\nBeneath this apparent uniformity lies a striking structural diversity, which\nleaves a clear imprint on system vulnerability. In this paper, we present\nharmonized topological models of the high-voltage grids of 15 European\ncountries, integrating all elements at voltage levels above 110 kV. Topological\nanalysis of these networks reveals a simple yet robust pattern: node degree\ndistributions consistently follow an exponential decay, but the rate of decay\nvaries significantly across countries. Through a detailed and systematic\nevaluation of network tolerance to node and edge removals, we show that the\ndecay rate delineates the boundary between systems that are more resilient to\nfailures and those that are prone to large-scale disruptions. Furthermore, we\ndemonstrate that this numerical boundary is highly sensitive to which layers of\nthe infrastructure are included in the models. To our knowledge, this study\nprovides the first quantitative cross-country comparison of 15 European\nhigh-voltage networks, linking topological properties with vulnerability\ncharacteristics."
                },
                "authors": [
                    {
                        "name": "BÃ¡lint Hartmann"
                    },
                    {
                        "name": "Michelle T. Cirunay"
                    }
                ],
                "author_detail": {
                    "name": "Michelle T. Cirunay"
                },
                "author": "Michelle T. Cirunay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12867v1",
                "updated": "2025-09-16T09:22:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    22,
                    21,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:22:21Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    22,
                    21,
                    1,
                    259,
                    0
                ],
                "title": "Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use"
                },
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding and reasoning, yet they remain limited when tackling\nreal-world tasks that require up-to-date knowledge, precise operations, or\nspecialized tool use. To address this, we propose Tool-R1, a reinforcement\nlearning framework that enables LLMs to perform general, compositional, and\nmulti-step tool use by generating executable Python code. Tool-R1 supports\nintegration of user-defined tools and standard libraries, with variable sharing\nacross steps to construct coherent workflows. An outcome-based reward function,\ncombining LLM-based answer judgment and code execution success, guides policy\noptimization. To improve training efficiency, we maintain a dynamic sample\nqueue to cache and reuse high-quality trajectories, reducing the overhead of\ncostly online sampling. Experiments on the GAIA benchmark show that Tool-R1\nsubstantially improves both accuracy and robustness, achieving about 10\\% gain\nover strong baselines, with larger improvements on complex multi-step tasks.\nThese results highlight the potential of Tool-R1 for enabling reliable and\nefficient tool-augmented reasoning in real-world applications. Our code will be\navailable at https://github.com/YBYBZhang/Tool-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding and reasoning, yet they remain limited when tackling\nreal-world tasks that require up-to-date knowledge, precise operations, or\nspecialized tool use. To address this, we propose Tool-R1, a reinforcement\nlearning framework that enables LLMs to perform general, compositional, and\nmulti-step tool use by generating executable Python code. Tool-R1 supports\nintegration of user-defined tools and standard libraries, with variable sharing\nacross steps to construct coherent workflows. An outcome-based reward function,\ncombining LLM-based answer judgment and code execution success, guides policy\noptimization. To improve training efficiency, we maintain a dynamic sample\nqueue to cache and reuse high-quality trajectories, reducing the overhead of\ncostly online sampling. Experiments on the GAIA benchmark show that Tool-R1\nsubstantially improves both accuracy and robustness, achieving about 10\\% gain\nover strong baselines, with larger improvements on complex multi-step tasks.\nThese results highlight the potential of Tool-R1 for enabling reliable and\nefficient tool-augmented reasoning in real-world applications. Our code will be\navailable at https://github.com/YBYBZhang/Tool-R1."
                },
                "authors": [
                    {
                        "name": "Yabo Zhang"
                    },
                    {
                        "name": "Yihan Zeng"
                    },
                    {
                        "name": "Qingyun Li"
                    },
                    {
                        "name": "Zhen Hu"
                    },
                    {
                        "name": "Kavin Han"
                    },
                    {
                        "name": "Wangmeng Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Wangmeng Zuo"
                },
                "author": "Wangmeng Zuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12817v1",
                "updated": "2025-09-16T08:36:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    8,
                    36,
                    5,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T08:36:05Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    8,
                    36,
                    5,
                    1,
                    259,
                    0
                ],
                "title": "SAGA: Selective Adaptive Gating for Efficient and Expressive Linear\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAGA: Selective Adaptive Gating for Efficient and Expressive Linear\n  Attention"
                },
                "summary": "While Transformer architecture excel at modeling long-range dependencies\ncontributing to its widespread adoption in vision tasks the quadratic\ncomplexity of softmax-based attention mechanisms imposes a major bottleneck,\nparticularly when processing high-resolution images. Linear attention presents\na promising alternative by reformulating the attention computation from $(QK)V$\nto $Q(KV)$, thereby reducing the complexity from $\\mathcal{O}(N^2)$ to\n$\\mathcal{O}(N)$ while preserving the global receptive field. However, most\nexisting methods compress historical key-value (KV) information uniformly,\nwhich can lead to feature redundancy and the loss of directional alignment with\nthe query (Q). This uniform compression results in low-rank $KV$ feature maps,\ncontributing to a performance gap compared to softmax attention. To mitigate\nthis limitation, we propose \\textbf{S}elective \\textbf{A}daptive\n\\textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which\nintroduces input-adaptive learnable gates to selectively modulate information\naggregation into the $KV$ feature map. These gates enhance semantic diversity\nand alleviate the low-rank constraint inherent in conventional linear\nattention. Additionally, we propose an efficient Hadamard-product decomposition\nmethod for gate computation, which introduces no additional memory overhead.\nExperiments demonstrate that SAGA achieves a 1.76$\\times$ improvement in\nthroughput and a 2.69$\\times$ reduction in peak GPU memory compared to PVT-T at\na resolution of $1280 \\times 1280$. Moreover, it improves top-1 accuracy by up\nto 4.4\\% on the ImageNet dataset, demonstrating both computational efficiency\nand model effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer architecture excel at modeling long-range dependencies\ncontributing to its widespread adoption in vision tasks the quadratic\ncomplexity of softmax-based attention mechanisms imposes a major bottleneck,\nparticularly when processing high-resolution images. Linear attention presents\na promising alternative by reformulating the attention computation from $(QK)V$\nto $Q(KV)$, thereby reducing the complexity from $\\mathcal{O}(N^2)$ to\n$\\mathcal{O}(N)$ while preserving the global receptive field. However, most\nexisting methods compress historical key-value (KV) information uniformly,\nwhich can lead to feature redundancy and the loss of directional alignment with\nthe query (Q). This uniform compression results in low-rank $KV$ feature maps,\ncontributing to a performance gap compared to softmax attention. To mitigate\nthis limitation, we propose \\textbf{S}elective \\textbf{A}daptive\n\\textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which\nintroduces input-adaptive learnable gates to selectively modulate information\naggregation into the $KV$ feature map. These gates enhance semantic diversity\nand alleviate the low-rank constraint inherent in conventional linear\nattention. Additionally, we propose an efficient Hadamard-product decomposition\nmethod for gate computation, which introduces no additional memory overhead.\nExperiments demonstrate that SAGA achieves a 1.76$\\times$ improvement in\nthroughput and a 2.69$\\times$ reduction in peak GPU memory compared to PVT-T at\na resolution of $1280 \\times 1280$. Moreover, it improves top-1 accuracy by up\nto 4.4\\% on the ImageNet dataset, demonstrating both computational efficiency\nand model effectiveness."
                },
                "authors": [
                    {
                        "name": "Yuan Cao"
                    },
                    {
                        "name": "Dong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dong Wang"
                },
                "author": "Dong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11156v2",
                "updated": "2025-09-16T07:49:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    7,
                    49,
                    41,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-14T08:22:37Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    22,
                    37,
                    6,
                    257,
                    0
                ],
                "title": "Adaptive K-PackCache: Cost-Centric Data Caching in Cloud",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive K-PackCache: Cost-Centric Data Caching in Cloud"
                },
                "summary": "Recent advances in data analytics have enabled the accurate prediction of\nuser access patterns, giving rise to the idea of packed caching delivering\nmultiple co accessed data items together as a bundle. This improves caching\nefficiency, as accessing one item often implies the need for others. Prior work\nhas explored only 2 item pairwise packing. In this paper, we extend the concept\nto general K packing, allowing variable size bundles for improved flexibility\nand performance. We formulate the K PackCache problem from a content delivery\nnetwork CDN operator perspective, aiming to minimize total cost comprising two\ncomponents: transfer cost modeled as a base cost plus a linearly increasing\nterm with the number of items packed, and memory rental cost for caching, which\ndepends on how long and how much is stored. Overpacking increases cost due to\nlow utility, underpacking leads to missed sharing opportunities. We propose an\nonline algorithm, Adaptive K PackCache AKPC, which dynamically forms, merges,\nand splits data cliques based on user access patterns and content correlation.\nOur approach supports batch requests, enables approximate clique merging, and\noffers a formal competitive guarantee. Through extensive evaluation on the\nNetflix and Spotify datasets, AKPC reduces total cost by up to 63 and 55\npercentage over online baselines, respectively, and achieves performance within\n15 and 13 percentage of the optimal. This demonstrates its scalability and\neffectiveness for real world caching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in data analytics have enabled the accurate prediction of\nuser access patterns, giving rise to the idea of packed caching delivering\nmultiple co accessed data items together as a bundle. This improves caching\nefficiency, as accessing one item often implies the need for others. Prior work\nhas explored only 2 item pairwise packing. In this paper, we extend the concept\nto general K packing, allowing variable size bundles for improved flexibility\nand performance. We formulate the K PackCache problem from a content delivery\nnetwork CDN operator perspective, aiming to minimize total cost comprising two\ncomponents: transfer cost modeled as a base cost plus a linearly increasing\nterm with the number of items packed, and memory rental cost for caching, which\ndepends on how long and how much is stored. Overpacking increases cost due to\nlow utility, underpacking leads to missed sharing opportunities. We propose an\nonline algorithm, Adaptive K PackCache AKPC, which dynamically forms, merges,\nand splits data cliques based on user access patterns and content correlation.\nOur approach supports batch requests, enables approximate clique merging, and\noffers a formal competitive guarantee. Through extensive evaluation on the\nNetflix and Spotify datasets, AKPC reduces total cost by up to 63 and 55\npercentage over online baselines, respectively, and achieves performance within\n15 and 13 percentage of the optimal. This demonstrates its scalability and\neffectiveness for real world caching systems."
                },
                "authors": [
                    {
                        "name": "Suvarthi Sarkar"
                    },
                    {
                        "name": "Aadarshraj Sah"
                    },
                    {
                        "name": "Poddutoori Sweeya Reddy"
                    },
                    {
                        "name": "Aryabartta Sahu"
                    }
                ],
                "author_detail": {
                    "name": "Aryabartta Sahu"
                },
                "author": "Aryabartta Sahu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13231v2",
                "updated": "2025-09-15T14:40:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    40,
                    16,
                    0,
                    258,
                    0
                ],
                "published": "2025-08-17T19:07:08Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "title": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System"
                },
                "summary": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference."
                },
                "authors": [
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Kaoutar El Maghraoui"
                    },
                    {
                        "name": "Naigang Wang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "IEEE Computer Architecture Letter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11815v1",
                "updated": "2025-09-15T11:53:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    53,
                    56,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T11:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    53,
                    56,
                    0,
                    258,
                    0
                ],
                "title": "SpecVLM: Fast Speculative Decoding in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecVLM: Fast Speculative Decoding in Vision-Language Models"
                },
                "summary": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM."
                },
                "authors": [
                    {
                        "name": "Haiduo Huang"
                    },
                    {
                        "name": "Fuwei Yang"
                    },
                    {
                        "name": "Zhenhua Liu"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Pengju Ren"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11628v1",
                "updated": "2025-09-15T06:46:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    6,
                    46,
                    22,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T06:46:22Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    6,
                    46,
                    22,
                    0,
                    258,
                    0
                ],
                "title": "SpeCa: Accelerating Diffusion Transformers with Speculative Feature\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeCa: Accelerating Diffusion Transformers with Speculative Feature\n  Caching"
                },
                "summary": "Diffusion models have revolutionized high-fidelity image and video synthesis,\nyet their computational demands remain prohibitive for real-time applications.\nThese models face two fundamental challenges: strict temporal dependencies\npreventing parallelization, and computationally intensive forward passes\nrequired at each denoising step. Drawing inspiration from speculative decoding\nin large language models, we present SpeCa, a novel 'Forecast-then-verify'\nacceleration framework that effectively addresses both limitations. SpeCa's\ncore innovation lies in introducing Speculative Sampling to diffusion models,\npredicting intermediate features for subsequent timesteps based on fully\ncomputed reference timesteps. Our approach implements a parameter-free\nverification mechanism that efficiently evaluates prediction reliability,\nenabling real-time decisions to accept or reject each prediction while\nincurring negligible computational overhead. Furthermore, SpeCa introduces\nsample-adaptive computation allocation that dynamically modulates resources\nbased on generation complexity, allocating reduced computation for simpler\nsamples while preserving intensive processing for complex instances.\nExperiments demonstrate 6.34x acceleration on FLUX with minimal quality\ndegradation (5.5% drop), 7.3x speedup on DiT while preserving generation\nfidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The\nverification mechanism incurs minimal overhead (1.67%-3.5% of full inference\ncosts), establishing a new paradigm for efficient diffusion model inference\nwhile maintaining generation quality even at aggressive acceleration ratios.\nOur codes have been released in Github:\n\\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have revolutionized high-fidelity image and video synthesis,\nyet their computational demands remain prohibitive for real-time applications.\nThese models face two fundamental challenges: strict temporal dependencies\npreventing parallelization, and computationally intensive forward passes\nrequired at each denoising step. Drawing inspiration from speculative decoding\nin large language models, we present SpeCa, a novel 'Forecast-then-verify'\nacceleration framework that effectively addresses both limitations. SpeCa's\ncore innovation lies in introducing Speculative Sampling to diffusion models,\npredicting intermediate features for subsequent timesteps based on fully\ncomputed reference timesteps. Our approach implements a parameter-free\nverification mechanism that efficiently evaluates prediction reliability,\nenabling real-time decisions to accept or reject each prediction while\nincurring negligible computational overhead. Furthermore, SpeCa introduces\nsample-adaptive computation allocation that dynamically modulates resources\nbased on generation complexity, allocating reduced computation for simpler\nsamples while preserving intensive processing for complex instances.\nExperiments demonstrate 6.34x acceleration on FLUX with minimal quality\ndegradation (5.5% drop), 7.3x speedup on DiT while preserving generation\nfidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The\nverification mechanism incurs minimal overhead (1.67%-3.5% of full inference\ncosts), establishing a new paradigm for efficient diffusion model inference\nwhile maintaining generation quality even at aggressive acceleration ratios.\nOur codes have been released in Github:\n\\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Fei Ren"
                    },
                    {
                        "name": "Shaobo Wang"
                    },
                    {
                        "name": "Kaixin Li"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_doi": "10.1145/3746027.3755331",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746027.3755331",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.11628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 9 figures, ACM Multimedia 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14089v2",
                "updated": "2025-09-15T01:15:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    1,
                    15,
                    50,
                    0,
                    258,
                    0
                ],
                "published": "2025-04-18T22:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average."
                },
                "authors": [
                    {
                        "name": "Kang He"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06261v2",
                "updated": "2025-09-15T00:51:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    0,
                    51,
                    47,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-08T00:57:50Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    0,
                    57,
                    50,
                    0,
                    251,
                    0
                ],
                "title": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving"
                },
                "summary": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems."
                },
                "authors": [
                    {
                        "name": "Kyungmin Bin"
                    },
                    {
                        "name": "Seungbeom Choi"
                    },
                    {
                        "name": "Jimyoung Son"
                    },
                    {
                        "name": "Jieun Choi"
                    },
                    {
                        "name": "Daseul Bae"
                    },
                    {
                        "name": "Daehyeon Baek"
                    },
                    {
                        "name": "Kihyo Moon"
                    },
                    {
                        "name": "Minsung Jang"
                    },
                    {
                        "name": "Hyojung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hyojung Lee"
                },
                "author": "Hyojung Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11239v1",
                "updated": "2025-09-14T12:29:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    12,
                    29,
                    49,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T12:29:49Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    12,
                    29,
                    49,
                    6,
                    257,
                    0
                ],
                "title": "Multi-Layer Perceptron-Based Relay Node Selection for Next-Generation\n  Intelligent Delay-Tolerant Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Layer Perceptron-Based Relay Node Selection for Next-Generation\n  Intelligent Delay-Tolerant Networks"
                },
                "summary": "Delay Tolerant Networks (DTNs) are critical for emergency communication in\nhighly dynamic and challenging scenarios characterized by intermittent\nconnectivity, frequent disruptions, and unpredictable node mobility. While some\nprotocols are widely adopted for simplicity and low overhead, their static\nreplication strategy lacks the ability to adaptively distinguish high-quality\nrelay nodes, often leading to inefficient and suboptimal message dissemination.\nTo address this challenge, we propose a novel intelligent routing enhancement\nthat integrates machine learning-based node evaluation into the Spray and Wait\nframework. Several dynamic, core features are extracted from simulation logs\nand are used to train multiple classifiers - Multi-Layer Perceptron (MLP),\nSupport Vector Machine (SVM), and Random Forest (RF) - to predict whether a\nnode is suitable as a relay under dynamic conditions. The trained models are\ndeployed via a lightweight Flask-based RESTful API, enabling real-time,\nadaptive predictions. We implement the enhanced router MLPBasedSprayRouter,\nwhich selectively forwards messages based on the predicted relay quality. A\ncaching mechanism is incorporated to reduce computational overhead and ensure\nstable, low-latency inference. Extensive experiments under realistic emergency\nmobility scenarios demonstrate that the proposed framework significantly\nimproves delivery ratio while reducing average latency compared to the baseline\nprotocols. Among all evaluated classifiers, MLP achieved the most robust\nperformance, consistently outperforming both SVM and RF in terms of accuracy,\nadaptability, and inference speed. These results confirm the novelty and\npracticality of integrating machine learning into DTN routing, paving the way\nfor resilient and intelligent communication systems in smart cities, disaster\nrecovery, and other dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delay Tolerant Networks (DTNs) are critical for emergency communication in\nhighly dynamic and challenging scenarios characterized by intermittent\nconnectivity, frequent disruptions, and unpredictable node mobility. While some\nprotocols are widely adopted for simplicity and low overhead, their static\nreplication strategy lacks the ability to adaptively distinguish high-quality\nrelay nodes, often leading to inefficient and suboptimal message dissemination.\nTo address this challenge, we propose a novel intelligent routing enhancement\nthat integrates machine learning-based node evaluation into the Spray and Wait\nframework. Several dynamic, core features are extracted from simulation logs\nand are used to train multiple classifiers - Multi-Layer Perceptron (MLP),\nSupport Vector Machine (SVM), and Random Forest (RF) - to predict whether a\nnode is suitable as a relay under dynamic conditions. The trained models are\ndeployed via a lightweight Flask-based RESTful API, enabling real-time,\nadaptive predictions. We implement the enhanced router MLPBasedSprayRouter,\nwhich selectively forwards messages based on the predicted relay quality. A\ncaching mechanism is incorporated to reduce computational overhead and ensure\nstable, low-latency inference. Extensive experiments under realistic emergency\nmobility scenarios demonstrate that the proposed framework significantly\nimproves delivery ratio while reducing average latency compared to the baseline\nprotocols. Among all evaluated classifiers, MLP achieved the most robust\nperformance, consistently outperforming both SVM and RF in terms of accuracy,\nadaptability, and inference speed. These results confirm the novelty and\npracticality of integrating machine learning into DTN routing, paving the way\nfor resilient and intelligent communication systems in smart cities, disaster\nrecovery, and other dynamic environments."
                },
                "authors": [
                    {
                        "name": "Zhekun Huang"
                    },
                    {
                        "name": "Milena Radenkovic"
                    }
                ],
                "author_detail": {
                    "name": "Milena Radenkovic"
                },
                "author": "Milena Radenkovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11181v1",
                "updated": "2025-09-14T09:26:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    9,
                    26,
                    44,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T09:26:44Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    9,
                    26,
                    44,
                    6,
                    257,
                    0
                ],
                "title": "Dislocation response to electric fields in strontium titanate: A\n  mesoscale indentation study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dislocation response to electric fields in strontium titanate: A\n  mesoscale indentation study"
                },
                "summary": "Dislocations in perovskite oxides have drawn increasing research interest due\nto their potential of tuning functional properties of electroceramics. Open\nquestions remain regarding the behavior of dislocations concerning their\nstability under strong externally applied electric fields. In this study, we\ninvestigate the dielectric breakdown strength of nominally undoped SrTiO3\ncrystals after the introduction of high-density dislocations. The\ndislocation-rich samples are prepared using the Brinell scratching method, and\nthey consistently exhibit lower dielectric breakdown strength as well as a\nlarger scatter in the breakdown probability. We also study the impact of\nelectric field on the introduction and movement of dislocations in SrTiO3\ncrystals using Brinell indentation coupled with an electric field of 2 kV/mm.\nNo changes on the dislocation plastic zone size, depth, and dislocation\ndistribution are observed under this electric field. Based on the charge state\nof the dislocations in SrTiO3 as well as the electrical and thermal\nconductivity modified by dislocations, we discuss the forces induced by the\nelectric field to act on the dislocations to underline the possible mechanisms\nfor such dislocation behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dislocations in perovskite oxides have drawn increasing research interest due\nto their potential of tuning functional properties of electroceramics. Open\nquestions remain regarding the behavior of dislocations concerning their\nstability under strong externally applied electric fields. In this study, we\ninvestigate the dielectric breakdown strength of nominally undoped SrTiO3\ncrystals after the introduction of high-density dislocations. The\ndislocation-rich samples are prepared using the Brinell scratching method, and\nthey consistently exhibit lower dielectric breakdown strength as well as a\nlarger scatter in the breakdown probability. We also study the impact of\nelectric field on the introduction and movement of dislocations in SrTiO3\ncrystals using Brinell indentation coupled with an electric field of 2 kV/mm.\nNo changes on the dislocation plastic zone size, depth, and dislocation\ndistribution are observed under this electric field. Based on the charge state\nof the dislocations in SrTiO3 as well as the electrical and thermal\nconductivity modified by dislocations, we discuss the forces induced by the\nelectric field to act on the dislocations to underline the possible mechanisms\nfor such dislocation behavior."
                },
                "authors": [
                    {
                        "name": "Alexander Frisch"
                    },
                    {
                        "name": "Daniel Isaia"
                    },
                    {
                        "name": "Oliver PreuÃ"
                    },
                    {
                        "name": "Xufei Fang"
                    }
                ],
                "author_detail": {
                    "name": "Xufei Fang"
                },
                "author": "Xufei Fang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11155v1",
                "updated": "2025-09-14T08:20:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    20,
                    48,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T08:20:48Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    20,
                    48,
                    6,
                    257,
                    0
                ],
                "title": "AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient\n  Inference in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient\n  Inference in LLMs"
                },
                "summary": "The quadratic complexity of the attention mechanism remains a fundamental\nbarrier to scaling Large Language Models (LLMs) to longer contexts, creating a\ncritical bottleneck in both computation and memory. To address this, we\nintroduce AQUA (Attention via QUery mAgnitudes) a novel and versatile\napproximation strategy that significantly reduces the cost of attention with a\ngraceful performance trade-off. Our method operates in two phases: an efficient\noffline step where we compute a universal, language agnostic projection matrix\nvia SVD on a calibration dataset, and an online inference step where we project\nquery and key vectors and dynamically select a sparse subset of dimensions\nbased on the query's magnitude. We provide a formal theoretical analysis of\nAQUA, establishing the break-even point at which it becomes more\ncomputationally efficient than standard attention. Our empirical evaluations on\nstate-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in\nthe attention dot-product computation can be achieved with a statistically\ninsignificant impact on performance across a wide range of benchmarks. We\nfurther showcase the versatility of AQUA by demonstrating its ability to\nsynergistically accelerate existing token eviction methods like H2O and to\ndirectly reduce KV-cache memory size. By offering a controllable knob to\nbalance efficiency and accuracy, AQUA provides a practical and powerful tool\nfor making large-scale LLM inference more accessible and sustainable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic complexity of the attention mechanism remains a fundamental\nbarrier to scaling Large Language Models (LLMs) to longer contexts, creating a\ncritical bottleneck in both computation and memory. To address this, we\nintroduce AQUA (Attention via QUery mAgnitudes) a novel and versatile\napproximation strategy that significantly reduces the cost of attention with a\ngraceful performance trade-off. Our method operates in two phases: an efficient\noffline step where we compute a universal, language agnostic projection matrix\nvia SVD on a calibration dataset, and an online inference step where we project\nquery and key vectors and dynamically select a sparse subset of dimensions\nbased on the query's magnitude. We provide a formal theoretical analysis of\nAQUA, establishing the break-even point at which it becomes more\ncomputationally efficient than standard attention. Our empirical evaluations on\nstate-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in\nthe attention dot-product computation can be achieved with a statistically\ninsignificant impact on performance across a wide range of benchmarks. We\nfurther showcase the versatility of AQUA by demonstrating its ability to\nsynergistically accelerate existing token eviction methods like H2O and to\ndirectly reduce KV-cache memory size. By offering a controllable knob to\nbalance efficiency and accuracy, AQUA provides a practical and powerful tool\nfor making large-scale LLM inference more accessible and sustainable."
                },
                "authors": [
                    {
                        "name": "Santhosh G S"
                    },
                    {
                        "name": "Saurav Prakash"
                    },
                    {
                        "name": "Balaraman Ravindran"
                    }
                ],
                "author_detail": {
                    "name": "Balaraman Ravindran"
                },
                "author": "Balaraman Ravindran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10798v1",
                "updated": "2025-09-13T03:34:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    13,
                    3,
                    34,
                    12,
                    5,
                    256,
                    0
                ],
                "published": "2025-09-13T03:34:12Z",
                "published_parsed": [
                    2025,
                    9,
                    13,
                    3,
                    34,
                    12,
                    5,
                    256,
                    0
                ],
                "title": "Judge Q: Trainable Queries for Optimized Information Retention in KV\n  Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judge Q: Trainable Queries for Optimized Information Retention in KV\n  Cache Eviction"
                },
                "summary": "Large language models (LLMs) utilize key-value (KV) cache to store historical\ninformation during sequence processing. The size of KV cache grows linearly as\nthe length of the sequence extends, which seriously affects memory usage and\ndecoding efficiency. Current methods for KV cache eviction typically utilize\nthe last window from the pre-filling phase as queries to compute the KV\nimportance scores for eviction. Although this scheme is simple to implement, it\ntends to overly focus on local information, potentially leading to the neglect\nor omission of crucial global information. To mitigate this issue, we propose\nJudge Q, a novel training method which incorporates a soft token list. This\nmethod only tunes the model's embedding layer at a low training cost. By\nconcatenating the soft token list at the end of the input sequence, we train\nthese tokens' attention map to the original input sequence to align with that\nof the actual decoded tokens. In this way, the queries corresponding to the\nsoft tokens can effectively capture global information and better evaluate the\nimportance of the keys and values within the KV cache, thus maintaining\ndecoding quality when KV cache is evicted. Under the same eviction budget, our\nmethod exhibits less performance degradation compared to existing eviction\napproaches. We validate our approach through experiments conducted on models\nsuch as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks\nincluding LongBench, RULER, and Needle-in-a-Haystack. Results indicate an\nimprovement of approximately 1 point on the LongBench and over 3 points on\nRULER. This proposed methodology can be seamlessly integrated into existing\nopen-source models with minimal training overhead, thereby enhancing\nperformance in KV cache eviction scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) utilize key-value (KV) cache to store historical\ninformation during sequence processing. The size of KV cache grows linearly as\nthe length of the sequence extends, which seriously affects memory usage and\ndecoding efficiency. Current methods for KV cache eviction typically utilize\nthe last window from the pre-filling phase as queries to compute the KV\nimportance scores for eviction. Although this scheme is simple to implement, it\ntends to overly focus on local information, potentially leading to the neglect\nor omission of crucial global information. To mitigate this issue, we propose\nJudge Q, a novel training method which incorporates a soft token list. This\nmethod only tunes the model's embedding layer at a low training cost. By\nconcatenating the soft token list at the end of the input sequence, we train\nthese tokens' attention map to the original input sequence to align with that\nof the actual decoded tokens. In this way, the queries corresponding to the\nsoft tokens can effectively capture global information and better evaluate the\nimportance of the keys and values within the KV cache, thus maintaining\ndecoding quality when KV cache is evicted. Under the same eviction budget, our\nmethod exhibits less performance degradation compared to existing eviction\napproaches. We validate our approach through experiments conducted on models\nsuch as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks\nincluding LongBench, RULER, and Needle-in-a-Haystack. Results indicate an\nimprovement of approximately 1 point on the LongBench and over 3 points on\nRULER. This proposed methodology can be seamlessly integrated into existing\nopen-source models with minimal training overhead, thereby enhancing\nperformance in KV cache eviction scenarios."
                },
                "authors": [
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Shiyu Ji"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10372v1",
                "updated": "2025-09-12T16:05:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    27,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T16:05:27Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    27,
                    4,
                    255,
                    0
                ],
                "title": "MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging\n  Bit-Slice-enabled Sparsity and Repetitiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging\n  Bit-Slice-enabled Sparsity and Repetitiveness"
                },
                "summary": "Large language models (LLMs) face significant inference latency due to\ninefficiencies in GEMM operations, weight access, and KV cache access,\nespecially in real-time scenarios. This highlights the need for a versatile\ncompute-memory efficient accelerator. Unfortunately, existing Transformer\naccelerators struggle to address both aspects simultaneously, as they focus on\nvalue-level processing, missing fine-grained opportunities to optimize\ncomputation and memory collaboratively. This paper introduces MCBP, a\nbit-grained compute-memory efficient algorithm-hardware co-design that\nleverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM\ninference. MCBP features three key innovations: 1) BS-repetitiveness-enabled\ncomputation reduction (BRCR), which eliminates redundant GEMM computations via\nleveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state\ncoding (BSTC), which reduces weight access via exploiting significant sparsity\nin high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),\nwhich reduces KV cache access by leveraging early-termination-based bit-grained\nprediction. These techniques, supported by custom accelerator designs,\neffectively alleviate the burden in GEMM, weight access, and KV cache access.\nExtensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up\nand 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA\nTransformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than\nSpatten, FACT and SOFA, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face significant inference latency due to\ninefficiencies in GEMM operations, weight access, and KV cache access,\nespecially in real-time scenarios. This highlights the need for a versatile\ncompute-memory efficient accelerator. Unfortunately, existing Transformer\naccelerators struggle to address both aspects simultaneously, as they focus on\nvalue-level processing, missing fine-grained opportunities to optimize\ncomputation and memory collaboratively. This paper introduces MCBP, a\nbit-grained compute-memory efficient algorithm-hardware co-design that\nleverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM\ninference. MCBP features three key innovations: 1) BS-repetitiveness-enabled\ncomputation reduction (BRCR), which eliminates redundant GEMM computations via\nleveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state\ncoding (BSTC), which reduces weight access via exploiting significant sparsity\nin high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),\nwhich reduces KV cache access by leveraging early-termination-based bit-grained\nprediction. These techniques, supported by custom accelerator designs,\neffectively alleviate the burden in GEMM, weight access, and KV cache access.\nExtensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up\nand 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA\nTransformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than\nSpatten, FACT and SOFA, respectively."
                },
                "authors": [
                    {
                        "name": "Huizheng Wang"
                    },
                    {
                        "name": "Zichuan Wang"
                    },
                    {
                        "name": "Zhiheng Yue"
                    },
                    {
                        "name": "Yousheng Long"
                    },
                    {
                        "name": "Taiquan Wei"
                    },
                    {
                        "name": "Jianxun Yang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Shaojun Wei"
                    },
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Shouyi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Shouyi Yin"
                },
                "author": "Shouyi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10312v1",
                "updated": "2025-09-12T14:53:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    53,
                    45,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    53,
                    45,
                    4,
                    255,
                    0
                ],
                "title": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion\n  Transformers with Cluster-Driven Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion\n  Transformers with Cluster-Driven Feature Caching"
                },
                "summary": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion."
                },
                "authors": [
                    {
                        "name": "Zhixin Zheng"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Shaobo Wang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "11 pages, 11 figures; Accepted by ACM MM2025; Mainly focus on feature\n  caching for diffusion transformers acceleration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10251v1",
                "updated": "2025-09-12T13:49:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    49,
                    27,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T13:49:27Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    49,
                    27,
                    4,
                    255,
                    0
                ],
                "title": "XBOF: A Cost-Efficient CXL JBOF with Inter-SSD Compute Resource Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XBOF: A Cost-Efficient CXL JBOF with Inter-SSD Compute Resource Sharing"
                },
                "summary": "Enterprise SSDs integrate numerous computing resources (e.g., ARM processor\nand onboard DRAM) to satisfy the ever-increasing performance requirements of\nI/O bursts. While these resources substantially elevate the monetary costs of\nSSDs, the sporadic nature of I/O bursts causes severe SSD resource\nunderutilization in just a bunch of flash (JBOF) level. Tackling this\nchallenge, we propose XBOF, a cost-efficient JBOF design, which only reserves\nmoderate computing resources in SSDs at low monetary cost, while achieving\ndemanded I/O performance through efficient inter-SSD resource sharing.\nSpecifically, XBOF first disaggregates SSD architecture into multiple disjoint\nparts based on their functionality, enabling fine-grained SSD internal resource\nmanagement. XBOF then employs a decentralized scheme to manage these\ndisaggregated resources and harvests the computing resources of idle SSDs to\nassist busy SSDs in handling I/O bursts. This idea is facilitated by the\ncache-coherent capability of Compute eXpress Link (CXL), with which the busy\nSSDs can directly utilize the harvested computing resources to accelerate\nmetadata processing. The evaluation results show that XBOF improves SSD\nresource utilization by 50.4% and saves 19.0% monetary costs with a negligible\nperformance loss, compared to existing JBOF designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise SSDs integrate numerous computing resources (e.g., ARM processor\nand onboard DRAM) to satisfy the ever-increasing performance requirements of\nI/O bursts. While these resources substantially elevate the monetary costs of\nSSDs, the sporadic nature of I/O bursts causes severe SSD resource\nunderutilization in just a bunch of flash (JBOF) level. Tackling this\nchallenge, we propose XBOF, a cost-efficient JBOF design, which only reserves\nmoderate computing resources in SSDs at low monetary cost, while achieving\ndemanded I/O performance through efficient inter-SSD resource sharing.\nSpecifically, XBOF first disaggregates SSD architecture into multiple disjoint\nparts based on their functionality, enabling fine-grained SSD internal resource\nmanagement. XBOF then employs a decentralized scheme to manage these\ndisaggregated resources and harvests the computing resources of idle SSDs to\nassist busy SSDs in handling I/O bursts. This idea is facilitated by the\ncache-coherent capability of Compute eXpress Link (CXL), with which the busy\nSSDs can directly utilize the harvested computing resources to accelerate\nmetadata processing. The evaluation results show that XBOF improves SSD\nresource utilization by 50.4% and saves 19.0% monetary costs with a negligible\nperformance loss, compared to existing JBOF designs."
                },
                "authors": [
                    {
                        "name": "Shushu Yi"
                    },
                    {
                        "name": "Yuda An"
                    },
                    {
                        "name": "Li Peng"
                    },
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Jieming Yin"
                    },
                    {
                        "name": "Guangyan Zhang"
                    },
                    {
                        "name": "Wenfei Wu"
                    },
                    {
                        "name": "Diyu Zhou"
                    },
                    {
                        "name": "Zhenlin Wang"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10016v1",
                "updated": "2025-09-12T07:20:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    20,
                    53,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T07:20:53Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    20,
                    53,
                    4,
                    255,
                    0
                ],
                "title": "SvalMIZ-25 Svalbard Marginal Ice Zone Campaign 2025 -- Cruise Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SvalMIZ-25 Svalbard Marginal Ice Zone Campaign 2025 -- Cruise Report"
                },
                "summary": "The coupling of weather, sea-ice, ocean, and wave forecasting systems has\nbeen a long-standing research focus to improve Arctic forecasting systems and\ntheir realism and is also a priority of international initiatives such as the\nWMO research project PCAPS. The goal of the Svalbard Marginal Ice Zone 2025\nCampaign (SvalMIZ-25) was to observe and better understand the complex\ninterplay between atmosphere, waves, and sea-ice in the winter Marginal Ice\nZone (MIZ) in order to advance predictive skill of coupled Arctic forecasting\nsystems. The main objective has been to set up a network of observations with a\nspatial distribution that allows for a representative comparison between in\nsitu observations and gridded model data. The observed variables include air\nand surface temperature, sea-ice drift, and wave energy spectra. With the\nsupport of the Norwegian Coast Guard, we participated in the research cruise\nwith KV Svalbard from 22.April - 11.May 2025. In total 21 buoys were deployed\nin the Marginal Ice Zone north of the Svalbard Archipelago.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coupling of weather, sea-ice, ocean, and wave forecasting systems has\nbeen a long-standing research focus to improve Arctic forecasting systems and\ntheir realism and is also a priority of international initiatives such as the\nWMO research project PCAPS. The goal of the Svalbard Marginal Ice Zone 2025\nCampaign (SvalMIZ-25) was to observe and better understand the complex\ninterplay between atmosphere, waves, and sea-ice in the winter Marginal Ice\nZone (MIZ) in order to advance predictive skill of coupled Arctic forecasting\nsystems. The main objective has been to set up a network of observations with a\nspatial distribution that allows for a representative comparison between in\nsitu observations and gridded model data. The observed variables include air\nand surface temperature, sea-ice drift, and wave energy spectra. With the\nsupport of the Norwegian Coast Guard, we participated in the research cruise\nwith KV Svalbard from 22.April - 11.May 2025. In total 21 buoys were deployed\nin the Marginal Ice Zone north of the Svalbard Archipelago."
                },
                "authors": [
                    {
                        "name": "M. MÃ¼ller"
                    },
                    {
                        "name": "J. Rabault"
                    },
                    {
                        "name": "C. Palerme"
                    },
                    {
                        "name": "J. TjernstrÃ¶m"
                    }
                ],
                "author_detail": {
                    "name": "J. TjernstrÃ¶m"
                },
                "author": "J. TjernstrÃ¶m",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09754v1",
                "updated": "2025-09-11T16:48:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    48,
                    24,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T16:48:24Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    48,
                    24,
                    3,
                    254,
                    0
                ],
                "title": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation"
                },
                "summary": "KV Cache is commonly used to accelerate LLM inference with long contexts, yet\nits high memory demand drives the need for cache compression. Existing\ncompression methods, however, are largely heuristic and lack dynamic budget\nallocation. To address this limitation, we introduce a unified framework for\ncache compression by minimizing information loss in Transformer residual\nstreams. Building on it, we analyze the layer attention output loss and derive\na new metric to compare cache entries across heads, enabling layer-wise\ncompression with dynamic head budgets. Additionally, by contrasting cross-layer\ninformation, we also achieve dynamic layer budgets. LAVa is the first unified\nstrategy for cache eviction and dynamic budget allocation that, unlike prior\nmethods, does not rely on training or the combination of multiple strategies.\nExperiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and\nInfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a\nnew insight: dynamic layer budgets are crucial for generation tasks (e.g., code\ncompletion), while dynamic head budgets play a key role in extraction tasks\n(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently\nmaintains top performance across task types. Our code is available at\nhttps://github.com/MGDDestiny/Lava.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache is commonly used to accelerate LLM inference with long contexts, yet\nits high memory demand drives the need for cache compression. Existing\ncompression methods, however, are largely heuristic and lack dynamic budget\nallocation. To address this limitation, we introduce a unified framework for\ncache compression by minimizing information loss in Transformer residual\nstreams. Building on it, we analyze the layer attention output loss and derive\na new metric to compare cache entries across heads, enabling layer-wise\ncompression with dynamic head budgets. Additionally, by contrasting cross-layer\ninformation, we also achieve dynamic layer budgets. LAVa is the first unified\nstrategy for cache eviction and dynamic budget allocation that, unlike prior\nmethods, does not rely on training or the combination of multiple strategies.\nExperiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and\nInfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a\nnew insight: dynamic layer budgets are crucial for generation tasks (e.g., code\ncompletion), while dynamic head budgets play a key role in extraction tasks\n(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently\nmaintains top performance across task types. Our code is available at\nhttps://github.com/MGDDestiny/Lava."
                },
                "authors": [
                    {
                        "name": "Yiqun Shen"
                    },
                    {
                        "name": "Song Yuan"
                    },
                    {
                        "name": "Zhengze Zhang"
                    },
                    {
                        "name": "Xiaoliang Wang"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Nguyen Cam-Tu"
                    }
                ],
                "author_detail": {
                    "name": "Nguyen Cam-Tu"
                },
                "author": "Nguyen Cam-Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09525v1",
                "updated": "2025-09-11T15:06:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    6,
                    3,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T15:06:03Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    6,
                    3,
                    3,
                    254,
                    0
                ],
                "title": "TrEnv: Transparently Share Serverless Execution Environments Across\n  Different Functions and Nodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrEnv: Transparently Share Serverless Execution Environments Across\n  Different Functions and Nodes"
                },
                "summary": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B."
                },
                "authors": [
                    {
                        "name": "Jialiang Huang"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Sixing Lin"
                    },
                    {
                        "name": "Kang Chen"
                    },
                    {
                        "name": "Jinlei Jiang"
                    },
                    {
                        "name": "Xia Liao"
                    },
                    {
                        "name": "Yingdi Shan"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Mengting Lu"
                    },
                    {
                        "name": "Tao Ma"
                    },
                    {
                        "name": "Haifeng Gong"
                    },
                    {
                        "name": "Mingxing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mingxing Zhang"
                },
                "author": "Mingxing Zhang",
                "arxiv_comment": "38 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09494v1",
                "updated": "2025-09-11T14:34:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    34,
                    1,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T14:34:01Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    34,
                    1,
                    3,
                    254,
                    0
                ],
                "title": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding"
                },
                "summary": "In-loop filtering (ILF) is a key technology in video coding standards to\nreduce artifacts and enhance visual quality. Recently, neural network-based ILF\nschemes have achieved remarkable coding gains, emerging as a powerful candidate\nfor next-generation video coding standards. However, the use of deep neural\nnetworks (DNN) brings significant computational and time complexity or high\ndemands for dedicated hardware, making it challenging for general use. To\naddress this limitation, we study a practical ILF solution by adopting look-up\ntables (LUTs). After training a DNN with a restricted reference range for ILF,\nall possible inputs are traversed, and the output values of the DNN are cached\ninto LUTs. During the coding process, the filtering process is performed by\nsimply retrieving the filtered pixel through locating the input pixels and\ninterpolating between the cached values, instead of relying on heavy inference\ncomputations. In this paper, we propose a universal LUT-based ILF framework,\ntermed LUT-ILF++. First, we introduce the cooperation of multiple kinds of\nfiltering LUTs and propose a series of customized indexing mechanisms to enable\nbetter filtering reference perception with limited storage consumption. Second,\nwe propose the cross-component indexing mechanism to enable the filtering of\ndifferent color components jointly. Third, in order to make our solution\npractical for coding uses, we propose the LUT compaction scheme to enable the\nLUT pruning, achieving a lower storage cost of the entire solution. The\nproposed framework is implemented in the VVC reference software. Experimental\nresults show that the proposed framework achieves on average 0.82%/2.97%/1.63%\nand 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI\nand RA configurations, respectively. Compared to DNN-based solutions, our\nproposed solution has much lower time complexity and storage cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology in video coding standards to\nreduce artifacts and enhance visual quality. Recently, neural network-based ILF\nschemes have achieved remarkable coding gains, emerging as a powerful candidate\nfor next-generation video coding standards. However, the use of deep neural\nnetworks (DNN) brings significant computational and time complexity or high\ndemands for dedicated hardware, making it challenging for general use. To\naddress this limitation, we study a practical ILF solution by adopting look-up\ntables (LUTs). After training a DNN with a restricted reference range for ILF,\nall possible inputs are traversed, and the output values of the DNN are cached\ninto LUTs. During the coding process, the filtering process is performed by\nsimply retrieving the filtered pixel through locating the input pixels and\ninterpolating between the cached values, instead of relying on heavy inference\ncomputations. In this paper, we propose a universal LUT-based ILF framework,\ntermed LUT-ILF++. First, we introduce the cooperation of multiple kinds of\nfiltering LUTs and propose a series of customized indexing mechanisms to enable\nbetter filtering reference perception with limited storage consumption. Second,\nwe propose the cross-component indexing mechanism to enable the filtering of\ndifferent color components jointly. Third, in order to make our solution\npractical for coding uses, we propose the LUT compaction scheme to enable the\nLUT pruning, achieving a lower storage cost of the entire solution. The\nproposed framework is implemented in the VVC reference software. Experimental\nresults show that the proposed framework achieves on average 0.82%/2.97%/1.63%\nand 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI\nand RA configurations, respectively. Compared to DNN-based solutions, our\nproposed solution has much lower time complexity and storage cost."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Jialin Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05211v2",
                "updated": "2025-09-11T12:06:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    6,
                    49,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-07T09:47:21Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "title": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization"
                },
                "summary": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Runsen Xu"
                    },
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19880v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19880v2",
                "updated": "2025-09-11T10:20:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    20,
                    20,
                    3,
                    254,
                    0
                ],
                "published": "2025-05-26T12:06:12Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    6,
                    12,
                    0,
                    146,
                    0
                ],
                "title": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing"
                },
                "summary": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead."
                },
                "authors": [
                    {
                        "name": "Saman Akbari"
                    },
                    {
                        "name": "Manfred Hauswirth"
                    }
                ],
                "author_detail": {
                    "name": "Manfred Hauswirth"
                },
                "author": "Manfred Hauswirth",
                "arxiv_doi": "10.1109/CLOUD67622.2025.00051",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/CLOUD67622.2025.00051",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.19880v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19880v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the 2025 IEEE 18th International Conference on Cloud\n  Computing (CLOUD)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19740v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19740v3",
                "updated": "2025-09-11T06:45:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    45,
                    58,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-27T10:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval"
                },
                "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19740v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19740v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01085v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01085v3",
                "updated": "2025-09-11T06:16:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    16,
                    31,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-01T03:16:52Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    16,
                    52,
                    0,
                    244,
                    0
                ],
                "title": "Bidirectional Sparse Attention for Faster Video Diffusion Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional Sparse Attention for Faster Video Diffusion Training"
                },
                "summary": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention."
                },
                "authors": [
                    {
                        "name": "Chenlu Zhan"
                    },
                    {
                        "name": "Wen Li"
                    },
                    {
                        "name": "Chuyu Shen"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Suhui Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01085v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01085v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09094v1",
                "updated": "2025-09-11T02:00:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    2,
                    0,
                    27,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T02:00:27Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    2,
                    0,
                    27,
                    3,
                    254,
                    0
                ],
                "title": "Coherence-Aware Task Graph Modeling for Realistic Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherence-Aware Task Graph Modeling for Realistic Application"
                },
                "summary": "As multicore systems continue to scale, cache coherence has emerged as a\ncritical determinant of system performance, with coherence behavior and task\nexecution closely intertwined, reshaping inter-task dependencies. Task graph\nmodeling provides a structured way to capture such dependencies and serves as\nthe foundation for many system-level design strategies. However, these\nstrategies typically rely on predefined task graphs, while many real-world\napplications lack explicit graphs and exhibit dynamic, data-dependent behavior,\nlimiting the effectiveness of static approaches. To address this, several task\ngraph modeling methods for realistic workloads have been developed. Yet, they\neither rely on implicit techniques that use application-specific features\nwithout producing explicit graphs, or they generate graphs tailored to fixed\nscheduling models, which limits generality. More importantly, they often\noverlook coherence interactions, creating a gap between design assumptions and\nactual runtime behavior. To overcome these limitations, we propose CoTAM, a\nCoherence-Aware Task Graph Modeling framework for realistic workloads that\nconstructs a unified task graph reflecting runtime behavior. CoTAM analyzes the\nimpact of coherence by decoupling its effects from overall execution,\nquantifies its influence through a learned weighting scheme, and infers\ninter-task dependencies for coherence-aware graph generation. Extensive\nexperiments show that CoTAM outperforms implicit methods, bridging the gap\nbetween dynamic workload behavior and existing designs while demonstrating the\nimportance of incorporating cache coherence into task graph modeling for\naccurate and generalizable system-level analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multicore systems continue to scale, cache coherence has emerged as a\ncritical determinant of system performance, with coherence behavior and task\nexecution closely intertwined, reshaping inter-task dependencies. Task graph\nmodeling provides a structured way to capture such dependencies and serves as\nthe foundation for many system-level design strategies. However, these\nstrategies typically rely on predefined task graphs, while many real-world\napplications lack explicit graphs and exhibit dynamic, data-dependent behavior,\nlimiting the effectiveness of static approaches. To address this, several task\ngraph modeling methods for realistic workloads have been developed. Yet, they\neither rely on implicit techniques that use application-specific features\nwithout producing explicit graphs, or they generate graphs tailored to fixed\nscheduling models, which limits generality. More importantly, they often\noverlook coherence interactions, creating a gap between design assumptions and\nactual runtime behavior. To overcome these limitations, we propose CoTAM, a\nCoherence-Aware Task Graph Modeling framework for realistic workloads that\nconstructs a unified task graph reflecting runtime behavior. CoTAM analyzes the\nimpact of coherence by decoupling its effects from overall execution,\nquantifies its influence through a learned weighting scheme, and infers\ninter-task dependencies for coherence-aware graph generation. Extensive\nexperiments show that CoTAM outperforms implicit methods, bridging the gap\nbetween dynamic workload behavior and existing designs while demonstrating the\nimportance of incorporating cache coherence into task graph modeling for\naccurate and generalizable system-level analysis."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_doi": "10.1145/3742875.3754678",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3742875.3754678",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.09094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by MEMOCODE'25, 10 pages",
                "arxiv_journal_ref": "International Symposium on Formal Methods and Models for System\n  Design (MEMOCODE '25), September 28-October 3, 2025, Taipei, Taiwan",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23674v2",
                "updated": "2025-09-10T17:59:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    59,
                    8,
                    2,
                    253,
                    0
                ],
                "published": "2025-07-31T15:50:57Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses"
                },
                "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience."
                },
                "authors": [
                    {
                        "name": "Muhammad Taha Cheema"
                    },
                    {
                        "name": "Abeer Aamir"
                    },
                    {
                        "name": "Khawaja Gul Muhammad"
                    },
                    {
                        "name": "Naveed Anwar Bhatti"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08696v1",
                "updated": "2025-09-10T15:41:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    41,
                    15,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T15:41:15Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    41,
                    15,
                    2,
                    253,
                    0
                ],
                "title": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer\n  Layer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer\n  Layer Caching"
                },
                "summary": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ ."
                },
                "authors": [
                    {
                        "name": "Siratish Sakpiboonchit"
                    }
                ],
                "author_detail": {
                    "name": "Siratish Sakpiboonchit"
                },
                "author": "Siratish Sakpiboonchit",
                "arxiv_comment": "9 pages, 2 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08542v1",
                "updated": "2025-09-10T12:46:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T12:46:29Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "title": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference"
                },
                "summary": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications."
                },
                "authors": [
                    {
                        "name": "Wenlun Zhang"
                    },
                    {
                        "name": "Xinyu Li"
                    },
                    {
                        "name": "Shimpei Ando"
                    },
                    {
                        "name": "Kentaro Yoshioka"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Yoshioka"
                },
                "author": "Kentaro Yoshioka",
                "arxiv_comment": "Accepted to ASP-DAC 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08342v1",
                "updated": "2025-09-10T07:28:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    28,
                    24,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T07:28:24Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    28,
                    24,
                    2,
                    253,
                    0
                ],
                "title": "Accelerating Mixture-of-Expert Inference with Adaptive Expert Split\n  Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Mixture-of-Expert Inference with Adaptive Expert Split\n  Mechanism"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a promising architecture for modern\nlarge language models (LLMs). However, massive parameters impose heavy GPU\nmemory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.\nOffloading the expert parameters to CPU RAM offers an effective way to\nalleviate the VRAM requirements for MoE inference. Existing approaches\ntypically cache a small subset of experts in VRAM and dynamically prefetch\nexperts from RAM during inference, leading to significant degradation in\ninference speed due to the poor cache hit rate and substantial expert loading\nlatency. In this work, we propose MoEpic, an efficient MoE inference system\nwith a novel expert split mechanism. Specifically, each expert is vertically\ndivided into two segments: top and bottom. MoEpic caches the top segment of hot\nexperts, so that more experts will be stored under the limited VRAM budget,\nthereby improving the cache hit rate. During each layer's inference, MoEpic\npredicts and prefetches the activated experts for the next layer. Since the top\nsegments of cached experts are exempt from fetching, the loading time is\nreduced, which allows efficient transfer-computation overlap. Nevertheless, the\nperformance of MoEpic critically depends on the cache configuration (i.e., each\nlayer's VRAM budget and expert split ratio). To this end, we propose a\ndivide-and-conquer algorithm based on fixed-point iteration for adaptive cache\nconfiguration. Extensive experiments on popular MoE LLMs demonstrate that\nMoEpic can save about half of the GPU cost, while lowering the inference\nlatency by about 37.51%-65.73% compared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a promising architecture for modern\nlarge language models (LLMs). However, massive parameters impose heavy GPU\nmemory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.\nOffloading the expert parameters to CPU RAM offers an effective way to\nalleviate the VRAM requirements for MoE inference. Existing approaches\ntypically cache a small subset of experts in VRAM and dynamically prefetch\nexperts from RAM during inference, leading to significant degradation in\ninference speed due to the poor cache hit rate and substantial expert loading\nlatency. In this work, we propose MoEpic, an efficient MoE inference system\nwith a novel expert split mechanism. Specifically, each expert is vertically\ndivided into two segments: top and bottom. MoEpic caches the top segment of hot\nexperts, so that more experts will be stored under the limited VRAM budget,\nthereby improving the cache hit rate. During each layer's inference, MoEpic\npredicts and prefetches the activated experts for the next layer. Since the top\nsegments of cached experts are exempt from fetching, the loading time is\nreduced, which allows efficient transfer-computation overlap. Nevertheless, the\nperformance of MoEpic critically depends on the cache configuration (i.e., each\nlayer's VRAM budget and expert split ratio). To this end, we propose a\ndivide-and-conquer algorithm based on fixed-point iteration for adaptive cache\nconfiguration. Extensive experiments on popular MoE LLMs demonstrate that\nMoEpic can save about half of the GPU cost, while lowering the inference\nlatency by about 37.51%-65.73% compared to the baselines."
                },
                "authors": [
                    {
                        "name": "Jiaming Yan"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08315v1",
                "updated": "2025-09-10T06:32:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    32,
                    49,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T06:32:49Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    32,
                    49,
                    2,
                    253,
                    0
                ],
                "title": "EvolKV: Evolutionary KV Cache Compression for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvolKV: Evolutionary KV Cache Compression for LLM Inference"
                },
                "summary": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation."
                },
                "authors": [
                    {
                        "name": "Bohan Yu"
                    },
                    {
                        "name": "Yekun Chai"
                    }
                ],
                "author_detail": {
                    "name": "Yekun Chai"
                },
                "author": "Yekun Chai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v3",
                "updated": "2025-09-09T13:30:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    30,
                    17,
                    1,
                    252,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "Accepted by EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07379v1",
                "updated": "2025-09-09T04:00:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    4,
                    0,
                    43,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T04:00:43Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    4,
                    0,
                    43,
                    1,
                    252,
                    0
                ],
                "title": "DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for\n  Efficient MoE LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for\n  Efficient MoE LLM Inference"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of deep learning tasks. Mixture of Experts (MoE) further enhances\ntheir capabilities by increasing model width through sparsely activated expert\nbranches, which keeps inference computation efficient. However, the large\nnumber of expert weights introduces significant GPU memory pressure, especially\nin resource-constrained environments such as single-GPU servers. More\nimportantly, MoE inference consists of two fundamentally different stages: a\nprefill stage where most experts are activated densely, and a decode stage\nwhere only a few experts are triggered sparsely. Treating these stages with a\nuniform scheduling strategy often leads to suboptimal latency and memory usage.\nTo address this, we propose DuoServe-MoE, an inference serving system that\nexplicitly separates prefill and decode stages and applies tailored expert\nscheduling strategies to each. In the prefill stage, DuoServe-MoE uses a\ntwo-stream CUDA pipeline that overlaps expert weight prefetching with the\ncomputation of non-MoE layers, limiting expert residency in GPU memory. In the\ndecode stage, a lightweight layer-level predictor trained offline from\nactivation traces is used to prefetch only the most likely activated experts,\nwithout requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B\nand 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to\n7.54 times while keeping peak memory usage at only 15 percent of the full model\nsize.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of deep learning tasks. Mixture of Experts (MoE) further enhances\ntheir capabilities by increasing model width through sparsely activated expert\nbranches, which keeps inference computation efficient. However, the large\nnumber of expert weights introduces significant GPU memory pressure, especially\nin resource-constrained environments such as single-GPU servers. More\nimportantly, MoE inference consists of two fundamentally different stages: a\nprefill stage where most experts are activated densely, and a decode stage\nwhere only a few experts are triggered sparsely. Treating these stages with a\nuniform scheduling strategy often leads to suboptimal latency and memory usage.\nTo address this, we propose DuoServe-MoE, an inference serving system that\nexplicitly separates prefill and decode stages and applies tailored expert\nscheduling strategies to each. In the prefill stage, DuoServe-MoE uses a\ntwo-stream CUDA pipeline that overlaps expert weight prefetching with the\ncomputation of non-MoE layers, limiting expert residency in GPU memory. In the\ndecode stage, a lightweight layer-level predictor trained offline from\nactivation traces is used to prefetch only the most likely activated experts,\nwithout requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B\nand 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to\n7.54 times while keeping peak memory usage at only 15 percent of the full model\nsize."
                },
                "authors": [
                    {
                        "name": "Yuning Zhang"
                    },
                    {
                        "name": "Grant Pinkert"
                    },
                    {
                        "name": "Nan Yang"
                    },
                    {
                        "name": "Yanli Li"
                    },
                    {
                        "name": "Dong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yuan"
                },
                "author": "Dong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01742v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01742v2",
                "updated": "2025-09-09T00:15:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    0,
                    15,
                    5,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-01T19:49:21Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    19,
                    49,
                    21,
                    0,
                    244,
                    0
                ],
                "title": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators"
                },
                "summary": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:\n(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache\nto accelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:\n(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache\nto accelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook."
                },
                "authors": [
                    {
                        "name": "Yitong Guo"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Haobin Hiroki Chen"
                    },
                    {
                        "name": "Yukui Luo"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Chenghong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chenghong Wang"
                },
                "author": "Chenghong Wang",
                "arxiv_comment": "Accepted by CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01742v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01742v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06949v1",
                "updated": "2025-09-08T17:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models"
                },
                "summary": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL"
                },
                "authors": [
                    {
                        "name": "Yinjie Wang"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ke Shen"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03377v2",
                "updated": "2025-09-08T17:22:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    22,
                    17,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-03T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    53,
                    45,
                    2,
                    246,
                    0
                ],
                "title": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing"
                },
                "summary": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v4",
                "updated": "2025-09-08T13:34:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    34,
                    54,
                    0,
                    251,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06579v1",
                "updated": "2025-09-08T11:49:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T11:49:51Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "title": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis"
                },
                "summary": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html."
                },
                "authors": [
                    {
                        "name": "Xin Kong"
                    },
                    {
                        "name": "Daniel Watson"
                    },
                    {
                        "name": "Yannick StrÃ¼mpler"
                    },
                    {
                        "name": "Michael Niemeyer"
                    },
                    {
                        "name": "Federico Tombari"
                    }
                ],
                "author_detail": {
                    "name": "Federico Tombari"
                },
                "author": "Federico Tombari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06493v1",
                "updated": "2025-09-08T09:54:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T09:54:18Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers"
                },
                "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search."
                },
                "authors": [
                    {
                        "name": "Ran Xin"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Yanchen Nie"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Xia Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xia Xiao"
                },
                "author": "Xia Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09822v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09822v4",
                "updated": "2025-09-08T09:09:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    9,
                    36,
                    0,
                    251,
                    0
                ],
                "published": "2025-08-13T13:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "title": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining"
                },
                "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/"
                },
                "authors": [
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sihan Qin"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09822v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09822v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06444v1",
                "updated": "2025-09-08T08:44:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    44,
                    24,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T08:44:24Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    44,
                    24,
                    0,
                    251,
                    0
                ],
                "title": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for\n  Heterogeneous and Privacy-Sensitive Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for\n  Heterogeneous and Privacy-Sensitive Data"
                },
                "summary": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments."
                },
                "authors": [
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Hainan Zhang"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Hong-Wei Zheng"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06436v1",
                "updated": "2025-09-08T08:34:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T08:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "title": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning"
                },
                "summary": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Xiaofei Xu"
                    },
                    {
                        "name": "Ke Deng"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Lin Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lin Tian"
                },
                "author": "Lin Tian",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06047v1",
                "updated": "2025-09-07T13:15:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    7,
                    13,
                    15,
                    17,
                    6,
                    250,
                    0
                ],
                "published": "2025-09-07T13:15:17Z",
                "published_parsed": [
                    2025,
                    9,
                    7,
                    13,
                    15,
                    17,
                    6,
                    250,
                    0
                ],
                "title": "A facile vector substrate platform via BaTiO3 membrane transfer enables\n  high quality solution processed epitaxial PZT on silicon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A facile vector substrate platform via BaTiO3 membrane transfer enables\n  high quality solution processed epitaxial PZT on silicon"
                },
                "summary": "The direct integration of high-performance ferroelectric oxides with silicon\nremains challenging due to lattice mismatch, thermal incompatibility, and the\nneed for high-temperature epitaxial growth. Here, a hybrid integration approach\nis demonstrated in which crystalline BaTiO3 (BTO) membranes are first\ntransferred onto Pt coated Si substrates and subsequently used as vector\nsubstrates (VS) for the growth of epitaxial (001) Pb(Zr0.52Ti0.48)O3 (PZT) thin\nfilms via chemical solution deposition (CSD). A KI and HCl based etchant\nenables rapid and complete dissolution of the SrVO3 sacrificial layer in about\n30 minutes, reducing the release time from days to minutes compared with\nconventional water based approaches to dissolve AVO3 and AMoO3 (A is Ca, Sr,\nBa). The BTO VS imposes dominant (00l) out of plane orientation and in plane\ncube on cube epitaxy in the overlying PZT. Devices exhibit remnant polarization\n10 to 12 micro coulomb/cm2 and coercive field of 100 kV/cm, with stable\nswitching to 10^8 cycles on the VS. From piezoelectric butterfly loops, we\nextract effective d33 of 70 pm/V for PZT on VS, and 54 pm/V for PZT grown on\nconventional Pt Si substrates. This approach demonstrates a scalable and cost\neffective route for integrating functional ferroelectric materials onto silicon\nand offers a promising platform for future CMOS compatible oxide electronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The direct integration of high-performance ferroelectric oxides with silicon\nremains challenging due to lattice mismatch, thermal incompatibility, and the\nneed for high-temperature epitaxial growth. Here, a hybrid integration approach\nis demonstrated in which crystalline BaTiO3 (BTO) membranes are first\ntransferred onto Pt coated Si substrates and subsequently used as vector\nsubstrates (VS) for the growth of epitaxial (001) Pb(Zr0.52Ti0.48)O3 (PZT) thin\nfilms via chemical solution deposition (CSD). A KI and HCl based etchant\nenables rapid and complete dissolution of the SrVO3 sacrificial layer in about\n30 minutes, reducing the release time from days to minutes compared with\nconventional water based approaches to dissolve AVO3 and AMoO3 (A is Ca, Sr,\nBa). The BTO VS imposes dominant (00l) out of plane orientation and in plane\ncube on cube epitaxy in the overlying PZT. Devices exhibit remnant polarization\n10 to 12 micro coulomb/cm2 and coercive field of 100 kV/cm, with stable\nswitching to 10^8 cycles on the VS. From piezoelectric butterfly loops, we\nextract effective d33 of 70 pm/V for PZT on VS, and 54 pm/V for PZT grown on\nconventional Pt Si substrates. This approach demonstrates a scalable and cost\neffective route for integrating functional ferroelectric materials onto silicon\nand offers a promising platform for future CMOS compatible oxide electronics."
                },
                "authors": [
                    {
                        "name": "Asraful Haque"
                    },
                    {
                        "name": "Antony Jeyaseelan"
                    },
                    {
                        "name": "Shubham Kumar Parate"
                    },
                    {
                        "name": "Srinivasan Raghavan"
                    },
                    {
                        "name": "Pavan Nukala"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Nukala"
                },
                "arxiv_affiliation": "Centre for Nanoscience and Engineering, Indian Institute of Science, Bengaluru, India",
                "author": "Pavan Nukala",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13863v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13863v2",
                "updated": "2025-09-06T05:58:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    6,
                    5,
                    58,
                    51,
                    5,
                    249,
                    0
                ],
                "published": "2025-08-19T14:30:41Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "title": "Tight Cache Contention Analysis for WCET Estimation on Multicore Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tight Cache Contention Analysis for WCET Estimation on Multicore Systems"
                },
                "summary": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Shenlin Cai"
                    },
                    {
                        "name": "Yaowei Liang"
                    },
                    {
                        "name": "Chen Jie"
                    },
                    {
                        "name": "Yinjie Fang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Guoquan Zhang"
                    },
                    {
                        "name": "Yaoyao Gu"
                    },
                    {
                        "name": "Xiang Xiao"
                    },
                    {
                        "name": "Wei Qin"
                    },
                    {
                        "name": "Xiangzhen Ouyang"
                    },
                    {
                        "name": "Wanli Chang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Chang"
                },
                "author": "Wanli Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13863v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05207v1",
                "updated": "2025-09-05T16:10:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    20,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T16:10:20Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    20,
                    4,
                    248,
                    0
                ],
                "title": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks"
                },
                "summary": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively."
                },
                "authors": [
                    {
                        "name": "Arefin Niam"
                    },
                    {
                        "name": "Tevfik Kosar"
                    },
                    {
                        "name": "M S Q Zulkar Nine"
                    }
                ],
                "author_detail": {
                    "name": "M S Q Zulkar Nine"
                },
                "author": "M S Q Zulkar Nine",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2505.10806",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05165v1",
                "updated": "2025-09-05T14:58:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T14:58:24Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "title": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens"
                },
                "summary": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment."
                },
                "authors": [
                    {
                        "name": "Dmitry Akulov"
                    },
                    {
                        "name": "Mohamed Sana"
                    },
                    {
                        "name": "Antonio De Domenico"
                    },
                    {
                        "name": "Tareq Si Salem"
                    },
                    {
                        "name": "Nicola Piovesan"
                    },
                    {
                        "name": "Fadhel Ayed"
                    }
                ],
                "author_detail": {
                    "name": "Fadhel Ayed"
                },
                "author": "Fadhel Ayed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09758v2",
                "updated": "2025-09-05T10:39:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    39,
                    3,
                    4,
                    248,
                    0
                ],
                "published": "2025-06-11T14:03:13Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "title": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems"
                },
                "summary": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs."
                },
                "authors": [
                    {
                        "name": "Zikai Liu"
                    },
                    {
                        "name": "Jasmin Schult"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3725783.3764403",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725783.3764403",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.09758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera-ready authors' version for APSys'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04377v1",
                "updated": "2025-09-04T16:40:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T16:40:01Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "title": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference"
                },
                "summary": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks."
                },
                "authors": [
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Jie Ye"
                    },
                    {
                        "name": "Xian-He Sun"
                    },
                    {
                        "name": "Anthony Kougkas"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12084v2",
                "updated": "2025-09-04T15:21:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    21,
                    11,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-21T12:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "title": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis"
                },
                "summary": "This study presents a comprehensive multi-level analysis of the NVIDIA Hopper\nGPU architecture, focusing on its performance characteristics and novel\nfeatures. We benchmark Hopper's memory subsystem, highlighting improvements in\nthe L2 partitioned cache and global memory access compared to Ampere and Ada\nLovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the\nbenefits of FP8 precision and asynchronous wgmma instructions for matrix\noperations. Additionally, we investigate the performance of DPX instructions\nfor dynamic programming, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. Through multi-level evaluation, we discover that the Hopper\narchitecture demonstrates significant acceleration potential in real-world\napplications. For instance, the asynchronous programming model supported by TMA\nachieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double\nthe performance of FP16, and DPX instructions accelerate a computational\nbiology algorithm by at least 4.75x. Our findings provide actionable insights\nfor optimizing compute-intensive workloads, from AI training to bioinformatics,\non Hopper GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a comprehensive multi-level analysis of the NVIDIA Hopper\nGPU architecture, focusing on its performance characteristics and novel\nfeatures. We benchmark Hopper's memory subsystem, highlighting improvements in\nthe L2 partitioned cache and global memory access compared to Ampere and Ada\nLovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the\nbenefits of FP8 precision and asynchronous wgmma instructions for matrix\noperations. Additionally, we investigate the performance of DPX instructions\nfor dynamic programming, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. Through multi-level evaluation, we discover that the Hopper\narchitecture demonstrates significant acceleration potential in real-world\napplications. For instance, the asynchronous programming model supported by TMA\nachieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double\nthe performance of FP16, and DPX instructions accelerate a computational\nbiology algorithm by at least 4.75x. Our findings provide actionable insights\nfor optimizing compute-intensive workloads, from AI training to bioinformatics,\non Hopper GPUs."
                },
                "authors": [
                    {
                        "name": "Weile Luo"
                    },
                    {
                        "name": "Ruibo Fan"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Hongyuan Liu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2402.13499",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04185v1",
                "updated": "2025-09-04T13:02:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T13:02:39Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "title": "Set Block Decoding is a Language Model Inference Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Set Block Decoding is a Language Model Inference Accelerator"
                },
                "summary": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training."
                },
                "authors": [
                    {
                        "name": "Itai Gat"
                    },
                    {
                        "name": "Heli Ben-Hamu"
                    },
                    {
                        "name": "Marton Havasi"
                    },
                    {
                        "name": "Daniel Haziza"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    },
                    {
                        "name": "David Lopez-Paz"
                    },
                    {
                        "name": "Brian Karrer"
                    },
                    {
                        "name": "Yaron Lipman"
                    }
                ],
                "author_detail": {
                    "name": "Yaron Lipman"
                },
                "author": "Yaron Lipman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04180v1",
                "updated": "2025-09-04T12:54:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    54,
                    32,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T12:54:32Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    54,
                    32,
                    3,
                    247,
                    0
                ],
                "title": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision"
                },
                "summary": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}."
                },
                "authors": [
                    {
                        "name": "Safouane El Ghazouali"
                    },
                    {
                        "name": "Umberto Michelucci"
                    }
                ],
                "author_detail": {
                    "name": "Umberto Michelucci"
                },
                "author": "Umberto Michelucci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04010v1",
                "updated": "2025-09-04T08:41:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    41,
                    6,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T08:41:06Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    41,
                    6,
                    3,
                    247,
                    0
                ],
                "title": "Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and\n  Lessons Learned",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and\n  Lessons Learned"
                },
                "summary": "The PQDSS standardization process requires cryptographic primitives to be\nfree from vulnerabilities, including timing and cache side-channels. Resistance\nto timing leakage is therefore an essential property, and achieving this\ntypically relies on software implementations that follow constant-time\nprinciples. Moreover, ensuring that all implementations are constant-time is\ncrucial for fair performance comparisons, as secure implementations often incur\nadditional overhead. Such analysis also helps identify scheme proposals that\nare inherently difficult to implement in constant time. Because constant-time\nproperties can be broken during compilation, it is often necessary to analyze\nthe compiled binary directly. Since manual binary analysis is extremely\nchallenging, automated analysis becomes highly important. Although several\ntools exist to assist with such analysis, they often have usability limitations\nand are difficult to set up correctly. To support the developers besides the\nNIST committee in verifying candidates, we developed a toolchain that automates\nconfiguration, execution, and result analysis for several widely used\nconstant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify\nconstant-time policy compliance at the binary level, and dudect and RTLF to\ndetect side-channel vulnerabilities through statistical analysis of execution\ntime behavior. We demonstrate its effectiveness and practicability by\nevaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26\nissues in total to the respective developers, and 5 of them have already been\nfixed. We also discuss our different findings, as well as the benefits of\nshortcomings of the different tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The PQDSS standardization process requires cryptographic primitives to be\nfree from vulnerabilities, including timing and cache side-channels. Resistance\nto timing leakage is therefore an essential property, and achieving this\ntypically relies on software implementations that follow constant-time\nprinciples. Moreover, ensuring that all implementations are constant-time is\ncrucial for fair performance comparisons, as secure implementations often incur\nadditional overhead. Such analysis also helps identify scheme proposals that\nare inherently difficult to implement in constant time. Because constant-time\nproperties can be broken during compilation, it is often necessary to analyze\nthe compiled binary directly. Since manual binary analysis is extremely\nchallenging, automated analysis becomes highly important. Although several\ntools exist to assist with such analysis, they often have usability limitations\nand are difficult to set up correctly. To support the developers besides the\nNIST committee in verifying candidates, we developed a toolchain that automates\nconfiguration, execution, and result analysis for several widely used\nconstant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify\nconstant-time policy compliance at the binary level, and dudect and RTLF to\ndetect side-channel vulnerabilities through statistical analysis of execution\ntime behavior. We demonstrate its effectiveness and practicability by\nevaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26\nissues in total to the respective developers, and 5 of them have already been\nfixed. We also discuss our different findings, as well as the benefits of\nshortcomings of the different tools."
                },
                "authors": [
                    {
                        "name": "Olivier Adjonyo"
                    },
                    {
                        "name": "Sebastien Bardin"
                    },
                    {
                        "name": "Emanuele Bellini"
                    },
                    {
                        "name": "Gilbert Ndollane Dione"
                    },
                    {
                        "name": "Mahmudul Faisal Al Ameen"
                    },
                    {
                        "name": "Robert Merget"
                    },
                    {
                        "name": "Frederic Recoules"
                    },
                    {
                        "name": "Yanis Sellami"
                    }
                ],
                "author_detail": {
                    "name": "Yanis Sellami"
                },
                "author": "Yanis Sellami",
                "arxiv_comment": "20 pages, 1 figure, to be published and presented at Sixth PQC\n  Standardization Conference by NIST, partially supported by the \"France 2030\"\n  government investment plan managed by the French National Research Agency,\n  under the reference ANR-22-PECY-0005",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v3",
                "updated": "2025-09-04T06:20:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    6,
                    20,
                    55,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "IC-Cache: Efficient Large Language Model Serving via In-context Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IC-Cache: Efficient Large Language Model Serving via In-context Caching"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 70% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge transfer among requests. However, naively caching and reusing past\nresponses leads to a big quality drop. In this paper, we introduce IC-Cache, a\ncaching system that enables live LLM capability augmentation to improve serving\nefficiency: by leveraging historical request-response pairs from larger models\nas in-context examples, IC-Cache empowers small LLMs to imitate and even exceed\nthe compositional abilities (e.g., reasoning) of their larger counterparts,\nenabling selective offloading of requests to reduce cost and latency. Achieving\nthis live augmentation at scale introduces intricate trade-offs between\nresponse quality, latency, and system throughput. For a new request, IC-Cache\nefficiently selects similar, high-utility examples to prepend them to the new\nrequest's input. At scale, it adaptively routes requests across LLMs of varying\ncapabilities, accounting for response quality and serving loads. IC-Cache\nemploys a cost-aware cache replay mechanism that refines example quality\noffline to maximize online cache utility and efficiency. Evaluations on\nmillions of realistic requests demonstrate that IC-Cache improves LLM serving\nthroughput by 1.4-5.9x and reduces latency by 28-71% without hurting response\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 70% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge transfer among requests. However, naively caching and reusing past\nresponses leads to a big quality drop. In this paper, we introduce IC-Cache, a\ncaching system that enables live LLM capability augmentation to improve serving\nefficiency: by leveraging historical request-response pairs from larger models\nas in-context examples, IC-Cache empowers small LLMs to imitate and even exceed\nthe compositional abilities (e.g., reasoning) of their larger counterparts,\nenabling selective offloading of requests to reduce cost and latency. Achieving\nthis live augmentation at scale introduces intricate trade-offs between\nresponse quality, latency, and system throughput. For a new request, IC-Cache\nefficiently selects similar, high-utility examples to prepend them to the new\nrequest's input. At scale, it adaptively routes requests across LLMs of varying\ncapabilities, accounting for response quality and serving loads. IC-Cache\nemploys a cost-aware cache replay mechanism that refines example quality\noffline to maximize online cache utility and efficiency. Evaluations on\nmillions of realistic requests demonstrate that IC-Cache improves LLM serving\nthroughput by 1.4-5.9x and reduces latency by 28-71% without hurting response\nquality."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Lillian Tsai"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "arxiv_doi": "10.1145/3731569.3764829",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731569.3764829",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.12689v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01228v2",
                "updated": "2025-09-03T20:54:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    20,
                    54,
                    57,
                    2,
                    246,
                    0
                ],
                "published": "2024-10-02T04:12:13Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    4,
                    12,
                    13,
                    2,
                    276,
                    0
                ],
                "title": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline\n  Co-Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline\n  Co-Serving"
                },
                "summary": "Large language model (LLM) serving demands low latency and high throughput,\nbut high load variability makes it challenging to achieve high GPU utilization.\nIn this paper, we identify a synergetic but overlooked opportunity to co-serve\nlatency-critical online requests alongside latency-tolerant offline tasks such\nas model benchmarking. While promising, existing serving systems fail to\nco-serve them efficiently, as their coarse-grained resource management at the\nrequest or iteration level cannot harvest millisecond-level GPU idle cycles\nwithout introducing interference that violates online latency objectives.\nConServe is a new LLM co-serving system that achieves high throughput and\nstrong online latency guarantees by managing resources at finer granularities.\nConServe introduces three techniques: (1) a latency-aware token-level scheduler\nthat precisely sizes offline batches and tokens to fit within online latency\nobjectives; (2) sub-iteration, layer-wise preemption that allows offline tasks\nto yield to online load spikes; and (3) incremental KV cache management that\nenables preempting and resuming offline requests at near-zero cost. Evaluations\nwith Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe\ndelivers an average of 2.2$\\times$ higher throughput and reduces online serving\ntail latency by 2.9$\\times$ on average compared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving demands low latency and high throughput,\nbut high load variability makes it challenging to achieve high GPU utilization.\nIn this paper, we identify a synergetic but overlooked opportunity to co-serve\nlatency-critical online requests alongside latency-tolerant offline tasks such\nas model benchmarking. While promising, existing serving systems fail to\nco-serve them efficiently, as their coarse-grained resource management at the\nrequest or iteration level cannot harvest millisecond-level GPU idle cycles\nwithout introducing interference that violates online latency objectives.\nConServe is a new LLM co-serving system that achieves high throughput and\nstrong online latency guarantees by managing resources at finer granularities.\nConServe introduces three techniques: (1) a latency-aware token-level scheduler\nthat precisely sizes offline batches and tokens to fit within online latency\nobjectives; (2) sub-iteration, layer-wise preemption that allows offline tasks\nto yield to online load spikes; and (3) incremental KV cache management that\nenables preempting and resuming offline requests at near-zero cost. Evaluations\nwith Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe\ndelivers an average of 2.2$\\times$ higher throughput and reduces online serving\ntail latency by 2.9$\\times$ on average compared to state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Yifan Qiao"
                    },
                    {
                        "name": "Shu Anzai"
                    },
                    {
                        "name": "Shan Yu"
                    },
                    {
                        "name": "Haoran Ma"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Miryung Kim"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Harry Xu"
                    }
                ],
                "author_detail": {
                    "name": "Harry Xu"
                },
                "author": "Harry Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03394v1",
                "updated": "2025-09-03T15:15:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T15:15:44Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "title": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload"
                },
                "summary": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%."
                },
                "authors": [
                    {
                        "name": "Amirhossein Shahbazinia"
                    },
                    {
                        "name": "Darong Huang"
                    },
                    {
                        "name": "Luis Costero"
                    },
                    {
                        "name": "David Atienza"
                    }
                ],
                "author_detail": {
                    "name": "David Atienza"
                },
                "author": "David Atienza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00079v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00079v4",
                "updated": "2025-09-03T14:56:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    56,
                    29,
                    2,
                    246,
                    0
                ],
                "published": "2024-06-24T02:05:32Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    2,
                    5,
                    32,
                    0,
                    176,
                    0
                ],
                "title": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving"
                },
                "summary": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests."
                },
                "authors": [
                    {
                        "name": "Ruoyu Qin"
                    },
                    {
                        "name": "Zheming Li"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Weimin Zheng"
                    },
                    {
                        "name": "Xinran Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xinran Xu"
                },
                "author": "Xinran Xu",
                "arxiv_comment": "23 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00079v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00079v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04416v2",
                "updated": "2025-09-03T14:28:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    28,
                    23,
                    2,
                    246,
                    0
                ],
                "published": "2025-07-06T15:08:49Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "title": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling"
                },
                "summary": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT."
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Anunay Yadav"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03560v1",
                "updated": "2025-09-03T11:23:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    23,
                    35,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T11:23:35Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    23,
                    35,
                    2,
                    246,
                    0
                ],
                "title": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine\n  Hybrid Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine\n  Hybrid Systems"
                },
                "summary": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits."
                },
                "authors": [
                    {
                        "name": "Atanu Kundu"
                    },
                    {
                        "name": "Pratyay Sarkar"
                    },
                    {
                        "name": "Rajarshi Ray"
                    }
                ],
                "author_detail": {
                    "name": "Rajarshi Ray"
                },
                "author": "Rajarshi Ray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03136v1",
                "updated": "2025-09-03T08:38:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T08:38:40Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "title": "Adaptive KV-Cache Compression without Manually Setting Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive KV-Cache Compression without Manually Setting Budget"
                },
                "summary": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable."
                },
                "authors": [
                    {
                        "name": "Chenxia Tang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20353v2",
                "updated": "2025-09-03T06:56:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    6,
                    56,
                    21,
                    2,
                    246,
                    0
                ],
                "published": "2025-05-26T05:58:49Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "title": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation"
                },
                "summary": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v2",
                "updated": "2025-09-02T18:10:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    18,
                    10,
                    0,
                    1,
                    245,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Yuchen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Liu"
                },
                "author": "Yuchen Liu",
                "arxiv_comment": "Under Major Revision in IEEE Network",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02532v1",
                "updated": "2025-09-02T17:35:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    17,
                    35,
                    42,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T17:35:42Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    17,
                    35,
                    42,
                    1,
                    245,
                    0
                ],
                "title": "A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device\n  Networks"
                },
                "summary": "Device-to-device (D2D) communication is one of the most promising techniques\nfor future wireless cellular communication systems. This paper considers coded\ncaching in a partially cooperative wireless D2D network, where only a subset of\nusers transmit during delivery, while all users request files. The\nnon-transmitting users are referred to as selfish users. All existing schemes\nthat do not require knowledge of the identity of selfish users before content\nplacement are limited to the high-memory regime, particularly when the number\nof selfish users is large. We propose a novel coded caching scheme for a\npartially cooperative D2D network that operates in all feasible memory regimes,\nregardless of the number of selfish users. We also derive a lower bound on the\ntransmission load of a partially cooperative D2D coded caching scheme. Using\nthis bound, the proposed scheme is shown to be optimal in the high-memory\nregime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Device-to-device (D2D) communication is one of the most promising techniques\nfor future wireless cellular communication systems. This paper considers coded\ncaching in a partially cooperative wireless D2D network, where only a subset of\nusers transmit during delivery, while all users request files. The\nnon-transmitting users are referred to as selfish users. All existing schemes\nthat do not require knowledge of the identity of selfish users before content\nplacement are limited to the high-memory regime, particularly when the number\nof selfish users is large. We propose a novel coded caching scheme for a\npartially cooperative D2D network that operates in all feasible memory regimes,\nregardless of the number of selfish users. We also derive a lower bound on the\ntransmission load of a partially cooperative D2D coded caching scheme. Using\nthis bound, the proposed scheme is shown to be optimal in the high-memory\nregime."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "7 pages and 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v5",
                "updated": "2025-09-02T16:39:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    39,
                    56,
                    1,
                    245,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. However, existing Ethernet-based solutions, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilization due to both increasing traffic demands and\nthe expanding scale of datacenter topologies, which also exacerbate network\nfailures. To address these limitations, we propose REPS, a lightweight\ndecentralized per-packet adaptive load balancing algorithm designed to optimize\nnetwork utilization while ensuring rapid recovery from link failures. REPS\nadapts to network conditions by caching good-performing paths. In case of a\nnetwork failure, REPS re-routes traffic away from it in less than 100\nmicroseconds. REPS is designed to be deployed with next-generation out-of-order\ntransports, such as Ultra Ethernet, and uses less than 25 bytes of\nper-connection state regardless of the topology size. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. However, existing Ethernet-based solutions, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilization due to both increasing traffic demands and\nthe expanding scale of datacenter topologies, which also exacerbate network\nfailures. To address these limitations, we propose REPS, a lightweight\ndecentralized per-packet adaptive load balancing algorithm designed to optimize\nnetwork utilization while ensuring rapid recovery from link failures. REPS\nadapts to network conditions by caching good-performing paths. In case of a\nnetwork failure, REPS re-routes traffic away from it in less than 100\nmicroseconds. REPS is designed to be deployed with next-generation out-of-order\ntransports, such as Ultra Ethernet, and uses less than 25 bytes of\nper-connection state regardless of the topology size. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02480v1",
                "updated": "2025-09-02T16:30:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    49,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T16:30:49Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    49,
                    1,
                    245,
                    0
                ],
                "title": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\n  Break the GPU Memory Wall",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\n  Break the GPU Memory Wall"
                },
                "summary": "Training LLMs larger than the aggregated memory of multiple GPUs is\nincreasingly necessary due to the faster growth of LLM sizes compared to GPU\nmemory. To this end, multi-tier host memory or disk offloading techniques are\nproposed by state of art. Despite advanced asynchronous multi-tier read/write\nstrategies, such offloading strategies result in significant I/O overheads in\nthe critical path of training, resulting in slower iterations. To this end, we\npropose MLP-Offload, a novel multi-level, multi-path offloading engine\nspecifically designed for optimizing LLM training on resource-constrained\nsetups by mitigating I/O bottlenecks. We make several key observations that\ndrive the design of MLP-Offload, such as I/O overheads during the update\ndominate the iteration time; I/O bandwidth of the third-level remote storage\ntier remains unutilized; and, contention due to concurrent offloading amplifies\nI/O bottlenecks. Driven by these insights, we design and implement MLP-Offload\nto offload the optimizer states across multiple tiers in a cache-efficient and\nconcurrency-controlled fashion to mitigate I/O bottlenecks during the backward\nand update phases. Evaluations on models up to 280B parameters shows that\nMLP-Offload achieves 2.5$\\times$ faster iterations compared to the\nstate-of-the-art LLM training runtimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLMs larger than the aggregated memory of multiple GPUs is\nincreasingly necessary due to the faster growth of LLM sizes compared to GPU\nmemory. To this end, multi-tier host memory or disk offloading techniques are\nproposed by state of art. Despite advanced asynchronous multi-tier read/write\nstrategies, such offloading strategies result in significant I/O overheads in\nthe critical path of training, resulting in slower iterations. To this end, we\npropose MLP-Offload, a novel multi-level, multi-path offloading engine\nspecifically designed for optimizing LLM training on resource-constrained\nsetups by mitigating I/O bottlenecks. We make several key observations that\ndrive the design of MLP-Offload, such as I/O overheads during the update\ndominate the iteration time; I/O bandwidth of the third-level remote storage\ntier remains unutilized; and, contention due to concurrent offloading amplifies\nI/O bottlenecks. Driven by these insights, we design and implement MLP-Offload\nto offload the optimizer states across multiple tiers in a cache-efficient and\nconcurrency-controlled fashion to mitigate I/O bottlenecks during the backward\nand update phases. Evaluations on models up to 280B parameters shows that\nMLP-Offload achieves 2.5$\\times$ faster iterations compared to the\nstate-of-the-art LLM training runtimes."
                },
                "authors": [
                    {
                        "name": "Avinash Maurya"
                    },
                    {
                        "name": "M. Mustafa Rafique"
                    },
                    {
                        "name": "Franck Cappello"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_doi": "10.1145/3712285.3759864",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3712285.3759864",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SC'25: The International Conference for High Performance Computing,\n  Networking, Storage and Analysis",
                "arxiv_journal_ref": "SC'25: The International Conference for High Performance\n  Computing, Networking, Storage and Analysis, 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.0; E.2; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02408v1",
                "updated": "2025-09-02T15:19:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    15,
                    19,
                    6,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T15:19:06Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    15,
                    19,
                    6,
                    1,
                    245,
                    0
                ],
                "title": "Cache Management for Mixture-of-Experts LLMs -- extended version",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Management for Mixture-of-Experts LLMs -- extended version"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks. One of the main challenges towards the successful\ndeployment of LLMs is memory management, since they typically involve billions\nof parameters. To this end, architectures based on Mixture-of-Experts have been\nproposed, which aim to reduce the size of the parameters that are activated\nwhen producing a token. This raises the equally critical issue of efficiently\nmanaging the limited cache of the system, in that frequently used experts\nshould be stored in the fast cache rather than in the slower secondary memory.\n  In this work, we introduce and study a new paging problem that models expert\nmanagement optimization. Our formulation captures both the layered architecture\nof LLMs and the requirement that experts are cached efficiently. We first\npresent lower bounds on the competitive ratio of both deterministic and\nrandomized algorithms, which show that under mild assumptions, LRU-like\npolicies have good theoretical competitive performance. We then propose a\nlayer-based extension of LRU that is tailored to the problem at hand.\n  Extensive simulations on both synthetic datasets and actual traces of MoE\nusage show that our algorithm outperforms policies for the classic paging\nproblem, such as the standard LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks. One of the main challenges towards the successful\ndeployment of LLMs is memory management, since they typically involve billions\nof parameters. To this end, architectures based on Mixture-of-Experts have been\nproposed, which aim to reduce the size of the parameters that are activated\nwhen producing a token. This raises the equally critical issue of efficiently\nmanaging the limited cache of the system, in that frequently used experts\nshould be stored in the fast cache rather than in the slower secondary memory.\n  In this work, we introduce and study a new paging problem that models expert\nmanagement optimization. Our formulation captures both the layered architecture\nof LLMs and the requirement that experts are cached efficiently. We first\npresent lower bounds on the competitive ratio of both deterministic and\nrandomized algorithms, which show that under mild assumptions, LRU-like\npolicies have good theoretical competitive performance. We then propose a\nlayer-based extension of LRU that is tailored to the problem at hand.\n  Extensive simulations on both synthetic datasets and actual traces of MoE\nusage show that our algorithm outperforms policies for the classic paging\nproblem, such as the standard LRU."
                },
                "authors": [
                    {
                        "name": "Spyros Angelopoulos"
                    },
                    {
                        "name": "Loris Marchal"
                    },
                    {
                        "name": "Adrien Obrecht"
                    },
                    {
                        "name": "Bertrand Simon"
                    }
                ],
                "author_detail": {
                    "name": "Bertrand Simon"
                },
                "author": "Bertrand Simon",
                "arxiv_doi": "10.1007/978-3-031-99872-0_2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-99872-0_2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v2",
                "updated": "2025-09-02T13:09:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    13,
                    9,
                    37,
                    1,
                    245,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing reliance on expensive vector\ndatabase lookups. To scale efficiently, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically skewed MedRAG workload reduces database calls by 78.9% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our work highlights that approximate caching\nis a viable and effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing reliance on expensive vector\ndatabase lookups. To scale efficiently, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically skewed MedRAG workload reduces database calls by 78.9% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our work highlights that approximate caching\nis a viable and effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Zhang Ji"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_doi": "10.1145/3721146.3721941",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721941",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02232v1",
                "updated": "2025-09-02T11:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    58,
                    6,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T11:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    58,
                    6,
                    1,
                    245,
                    0
                ],
                "title": "Efficient Geometry Compression and Communication for 3D Gaussian\n  Splatting Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Geometry Compression and Communication for 3D Gaussian\n  Splatting Point Clouds"
                },
                "summary": "Storage and transmission challenges in dynamic 3D scene representation based\non the i3DV platform, With increasing scene complexity, the explosive growth of\n3D Gaussian data volume causes excessive storage space occupancy. To address\nthis issue, we propose adopting the AVS PCRM reference software for efficient\ncompression of Gaussian point cloud geometry data. The strategy deeply\nintegrates the advanced encoding capabilities of AVS PCRM into the i3DV\nplatform, forming technical complementarity with the original rate-distortion\noptimization mechanism based on binary hash tables. On one hand, the hash table\nefficiently caches inter-frame Gaussian point transformation relationships,\nwhich allows for high-fidelity transmission within a 40 Mbps bandwidth\nconstraint. On the other hand, AVS PCRM performs precise compression on\ngeometry data. Experimental results demonstrate that the joint framework\nmaintains the advantages of fast rendering and high-quality synthesis in 3D\nGaussian technology while achieving significant 10\\%-25\\% bitrate savings on\nuniversal test sets. It provides a superior rate-distortion tradeoff solution\nfor the storage, transmission, and interaction of 3D volumetric video.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Storage and transmission challenges in dynamic 3D scene representation based\non the i3DV platform, With increasing scene complexity, the explosive growth of\n3D Gaussian data volume causes excessive storage space occupancy. To address\nthis issue, we propose adopting the AVS PCRM reference software for efficient\ncompression of Gaussian point cloud geometry data. The strategy deeply\nintegrates the advanced encoding capabilities of AVS PCRM into the i3DV\nplatform, forming technical complementarity with the original rate-distortion\noptimization mechanism based on binary hash tables. On one hand, the hash table\nefficiently caches inter-frame Gaussian point transformation relationships,\nwhich allows for high-fidelity transmission within a 40 Mbps bandwidth\nconstraint. On the other hand, AVS PCRM performs precise compression on\ngeometry data. Experimental results demonstrate that the joint framework\nmaintains the advantages of fast rendering and high-quality synthesis in 3D\nGaussian technology while achieving significant 10\\%-25\\% bitrate savings on\nuniversal test sets. It provides a superior rate-distortion tradeoff solution\nfor the storage, transmission, and interaction of 3D volumetric video."
                },
                "authors": [
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Yanting Li"
                    },
                    {
                        "name": "Luyang Tang"
                    },
                    {
                        "name": "Wei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Gao"
                },
                "author": "Wei Gao",
                "arxiv_doi": "10.1145/3680207.3765659",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3680207.3765659",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages,5 figures",
                "arxiv_journal_ref": "ACM MOBICOM 2025",
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15212v2",
                "updated": "2025-09-02T11:29:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    29,
                    34,
                    1,
                    245,
                    0
                ],
                "published": "2025-08-21T03:48:28Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning"
                },
                "summary": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Guanchen Li"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02121v1",
                "updated": "2025-09-02T09:17:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    9,
                    17,
                    40,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T09:17:40Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    9,
                    17,
                    40,
                    1,
                    245,
                    0
                ],
                "title": "Batch Query Processing and Optimization for Agentic Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch Query Processing and Optimization for Agentic Workflows"
                },
                "summary": "Large Language Models (LLMs) in agentic workflows combine multi-step\nreasoning, tool use, and collaboration across multiple specialized agents.\nExisting LLM serving engines optimize individual calls in isolation, while\nmulti-agent frameworks focus on orchestration without system-level performance\nplanning. As a result, repeated prompts, overlapping contexts, and concurrent\nexecutions create substantial redundancy and poor GPU utilization, especially\nin batch analytics scenarios. We introduce Halo, a system that brings batch\nquery processing and optimization into agentic LLM workflows. Halo represents\neach workflow as a structured query plan DAG and constructs a consolidated\ngraph for batched queries that exposes shared computation. Guided by a cost\nmodel that jointly considers prefill and decode costs, cache reuse, and GPU\nplacement, Halo performs plan-level optimization to minimize redundant\nexecution. Its runtime integrates adaptive batching, KV-cache sharing and\nmigration, along with compute-communication overlap to maximize hardware\nefficiency. Evaluation across six benchmarks shows that Halo achieves up to\n18.6x speedup for batch inference and 4.7x throughput improvement under online\nserving, scaling to workloads of tens of thousands of queries and complex\ngraphs. These gains are achieved without compromising output quality. By\nunifying query optimization with LLM serving, Halo enables efficient agentic\nworkflows in data analytics and decision-making applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) in agentic workflows combine multi-step\nreasoning, tool use, and collaboration across multiple specialized agents.\nExisting LLM serving engines optimize individual calls in isolation, while\nmulti-agent frameworks focus on orchestration without system-level performance\nplanning. As a result, repeated prompts, overlapping contexts, and concurrent\nexecutions create substantial redundancy and poor GPU utilization, especially\nin batch analytics scenarios. We introduce Halo, a system that brings batch\nquery processing and optimization into agentic LLM workflows. Halo represents\neach workflow as a structured query plan DAG and constructs a consolidated\ngraph for batched queries that exposes shared computation. Guided by a cost\nmodel that jointly considers prefill and decode costs, cache reuse, and GPU\nplacement, Halo performs plan-level optimization to minimize redundant\nexecution. Its runtime integrates adaptive batching, KV-cache sharing and\nmigration, along with compute-communication overlap to maximize hardware\nefficiency. Evaluation across six benchmarks shows that Halo achieves up to\n18.6x speedup for batch inference and 4.7x throughput improvement under online\nserving, scaling to workloads of tens of thousands of queries and complex\ngraphs. These gains are achieved without compromising output quality. By\nunifying query optimization with LLM serving, Halo enables efficient agentic\nworkflows in data analytics and decision-making applications."
                },
                "authors": [
                    {
                        "name": "Junyi Shen"
                    },
                    {
                        "name": "Noppanat Wadlom"
                    },
                    {
                        "name": "Yao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Lu"
                },
                "author": "Yao Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02004v1",
                "updated": "2025-09-02T06:40:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    6,
                    40,
                    45,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T06:40:45Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    6,
                    40,
                    45,
                    1,
                    245,
                    0
                ],
                "title": "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data"
                },
                "summary": "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols."
                },
                "authors": [
                    {
                        "name": "Takao Murakami"
                    },
                    {
                        "name": "Yuichi Sei"
                    },
                    {
                        "name": "Reo Eriguchi"
                    }
                ],
                "author_detail": {
                    "name": "Reo Eriguchi"
                },
                "author": "Reo Eriguchi",
                "arxiv_comment": "Full version of the paper accepted at NDSS 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01395v1",
                "updated": "2025-09-01T11:41:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    41,
                    10,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T11:41:10Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    41,
                    10,
                    0,
                    244,
                    0
                ],
                "title": "LLMs cannot spot math errors, even when allowed to peek into the\n  solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs cannot spot math errors, even when allowed to peek into the\n  solution"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance."
                },
                "authors": [
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15779v2",
                "updated": "2025-09-01T07:26:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    7,
                    26,
                    57,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-17T08:12:34Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    12,
                    34,
                    0,
                    48,
                    0
                ],
                "title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer"
                },
                "summary": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP."
                },
                "authors": [
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sumin Song"
                    },
                    {
                        "name": "Woosang Lim"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v3",
                "updated": "2025-09-01T03:51:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    51,
                    9,
                    0,
                    244,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01092v1",
                "updated": "2025-09-01T03:31:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T03:31:44Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "title": "REFRAG: Rethinking RAG based Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REFRAG: Rethinking RAG based Decoding"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes."
                },
                "authors": [
                    {
                        "name": "Xiaoqiang Lin"
                    },
                    {
                        "name": "Aritra Ghosh"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Anshumali Shrivastava"
                    },
                    {
                        "name": "Vijai Mohan"
                    }
                ],
                "author_detail": {
                    "name": "Vijai Mohan"
                },
                "author": "Vijai Mohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06133v2",
                "updated": "2025-08-31T15:09:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    15,
                    9,
                    36,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-08T08:54:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "title": "LLM Serving Optimization with Variable Prefill and Decode Lengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Serving Optimization with Variable Prefill and Decode Lengths"
                },
                "summary": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Meixuan Wang"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00883v1",
                "updated": "2025-08-31T14:51:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    51,
                    19,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-31T14:51:19Z",
                "published_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    51,
                    19,
                    6,
                    243,
                    0
                ],
                "title": "Accelerating Latency-Critical Applications with AI-Powered\n  Semi-Automatic Fine-Grained Parallelization on SMT Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Latency-Critical Applications with AI-Powered\n  Semi-Automatic Fine-Grained Parallelization on SMT Processors"
                },
                "summary": "Latency-critical applications tend to show low utilization of functional\nunits due to frequent cache misses and mispredictions during speculative\nexecution in high-performance superscalar processors. However, due to\nsignificant impact on single-thread performance, Simultaneous Multithreading\n(SMT) technology is rarely used with heavy threads of latency-critical\napplications. In this paper, we explore utilization of SMT technology to\nsupport fine-grained parallelization of latency-critical applications.\nFollowing the advancements in the development of Large Language Models (LLMs),\nwe introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we\nextend AI Coding Agent in Cursor IDE with additional tools connected through\nModel Context Protocol, enabling end-to-end AI Agent for parallelization.\nAdditional connected tools enable LLM-guided hotspot detection, collection of\ndynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance\nsimulation to estimate performance gains. We apply Aira with Relic parallel\nframework for fine-grained task parallelism on SMT cores to parallelize\nlatency-critical benchmarks representing real-world applications used in\nindustry. We show 17% geomean performance gain from parallelization of\nlatency-critical benchmarks using Aira with Relic framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency-critical applications tend to show low utilization of functional\nunits due to frequent cache misses and mispredictions during speculative\nexecution in high-performance superscalar processors. However, due to\nsignificant impact on single-thread performance, Simultaneous Multithreading\n(SMT) technology is rarely used with heavy threads of latency-critical\napplications. In this paper, we explore utilization of SMT technology to\nsupport fine-grained parallelization of latency-critical applications.\nFollowing the advancements in the development of Large Language Models (LLMs),\nwe introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we\nextend AI Coding Agent in Cursor IDE with additional tools connected through\nModel Context Protocol, enabling end-to-end AI Agent for parallelization.\nAdditional connected tools enable LLM-guided hotspot detection, collection of\ndynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance\nsimulation to estimate performance gains. We apply Aira with Relic parallel\nframework for fine-grained task parallelism on SMT cores to parallelize\nlatency-critical benchmarks representing real-world applications used in\nindustry. We show 17% geomean performance gain from parallelization of\nlatency-critical benchmarks using Aira with Relic framework."
                },
                "authors": [
                    {
                        "name": "Denis Los"
                    },
                    {
                        "name": "Igor Petushkov"
                    }
                ],
                "author_detail": {
                    "name": "Igor Petushkov"
                },
                "author": "Igor Petushkov",
                "arxiv_journal_ref": "International Journal of Open Information Technologies, vol. 13,\n  no. 9, pp. 129-134, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10431v3",
                "updated": "2025-08-31T05:43:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    5,
                    43,
                    55,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-14T08:04:15Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    4,
                    15,
                    3,
                    226,
                    0
                ],
                "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches"
                },
                "summary": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE."
                },
                "authors": [
                    {
                        "name": "Chris Cao"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "This version includes updated analysis of RCO Bugs (one additional\n  bug identified). Appendix added with code snippets for bug fixes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00625v1",
                "updated": "2025-08-30T22:47:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T22:47:15Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "title": "NetGent: Agent-Based Automation of Network Application Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NetGent: Agent-Based Automation of Network Application Workflows"
                },
                "summary": "We present NetGent, an AI-agent framework for automating complex application\nworkflows to generate realistic network traffic datasets. Developing\ngeneralizable ML models for networking requires data collection from network\nenvironments with traffic that results from a diverse set of real-world web\napplications. However, using existing browser automation tools that are\ndiverse, repeatable, realistic, and efficient remains fragile and costly.\nNetGent addresses this challenge by allowing users to specify workflows as\nnatural-language rules that define state-dependent actions. These abstract\nspecifications are compiled into nondeterministic finite automata (NFAs), which\na state synthesis component translates into reusable, executable code. This\ndesign enables deterministic replay, reduces redundant LLM calls through state\ncaching, and adapts quickly when application interfaces change. In experiments,\nNetGent automated more than 50+ workflows spanning video-on-demand streaming,\nlive video streaming, video conferencing, social media, and web scraping,\nproducing realistic traffic traces while remaining robust to UI variability. By\ncombining the flexibility of language-based agents with the reliability of\ncompiled execution, NetGent provides a scalable foundation for generating the\ndiverse, repeatable datasets needed to advance ML in networking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present NetGent, an AI-agent framework for automating complex application\nworkflows to generate realistic network traffic datasets. Developing\ngeneralizable ML models for networking requires data collection from network\nenvironments with traffic that results from a diverse set of real-world web\napplications. However, using existing browser automation tools that are\ndiverse, repeatable, realistic, and efficient remains fragile and costly.\nNetGent addresses this challenge by allowing users to specify workflows as\nnatural-language rules that define state-dependent actions. These abstract\nspecifications are compiled into nondeterministic finite automata (NFAs), which\na state synthesis component translates into reusable, executable code. This\ndesign enables deterministic replay, reduces redundant LLM calls through state\ncaching, and adapts quickly when application interfaces change. In experiments,\nNetGent automated more than 50+ workflows spanning video-on-demand streaming,\nlive video streaming, video conferencing, social media, and web scraping,\nproducing realistic traffic traces while remaining robust to UI variability. By\ncombining the flexibility of language-based agents with the reliability of\ncompiled execution, NetGent provides a scalable foundation for generating the\ndiverse, repeatable datasets needed to advance ML in networking."
                },
                "authors": [
                    {
                        "name": "Jaber Daneshamooz"
                    },
                    {
                        "name": "Eugene Vuong"
                    },
                    {
                        "name": "Laasya Koduru"
                    },
                    {
                        "name": "Sanjay Chandrasekaran"
                    },
                    {
                        "name": "Arpit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Arpit Gupta"
                },
                "author": "Arpit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00579v1",
                "updated": "2025-08-30T18:25:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    18,
                    25,
                    19,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T18:25:19Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    18,
                    25,
                    19,
                    5,
                    242,
                    0
                ],
                "title": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for\n  KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for\n  KV Cache"
                },
                "summary": "Transformer-based large language models (LLMs) demonstrate impressive\npotential in various practical applications. However, long context inference\nposes a significant challenge due to the enormous memory requirements of the\nkey-value (KV) cache, which can scale to multiple gigabytes as sequence length\nand batch size increase. In this paper, we present KVComp, a generic and\nefficient KV cache management framework optimized for long-text generation that\nsynergistically works with both latency-critical and throughput-critical\ninference systems. KVComp employs novel lossy compression techniques\nspecifically designed for KV cache data characteristics, featuring careful\nco-design of compression algorithms and system architecture. Our approach\nmaintains compatibility with the growing nature of KV cache while preserving\nhigh computational efficiency. Experimental results show that KVComp achieves\non average 47\\% and up to 83\\% higher memory reduction rate compared to\nexisting methods with little/no model accuracy degradation. Furthermore, KVComp\nachieves extremely high execution throughput, effectively reducing\ndecompression overhead and, in some cases, even accelerating the matrix-vector\nmultiplication operation and outperform cuBLAS-based attention kernels with\nless data movement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) demonstrate impressive\npotential in various practical applications. However, long context inference\nposes a significant challenge due to the enormous memory requirements of the\nkey-value (KV) cache, which can scale to multiple gigabytes as sequence length\nand batch size increase. In this paper, we present KVComp, a generic and\nefficient KV cache management framework optimized for long-text generation that\nsynergistically works with both latency-critical and throughput-critical\ninference systems. KVComp employs novel lossy compression techniques\nspecifically designed for KV cache data characteristics, featuring careful\nco-design of compression algorithms and system architecture. Our approach\nmaintains compatibility with the growing nature of KV cache while preserving\nhigh computational efficiency. Experimental results show that KVComp achieves\non average 47\\% and up to 83\\% higher memory reduction rate compared to\nexisting methods with little/no model accuracy degradation. Furthermore, KVComp\nachieves extremely high execution throughput, effectively reducing\ndecompression overhead and, in some cases, even accelerating the matrix-vector\nmultiplication operation and outperform cuBLAS-based attention kernels with\nless data movement."
                },
                "authors": [
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Taolue Yang"
                    },
                    {
                        "name": "Youyuan Liu"
                    },
                    {
                        "name": "Chengming Zhang"
                    },
                    {
                        "name": "Xubin He"
                    },
                    {
                        "name": "Sian Jin"
                    }
                ],
                "author_detail": {
                    "name": "Sian Jin"
                },
                "author": "Sian Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.13777v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.13777v2",
                "updated": "2025-08-30T14:49:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    14,
                    49,
                    34,
                    5,
                    242,
                    0
                ],
                "published": "2023-10-20T19:22:58Z",
                "published_parsed": [
                    2023,
                    10,
                    20,
                    19,
                    22,
                    58,
                    4,
                    293,
                    0
                ],
                "title": "Discrete and Continuous Caching Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete and Continuous Caching Games"
                },
                "summary": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values."
                },
                "authors": [
                    {
                        "name": "Ãron JÃ¡nosik"
                    },
                    {
                        "name": "Csenge MiklÃ³s"
                    },
                    {
                        "name": "DÃ¡niel G. Simon"
                    },
                    {
                        "name": "KristÃ³f ZÃ³lomy"
                    }
                ],
                "author_detail": {
                    "name": "KristÃ³f ZÃ³lomy"
                },
                "author": "KristÃ³f ZÃ³lomy",
                "arxiv_doi": "10.1142/S0219198925500057",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1142/S0219198925500057",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.13777v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.13777v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "International Game Theory Review 27 (3), 2025",
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91A05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v3",
                "updated": "2025-08-30T09:35:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    9,
                    35,
                    22,
                    5,
                    242,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "SOSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00419v1",
                "updated": "2025-08-30T08:57:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    8,
                    57,
                    53,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T08:57:53Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    8,
                    57,
                    53,
                    5,
                    242,
                    0
                ],
                "title": "LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging\n  and KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging\n  and KV Cache Compression"
                },
                "summary": "In this paper, we introduce LightVLM, a simple but effective method that can\nbe seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly\naccelerate the inference process in a training-free manner. We divide the\ninference procedure of VLMs into two stages, i.e., encoding and decoding, and\npropose to simultaneously accelerate VLMs in both stages to largely improve\nmodel efficiency. During encoding, we propose pyramid token merging to reduce\ntokens of different LLM layers in a hierarchical manner by finally only keeping\na few dominant tokens to achieve high efficiency. During decoding, aimed at\nreducing the high latency of outputting long sequences, we propose KV Cache\ncompression to remove unnecessary caches to increase the network throughput.\nExperimental results show that LightVLM successfully retains 100% performance\nwhen only preserving 35% image tokens, and maintains around 98% performance\nwhen keeping only 3% image tokens. LightVLM could 2.02$\\times$ the network\nthroughput and reduce the prefilling time by 3.65$\\times$. LightVLM also makes\nlarge VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to\ninfer faster than significantly smaller models (e.g., InternVL2.5 8B),\nhopefully facilitating the real-world deployment. When generating long text\nsequences (e.g., 4096 tokens), LightVLM could reduce the inference time by\n3.21$\\times$, largely outperforming existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce LightVLM, a simple but effective method that can\nbe seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly\naccelerate the inference process in a training-free manner. We divide the\ninference procedure of VLMs into two stages, i.e., encoding and decoding, and\npropose to simultaneously accelerate VLMs in both stages to largely improve\nmodel efficiency. During encoding, we propose pyramid token merging to reduce\ntokens of different LLM layers in a hierarchical manner by finally only keeping\na few dominant tokens to achieve high efficiency. During decoding, aimed at\nreducing the high latency of outputting long sequences, we propose KV Cache\ncompression to remove unnecessary caches to increase the network throughput.\nExperimental results show that LightVLM successfully retains 100% performance\nwhen only preserving 35% image tokens, and maintains around 98% performance\nwhen keeping only 3% image tokens. LightVLM could 2.02$\\times$ the network\nthroughput and reduce the prefilling time by 3.65$\\times$. LightVLM also makes\nlarge VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to\ninfer faster than significantly smaller models (e.g., InternVL2.5 8B),\nhopefully facilitating the real-world deployment. When generating long text\nsequences (e.g., 4096 tokens), LightVLM could reduce the inference time by\n3.21$\\times$, largely outperforming existing methods."
                },
                "authors": [
                    {
                        "name": "Lianyu Hu"
                    },
                    {
                        "name": "Fanhua Shang"
                    },
                    {
                        "name": "Wei Feng"
                    },
                    {
                        "name": "Liang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wan"
                },
                "author": "Liang Wan",
                "arxiv_comment": "EMNLP2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00388v1",
                "updated": "2025-08-30T06:56:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    6,
                    56,
                    28,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T06:56:28Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    6,
                    56,
                    28,
                    5,
                    242,
                    0
                ],
                "title": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV\n  Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV\n  Cache Eviction"
                },
                "summary": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github."
                },
                "authors": [
                    {
                        "name": "Xuelin Li"
                    },
                    {
                        "name": "Xiangqi Jin"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11435v2",
                "updated": "2025-08-29T20:39:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    20,
                    39,
                    21,
                    4,
                    241,
                    0
                ],
                "published": "2025-04-15T17:51:39Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    51,
                    39,
                    1,
                    105,
                    0
                ],
                "title": "Robust Containment Queries over Collections of Trimmed NURBS Surfaces\n  via Generalized Winding Numbers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Containment Queries over Collections of Trimmed NURBS Surfaces\n  via Generalized Winding Numbers"
                },
                "summary": "We propose a containment query that is robust to the watertightness of\nregions bound by trimmed NURBS surfaces, as this property is difficult to\nguarantee for in-the-wild CAD models. Containment is determined through the\ngeneralized winding number (GWN), a mathematical construction that is\nindifferent to the arrangement of surfaces in the shape. Applying contemporary\ntechniques for the 3D GWN to trimmed NURBS surfaces requires some form of\ngeometric discretization, introducing computational inefficiency to the\nalgorithm and even risking containment misclassifications near the surface. In\ncontrast, our proposed method uses a novel reformulation of the relevant\nsurface integral based on Stokes' theorem, which operates on the boundary and\ntrimming curves as provided through rapidly converging adaptive quadrature.\nBatches of queries are further accelerated by memoizing (i.e.\\ caching and\nreusing) quadrature node positions and tangents as they are evaluated. We\ndemonstrate that our GWN method is robust to complex trimming geometry in a CAD\nmodel, and is accurate up to arbitrary precision at arbitrary distances from\nthe surface. The derived containment query is therefore robust to model\nnon-watertightness while respecting all curved features of the input shape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a containment query that is robust to the watertightness of\nregions bound by trimmed NURBS surfaces, as this property is difficult to\nguarantee for in-the-wild CAD models. Containment is determined through the\ngeneralized winding number (GWN), a mathematical construction that is\nindifferent to the arrangement of surfaces in the shape. Applying contemporary\ntechniques for the 3D GWN to trimmed NURBS surfaces requires some form of\ngeometric discretization, introducing computational inefficiency to the\nalgorithm and even risking containment misclassifications near the surface. In\ncontrast, our proposed method uses a novel reformulation of the relevant\nsurface integral based on Stokes' theorem, which operates on the boundary and\ntrimming curves as provided through rapidly converging adaptive quadrature.\nBatches of queries are further accelerated by memoizing (i.e.\\ caching and\nreusing) quadrature node positions and tangents as they are evaluated. We\ndemonstrate that our GWN method is robust to complex trimming geometry in a CAD\nmodel, and is accurate up to arbitrary precision at arbitrary distances from\nthe surface. The derived containment query is therefore robust to model\nnon-watertightness while respecting all curved features of the input shape."
                },
                "authors": [
                    {
                        "name": "Jacob Spainhour"
                    },
                    {
                        "name": "Kenneth Weiss"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth Weiss"
                },
                "author": "Kenneth Weiss",
                "arxiv_comment": "18 Pages, 16 Figures, 1 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68U05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.3.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00202v1",
                "updated": "2025-08-29T19:23:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    23,
                    35,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T19:23:35Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    23,
                    35,
                    4,
                    241,
                    0
                ],
                "title": "From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer\n  Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer\n  Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive\n  Inference"
                },
                "summary": "Although the Transformer has become the cornerstone of modern AI, its\nautoregressive inference suffers from a linearly growing KV Cache and a\ncomputational complexity of O(N^2 d), severely hindering its ability to process\nultra-long sequences. To overcome this limitation, this paper introduces the\nTConstFormer architecture, building upon our previous work, TLinFormer.\nTConstFormer employs an innovative periodic state update mechanism to achieve a\ntruly constant-size O(1) KV Cache. The computational complexity of this\nmechanism is also O(1) in an amortized sense: it performs purely constant-time\ncomputations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single\nlinear-time global information synchronization only on the $k$-th step.\nTheoretical calculations and experimental results demonstrate that TConstFormer\nexhibits an overwhelming advantage over baseline models in terms of speed,\nmemory efficiency, and overall performance on long-text inference tasks. This\nbreakthrough paves the way for efficient and robust streaming language model\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although the Transformer has become the cornerstone of modern AI, its\nautoregressive inference suffers from a linearly growing KV Cache and a\ncomputational complexity of O(N^2 d), severely hindering its ability to process\nultra-long sequences. To overcome this limitation, this paper introduces the\nTConstFormer architecture, building upon our previous work, TLinFormer.\nTConstFormer employs an innovative periodic state update mechanism to achieve a\ntruly constant-size O(1) KV Cache. The computational complexity of this\nmechanism is also O(1) in an amortized sense: it performs purely constant-time\ncomputations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single\nlinear-time global information synchronization only on the $k$-th step.\nTheoretical calculations and experimental results demonstrate that TConstFormer\nexhibits an overwhelming advantage over baseline models in terms of speed,\nmemory efficiency, and overall performance on long-text inference tasks. This\nbreakthrough paves the way for efficient and robust streaming language model\napplications."
                },
                "authors": [
                    {
                        "name": "Zhongpan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpan Tang"
                },
                "author": "Zhongpan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00195v1",
                "updated": "2025-08-29T19:12:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    12,
                    4,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T19:12:04Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    12,
                    4,
                    4,
                    241,
                    0
                ],
                "title": "Democratizing Agentic AI with Fast Test-Time Scaling on the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Democratizing Agentic AI with Fast Test-Time Scaling on the Edge"
                },
                "summary": "Deploying agentic AI on edge devices is crucial for privacy and\nresponsiveness, but memory constraints typically relegate these systems to\nsmaller Large Language Models (LLMs) with inferior reasoning capabilities.\nTest-Time Scaling (TTS) can bridge this reasoning gap by dedicating more\ncompute during inference, but existing methods incur prohibitive overhead on\nedge hardware. To overcome this, we introduce FlashTTS, a serving system that\nmakes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces\nthree synergistic optimizations: (i) Speculative Beam Extension to mitigate\nsystem stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model\nMemory Allocation to dynamically balance memory between generation and\nverification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache\nreuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on\na single consumer GPU (24 GB) to match the accuracy and latency of large cloud\nmodels. Our evaluation demonstrates that FlashTTS achieves an average 2.2x\nhigher goodput and reduces latency by 38%-68% compared to a vLLM baseline,\npaving the way for democratized, high-performance agentic AI on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying agentic AI on edge devices is crucial for privacy and\nresponsiveness, but memory constraints typically relegate these systems to\nsmaller Large Language Models (LLMs) with inferior reasoning capabilities.\nTest-Time Scaling (TTS) can bridge this reasoning gap by dedicating more\ncompute during inference, but existing methods incur prohibitive overhead on\nedge hardware. To overcome this, we introduce FlashTTS, a serving system that\nmakes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces\nthree synergistic optimizations: (i) Speculative Beam Extension to mitigate\nsystem stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model\nMemory Allocation to dynamically balance memory between generation and\nverification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache\nreuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on\na single consumer GPU (24 GB) to match the accuracy and latency of large cloud\nmodels. Our evaluation demonstrates that FlashTTS achieves an average 2.2x\nhigher goodput and reduces latency by 38%-68% compared to a vLLM baseline,\npaving the way for democratized, high-performance agentic AI on edge devices."
                },
                "authors": [
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Zhiwen Mo"
                    },
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Shuang Liang"
                    },
                    {
                        "name": "Lingxiao Ma"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "author": "Hongxiang Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16217v2",
                "updated": "2025-08-29T18:45:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    18,
                    45,
                    22,
                    4,
                    241,
                    0
                ],
                "published": "2025-07-22T04:21:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Towards Compute-Optimal Many-Shot In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Compute-Optimal Many-Shot In-Context Learning"
                },
                "summary": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL."
                },
                "authors": [
                    {
                        "name": "Shahriar Golchin"
                    },
                    {
                        "name": "Yanfei Chen"
                    },
                    {
                        "name": "Rujun Han"
                    },
                    {
                        "name": "Manan Gandhi"
                    },
                    {
                        "name": "Tianli Yu"
                    },
                    {
                        "name": "Swaroop Mishra"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Chen-Yu Lee"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "arxiv_comment": "Final version; accepted at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05930v2",
                "updated": "2025-08-29T09:58:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    58,
                    17,
                    4,
                    241,
                    0
                ],
                "published": "2025-06-06T09:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    9,
                    55,
                    59,
                    4,
                    157,
                    0
                ],
                "title": "Neural Visibility Cache for Real-Time Light Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Visibility Cache for Real-Time Light Sampling"
                },
                "summary": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR)."
                },
                "authors": [
                    {
                        "name": "Jakub BokÅ¡anskÃ½"
                    },
                    {
                        "name": "Daniel Meister"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Meister"
                },
                "author": "Daniel Meister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15683v2",
                "updated": "2025-08-29T07:40:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    40,
                    34,
                    4,
                    241,
                    0
                ],
                "published": "2025-05-21T15:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "title": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models"
                },
                "summary": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability."
                },
                "authors": [
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Hainan zhang"
                    },
                    {
                        "name": "Weihua Li"
                    },
                    {
                        "name": "Qinnan zhang"
                    },
                    {
                        "name": "jin Dong"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04467v1",
                "updated": "2025-08-29T02:29:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T02:29:52Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "title": "Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode\n  Disaggregation in Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode\n  Disaggregation in Inference"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the default settings, our method\nachieves a 20.56% inference speedup and a 4.95 times reduction in data\ntransmission bandwidth consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the default settings, our method\nachieves a 20.56% inference speedup and a 4.95 times reduction in data\ntransmission bandwidth consumption."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Mengsi Lyu"
                    },
                    {
                        "name": "Yulong Ao"
                    },
                    {
                        "name": "Yonghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yonghua Lin"
                },
                "author": "Yonghua Lin",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12211v1",
                "updated": "2025-08-28T16:17:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    17,
                    18,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T16:17:18Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    17,
                    18,
                    3,
                    240,
                    0
                ],
                "title": "TinyServe: Query-Aware Cache Selection for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyServe: Query-Aware Cache Selection for Efficient LLM Serving"
                },
                "summary": "Serving large language models (LLMs) efficiently remains challenging due to\nthe high memory and latency overhead of key-value (KV) cache access during\nautoregressive decoding. We present \\textbf{TinyServe}, a lightweight and\nextensible serving system for deploying tiny LLMs (e.g., TinyLLaMA, GPT2-345M)\nwith support for structured KV sparsity, plugin-based token selection, and\nhardware-efficient attention kernels. Unlike prior simulation frameworks,\nTinyServe executes real-time decoding with configurable sparsity strategies and\nfine-grained instrumentation.\n  To reduce decoding cost, we introduce a \\textit{query-aware page selection}\nmechanism that leverages bounding-box metadata to estimate attention relevance\nbetween the query and KV cache blocks. This enables selective KV loading with\nminimal overhead and no model modifications. Our fused CUDA kernel integrates\npage scoring, sparse memory access, and masked attention in a single pass.\n  Experiments show that TinyServe achieves up to \\textbf{3.4x} speedup and over\n\\textbf{2x} memory savings with negligible accuracy drop. Additional analysis\nof cache reuse, page hit rate, and multi-GPU scaling confirms its practicality\nas an efficient system-level design for LLM training and inference research on\nresource-constrained hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) efficiently remains challenging due to\nthe high memory and latency overhead of key-value (KV) cache access during\nautoregressive decoding. We present \\textbf{TinyServe}, a lightweight and\nextensible serving system for deploying tiny LLMs (e.g., TinyLLaMA, GPT2-345M)\nwith support for structured KV sparsity, plugin-based token selection, and\nhardware-efficient attention kernels. Unlike prior simulation frameworks,\nTinyServe executes real-time decoding with configurable sparsity strategies and\nfine-grained instrumentation.\n  To reduce decoding cost, we introduce a \\textit{query-aware page selection}\nmechanism that leverages bounding-box metadata to estimate attention relevance\nbetween the query and KV cache blocks. This enables selective KV loading with\nminimal overhead and no model modifications. Our fused CUDA kernel integrates\npage scoring, sparse memory access, and masked attention in a single pass.\n  Experiments show that TinyServe achieves up to \\textbf{3.4x} speedup and over\n\\textbf{2x} memory savings with negligible accuracy drop. Additional analysis\nof cache reuse, page hit rate, and multi-GPU scaling confirms its practicality\nas an efficient system-level design for LLM training and inference research on\nresource-constrained hardware."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yanxuan Yu"
                },
                "author": "Yanxuan Yu",
                "arxiv_comment": "Accepted to ACM MM as Oral Paper, also accepted to ICML MOSS\n  workshop, publicly available as https://openreview.net/forum?id=sOdtl4jLci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20865v1",
                "updated": "2025-08-28T14:58:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:58:47Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "title": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction"
                },
                "summary": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%."
                },
                "authors": [
                    {
                        "name": "Zhuoxing Wei"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Qingchen Xie"
                    }
                ],
                "author_detail": {
                    "name": "Qingchen Xie"
                },
                "author": "Qingchen Xie",
                "arxiv_doi": "10.1145/3726302.3730177",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730177",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.20865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, 1 figures, SIGIR 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18250v2",
                "updated": "2025-08-28T08:49:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    49,
                    24,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-25T17:41:13Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    13,
                    0,
                    237,
                    0
                ],
                "title": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study"
                },
                "summary": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM."
                },
                "authors": [
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Fernando GarcÃ­a-Redondo"
                    },
                    {
                        "name": "Arvind Sharma"
                    },
                    {
                        "name": "Van Dai Nguyen"
                    },
                    {
                        "name": "Andrea Fantini"
                    },
                    {
                        "name": "Philippe Matagne"
                    },
                    {
                        "name": "Siddharth Rao"
                    },
                    {
                        "name": "Subhali Subhechha"
                    },
                    {
                        "name": "Lynn Verschueren"
                    },
                    {
                        "name": "Mohammed Aftab Baig"
                    },
                    {
                        "name": "Marie Garcia Bardon"
                    },
                    {
                        "name": "Geert Hellings"
                    }
                ],
                "author_detail": {
                    "name": "Geert Hellings"
                },
                "author": "Geert Hellings",
                "arxiv_comment": "Manuscript submitted to IEEE Trans. Elec. Dev. Work enabled in part\n  by NanoIC pilot line; acquisition and operation jointly funded by Chips Joint\n  Undertaking, through EU's Digital Europe (101183266) and Horizon Europe\n  programs (101183277), as well as by the participating states\n  (Belgium-Flanders, France, Germany, Finland, Ireland, Romania)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20524v1",
                "updated": "2025-08-28T08:05:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    5,
                    42,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T08:05:42Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    5,
                    42,
                    3,
                    240,
                    0
                ],
                "title": "Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport\n  Equation Solver for Fast Scatter Correction in Multi-Spectral CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport\n  Equation Solver for Fast Scatter Correction in Multi-Spectral CT"
                },
                "summary": "X-ray scatter has been a serious concern in computed tomography (CT), leading\nto image artifacts and distortion of CT values. The linear Boltzmann transport\nequation (LBTE) is recognized as a fast and accurate approach for scatter\nestimation. However, for multi-spectral CT, it is cumbersome to compute\nmultiple scattering components for different spectra separately when applying\nLBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum\nDecomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute\nX-ray scatter distributions from CT acquisitions at two or more different\nspectra simultaneously, in a unified framework with no sacrifice in accuracy\nand nearly no increase in computation in theory. First, a matrixed-spectrum\nsolver of LBTE is obtained by introducing an additional label dimension to\nexpand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a\nprinciple of selection of basis using the QR decomposition, along with the\nabove solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter\ncorrection method can be established for multi-spectral CT. We validate the\neffectiveness and accuracy of our method by comparing it with the Monte Carlo\nmethod, including the computational time. We also evaluate the scatter\ncorrection performance using two different phantoms for fast-kV switching based\ndual-energy CT, and using an elliptical phantom in a numerical simulation for\nkV-modulation enabled CT scans, validating that our proposed method can\nsignificantly reduce the computational cost at multiple spectra and effectively\nreduce scatter artifact in reconstructed CT images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-ray scatter has been a serious concern in computed tomography (CT), leading\nto image artifacts and distortion of CT values. The linear Boltzmann transport\nequation (LBTE) is recognized as a fast and accurate approach for scatter\nestimation. However, for multi-spectral CT, it is cumbersome to compute\nmultiple scattering components for different spectra separately when applying\nLBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum\nDecomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute\nX-ray scatter distributions from CT acquisitions at two or more different\nspectra simultaneously, in a unified framework with no sacrifice in accuracy\nand nearly no increase in computation in theory. First, a matrixed-spectrum\nsolver of LBTE is obtained by introducing an additional label dimension to\nexpand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a\nprinciple of selection of basis using the QR decomposition, along with the\nabove solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter\ncorrection method can be established for multi-spectral CT. We validate the\neffectiveness and accuracy of our method by comparing it with the Monte Carlo\nmethod, including the computational time. We also evaluate the scatter\ncorrection performance using two different phantoms for fast-kV switching based\ndual-energy CT, and using an elliptical phantom in a numerical simulation for\nkV-modulation enabled CT scans, validating that our proposed method can\nsignificantly reduce the computational cost at multiple spectra and effectively\nreduce scatter artifact in reconstructed CT images."
                },
                "authors": [
                    {
                        "name": "Guoxi Zhu"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Zhiqiang Chen"
                    },
                    {
                        "name": "Hewei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Hewei Gao"
                },
                "author": "Hewei Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20433v1",
                "updated": "2025-08-28T05:22:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    5,
                    22,
                    25,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T05:22:25Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    5,
                    22,
                    25,
                    3,
                    240,
                    0
                ],
                "title": "MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content\n  Caching in Emerging Mega-Constellations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content\n  Caching in Emerging Mega-Constellations"
                },
                "summary": "Significant latency in global content delivery primarily arises from\ninsufficient terrestrial infrastructure. Deploying space-based content delivery\nnetworks within emerging mega-constellations provides an effective means to\nbridge the digital divide. However, space-based caching faces constraints from\nphysical-layer dynamics, including dynamic topologies, time-varying\ninter-satellite link conditions, and limited onboard energy. In addition,\nexisting mechanisms often lack fine-grained content categorization and global\noptimization. This paper proposes MegaCacheX, a cost-effective hierarchical\nframework for collaborative content distribution that achieves\n\"Earth-independence\" by providing cloud services directly from space.\nSpecifically, data centers in Sun-synchronous orbit act as primary content\nsources, while caching nodes in mega-constellations and ground stations\ncollaboratively form a distributed edge layer. MegaCacheX optimizes caching\nstrategies by integrating content popularity, regional user distribution, and\nsatellite trajectory predictions. Multi-tier caching nodes serve as service\nanchors, enabling seamless content delivery with low latency. A prototype\nimplemented on a microservices-based, containerized testbed demonstrates that\nMegaCacheX reduces global content access latency by about 36% compared to\nbaseline approaches, while maintaining cost efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant latency in global content delivery primarily arises from\ninsufficient terrestrial infrastructure. Deploying space-based content delivery\nnetworks within emerging mega-constellations provides an effective means to\nbridge the digital divide. However, space-based caching faces constraints from\nphysical-layer dynamics, including dynamic topologies, time-varying\ninter-satellite link conditions, and limited onboard energy. In addition,\nexisting mechanisms often lack fine-grained content categorization and global\noptimization. This paper proposes MegaCacheX, a cost-effective hierarchical\nframework for collaborative content distribution that achieves\n\"Earth-independence\" by providing cloud services directly from space.\nSpecifically, data centers in Sun-synchronous orbit act as primary content\nsources, while caching nodes in mega-constellations and ground stations\ncollaboratively form a distributed edge layer. MegaCacheX optimizes caching\nstrategies by integrating content popularity, regional user distribution, and\nsatellite trajectory predictions. Multi-tier caching nodes serve as service\nanchors, enabling seamless content delivery with low latency. A prototype\nimplemented on a microservices-based, containerized testbed demonstrates that\nMegaCacheX reduces global content access latency by about 36% compared to\nbaseline approaches, while maintaining cost efficiency."
                },
                "authors": [
                    {
                        "name": "Haoyang Shi"
                    },
                    {
                        "name": "Xing Zhang"
                    },
                    {
                        "name": "Sitong Li"
                    },
                    {
                        "name": "Minghang Li"
                    },
                    {
                        "name": "Xinming Lu"
                    },
                    {
                        "name": "Shaoxiang Xu"
                    },
                    {
                        "name": "Guoquan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoquan Wang"
                },
                "author": "Guoquan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20424v1",
                "updated": "2025-08-28T04:46:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    46,
                    44,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T04:46:44Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    46,
                    44,
                    3,
                    240,
                    0
                ],
                "title": "Breaking Diffusion with Cache: Exploiting Approximate Caches in\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Diffusion with Cache: Exploiting Approximate Caches in\n  Diffusion Models"
                },
                "summary": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Shuncheng Jie"
                    },
                    {
                        "name": "Sihang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sihang Liu"
                },
                "author": "Sihang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20407v1",
                "updated": "2025-08-28T04:10:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    10,
                    19,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T04:10:19Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    10,
                    19,
                    3,
                    240,
                    0
                ],
                "title": "Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full\n  Context-Aware Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full\n  Context-Aware Linear Attention"
                },
                "summary": "The Transformer architecture has become a cornerstone of modern artificial\nintelligence, but its core self-attention mechanism suffers from a complexity\nbottleneck that scales quadratically with sequence length, severely limiting\nits application in long-sequence tasks. To address this challenge, existing\nlinear attention methods typically sacrifice model performance by relying on\ndata-agnostic kernel approximations or restrictive context selection. This\npaper returns to the first principles of connectionism, starting from the\ntopological structure of information flow, to introduce a novel linear\nattention architecture-\\textbf{TLinFormer}. By reconfiguring neuron connection\npatterns, TLinFormer achieves strict linear complexity while computing exact\nattention scores and ensuring information flow remains aware of the full\nhistorical context. This design aims to bridge the performance gap prevalent\nbetween existing efficient attention methods and standard attention. Through a\nseries of experiments, we systematically evaluate the performance of TLinFormer\nagainst a standard Transformer baseline on long-sequence inference tasks. The\nresults demonstrate that TLinFormer exhibits overwhelming advantages in key\nmetrics such as \\textbf{inference latency}, \\textbf{KV cache efficiency},\n\\textbf{memory footprint}, and \\textbf{overall speedup}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Transformer architecture has become a cornerstone of modern artificial\nintelligence, but its core self-attention mechanism suffers from a complexity\nbottleneck that scales quadratically with sequence length, severely limiting\nits application in long-sequence tasks. To address this challenge, existing\nlinear attention methods typically sacrifice model performance by relying on\ndata-agnostic kernel approximations or restrictive context selection. This\npaper returns to the first principles of connectionism, starting from the\ntopological structure of information flow, to introduce a novel linear\nattention architecture-\\textbf{TLinFormer}. By reconfiguring neuron connection\npatterns, TLinFormer achieves strict linear complexity while computing exact\nattention scores and ensuring information flow remains aware of the full\nhistorical context. This design aims to bridge the performance gap prevalent\nbetween existing efficient attention methods and standard attention. Through a\nseries of experiments, we systematically evaluate the performance of TLinFormer\nagainst a standard Transformer baseline on long-sequence inference tasks. The\nresults demonstrate that TLinFormer exhibits overwhelming advantages in key\nmetrics such as \\textbf{inference latency}, \\textbf{KV cache efficiency},\n\\textbf{memory footprint}, and \\textbf{overall speedup}."
                },
                "authors": [
                    {
                        "name": "Zhongpan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpan Tang"
                },
                "author": "Zhongpan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v5",
                "updated": "2025-08-28T03:57:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    3,
                    57,
                    52,
                    3,
                    240,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from (1) the distribution variance in the LLM activations and (2) the\nsensitivity difference among various kinds of layers. To address these issues,\nwe propose a training-free approach called Activation-aware Singular Value\nDecomposition (ASVD). Specifically, ASVD manages activation outliers by\ntransforming the weight matrix based on the activation distribution. This\ntransformation allows the outliers in the activation matrix to be absorbed into\nthe transformed weight matrix, thereby enhancing decomposition accuracy.\nAdditionally, we propose an efficient iterative calibration process to optimize\nlayer-specific decomposition by addressing the varying sensitivity of different\nLLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the\nsuccess of the low-rank decomposition of projection matrices in the\nself-attention module, we further introduce ASVD to compress the KV cache. By\nreducing the channel dimension of KV activations, memory requirements for KV\ncache can be largely reduced. ASVD can further achieve 50% KV cache reductions\nwithout performance drop in a training-free manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from (1) the distribution variance in the LLM activations and (2) the\nsensitivity difference among various kinds of layers. To address these issues,\nwe propose a training-free approach called Activation-aware Singular Value\nDecomposition (ASVD). Specifically, ASVD manages activation outliers by\ntransforming the weight matrix based on the activation distribution. This\ntransformation allows the outliers in the activation matrix to be absorbed into\nthe transformed weight matrix, thereby enhancing decomposition accuracy.\nAdditionally, we propose an efficient iterative calibration process to optimize\nlayer-specific decomposition by addressing the varying sensitivity of different\nLLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the\nsuccess of the low-rank decomposition of projection matrices in the\nself-attention module, we further introduce ASVD to compress the KV cache. By\nreducing the channel dimension of KV activations, memory requirements for KV\ncache can be largely reduced. ASVD can further achieve 50% KV cache reductions\nwithout performance drop in a training-free manner."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09888v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09888v2",
                "updated": "2025-08-28T01:40:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    1,
                    40,
                    30,
                    3,
                    240,
                    0
                ],
                "published": "2025-02-14T03:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "title": "Climber: Toward Efficient Scaling Laws for Large Recommendation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Climber: Toward Efficient Scaling Laws for Large Recommendation Models"
                },
                "summary": "Transformer-based generative models have achieved remarkable success across\ndomains with various scaling law manifestations. However, our extensive\nexperiments reveal persistent challenges when applying Transformer to\nrecommendation systems: (1) Transformer scaling is not ideal with increased\ncomputational resources, due to structural incompatibilities with\nrecommendation-specific features such as multi-source data heterogeneity; (2)\ncritical online inference latency constraints (tens of milliseconds) that\nintensify with longer user behavior sequences and growing computational\ndemands. We propose Climber, an efficient recommendation framework comprising\ntwo synergistic components: the model architecture for efficient scaling and\nthe co-designed acceleration techniques. Our proposed model adopts two core\ninnovations: (1) multi-scale sequence extraction that achieves a time\ncomplexity reduction by a constant factor, enabling more efficient scaling with\nsequence length; (2) dynamic temperature modulation adapting attention\ndistributions to the multi-scenario and multi-behavior patterns. Complemented\nby acceleration techniques, Climber achieves a 5.15$\\times$ throughput gain\nwithout performance degradation by adopting a \"single user, multiple item\"\nbatched processing and memory-efficient Key-Value caching. Comprehensive\noffline experiments on multiple datasets validate that Climber exhibits a more\nideal scaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19\\% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based generative models have achieved remarkable success across\ndomains with various scaling law manifestations. However, our extensive\nexperiments reveal persistent challenges when applying Transformer to\nrecommendation systems: (1) Transformer scaling is not ideal with increased\ncomputational resources, due to structural incompatibilities with\nrecommendation-specific features such as multi-source data heterogeneity; (2)\ncritical online inference latency constraints (tens of milliseconds) that\nintensify with longer user behavior sequences and growing computational\ndemands. We propose Climber, an efficient recommendation framework comprising\ntwo synergistic components: the model architecture for efficient scaling and\nthe co-designed acceleration techniques. Our proposed model adopts two core\ninnovations: (1) multi-scale sequence extraction that achieves a time\ncomplexity reduction by a constant factor, enabling more efficient scaling with\nsequence length; (2) dynamic temperature modulation adapting attention\ndistributions to the multi-scenario and multi-behavior patterns. Complemented\nby acceleration techniques, Climber achieves a 5.15$\\times$ throughput gain\nwithout performance degradation by adopting a \"single user, multiple item\"\nbatched processing and memory-efficient Key-Value caching. Comprehensive\noffline experiments on multiple datasets validate that Climber exhibits a more\nideal scaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19\\% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily."
                },
                "authors": [
                    {
                        "name": "Songpei Xu"
                    },
                    {
                        "name": "Shijia Wang"
                    },
                    {
                        "name": "Da Guo"
                    },
                    {
                        "name": "Xianwen Guo"
                    },
                    {
                        "name": "Qiang Xiao"
                    },
                    {
                        "name": "Bin Huang"
                    },
                    {
                        "name": "Guanlin Wu"
                    },
                    {
                        "name": "Chuanjiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Chuanjiang Luo"
                },
                "author": "Chuanjiang Luo",
                "arxiv_doi": "10.1145/3746252.3761561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09888v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09888v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00105v1",
                "updated": "2025-08-28T00:46:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    0,
                    46,
                    51,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T00:46:51Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    0,
                    46,
                    51,
                    3,
                    240,
                    0
                ],
                "title": "AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and\n  High-Quality Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and\n  High-Quality Language Model Serving"
                },
                "summary": "Large language model (LLM) applications often reuse previously processed\ncontext, such as chat history and documents, which introduces significant\nredundant computation. Existing LLM serving systems address such redundant\ncomputation by storing the KV caches of processed context and loading the\ncorresponding KV cache when a new request reuses the context. Further, as these\nLLM applications scale, the total size of KV caches becomes excessively large\nand requires both DRAM and SSD for full storage.\n  However, prior work that stores KV caches in DRAM and SSD suffers from high\nloading delays, as most KV cache hits come from SSD, which is slow to load. To\nincrease the KV cache hit rate on DRAM, we identify lossy KV cache compression\nas a promising approach. We design a lossy compression system that decides the\ncompression algorithm, compression rate and device placement for each KV cache\nentry to maximise DRAM hits and minimise loading delay without significantly\ndegrading generation quality. Compared to various static compression baselines\nacross three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at\nthe same quality and 6--55% quality improvements at the same delay.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) applications often reuse previously processed\ncontext, such as chat history and documents, which introduces significant\nredundant computation. Existing LLM serving systems address such redundant\ncomputation by storing the KV caches of processed context and loading the\ncorresponding KV cache when a new request reuses the context. Further, as these\nLLM applications scale, the total size of KV caches becomes excessively large\nand requires both DRAM and SSD for full storage.\n  However, prior work that stores KV caches in DRAM and SSD suffers from high\nloading delays, as most KV cache hits come from SSD, which is slow to load. To\nincrease the KV cache hit rate on DRAM, we identify lossy KV cache compression\nas a promising approach. We design a lossy compression system that decides the\ncompression algorithm, compression rate and device placement for each KV cache\nentry to maximise DRAM hits and minimise loading delay without significantly\ndegrading generation quality. Compared to various static compression baselines\nacross three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at\nthe same quality and 6--55% quality improvements at the same delay."
                },
                "authors": [
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Samuel Shen"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Ganesh Ananthanarayanan"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.13318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13318v1",
                "updated": "2025-09-16T17:59:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    59,
                    31,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T17:59:31Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    59,
                    31,
                    1,
                    259,
                    0
                ],
                "title": "How Theory-Informed Priors Affect DESI Evidence for Evolving Dark Energy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Theory-Informed Priors Affect DESI Evidence for Evolving Dark Energy"
                },
                "summary": "Recent measurements of baryon acoustic oscillations (BAO) from the Dark\nEnergy Spectroscopic Instrument (DESI) have been interpreted to suggest that\ndark energy may be evolving. In this work, we examine how prior choices affect\nsuch conclusions. Specifically, we study the biases introduced by the customary\nuse of uniform priors on the Chevallier-Polarski-Linder (CPL) parameters, $w_0$\nand $w_a$, when assessing evidence for evolving dark energy. To do so, we\nconstruct theory-informed priors on $(w_0, w_a)$ using a normalizing flow (NF),\ntrained on two representative quintessence models, which learns the\ndistribution of these parameters conditional on the underlying $\\Lambda$CDM\nparameters. In the combined $\\textit{Planck}$ CMB + DESI BAO analysis we find\nthat the apparent tension with a cosmological constant in the CPL framework can\nbe reduced from $\\sim 3.1\\sigma$ to $\\sim 1.3\\sigma$ once theory-informed\npriors are applied, rendering the result effectively consistent with\n$\\Lambda$CDM. For completeness, we also analyze combinations that include Type\nIa supernova data, showing similar shifts toward the $\\Lambda$CDM limit. Taken\ntogether, the observed sensitivity to prior choices in these analyses arises\nbecause uniform priors - often mischaracterized as \"uninformative\" - can\nactually bias inferences toward unphysical parameter regions. Consequently, our\nresults underscore the importance of adopting physically motivated priors to\nensure robust cosmological inferences, especially when evaluating new\nhypotheses with only marginal statistical support. Lastly, our NF-based\nframework achieves these results by post-processing existing MCMC chains,\nrequiring $\\approx 1$ hour of additional CPU compute time on top of the base\nanalysis - a dramatic speedup over direct model sampling that highlights the\nscalability of this approach for testing diverse theoretical models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent measurements of baryon acoustic oscillations (BAO) from the Dark\nEnergy Spectroscopic Instrument (DESI) have been interpreted to suggest that\ndark energy may be evolving. In this work, we examine how prior choices affect\nsuch conclusions. Specifically, we study the biases introduced by the customary\nuse of uniform priors on the Chevallier-Polarski-Linder (CPL) parameters, $w_0$\nand $w_a$, when assessing evidence for evolving dark energy. To do so, we\nconstruct theory-informed priors on $(w_0, w_a)$ using a normalizing flow (NF),\ntrained on two representative quintessence models, which learns the\ndistribution of these parameters conditional on the underlying $\\Lambda$CDM\nparameters. In the combined $\\textit{Planck}$ CMB + DESI BAO analysis we find\nthat the apparent tension with a cosmological constant in the CPL framework can\nbe reduced from $\\sim 3.1\\sigma$ to $\\sim 1.3\\sigma$ once theory-informed\npriors are applied, rendering the result effectively consistent with\n$\\Lambda$CDM. For completeness, we also analyze combinations that include Type\nIa supernova data, showing similar shifts toward the $\\Lambda$CDM limit. Taken\ntogether, the observed sensitivity to prior choices in these analyses arises\nbecause uniform priors - often mischaracterized as \"uninformative\" - can\nactually bias inferences toward unphysical parameter regions. Consequently, our\nresults underscore the importance of adopting physically motivated priors to\nensure robust cosmological inferences, especially when evaluating new\nhypotheses with only marginal statistical support. Lastly, our NF-based\nframework achieves these results by post-processing existing MCMC chains,\nrequiring $\\approx 1$ hour of additional CPU compute time on top of the base\nanalysis - a dramatic speedup over direct model sampling that highlights the\nscalability of this approach for testing diverse theoretical models."
                },
                "authors": [
                    {
                        "name": "Michael W. Toomey"
                    },
                    {
                        "name": "Gabriele Montefalcone"
                    },
                    {
                        "name": "Evan McDonough"
                    },
                    {
                        "name": "Katherine Freese"
                    }
                ],
                "author_detail": {
                    "name": "Katherine Freese"
                },
                "author": "Katherine Freese",
                "arxiv_comment": "16 pages, 7 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13317v1",
                "updated": "2025-09-16T17:59:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    59,
                    6,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T17:59:06Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    59,
                    6,
                    1,
                    259,
                    0
                ],
                "title": "3D Aware Region Prompted Vision Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Aware Region Prompted Vision Language Model"
                },
                "summary": "We present Spatial Region 3D (SR-3D) aware vision-language model that\nconnects single-view 2D images and multi-view 3D data through a shared visual\ntoken space. SR-3D supports flexible region prompting, allowing users to\nannotate regions with bounding boxes, segmentation masks on any frame, or\ndirectly in 3D, without the need for exhaustive multi-frame labeling. We\nachieve this by enriching 2D visual features with 3D positional embeddings,\nwhich allows the 3D model to draw upon strong 2D priors for more accurate\nspatial reasoning across frames, even when objects of interest do not co-occur\nwithin the same view. Extensive experiments on both general 2D vision language\nand specialized 3D spatial benchmarks demonstrate that SR-3D achieves\nstate-of-the-art performance, underscoring its effectiveness for unifying 2D\nand 3D representation space on scene understanding. Moreover, we observe\napplicability to in-the-wild videos without sensory 3D inputs or ground-truth\n3D annotations, where SR-3D accurately infers spatial relationships and metric\nmeasurements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Spatial Region 3D (SR-3D) aware vision-language model that\nconnects single-view 2D images and multi-view 3D data through a shared visual\ntoken space. SR-3D supports flexible region prompting, allowing users to\nannotate regions with bounding boxes, segmentation masks on any frame, or\ndirectly in 3D, without the need for exhaustive multi-frame labeling. We\nachieve this by enriching 2D visual features with 3D positional embeddings,\nwhich allows the 3D model to draw upon strong 2D priors for more accurate\nspatial reasoning across frames, even when objects of interest do not co-occur\nwithin the same view. Extensive experiments on both general 2D vision language\nand specialized 3D spatial benchmarks demonstrate that SR-3D achieves\nstate-of-the-art performance, underscoring its effectiveness for unifying 2D\nand 3D representation space on scene understanding. Moreover, we observe\napplicability to in-the-wild videos without sensory 3D inputs or ground-truth\n3D annotations, where SR-3D accurately infers spatial relationships and metric\nmeasurements."
                },
                "authors": [
                    {
                        "name": "An-Chieh Cheng"
                    },
                    {
                        "name": "Yang Fu"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Xiaolong Li"
                    },
                    {
                        "name": "Subhashree Radhakrishnan"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Xiaolong Wang"
                    },
                    {
                        "name": "Sifei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sifei Liu"
                },
                "author": "Sifei Liu",
                "arxiv_comment": "Project Website: https://www.anjiecheng.me/sr3d",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13316v1",
                "updated": "2025-09-16T17:59:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    59,
                    4,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T17:59:04Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    59,
                    4,
                    1,
                    259,
                    0
                ],
                "title": "Do Natural Language Descriptions of Model Activations Convey Privileged\n  Information?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Natural Language Descriptions of Model Activations Convey Privileged\n  Information?"
                },
                "summary": "Recent interpretability methods have proposed to translate LLM internal\nrepresentations into natural language descriptions using a second verbalizer\nLLM. This is intended to illuminate how the target model represents and\noperates on inputs. But do such activation verbalization approaches actually\nprovide privileged knowledge about the internal workings of the target model,\nor do they merely convey information about its inputs? We critically evaluate\npopular verbalization methods across datasets used in prior work and find that\nthey succeed at benchmarks without any access to target model internals,\nsuggesting that these datasets are not ideal for evaluating verbalization\nmethods. We then run controlled experiments which reveal that verbalizations\noften reflect the parametric knowledge of the verbalizer LLM which generated\nthem, rather than the activations of the target LLM being decoded. Taken\ntogether, our results indicate a need for targeted benchmarks and experimental\ncontrols to rigorously assess whether verbalization methods provide meaningful\ninsights into the operations of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent interpretability methods have proposed to translate LLM internal\nrepresentations into natural language descriptions using a second verbalizer\nLLM. This is intended to illuminate how the target model represents and\noperates on inputs. But do such activation verbalization approaches actually\nprovide privileged knowledge about the internal workings of the target model,\nor do they merely convey information about its inputs? We critically evaluate\npopular verbalization methods across datasets used in prior work and find that\nthey succeed at benchmarks without any access to target model internals,\nsuggesting that these datasets are not ideal for evaluating verbalization\nmethods. We then run controlled experiments which reveal that verbalizations\noften reflect the parametric knowledge of the verbalizer LLM which generated\nthem, rather than the activations of the target LLM being decoded. Taken\ntogether, our results indicate a need for targeted benchmarks and experimental\ncontrols to rigorously assess whether verbalization methods provide meaningful\ninsights into the operations of LLMs."
                },
                "authors": [
                    {
                        "name": "Millicent Li"
                    },
                    {
                        "name": "Alberto Mario Ceballos Arroyo"
                    },
                    {
                        "name": "Giordano Rogers"
                    },
                    {
                        "name": "Naomi Saphra"
                    },
                    {
                        "name": "Byron C. Wallace"
                    }
                ],
                "author_detail": {
                    "name": "Byron C. Wallace"
                },
                "author": "Byron C. Wallace",
                "arxiv_comment": "34 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13313v1",
                "updated": "2025-09-16T17:57:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    57,
                    22,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T17:57:22Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    57,
                    22,
                    1,
                    259,
                    0
                ],
                "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context\n  Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReSum: Unlocking Long-Horizon Search Intelligence via Context\n  Summarization"
                },
                "summary": "Large Language Model (LLM)-based web agents demonstrate strong performance on\nknowledge-intensive tasks but are hindered by context window limitations in\nparadigms like ReAct. Complex queries involving multiple entities, intertwined\nrelationships, and high uncertainty demand extensive search cycles that rapidly\nexhaust context budgets before reaching complete solutions. To overcome this\nchallenge, we introduce ReSum, a novel paradigm that enables indefinite\nexploration through periodic context summarization. ReSum converts growing\ninteraction histories into compact reasoning states, maintaining awareness of\nprior discoveries while bypassing context constraints. For paradigm adaptation,\nwe propose ReSum-GRPO, integrating GRPO with segmented trajectory training and\nadvantage broadcasting to familiarize agents with summary-conditioned\nreasoning. Extensive experiments on web agents of varying scales across three\nbenchmarks demonstrate that ReSum delivers an average absolute improvement of\n4.5\\% over ReAct, with further gains of up to 8.2\\% following ReSum-GRPO\ntraining. Notably, with only 1K training samples, our WebResummer-30B (a\nReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\\% Pass@1 on\nBrowseComp-zh and 18.3\\% on BrowseComp-en, surpassing existing open-source web\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based web agents demonstrate strong performance on\nknowledge-intensive tasks but are hindered by context window limitations in\nparadigms like ReAct. Complex queries involving multiple entities, intertwined\nrelationships, and high uncertainty demand extensive search cycles that rapidly\nexhaust context budgets before reaching complete solutions. To overcome this\nchallenge, we introduce ReSum, a novel paradigm that enables indefinite\nexploration through periodic context summarization. ReSum converts growing\ninteraction histories into compact reasoning states, maintaining awareness of\nprior discoveries while bypassing context constraints. For paradigm adaptation,\nwe propose ReSum-GRPO, integrating GRPO with segmented trajectory training and\nadvantage broadcasting to familiarize agents with summary-conditioned\nreasoning. Extensive experiments on web agents of varying scales across three\nbenchmarks demonstrate that ReSum delivers an average absolute improvement of\n4.5\\% over ReAct, with further gains of up to 8.2\\% following ReSum-GRPO\ntraining. Notably, with only 1K training samples, our WebResummer-30B (a\nReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\\% Pass@1 on\nBrowseComp-zh and 18.3\\% on BrowseComp-en, surpassing existing open-source web\nagents."
                },
                "authors": [
                    {
                        "name": "Xixi Wu"
                    },
                    {
                        "name": "Kuan Li"
                    },
                    {
                        "name": "Yida Zhao"
                    },
                    {
                        "name": "Liwen Zhang"
                    },
                    {
                        "name": "Litu Ou"
                    },
                    {
                        "name": "Huifeng Yin"
                    },
                    {
                        "name": "Zhongwang Zhang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Minhao Cheng"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Hong Cheng"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13310v1",
                "updated": "2025-09-16T17:57:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    57,
                    19,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T17:57:19Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    57,
                    19,
                    1,
                    259,
                    0
                ],
                "title": "Scaling Agents via Continual Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Agents via Continual Pre-training"
                },
                "summary": "Large language models (LLMs) have evolved into agentic systems capable of\nautonomous tool use and multi-step reasoning for complex problem-solving.\nHowever, post-training approaches building upon general-purpose foundation\nmodels consistently underperform in agentic tasks, particularly in open-source\nimplementations. We identify the root cause: the absence of robust agentic\nfoundation models forces models during post-training to simultaneously learn\ndiverse agentic behaviors while aligning them to expert demonstrations, thereby\ncreating fundamental optimization tensions. To this end, we are the first to\npropose incorporating Agentic Continual Pre-training (Agentic CPT) into the\ndeep research agents training pipeline to build powerful agentic foundational\nmodels. Based on this approach, we develop a deep research agent model named\nAgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve\nstate-of-the-art performance while retains strong tool-use ability, notably\n39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have evolved into agentic systems capable of\nautonomous tool use and multi-step reasoning for complex problem-solving.\nHowever, post-training approaches building upon general-purpose foundation\nmodels consistently underperform in agentic tasks, particularly in open-source\nimplementations. We identify the root cause: the absence of robust agentic\nfoundation models forces models during post-training to simultaneously learn\ndiverse agentic behaviors while aligning them to expert demonstrations, thereby\ncreating fundamental optimization tensions. To this end, we are the first to\npropose incorporating Agentic Continual Pre-training (Agentic CPT) into the\ndeep research agents training pipeline to build powerful agentic foundational\nmodels. Based on this approach, we develop a deep research agent model named\nAgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve\nstate-of-the-art performance while retains strong tool-use ability, notably\n39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE."
                },
                "authors": [
                    {
                        "name": "Liangcai Su"
                    },
                    {
                        "name": "Zhen Zhang"
                    },
                    {
                        "name": "Guangyu Li"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Maojia Song"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Kuan Li"
                    },
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Xuanzhong Chen"
                    },
                    {
                        "name": "Zile Qiao"
                    },
                    {
                        "name": "Zhongwang Zhang"
                    },
                    {
                        "name": "Huifeng Yin"
                    },
                    {
                        "name": "Shihao Cai"
                    },
                    {
                        "name": "Runnan Fang"
                    },
                    {
                        "name": "Zhengwei Tao"
                    },
                    {
                        "name": "Wenbiao Yin"
                    },
                    {
                        "name": "Chenxiong Qian"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13307v1",
                "updated": "2025-09-16T17:57:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    57,
                    5,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T17:57:05Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    57,
                    5,
                    1,
                    259,
                    0
                ],
                "title": "High-Dimensional Bayesian Model Comparison in Cosmology with\n  GPU-accelerated Nested Sampling and Neural Emulators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Dimensional Bayesian Model Comparison in Cosmology with\n  GPU-accelerated Nested Sampling and Neural Emulators"
                },
                "summary": "We demonstrate a GPU-accelerated nested sampling framework for efficient\nhigh-dimensional Bayesian inference in cosmology. Using JAX-based neural\nemulators and likelihoods for cosmic microwave background and cosmic shear\nanalyses, our approach provides parameter constraints and direct calculation of\nBayesian evidence. In the 39 dimensional $\\Lambda$CDM vs $w_0w_a$ shear\nanalysis, we produce Bayes Factors and a robust error bar in just 2 days on a\nsingle A100 GPU, without loss of accuracy. Where CPU-based nested sampling can\nnow be outpaced by methods relying on MCMC sampling and decoupled evidence\nestimation, we demonstrate that with GPU acceleration nested sampling offers\nthe necessary speed-up to put it on equal computational footing with these\nmethods, especially where reliable model comparison is paramount. We put\nforward both nested and gradient-based sampling as useful tools for the modern\ncosmologist, where cutting-edge inference pipelines can yield orders of\nmagnitude improvements in computation time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate a GPU-accelerated nested sampling framework for efficient\nhigh-dimensional Bayesian inference in cosmology. Using JAX-based neural\nemulators and likelihoods for cosmic microwave background and cosmic shear\nanalyses, our approach provides parameter constraints and direct calculation of\nBayesian evidence. In the 39 dimensional $\\Lambda$CDM vs $w_0w_a$ shear\nanalysis, we produce Bayes Factors and a robust error bar in just 2 days on a\nsingle A100 GPU, without loss of accuracy. Where CPU-based nested sampling can\nnow be outpaced by methods relying on MCMC sampling and decoupled evidence\nestimation, we demonstrate that with GPU acceleration nested sampling offers\nthe necessary speed-up to put it on equal computational footing with these\nmethods, especially where reliable model comparison is paramount. We put\nforward both nested and gradient-based sampling as useful tools for the modern\ncosmologist, where cutting-edge inference pipelines can yield orders of\nmagnitude improvements in computation time."
                },
                "authors": [
                    {
                        "name": "Toby Lovick"
                    },
                    {
                        "name": "David Yallup"
                    },
                    {
                        "name": "Davide Piras"
                    },
                    {
                        "name": "Alessio Spurio Mancini"
                    },
                    {
                        "name": "Will Handley"
                    }
                ],
                "author_detail": {
                    "name": "Will Handley"
                },
                "author": "Will Handley",
                "arxiv_comment": "10 pages 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13305v1",
                "updated": "2025-09-16T17:57:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    57,
                    3,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T17:57:03Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    57,
                    3,
                    1,
                    259,
                    0
                ],
                "title": "WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic\n  Data and Scalable Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic\n  Data and Scalable Reinforcement Learning"
                },
                "summary": "Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all open-source agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all open-source agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap."
                },
                "authors": [
                    {
                        "name": "Kuan Li"
                    },
                    {
                        "name": "Zhongwang Zhang"
                    },
                    {
                        "name": "Huifeng Yin"
                    },
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Yida Zhao"
                    },
                    {
                        "name": "Liwen Zhang"
                    },
                    {
                        "name": "Litu Ou"
                    },
                    {
                        "name": "Dingchu Zhang"
                    },
                    {
                        "name": "Xixi Wu"
                    },
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Zile Qiao"
                    },
                    {
                        "name": "Zhen Zhang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20404v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20404v3",
                "updated": "2025-09-16T17:48:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    48,
                    53,
                    1,
                    259,
                    0
                ],
                "published": "2024-05-30T18:16:41Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    18,
                    16,
                    41,
                    3,
                    151,
                    0
                ],
                "title": "JoPA:Explaining Large Language Model's Generation via Joint Prompt\n  Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JoPA:Explaining Large Language Model's Generation via Joint Prompt\n  Attribution"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performances in\ncomplex text generation tasks. However, the contribution of the input prompt to\nthe generated content still remains obscure to humans, underscoring the\nnecessity of understanding the causality between input and output pairs.\nExisting works for providing prompt-specific explanation often confine model\noutput to be classification or next-word prediction. Few initial attempts\naiming to explain the entire language generation often treat input prompt texts\nindependently, ignoring their combinatorial effects on the follow-up\ngeneration. In this study, we introduce a counterfactual explanation framework\nbased on Joint Prompt Attribution, JoPA, which aims to explain how a few prompt\ntexts collaboratively influences the LLM's complete generation. Particularly,\nwe formulate the task of prompt attribution for generation interpretation as a\ncombinatorial optimization problem, and introduce a probabilistic algorithm to\nsearch for the casual input combination in the discrete space. We define and\nutilize multiple metrics to evaluate the produced explanations, demonstrating\nboth the faithfulness and efficiency of our framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performances in\ncomplex text generation tasks. However, the contribution of the input prompt to\nthe generated content still remains obscure to humans, underscoring the\nnecessity of understanding the causality between input and output pairs.\nExisting works for providing prompt-specific explanation often confine model\noutput to be classification or next-word prediction. Few initial attempts\naiming to explain the entire language generation often treat input prompt texts\nindependently, ignoring their combinatorial effects on the follow-up\ngeneration. In this study, we introduce a counterfactual explanation framework\nbased on Joint Prompt Attribution, JoPA, which aims to explain how a few prompt\ntexts collaboratively influences the LLM's complete generation. Particularly,\nwe formulate the task of prompt attribution for generation interpretation as a\ncombinatorial optimization problem, and introduce a probabilistic algorithm to\nsearch for the casual input combination in the discrete space. We define and\nutilize multiple metrics to evaluate the produced explanations, demonstrating\nboth the faithfulness and efficiency of our framework."
                },
                "authors": [
                    {
                        "name": "Yurui Chang"
                    },
                    {
                        "name": "Bochuan Cao"
                    },
                    {
                        "name": "Yujia Wang"
                    },
                    {
                        "name": "Jinghui Chen"
                    },
                    {
                        "name": "Lu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lu Lin"
                },
                "author": "Lu Lin",
                "arxiv_doi": "10.18653/v1/2025.acl-long.1074",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2025.acl-long.1074",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.20404v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20404v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to ACL 2025 (Main)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13293v1",
                "updated": "2025-09-16T17:44:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    44,
                    58,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T17:44:58Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    44,
                    58,
                    1,
                    259,
                    0
                ],
                "title": "Inferring Soil Drydown Behaviour with Adaptive Bayesian Online\n  Changepoint Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Soil Drydown Behaviour with Adaptive Bayesian Online\n  Changepoint Analysis"
                },
                "summary": "Continuous soil-moisture measurements provide a direct lens on subsurface\nhydrological processes, notably the post-rainfall \"drydown\" phase. Because\nthese records consist of distinct, segment-specific behaviours whose forms and\nscales vary over time, realistic inference demands a model that captures\npiecewise dynamics while accommodating parameters that are unknown a priori.\nBuilding on Bayesian Online Changepoint Detection (BOCPD), we introduce two\ncomplementary extensions: a particle-filter variant that substitutes exact\nmarginalisation with sequential Monte Carlo to enable real-time inference when\ncritical parameters cannot be integrated out analytically, and an\nonline-gradient variant that embeds stochastic gradient updates within BOCPD to\nlearn application-relevant parameters on the fly without prohibitive\ncomputational cost. After validating both algorithms on synthetic data that\nreplicate the temporal structure of field observations-detailing hyperparameter\nchoices, priors, and cost-saving strategies-we apply them to soil-moisture\nseries from experimental sites in Austria and the United States, quantifying\nsite-specific drydown rates and demonstrating the advantages of our adaptive\nframework over static models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous soil-moisture measurements provide a direct lens on subsurface\nhydrological processes, notably the post-rainfall \"drydown\" phase. Because\nthese records consist of distinct, segment-specific behaviours whose forms and\nscales vary over time, realistic inference demands a model that captures\npiecewise dynamics while accommodating parameters that are unknown a priori.\nBuilding on Bayesian Online Changepoint Detection (BOCPD), we introduce two\ncomplementary extensions: a particle-filter variant that substitutes exact\nmarginalisation with sequential Monte Carlo to enable real-time inference when\ncritical parameters cannot be integrated out analytically, and an\nonline-gradient variant that embeds stochastic gradient updates within BOCPD to\nlearn application-relevant parameters on the fly without prohibitive\ncomputational cost. After validating both algorithms on synthetic data that\nreplicate the temporal structure of field observations-detailing hyperparameter\nchoices, priors, and cost-saving strategies-we apply them to soil-moisture\nseries from experimental sites in Austria and the United States, quantifying\nsite-specific drydown rates and demonstrating the advantages of our adaptive\nframework over static models."
                },
                "authors": [
                    {
                        "name": "Mengyi Gong"
                    },
                    {
                        "name": "Christopher Nemeth"
                    },
                    {
                        "name": "Rebecca Killick"
                    },
                    {
                        "name": "Peter Strauss"
                    },
                    {
                        "name": "John Quinton"
                    }
                ],
                "author_detail": {
                    "name": "John Quinton"
                },
                "author": "John Quinton",
                "arxiv_comment": "21 pages of main manuscript and 3 pages if supplemental document",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62P12",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13281v1",
                "updated": "2025-09-16T17:35:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    35,
                    36,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T17:35:36Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    35,
                    36,
                    1,
                    259,
                    0
                ],
                "title": "RepIt: Representing Isolated Targets to Steer Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepIt: Representing Isolated Targets to Steer Language Models"
                },
                "summary": "While activation steering in large language models (LLMs) is a growing area\nof research, methods can often incur broader effects than desired. This\nmotivates isolation of purer concept vectors to enable targeted interventions\nand understand LLM behavior at a more granular level. We present RepIt, a\nsimple and data-efficient framework for isolating concept-specific\nrepresentations. Across five frontier LLMs, RepIt enables precise\ninterventions: it selectively suppresses refusal on targeted concepts while\npreserving refusal elsewhere, producing models that answer WMD-related\nquestions while still scoring as safe on standard benchmarks. We further show\nthat the corrective signal localizes to just 100-200 neurons and that robust\ntarget representations can be extracted from as few as a dozen examples on a\nsingle A6000. This efficiency raises a dual concern: manipulations can be\nperformed with modest compute and data to extend to underrepresented\ndata-scarce topics while evading existing benchmarks. By disentangling refusal\nvectors with RepIt, this work demonstrates that targeted interventions can\ncounteract overgeneralization, laying the foundation for more granular control\nof model behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While activation steering in large language models (LLMs) is a growing area\nof research, methods can often incur broader effects than desired. This\nmotivates isolation of purer concept vectors to enable targeted interventions\nand understand LLM behavior at a more granular level. We present RepIt, a\nsimple and data-efficient framework for isolating concept-specific\nrepresentations. Across five frontier LLMs, RepIt enables precise\ninterventions: it selectively suppresses refusal on targeted concepts while\npreserving refusal elsewhere, producing models that answer WMD-related\nquestions while still scoring as safe on standard benchmarks. We further show\nthat the corrective signal localizes to just 100-200 neurons and that robust\ntarget representations can be extracted from as few as a dozen examples on a\nsingle A6000. This efficiency raises a dual concern: manipulations can be\nperformed with modest compute and data to extend to underrepresented\ndata-scarce topics while evading existing benchmarks. By disentangling refusal\nvectors with RepIt, this work demonstrates that targeted interventions can\ncounteract overgeneralization, laying the foundation for more granular control\nof model behavior."
                },
                "authors": [
                    {
                        "name": "Vincent Siu"
                    },
                    {
                        "name": "Nathan W. Henry"
                    },
                    {
                        "name": "Nicholas Crispino"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Chenguang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chenguang Wang"
                },
                "author": "Chenguang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13272v1",
                "updated": "2025-09-16T17:29:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    29,
                    32,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T17:29:32Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    29,
                    32,
                    1,
                    259,
                    0
                ],
                "title": "Ionization and temperature measurements in warm dense copper using x-ray\n  absorption spectroscopy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ionization and temperature measurements in warm dense copper using x-ray\n  absorption spectroscopy"
                },
                "summary": "We detail experimental results inferring ionization and temperature for warm\ndense copper plasmas at several times solid density (15 to 25 g/cm$^3$) and\ntemperatures of 10 to 21 eV. Experiments performed at the OMEGA Laser Facility\ngenerate uniform warm dense matter conditions via symmetric shock compression\nof a buried copper layer. The plasma is probed with a laser-generated x-ray\nsource to collect the K-shell x-ray absorption spectrum. Fitting bound-bound\nabsorption contributions from constituent charge states of copper provides an\nestimated $\\overline{Z}$ of approximately 4 to 7 for these warm dense copper\nplasmas. We find that these partially ionized plasmas have K-edge shifts of 12\nto 30 eV and bound-bound resonance 1s$\\rightarrow$3p absorption shifts of 4 to\n26 eV with respect to the cold K-edge. This study provides necessary\nexperimental data to improve ionization and opacity models in the warm dense\nmatter regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We detail experimental results inferring ionization and temperature for warm\ndense copper plasmas at several times solid density (15 to 25 g/cm$^3$) and\ntemperatures of 10 to 21 eV. Experiments performed at the OMEGA Laser Facility\ngenerate uniform warm dense matter conditions via symmetric shock compression\nof a buried copper layer. The plasma is probed with a laser-generated x-ray\nsource to collect the K-shell x-ray absorption spectrum. Fitting bound-bound\nabsorption contributions from constituent charge states of copper provides an\nestimated $\\overline{Z}$ of approximately 4 to 7 for these warm dense copper\nplasmas. We find that these partially ionized plasmas have K-edge shifts of 12\nto 30 eV and bound-bound resonance 1s$\\rightarrow$3p absorption shifts of 4 to\n26 eV with respect to the cold K-edge. This study provides necessary\nexperimental data to improve ionization and opacity models in the warm dense\nmatter regime."
                },
                "authors": [
                    {
                        "name": "T. Cordova"
                    },
                    {
                        "name": "E. V. Marley"
                    },
                    {
                        "name": "D. A. Chin"
                    },
                    {
                        "name": "R. A. London"
                    },
                    {
                        "name": "H. A. Scott"
                    },
                    {
                        "name": "T. DÃ¶ppner"
                    },
                    {
                        "name": "F. N. Beg"
                    },
                    {
                        "name": "F. Coppari"
                    },
                    {
                        "name": "M. Millot"
                    },
                    {
                        "name": "J. Emig"
                    },
                    {
                        "name": "S. B. Hansen"
                    },
                    {
                        "name": "P. M. Nilson"
                    },
                    {
                        "name": "P. Sterne"
                    },
                    {
                        "name": "M. J. MacDonald"
                    }
                ],
                "author_detail": {
                    "name": "M. J. MacDonald"
                },
                "author": "M. J. MacDonald",
                "arxiv_comment": "10 Pages, 7 Figures, In submission for Physical Review Research",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13268v1",
                "updated": "2025-09-16T17:26:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    26,
                    17,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T17:26:17Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    26,
                    17,
                    1,
                    259,
                    0
                ],
                "title": "LLMs for energy and macronutrients estimation using only text data from\n  24-hour dietary recalls: a parameter-efficient fine-tuning experiment using a\n  10-shot prompt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for energy and macronutrients estimation using only text data from\n  24-hour dietary recalls: a parameter-efficient fine-tuning experiment using a\n  10-shot prompt"
                },
                "summary": "BACKGROUND: Most artificial intelligence tools used to estimate nutritional\ncontent rely on image input. However, whether large language models (LLMs) can\naccurately predict nutritional values based solely on text descriptions of\nfoods consumed remains unknown. If effective, this approach could enable\nsimpler dietary monitoring without the need for photographs. METHODS: We used\n24-hour dietary recalls from adolescents aged 12-19 years in the National\nHealth and Nutrition Examination Survey (NHANES). An open-source quantized LLM\nwas prompted using a 10-shot, chain-of-thought approach to estimate energy and\nfive macronutrients based solely on text strings listing foods and their\nquantities. We then applied parameter-efficient fine-tuning (PEFT) to evaluate\nwhether predictive accuracy improved. NHANES-calculated values served as the\nground truth for energy, proteins, carbohydrates, total sugar, dietary fiber\nand total fat. RESULTS: In a pooled dataset of 11,281 adolescents (49.9% male,\nmean age 15.4 years), the vanilla LLM yielded poor predictions. The mean\nabsolute error (MAE) was 652.08 for energy and the Lin's CCC <0.46 across\nendpoints. In contrast, the fine-tuned model performed substantially better,\nwith energy MAEs ranging from 171.34 to 190.90 across subsets, and Lin's CCC\nexceeding 0.89 for all outcomes. CONCLUSIONS: When prompted using a\nchain-of-thought approach and fine-tuned with PEFT, open-source LLMs exposed\nsolely to text input can accurately predict energy and macronutrient values\nfrom 24-hour dietary recalls. This approach holds promise for low-burden,\ntext-based dietary monitoring tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BACKGROUND: Most artificial intelligence tools used to estimate nutritional\ncontent rely on image input. However, whether large language models (LLMs) can\naccurately predict nutritional values based solely on text descriptions of\nfoods consumed remains unknown. If effective, this approach could enable\nsimpler dietary monitoring without the need for photographs. METHODS: We used\n24-hour dietary recalls from adolescents aged 12-19 years in the National\nHealth and Nutrition Examination Survey (NHANES). An open-source quantized LLM\nwas prompted using a 10-shot, chain-of-thought approach to estimate energy and\nfive macronutrients based solely on text strings listing foods and their\nquantities. We then applied parameter-efficient fine-tuning (PEFT) to evaluate\nwhether predictive accuracy improved. NHANES-calculated values served as the\nground truth for energy, proteins, carbohydrates, total sugar, dietary fiber\nand total fat. RESULTS: In a pooled dataset of 11,281 adolescents (49.9% male,\nmean age 15.4 years), the vanilla LLM yielded poor predictions. The mean\nabsolute error (MAE) was 652.08 for energy and the Lin's CCC <0.46 across\nendpoints. In contrast, the fine-tuned model performed substantially better,\nwith energy MAEs ranging from 171.34 to 190.90 across subsets, and Lin's CCC\nexceeding 0.89 for all outcomes. CONCLUSIONS: When prompted using a\nchain-of-thought approach and fine-tuned with PEFT, open-source LLMs exposed\nsolely to text input can accurately predict energy and macronutrient values\nfrom 24-hour dietary recalls. This approach holds promise for low-burden,\ntext-based dietary monitoring tools."
                },
                "authors": [
                    {
                        "name": "Rodrigo M Carrillo-Larco"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo M Carrillo-Larco"
                },
                "author": "Rodrigo M Carrillo-Larco",
                "arxiv_comment": "https://github.com/rodrigo-carrillo/LLMs-Macronutrient-Estimation-NHANES-Adolescents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13267v1",
                "updated": "2025-09-16T17:24:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    24,
                    35,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T17:24:35Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    24,
                    35,
                    1,
                    259,
                    0
                ],
                "title": "Learning Discrete Bayesian Networks with Hierarchical Dirichlet\n  Shrinkage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Discrete Bayesian Networks with Hierarchical Dirichlet\n  Shrinkage"
                },
                "summary": "Discrete Bayesian networks (DBNs) provide a broadly useful framework for\nmodeling dependence structures in multivariate categorical data. There is a\nvast literature on methods for inferring conditional probabilities and\ngraphical structure in DBNs, but data sparsity and parametric assumptions are\nmajor practical issues. In this article, we detail a comprehensive Bayesian\nframework for learning DBNs. First, we propose a hierarchical prior for the\nconditional probabilities that enables complicated interactions between parent\nvariables and stability in sparse regimes. We give a novel Markov chain Monte\nCarlo (MCMC) algorithm utilizing parallel Langevin proposals to generate exact\nposterior samples, avoiding the pitfalls of variational approximations.\nMoreover, we verify that the full conditional distribution of the concentration\nparameters is log-concave under mild conditions, facilitating efficient\nsampling. We then propose two methods for learning network structures,\nincluding parent sets, Markov blankets, and DAGs, from categorical data. The\nfirst cycles through individual edges each MCMC iteration, whereas the second\nupdates the entire structure as a single step. We evaluate the accuracy, power,\nand MCMC performance of our methods on several simulation studies. Finally, we\napply our methodology to uncover prognostic network structure from primary\nbreast cancer samples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete Bayesian networks (DBNs) provide a broadly useful framework for\nmodeling dependence structures in multivariate categorical data. There is a\nvast literature on methods for inferring conditional probabilities and\ngraphical structure in DBNs, but data sparsity and parametric assumptions are\nmajor practical issues. In this article, we detail a comprehensive Bayesian\nframework for learning DBNs. First, we propose a hierarchical prior for the\nconditional probabilities that enables complicated interactions between parent\nvariables and stability in sparse regimes. We give a novel Markov chain Monte\nCarlo (MCMC) algorithm utilizing parallel Langevin proposals to generate exact\nposterior samples, avoiding the pitfalls of variational approximations.\nMoreover, we verify that the full conditional distribution of the concentration\nparameters is log-concave under mild conditions, facilitating efficient\nsampling. We then propose two methods for learning network structures,\nincluding parent sets, Markov blankets, and DAGs, from categorical data. The\nfirst cycles through individual edges each MCMC iteration, whereas the second\nupdates the entire structure as a single step. We evaluate the accuracy, power,\nand MCMC performance of our methods on several simulation studies. Finally, we\napply our methodology to uncover prognostic network structure from primary\nbreast cancer samples."
                },
                "authors": [
                    {
                        "name": "Alexander Dombowsky"
                    },
                    {
                        "name": "David B. Dunson"
                    }
                ],
                "author_detail": {
                    "name": "David B. Dunson"
                },
                "author": "David B. Dunson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13265v1",
                "updated": "2025-09-16T17:22:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    22,
                    0,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T17:22:00Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    22,
                    0,
                    1,
                    259,
                    0
                ],
                "title": "Beyond Private or Public: Large Language Models as Quasi-Public Goods in\n  the AI Economy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Private or Public: Large Language Models as Quasi-Public Goods in\n  the AI Economy"
                },
                "summary": "This paper conceptualizes Large Language Models (LLMs) as a form of mixed\npublic goods within digital infrastructure, analyzing their economic properties\nthrough a comprehensive theoretical framework. We develop mathematical models\nto quantify the non-rivalry characteristics, partial excludability, and\npositive externalities of LLMs. Through comparative analysis of open-source and\nclosed-source development paths, we identify systematic differences in resource\nallocation efficiency, innovation trajectories, and access equity. Our\nempirical research evaluates the spillover effects and network externalities of\nLLMs across different domains, including knowledge diffusion, innovation\nacceleration, and industry transformation. Based on these findings, we propose\npolicy recommendations for balancing innovation incentives with equitable\naccess, including public-private partnership mechanisms, computational resource\ndemocratization, and governance structures that optimize social welfare. This\ninterdisciplinary approach contributes to understanding the economic nature of\nfoundation AI models and provides policy guidance for their development as\ncritical digital infrastructure",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper conceptualizes Large Language Models (LLMs) as a form of mixed\npublic goods within digital infrastructure, analyzing their economic properties\nthrough a comprehensive theoretical framework. We develop mathematical models\nto quantify the non-rivalry characteristics, partial excludability, and\npositive externalities of LLMs. Through comparative analysis of open-source and\nclosed-source development paths, we identify systematic differences in resource\nallocation efficiency, innovation trajectories, and access equity. Our\nempirical research evaluates the spillover effects and network externalities of\nLLMs across different domains, including knowledge diffusion, innovation\nacceleration, and industry transformation. Based on these findings, we propose\npolicy recommendations for balancing innovation incentives with equitable\naccess, including public-private partnership mechanisms, computational resource\ndemocratization, and governance structures that optimize social welfare. This\ninterdisciplinary approach contributes to understanding the economic nature of\nfoundation AI models and provides policy guidance for their development as\ncritical digital infrastructure"
                },
                "authors": [
                    {
                        "name": "Yukun Zhang"
                    },
                    {
                        "name": "TianYang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "TianYang Zhang"
                },
                "author": "TianYang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13255v1",
                "updated": "2025-09-16T17:12:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    12,
                    23,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T17:12:23Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    12,
                    23,
                    1,
                    259,
                    0
                ],
                "title": "ResidualViT for Efficient Temporally Dense Video Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResidualViT for Efficient Temporally Dense Video Encoding"
                },
                "summary": "Several video understanding tasks, such as natural language temporal video\ngrounding, temporal activity localization, and audio description generation,\nrequire \"temporally dense\" reasoning over frames sampled at high temporal\nresolution. However, computing frame-level features for these tasks is\ncomputationally expensive given the temporal resolution requirements. In this\npaper, we make three contributions to reduce the cost of computing features for\ntemporally dense tasks. First, we introduce a vision transformer (ViT)\narchitecture, dubbed ResidualViT, that leverages the large temporal redundancy\nin videos to efficiently compute temporally dense frame-level features. Our\narchitecture incorporates (i) learnable residual connections that ensure\ntemporal consistency across consecutive frames and (ii) a token reduction\nmodule that enhances processing speed by selectively discarding temporally\nredundant information while reusing weights of a pretrained foundation model.\nSecond, we propose a lightweight distillation strategy to approximate the\nframe-level features of the original foundation model. Finally, we evaluate our\napproach across four tasks and five datasets, in both zero-shot and fully\nsupervised settings, demonstrating significant reductions in computational cost\n(up to 60%) and improvements in inference speed (up to 2.5x faster), all while\nclosely approximating the accuracy of the original foundation model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several video understanding tasks, such as natural language temporal video\ngrounding, temporal activity localization, and audio description generation,\nrequire \"temporally dense\" reasoning over frames sampled at high temporal\nresolution. However, computing frame-level features for these tasks is\ncomputationally expensive given the temporal resolution requirements. In this\npaper, we make three contributions to reduce the cost of computing features for\ntemporally dense tasks. First, we introduce a vision transformer (ViT)\narchitecture, dubbed ResidualViT, that leverages the large temporal redundancy\nin videos to efficiently compute temporally dense frame-level features. Our\narchitecture incorporates (i) learnable residual connections that ensure\ntemporal consistency across consecutive frames and (ii) a token reduction\nmodule that enhances processing speed by selectively discarding temporally\nredundant information while reusing weights of a pretrained foundation model.\nSecond, we propose a lightweight distillation strategy to approximate the\nframe-level features of the original foundation model. Finally, we evaluate our\napproach across four tasks and five datasets, in both zero-shot and fully\nsupervised settings, demonstrating significant reductions in computational cost\n(up to 60%) and improvements in inference speed (up to 2.5x faster), all while\nclosely approximating the accuracy of the original foundation model."
                },
                "authors": [
                    {
                        "name": "Mattia Soldan"
                    },
                    {
                        "name": "Fabian Caba Heilbron"
                    },
                    {
                        "name": "Bernard Ghanem"
                    },
                    {
                        "name": "Josef Sivic"
                    },
                    {
                        "name": "Bryan Russell"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Russell"
                },
                "author": "Bryan Russell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13251v1",
                "updated": "2025-09-16T17:02:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    2,
                    24,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T17:02:24Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    2,
                    24,
                    1,
                    259,
                    0
                ],
                "title": "Large Language Model-assisted Meta-optimizer for Automated Design of\n  Constrained Evolutionary Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-assisted Meta-optimizer for Automated Design of\n  Constrained Evolutionary Algorithm"
                },
                "summary": "Meta-black-box optimization has been significantly advanced through the use\nof large language models (LLMs), yet in fancy on constrained evolutionary\noptimization. In this work, AwesomeDE is proposed that leverages LLMs as the\nstrategy of meta-optimizer to generate update rules for constrained\nevolutionary algorithm without human intervention. On the meanwhile, $RTO^2H$\nframework is introduced for standardize prompt design of LLMs. The\nmeta-optimizer is trained on a diverse set of constrained optimization\nproblems. Key components, including prompt design and iterative refinement, are\nsystematically analyzed to determine their impact on design quality.\nExperimental results demonstrate that the proposed approach outperforms\nexisting methods in terms of computational efficiency and solution accuracy.\nFurthermore, AwesomeDE is shown to generalize well across distinct problem\ndomains, suggesting its potential for broad applicability. This research\ncontributes to the field by providing a scalable and data-driven methodology\nfor automated constrained algorithm design, while also highlighting limitations\nand directions for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-black-box optimization has been significantly advanced through the use\nof large language models (LLMs), yet in fancy on constrained evolutionary\noptimization. In this work, AwesomeDE is proposed that leverages LLMs as the\nstrategy of meta-optimizer to generate update rules for constrained\nevolutionary algorithm without human intervention. On the meanwhile, $RTO^2H$\nframework is introduced for standardize prompt design of LLMs. The\nmeta-optimizer is trained on a diverse set of constrained optimization\nproblems. Key components, including prompt design and iterative refinement, are\nsystematically analyzed to determine their impact on design quality.\nExperimental results demonstrate that the proposed approach outperforms\nexisting methods in terms of computational efficiency and solution accuracy.\nFurthermore, AwesomeDE is shown to generalize well across distinct problem\ndomains, suggesting its potential for broad applicability. This research\ncontributes to the field by providing a scalable and data-driven methodology\nfor automated constrained algorithm design, while also highlighting limitations\nand directions for future work."
                },
                "authors": [
                    {
                        "name": "Xu Yang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Kaiwen Li"
                    },
                    {
                        "name": "Wenhua Li"
                    },
                    {
                        "name": "Weixiong Huang"
                    }
                ],
                "author_detail": {
                    "name": "Weixiong Huang"
                },
                "author": "Weixiong Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13244v1",
                "updated": "2025-09-16T16:54:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    54,
                    35,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T16:54:35Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    54,
                    35,
                    1,
                    259,
                    0
                ],
                "title": "Evaluating LLM Alignment on Personality Inference from Real-World\n  Interview Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLM Alignment on Personality Inference from Real-World\n  Interview Data"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in roles requiring\nnuanced psychological understanding, such as emotional support agents,\ncounselors, and decision-making assistants. However, their ability to interpret\nhuman personality traits, a critical aspect of such applications, remains\nunexplored, particularly in ecologically valid conversational settings. While\nprior work has simulated LLM \"personas\" using discrete Big Five labels on\nsocial media data, the alignment of LLMs with continuous, ground-truth\npersonality assessments derived from natural interactions is largely\nunexamined. To address this gap, we introduce a novel benchmark comprising\nsemi-structured interview transcripts paired with validated continuous Big Five\ntrait scores. Using this dataset, we systematically evaluate LLM performance\nacross three paradigms: (1) zero-shot and chain-of-thought prompting with\nGPT-4.1 Mini, (2) LoRA-based fine-tuning applied to both RoBERTa and Meta-LLaMA\narchitectures, and (3) regression using static embeddings from pretrained BERT\nand OpenAI's text-embedding-3-small. Our results reveal that all Pearson\ncorrelations between model predictions and ground-truth personality traits\nremain below 0.26, highlighting the limited alignment of current LLMs with\nvalidated psychological constructs. Chain-of-thought prompting offers minimal\ngains over zero-shot, suggesting that personality inference relies more on\nlatent semantic representation than explicit reasoning. These findings\nunderscore the challenges of aligning LLMs with complex human attributes and\nmotivate future work on trait-specific prompting, context-aware modeling, and\nalignment-oriented fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in roles requiring\nnuanced psychological understanding, such as emotional support agents,\ncounselors, and decision-making assistants. However, their ability to interpret\nhuman personality traits, a critical aspect of such applications, remains\nunexplored, particularly in ecologically valid conversational settings. While\nprior work has simulated LLM \"personas\" using discrete Big Five labels on\nsocial media data, the alignment of LLMs with continuous, ground-truth\npersonality assessments derived from natural interactions is largely\nunexamined. To address this gap, we introduce a novel benchmark comprising\nsemi-structured interview transcripts paired with validated continuous Big Five\ntrait scores. Using this dataset, we systematically evaluate LLM performance\nacross three paradigms: (1) zero-shot and chain-of-thought prompting with\nGPT-4.1 Mini, (2) LoRA-based fine-tuning applied to both RoBERTa and Meta-LLaMA\narchitectures, and (3) regression using static embeddings from pretrained BERT\nand OpenAI's text-embedding-3-small. Our results reveal that all Pearson\ncorrelations between model predictions and ground-truth personality traits\nremain below 0.26, highlighting the limited alignment of current LLMs with\nvalidated psychological constructs. Chain-of-thought prompting offers minimal\ngains over zero-shot, suggesting that personality inference relies more on\nlatent semantic representation than explicit reasoning. These findings\nunderscore the challenges of aligning LLMs with complex human attributes and\nmotivate future work on trait-specific prompting, context-aware modeling, and\nalignment-oriented fine-tuning."
                },
                "authors": [
                    {
                        "name": "Jianfeng Zhu"
                    },
                    {
                        "name": "Julina Maharjan"
                    },
                    {
                        "name": "Xinyu Li"
                    },
                    {
                        "name": "Karin G. Coifman"
                    },
                    {
                        "name": "Ruoming Jin"
                    }
                ],
                "author_detail": {
                    "name": "Ruoming Jin"
                },
                "author": "Ruoming Jin",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19668v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19668v3",
                "updated": "2025-09-16T16:49:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    49,
                    11,
                    1,
                    259,
                    0
                ],
                "published": "2025-02-27T01:29:51Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    1,
                    29,
                    51,
                    3,
                    58,
                    0
                ],
                "title": "SuPreME: A Supervised Pre-training Framework for Multimodal ECG\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuPreME: A Supervised Pre-training Framework for Multimodal ECG\n  Representation Learning"
                },
                "summary": "Cardiovascular diseases are a leading cause of death and disability\nworldwide. Electrocardiogram (ECG) is critical for diagnosing and monitoring\ncardiac health, but obtaining large-scale annotated ECG datasets is\nlabor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL)\nmethods mitigate this by learning features without extensive labels but fail to\ncapture fine-grained clinical semantics and require extensive task-specific\nfine-tuning. To address these challenges, we propose $\\textbf{SuPreME}$, a\n$\\textbf{Su}$pervised $\\textbf{Pre}$-training framework for\n$\\textbf{M}$ultimodal $\\textbf{E}$CG representation learning. SuPreME is\npre-trained using structured diagnostic labels derived from ECG report entities\nthrough a one-time offline extraction with Large Language Models (LLMs), which\nhelp denoise, standardize cardiac concepts, and improve clinical representation\nlearning. By fusing ECG signals with textual cardiac queries instead of fixed\nlabels, SuPreME enables zero-shot classification of unseen conditions without\nfurther fine-tuning. We evaluate SuPreME on six downstream datasets covering\n106 cardiac conditions, achieving superior zero-shot AUC performance of\n$77.20\\%$, surpassing state-of-the-art eSSLs by $4.98\\%$. Results demonstrate\nSuPreME's effectiveness in leveraging structured, clinically relevant knowledge\nfor high-quality ECG representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cardiovascular diseases are a leading cause of death and disability\nworldwide. Electrocardiogram (ECG) is critical for diagnosing and monitoring\ncardiac health, but obtaining large-scale annotated ECG datasets is\nlabor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL)\nmethods mitigate this by learning features without extensive labels but fail to\ncapture fine-grained clinical semantics and require extensive task-specific\nfine-tuning. To address these challenges, we propose $\\textbf{SuPreME}$, a\n$\\textbf{Su}$pervised $\\textbf{Pre}$-training framework for\n$\\textbf{M}$ultimodal $\\textbf{E}$CG representation learning. SuPreME is\npre-trained using structured diagnostic labels derived from ECG report entities\nthrough a one-time offline extraction with Large Language Models (LLMs), which\nhelp denoise, standardize cardiac concepts, and improve clinical representation\nlearning. By fusing ECG signals with textual cardiac queries instead of fixed\nlabels, SuPreME enables zero-shot classification of unseen conditions without\nfurther fine-tuning. We evaluate SuPreME on six downstream datasets covering\n106 cardiac conditions, achieving superior zero-shot AUC performance of\n$77.20\\%$, surpassing state-of-the-art eSSLs by $4.98\\%$. Results demonstrate\nSuPreME's effectiveness in leveraging structured, clinically relevant knowledge\nfor high-quality ECG representations."
                },
                "authors": [
                    {
                        "name": "Mingsheng Cai"
                    },
                    {
                        "name": "Jiuming Jiang"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Rossella Arcucci"
                    }
                ],
                "author_detail": {
                    "name": "Rossella Arcucci"
                },
                "author": "Rossella Arcucci",
                "arxiv_comment": "Findings of The 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19668v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19668v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01132v2",
                "updated": "2025-09-16T16:47:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    47,
                    26,
                    1,
                    259,
                    0
                ],
                "published": "2025-04-01T19:08:24Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    8,
                    24,
                    1,
                    91,
                    0
                ],
                "title": "Is the Top Still Spinning? Evaluating Subjectivity in Narrative\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is the Top Still Spinning? Evaluating Subjectivity in Narrative\n  Understanding"
                },
                "summary": "Determining faithfulness of a claim to a source document is an important\nproblem across many domains. This task is generally treated as a binary\njudgment of whether the claim is supported or unsupported in relation to the\nsource. In many cases, though, whether a claim is supported can be ambiguous.\nFor instance, it may depend on making inferences from given evidence, and\ndifferent people can reasonably interpret the claim as either supported or\nunsupported based on their agreement with those inferences. Forcing binary\nlabels upon such claims lowers the reliability of evaluation. In this work, we\nreframe the task to manage the subjectivity involved with factuality judgments\nof ambiguous claims. We introduce LLM-generated edits of summaries as a method\nof providing a nuanced evaluation of claims: how much does a summary need to be\nedited to be unambiguous? Whether a claim gets rewritten and how much it\nchanges can be used as an automatic evaluation metric, the Ambiguity Rewrite\nMetric (ARM), with a much richer feedback signal than a binary judgment of\nfaithfulness. We focus on the area of narrative summarization as it is\nparticularly rife with ambiguity and subjective interpretation. We show that\nARM produces a 21% absolute improvement in annotator agreement on claim\nfaithfulness, indicating that subjectivity is reduced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Determining faithfulness of a claim to a source document is an important\nproblem across many domains. This task is generally treated as a binary\njudgment of whether the claim is supported or unsupported in relation to the\nsource. In many cases, though, whether a claim is supported can be ambiguous.\nFor instance, it may depend on making inferences from given evidence, and\ndifferent people can reasonably interpret the claim as either supported or\nunsupported based on their agreement with those inferences. Forcing binary\nlabels upon such claims lowers the reliability of evaluation. In this work, we\nreframe the task to manage the subjectivity involved with factuality judgments\nof ambiguous claims. We introduce LLM-generated edits of summaries as a method\nof providing a nuanced evaluation of claims: how much does a summary need to be\nedited to be unambiguous? Whether a claim gets rewritten and how much it\nchanges can be used as an automatic evaluation metric, the Ambiguity Rewrite\nMetric (ARM), with a much richer feedback signal than a binary judgment of\nfaithfulness. We focus on the area of narrative summarization as it is\nparticularly rife with ambiguity and subjective interpretation. We show that\nARM produces a 21% absolute improvement in annotator agreement on claim\nfaithfulness, indicating that subjectivity is reduced."
                },
                "authors": [
                    {
                        "name": "Melanie Subbiah"
                    },
                    {
                        "name": "Akankshya Mishra"
                    },
                    {
                        "name": "Grace Kim"
                    },
                    {
                        "name": "Liyan Tang"
                    },
                    {
                        "name": "Greg Durrett"
                    },
                    {
                        "name": "Kathleen McKeown"
                    }
                ],
                "author_detail": {
                    "name": "Kathleen McKeown"
                },
                "author": "Kathleen McKeown",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13237v1",
                "updated": "2025-09-16T16:44:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    44,
                    26,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T16:44:26Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    44,
                    26,
                    1,
                    259,
                    0
                ],
                "title": "Metacognitive Reuse: Turning Recurring LLM Reasoning Into Concise\n  Behaviors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metacognitive Reuse: Turning Recurring LLM Reasoning Into Concise\n  Behaviors"
                },
                "summary": "Large language models (LLMs) now solve multi-step problems by emitting\nextended chains of thought. During the process, they often re-derive the same\nintermediate steps across problems, inflating token usage and latency. This\nsaturation of the context window leaves less capacity for exploration. We study\na simple mechanism that converts recurring reasoning fragments into concise,\nreusable \"behaviors\" (name + instruction) via the model's own metacognitive\nanalysis of prior traces. These behaviors are stored in a \"behavior handbook\"\nwhich supplies them to the model in-context at inference or distills them into\nparameters via supervised fine-tuning. This approach achieves improved\ntest-time reasoning across three different settings - 1) Behavior-conditioned\ninference: Providing the LLM relevant behaviors in-context during reasoning\nreduces number of reasoning tokens by up to 46% while matching or improving\nbaseline accuracy; 2) Behavior-guided self-improvement: Without any parameter\nupdates, the model improves its own future reasoning by leveraging behaviors\nfrom its own past problem solving attempts. This yields up to 10% higher\naccuracy than a naive critique-and-revise baseline; and 3) Behavior-conditioned\nSFT: SFT on behavior-conditioned reasoning traces is more effective at\nconverting non-reasoning models into reasoning models as compared to vanilla\nSFT. Together, these results indicate that turning slow derivations into fast\nprocedural hints enables LLMs to remember how to reason, not just what to\nconclude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) now solve multi-step problems by emitting\nextended chains of thought. During the process, they often re-derive the same\nintermediate steps across problems, inflating token usage and latency. This\nsaturation of the context window leaves less capacity for exploration. We study\na simple mechanism that converts recurring reasoning fragments into concise,\nreusable \"behaviors\" (name + instruction) via the model's own metacognitive\nanalysis of prior traces. These behaviors are stored in a \"behavior handbook\"\nwhich supplies them to the model in-context at inference or distills them into\nparameters via supervised fine-tuning. This approach achieves improved\ntest-time reasoning across three different settings - 1) Behavior-conditioned\ninference: Providing the LLM relevant behaviors in-context during reasoning\nreduces number of reasoning tokens by up to 46% while matching or improving\nbaseline accuracy; 2) Behavior-guided self-improvement: Without any parameter\nupdates, the model improves its own future reasoning by leveraging behaviors\nfrom its own past problem solving attempts. This yields up to 10% higher\naccuracy than a naive critique-and-revise baseline; and 3) Behavior-conditioned\nSFT: SFT on behavior-conditioned reasoning traces is more effective at\nconverting non-reasoning models into reasoning models as compared to vanilla\nSFT. Together, these results indicate that turning slow derivations into fast\nprocedural hints enables LLMs to remember how to reason, not just what to\nconclude."
                },
                "authors": [
                    {
                        "name": "Aniket Didolkar"
                    },
                    {
                        "name": "Nicolas Ballas"
                    },
                    {
                        "name": "Sanjeev Arora"
                    },
                    {
                        "name": "Anirudh Goyal"
                    }
                ],
                "author_detail": {
                    "name": "Anirudh Goyal"
                },
                "author": "Anirudh Goyal",
                "arxiv_comment": "18 pages, 9 Figures, 5 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13234v1",
                "updated": "2025-09-16T16:42:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    42,
                    19,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T16:42:19Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    42,
                    19,
                    1,
                    259,
                    0
                ],
                "title": "Simulating Clinical AI Assistance using Multimodal LLMs: A Case Study in\n  Diabetic Retinopathy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Clinical AI Assistance using Multimodal LLMs: A Case Study in\n  Diabetic Retinopathy"
                },
                "summary": "Diabetic retinopathy (DR) is a leading cause of blindness worldwide, and AI\nsystems can expand access to fundus photography screening. Current FDA-cleared\nsystems primarily provide binary referral outputs, where this minimal output\nmay limit clinical trust and utility. Yet, determining the most effective\noutput format to enhance clinician-AI performance is an empirical challenge\nthat is difficult to assess at scale. We evaluated multimodal large language\nmodels (MLLMs) for DR detection and their ability to simulate clinical AI\nassistance across different output types. Two models were tested on IDRiD and\nMessidor-2: GPT-4o, a general-purpose MLLM, and MedGemma, an open-source\nmedical model. Experiments included: (1) baseline evaluation, (2) simulated AI\nassistance with synthetic predictions, and (3) actual AI-to-AI collaboration\nwhere GPT-4o incorporated MedGemma outputs. MedGemma outperformed GPT-4o at\nbaseline, achieving higher sensitivity and AUROC, while GPT-4o showed\nnear-perfect specificity but low sensitivity. Both models adjusted predictions\nbased on simulated AI inputs, but GPT-4o's performance collapsed with incorrect\nones, whereas MedGemma remained more stable. In actual collaboration, GPT-4o\nachieved strong results when guided by MedGemma's descriptive outputs, even\nwithout direct image access (AUROC up to 0.96). These findings suggest MLLMs\nmay improve DR screening pipelines and serve as scalable simulators for\nstudying clinical AI assistance across varying output configurations. Open,\nlightweight models such as MedGemma may be especially valuable in low-resource\nsettings, while descriptive outputs could enhance explainability and clinician\ntrust in clinical workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diabetic retinopathy (DR) is a leading cause of blindness worldwide, and AI\nsystems can expand access to fundus photography screening. Current FDA-cleared\nsystems primarily provide binary referral outputs, where this minimal output\nmay limit clinical trust and utility. Yet, determining the most effective\noutput format to enhance clinician-AI performance is an empirical challenge\nthat is difficult to assess at scale. We evaluated multimodal large language\nmodels (MLLMs) for DR detection and their ability to simulate clinical AI\nassistance across different output types. Two models were tested on IDRiD and\nMessidor-2: GPT-4o, a general-purpose MLLM, and MedGemma, an open-source\nmedical model. Experiments included: (1) baseline evaluation, (2) simulated AI\nassistance with synthetic predictions, and (3) actual AI-to-AI collaboration\nwhere GPT-4o incorporated MedGemma outputs. MedGemma outperformed GPT-4o at\nbaseline, achieving higher sensitivity and AUROC, while GPT-4o showed\nnear-perfect specificity but low sensitivity. Both models adjusted predictions\nbased on simulated AI inputs, but GPT-4o's performance collapsed with incorrect\nones, whereas MedGemma remained more stable. In actual collaboration, GPT-4o\nachieved strong results when guided by MedGemma's descriptive outputs, even\nwithout direct image access (AUROC up to 0.96). These findings suggest MLLMs\nmay improve DR screening pipelines and serve as scalable simulators for\nstudying clinical AI assistance across varying output configurations. Open,\nlightweight models such as MedGemma may be especially valuable in low-resource\nsettings, while descriptive outputs could enhance explainability and clinician\ntrust in clinical workflows."
                },
                "authors": [
                    {
                        "name": "Nadim Barakat"
                    },
                    {
                        "name": "William Lotter"
                    }
                ],
                "author_detail": {
                    "name": "William Lotter"
                },
                "author": "William Lotter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13232v1",
                "updated": "2025-09-16T16:39:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    39,
                    11,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T16:39:11Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    39,
                    11,
                    1,
                    259,
                    0
                ],
                "title": "Single-stream Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-stream Policy Optimization"
                },
                "summary": "We revisit policy-gradient optimization for Large Language Models (LLMs) from\na single-stream perspective. Prevailing group-based methods like GRPO reduce\nvariance with on-the-fly baselines but suffer from critical flaws: frequent\ndegenerate groups erase learning signals, and synchronization barriers hinder\nscalability. We introduce Single-stream Policy Optimization (SPO), which\neliminates these issues by design. SPO replaces per-group baselines with a\npersistent, KL-adaptive value tracker and normalizes advantages globally across\nthe batch, providing a stable, low-variance learning signal for every sample.\nBeing group-free, SPO enables higher throughput and scales effectively in\nlong-horizon or tool-integrated settings where generation times vary.\nFurthermore, the persistent value tracker naturally enables an adaptive\ncurriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO\nconverges more smoothly and attains higher accuracy than GRPO, while\neliminating computation wasted on degenerate groups. Ablation studies confirm\nthat SPO's gains stem from its principled approach to baseline estimation and\nadvantage normalization, offering a more robust and efficient path for LLM\nreasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the\naverage maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial\nabsolute point gains on challenging datasets, including +7.3 pp on BRUMO 25,\n+4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain\nin pass@$k$ across the evaluated $k$ values. SPO's success challenges the\nprevailing trend of adding incidental complexity to RL algorithms, highlighting\na path where fundamental principles, not architectural workarounds, drive the\nnext wave of progress in LLM reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We revisit policy-gradient optimization for Large Language Models (LLMs) from\na single-stream perspective. Prevailing group-based methods like GRPO reduce\nvariance with on-the-fly baselines but suffer from critical flaws: frequent\ndegenerate groups erase learning signals, and synchronization barriers hinder\nscalability. We introduce Single-stream Policy Optimization (SPO), which\neliminates these issues by design. SPO replaces per-group baselines with a\npersistent, KL-adaptive value tracker and normalizes advantages globally across\nthe batch, providing a stable, low-variance learning signal for every sample.\nBeing group-free, SPO enables higher throughput and scales effectively in\nlong-horizon or tool-integrated settings where generation times vary.\nFurthermore, the persistent value tracker naturally enables an adaptive\ncurriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO\nconverges more smoothly and attains higher accuracy than GRPO, while\neliminating computation wasted on degenerate groups. Ablation studies confirm\nthat SPO's gains stem from its principled approach to baseline estimation and\nadvantage normalization, offering a more robust and efficient path for LLM\nreasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the\naverage maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial\nabsolute point gains on challenging datasets, including +7.3 pp on BRUMO 25,\n+4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain\nin pass@$k$ across the evaluated $k$ values. SPO's success challenges the\nprevailing trend of adding incidental complexity to RL algorithms, highlighting\na path where fundamental principles, not architectural workarounds, drive the\nnext wave of progress in LLM reasoning."
                },
                "authors": [
                    {
                        "name": "Zhongwen Xu"
                    },
                    {
                        "name": "Zihan Ding"
                    }
                ],
                "author_detail": {
                    "name": "Zihan Ding"
                },
                "author": "Zihan Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13201v1",
                "updated": "2025-09-16T16:06:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    6,
                    33,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T16:06:33Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    6,
                    33,
                    1,
                    259,
                    0
                ],
                "title": "Scaling Up Throughput-oriented LLM Inference Applications on\n  Heterogeneous Opportunistic GPU Clusters with Pervasive Context Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up Throughput-oriented LLM Inference Applications on\n  Heterogeneous Opportunistic GPU Clusters with Pervasive Context Management"
                },
                "summary": "The widespread growth in LLM developments increasingly demands more\ncomputational power from clusters than what they can supply. Traditional LLM\napplications inherently require huge static resource allocations, which force\nusers to either wait in a long job queue and accept progress delay, or buy\nexpensive hardware to fulfill their needs and exacerbate the demand-supply\nproblem. However, not all LLM applications are latency-sensitive and can\ninstead be executed in a throughput-oriented way. This throughput orientation\nallows a dynamic allocation that opportunistically pools available resources\nover time, avoiding both the long queue and expensive GPU purchases.\nEffectively utilizing opportunistic resources brings numerous challenges\nnevertheless. Our solution, pervasive context management, exploits the common\ncomputational context in LLM applications and provides mechanisms and policies\nthat allow seamless context reuse on opportunistic resources. Our evaluation\nshows an LLM application with pervasive context management on opportunistic\nresources reduces its execution time by 98.1%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread growth in LLM developments increasingly demands more\ncomputational power from clusters than what they can supply. Traditional LLM\napplications inherently require huge static resource allocations, which force\nusers to either wait in a long job queue and accept progress delay, or buy\nexpensive hardware to fulfill their needs and exacerbate the demand-supply\nproblem. However, not all LLM applications are latency-sensitive and can\ninstead be executed in a throughput-oriented way. This throughput orientation\nallows a dynamic allocation that opportunistically pools available resources\nover time, avoiding both the long queue and expensive GPU purchases.\nEffectively utilizing opportunistic resources brings numerous challenges\nnevertheless. Our solution, pervasive context management, exploits the common\ncomputational context in LLM applications and provides mechanisms and policies\nthat allow seamless context reuse on opportunistic resources. Our evaluation\nshows an LLM application with pervasive context management on opportunistic\nresources reduces its execution time by 98.1%."
                },
                "authors": [
                    {
                        "name": "Thanh Son Phung"
                    },
                    {
                        "name": "Douglas Thain"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Thain"
                },
                "author": "Douglas Thain",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13196v1",
                "updated": "2025-09-16T16:00:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    0,
                    6,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T16:00:06Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    0,
                    6,
                    1,
                    259,
                    0
                ],
                "title": "The Few-shot Dilemma: Over-prompting Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Few-shot Dilemma: Over-prompting Large Language Models"
                },
                "summary": "Over-prompting, a phenomenon where excessive examples in prompts lead to\ndiminished performance in Large Language Models (LLMs), challenges the\nconventional wisdom about in-context few-shot learning. To investigate this\nfew-shot dilemma, we outline a prompting framework that leverages three\nstandard few-shot selection methods - random sampling, semantic embedding, and\nTF-IDF vectors - and evaluate these methods across multiple LLMs, including\nGPT-4o, GPT-3.5-turbo, DeepSeek-V3, Gemma-3, LLaMA-3.1, LLaMA-3.2, and Mistral.\nOur experimental results reveal that incorporating excessive domain-specific\nexamples into prompts can paradoxically degrade performance in certain LLMs,\nwhich contradicts the prior empirical conclusion that more relevant few-shot\nexamples universally benefit LLMs. Given the trend of LLM-assisted software\nengineering and requirement analysis, we experiment with two real-world\nsoftware requirement classification datasets. By gradually increasing the\nnumber of TF-IDF-selected and stratified few-shot examples, we identify their\noptimal quantity for each LLM. This combined approach achieves superior\nperformance with fewer examples, avoiding the over-prompting problem, thus\nsurpassing the state-of-the-art by 1% in classifying functional and\nnon-functional requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over-prompting, a phenomenon where excessive examples in prompts lead to\ndiminished performance in Large Language Models (LLMs), challenges the\nconventional wisdom about in-context few-shot learning. To investigate this\nfew-shot dilemma, we outline a prompting framework that leverages three\nstandard few-shot selection methods - random sampling, semantic embedding, and\nTF-IDF vectors - and evaluate these methods across multiple LLMs, including\nGPT-4o, GPT-3.5-turbo, DeepSeek-V3, Gemma-3, LLaMA-3.1, LLaMA-3.2, and Mistral.\nOur experimental results reveal that incorporating excessive domain-specific\nexamples into prompts can paradoxically degrade performance in certain LLMs,\nwhich contradicts the prior empirical conclusion that more relevant few-shot\nexamples universally benefit LLMs. Given the trend of LLM-assisted software\nengineering and requirement analysis, we experiment with two real-world\nsoftware requirement classification datasets. By gradually increasing the\nnumber of TF-IDF-selected and stratified few-shot examples, we identify their\noptimal quantity for each LLM. This combined approach achieves superior\nperformance with fewer examples, avoiding the over-prompting problem, thus\nsurpassing the state-of-the-art by 1% in classifying functional and\nnon-functional requirements."
                },
                "authors": [
                    {
                        "name": "Yongjian Tang"
                    },
                    {
                        "name": "Doruk Tuncel"
                    },
                    {
                        "name": "Christian Koerner"
                    },
                    {
                        "name": "Thomas Runkler"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Runkler"
                },
                "author": "Thomas Runkler",
                "arxiv_comment": "accepted for the main track of FLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09004v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09004v3",
                "updated": "2025-09-16T15:59:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    59,
                    27,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-10T21:05:27Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    21,
                    5,
                    27,
                    2,
                    253,
                    0
                ],
                "title": "Implicit Neural Representations of Intramyocardial Motion and Strain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Neural Representations of Intramyocardial Motion and Strain"
                },
                "summary": "Automatic quantification of intramyocardial motion and strain from tagging\nMRI remains an important but challenging task. We propose a method using\nimplicit neural representations (INRs), conditioned on learned latent codes, to\npredict continuous left ventricular (LV) displacement -- without requiring\ninference-time optimisation. Evaluated on 452 UK Biobank test cases, our method\nachieved the best tracking accuracy (2.14 mm RMSE) and the lowest combined\nerror in global circumferential (2.86%) and radial (6.42%) strain compared to\nthree deep learning baselines. In addition, our method is $\\sim$380$\\times$\nfaster than the most accurate baseline. These results highlight the suitability\nof INR-based models for accurate and scalable analysis of myocardial strain in\nlarge CMR datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic quantification of intramyocardial motion and strain from tagging\nMRI remains an important but challenging task. We propose a method using\nimplicit neural representations (INRs), conditioned on learned latent codes, to\npredict continuous left ventricular (LV) displacement -- without requiring\ninference-time optimisation. Evaluated on 452 UK Biobank test cases, our method\nachieved the best tracking accuracy (2.14 mm RMSE) and the lowest combined\nerror in global circumferential (2.86%) and radial (6.42%) strain compared to\nthree deep learning baselines. In addition, our method is $\\sim$380$\\times$\nfaster than the most accurate baseline. These results highlight the suitability\nof INR-based models for accurate and scalable analysis of myocardial strain in\nlarge CMR datasets."
                },
                "authors": [
                    {
                        "name": "Andrew Bell"
                    },
                    {
                        "name": "Yan Kit Choi"
                    },
                    {
                        "name": "Steffen E Petersen"
                    },
                    {
                        "name": "Andrew King"
                    },
                    {
                        "name": "Muhummad Sohaib Nazir"
                    },
                    {
                        "name": "Alistair A Young"
                    }
                ],
                "author_detail": {
                    "name": "Alistair A Young"
                },
                "author": "Alistair A Young",
                "arxiv_comment": "STACOM 2025 @ MICCAI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09004v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09004v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13189v1",
                "updated": "2025-09-16T15:42:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    42,
                    22,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T15:42:22Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    42,
                    22,
                    1,
                    259,
                    0
                ],
                "title": "SURGIN: SURrogate-guided Generative INversion for subsurface multiphase\n  flow with quantified uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SURGIN: SURrogate-guided Generative INversion for subsurface multiphase\n  flow with quantified uncertainty"
                },
                "summary": "We present a direct inverse modeling method named SURGIN, a SURrogate-guided\nGenerative INversion framework tailed for subsurface multiphase flow data\nassimilation. Unlike existing inversion methods that require adaptation for\neach new observational configuration, SURGIN features a zero-shot conditional\ngeneration capability, enabling real-time assimilation of unseen monitoring\ndata without task-specific retraining. Specifically, SURGIN synergistically\nintegrates a U-Net enhanced Fourier Neural Operator (U-FNO) surrogate with a\nscore-based generative model (SGM), framing the conditional generation as a\nsurrogate prediction-guidance process in a Bayesian perspective. Instead of\ndirectly learning the conditional generation of geological parameters, an\nunconditional SGM is first pretrained in a self-supervised manner to capture\nthe geological prior, after which posterior sampling is performed by leveraging\na differentiable U-FNO surrogate to enable efficient forward evaluations\nconditioned on unseen observations. Extensive numerical experiments demonstrate\nSURGIN's capability to decently infer heterogeneous geological fields and\npredict spatiotemporal flow dynamics with quantified uncertainty across diverse\nmeasurement settings. By unifying generative learning with surrogate-guided\nBayesian inference, SURGIN establishes a new paradigm for inverse modeling and\nuncertainty quantification in parametric functional spaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a direct inverse modeling method named SURGIN, a SURrogate-guided\nGenerative INversion framework tailed for subsurface multiphase flow data\nassimilation. Unlike existing inversion methods that require adaptation for\neach new observational configuration, SURGIN features a zero-shot conditional\ngeneration capability, enabling real-time assimilation of unseen monitoring\ndata without task-specific retraining. Specifically, SURGIN synergistically\nintegrates a U-Net enhanced Fourier Neural Operator (U-FNO) surrogate with a\nscore-based generative model (SGM), framing the conditional generation as a\nsurrogate prediction-guidance process in a Bayesian perspective. Instead of\ndirectly learning the conditional generation of geological parameters, an\nunconditional SGM is first pretrained in a self-supervised manner to capture\nthe geological prior, after which posterior sampling is performed by leveraging\na differentiable U-FNO surrogate to enable efficient forward evaluations\nconditioned on unseen observations. Extensive numerical experiments demonstrate\nSURGIN's capability to decently infer heterogeneous geological fields and\npredict spatiotemporal flow dynamics with quantified uncertainty across diverse\nmeasurement settings. By unifying generative learning with surrogate-guided\nBayesian inference, SURGIN establishes a new paradigm for inverse modeling and\nuncertainty quantification in parametric functional spaces."
                },
                "authors": [
                    {
                        "name": "Zhao Feng"
                    },
                    {
                        "name": "Bicheng Yan"
                    },
                    {
                        "name": "Luanxiao Zhao"
                    },
                    {
                        "name": "Xianda Shen"
                    },
                    {
                        "name": "Renyu Zhao"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Fengshou Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Fengshou Zhang"
                },
                "author": "Fengshou Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13886v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13886v4",
                "updated": "2025-09-16T15:33:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    33,
                    16,
                    1,
                    259,
                    0
                ],
                "published": "2025-05-20T03:47:44Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    3,
                    47,
                    44,
                    1,
                    140,
                    0
                ],
                "title": "Game-RL: Synthesizing Verifiable Game Tasks at Scale to Boost VLMs\n  General Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Game-RL: Synthesizing Verifiable Game Tasks at Scale to Boost VLMs\n  General Reasoning"
                },
                "summary": "Real-world vision language reasoning scenarios often include diverse and\ncomplex tasks. However, vision language reinforcement learning has primarily\nfocused on a narrow set of tasks (e.g. geometry or chart reasoning), limiting\nthe improvement of Vision Language Models' (VLMs) general reasoning. Therefore,\nwe propose a novel Code2Logic approach, using Large Language Models (LLMs) to\nsynthesize verifiable game reasoning tasks at scale via adapting game code.\nUsing the Code2Logic, we developed the GameQA dataset to train and evaluate\nVLMs. GameQA is verifiable and scalable, offers controllable difficulty\ngradation and is diverse with 30 games and 158 tasks. Then we apply Game-RL,\nwhich is simple reinforcement learning on GameQA. Surprisingly, despite\ntraining solely on game tasks, VLMs demonstrated out of domain generalization,\nspecifically Qwen2.5-VL-7B improving performance by 2.33% across 7 diverse\nvision-language benchmarks. Our code, dataset and models are available at the\nGitHub repository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world vision language reasoning scenarios often include diverse and\ncomplex tasks. However, vision language reinforcement learning has primarily\nfocused on a narrow set of tasks (e.g. geometry or chart reasoning), limiting\nthe improvement of Vision Language Models' (VLMs) general reasoning. Therefore,\nwe propose a novel Code2Logic approach, using Large Language Models (LLMs) to\nsynthesize verifiable game reasoning tasks at scale via adapting game code.\nUsing the Code2Logic, we developed the GameQA dataset to train and evaluate\nVLMs. GameQA is verifiable and scalable, offers controllable difficulty\ngradation and is diverse with 30 games and 158 tasks. Then we apply Game-RL,\nwhich is simple reinforcement learning on GameQA. Surprisingly, despite\ntraining solely on game tasks, VLMs demonstrated out of domain generalization,\nspecifically Qwen2.5-VL-7B improving performance by 2.33% across 7 diverse\nvision-language benchmarks. Our code, dataset and models are available at the\nGitHub repository."
                },
                "authors": [
                    {
                        "name": "Jingqi Tong"
                    },
                    {
                        "name": "Jixin Tang"
                    },
                    {
                        "name": "Hangcheng Li"
                    },
                    {
                        "name": "Yurong Mou"
                    },
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Yanbo Wen"
                    },
                    {
                        "name": "Fan Song"
                    },
                    {
                        "name": "Jiahao Zhan"
                    },
                    {
                        "name": "Yuyang Lu"
                    },
                    {
                        "name": "Chaoran Tao"
                    },
                    {
                        "name": "Zhiyuan Guo"
                    },
                    {
                        "name": "Jizhou Yu"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Changhao Jiang"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Tao Liang"
                    },
                    {
                        "name": "Zhihui Fei"
                    },
                    {
                        "name": "Mingyang Wan"
                    },
                    {
                        "name": "Guojun Ma"
                    },
                    {
                        "name": "Weifeng Ge"
                    },
                    {
                        "name": "Guanhua Chen"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "63 pages, 23 figures, submitted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13886v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13886v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13179v1",
                "updated": "2025-09-16T15:32:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    32,
                    51,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T15:32:51Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    32,
                    51,
                    1,
                    259,
                    0
                ],
                "title": "Efficient Cold-Start Recommendation via BPE Token-Level Embedding\n  Initialization with LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Cold-Start Recommendation via BPE Token-Level Embedding\n  Initialization with LLM"
                },
                "summary": "The cold-start issue is the challenge when we talk about recommender systems,\nespecially in the case when we do not have the past interaction data of new\nusers or new items. Content-based features or hybrid solutions are common as\nconventional solutions, but they can only work in a sparse metadata environment\nwith shallow patterns. In this paper, the efficient cold-start recommendation\nstrategy is presented, which is based on the sub word-level representations by\napplying Byte Pair Encoding (BPE) tokenization and pre-trained Large Language\nModel (LLM) embedding in the initialization procedure. We obtain fine-grained\ntoken-level vectors that are aligned with the BPE vocabulary as opposed to\nusing coarse-grained sentence embeddings. Together, these token embeddings can\nbe used as dense semantic priors on unseen entities, making immediate\nrecommendation performance possible without user-item interaction history. Our\nmechanism can be compared to collaborative filtering systems and tested over\nbenchmark datasets with stringent cold-start assumptions. Experimental findings\nshow that the given BPE-LLM method achieves higher Recall@k, NDCG@k, and Hit\nRate measurements compared to the standard baseline and displays the same\ncapability of sufficient computational performance. Furthermore, we demonstrate\nthat using subword-aware embeddings yields better generalizability and is more\ninterpretable, especially within a multilingual and sparse input setting. The\npractical application of token-level semantic initialization as a lightweight,\nbut nevertheless effective extension to modern recommender systems in the\nzero-shot setting is indicated within this work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cold-start issue is the challenge when we talk about recommender systems,\nespecially in the case when we do not have the past interaction data of new\nusers or new items. Content-based features or hybrid solutions are common as\nconventional solutions, but they can only work in a sparse metadata environment\nwith shallow patterns. In this paper, the efficient cold-start recommendation\nstrategy is presented, which is based on the sub word-level representations by\napplying Byte Pair Encoding (BPE) tokenization and pre-trained Large Language\nModel (LLM) embedding in the initialization procedure. We obtain fine-grained\ntoken-level vectors that are aligned with the BPE vocabulary as opposed to\nusing coarse-grained sentence embeddings. Together, these token embeddings can\nbe used as dense semantic priors on unseen entities, making immediate\nrecommendation performance possible without user-item interaction history. Our\nmechanism can be compared to collaborative filtering systems and tested over\nbenchmark datasets with stringent cold-start assumptions. Experimental findings\nshow that the given BPE-LLM method achieves higher Recall@k, NDCG@k, and Hit\nRate measurements compared to the standard baseline and displays the same\ncapability of sufficient computational performance. Furthermore, we demonstrate\nthat using subword-aware embeddings yields better generalizability and is more\ninterpretable, especially within a multilingual and sparse input setting. The\npractical application of token-level semantic initialization as a lightweight,\nbut nevertheless effective extension to modern recommender systems in the\nzero-shot setting is indicated within this work."
                },
                "authors": [
                    {
                        "name": "Yushang Zhao"
                    },
                    {
                        "name": "Xinyue Han"
                    },
                    {
                        "name": "Qian Leng"
                    },
                    {
                        "name": "Qianyi Sun"
                    },
                    {
                        "name": "Haotian Lyu"
                    },
                    {
                        "name": "Chengrui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Chengrui Zhou"
                },
                "author": "Chengrui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13176v1",
                "updated": "2025-09-16T15:29:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    29,
                    45,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T15:29:45Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    29,
                    45,
                    1,
                    259,
                    0
                ],
                "title": "Semiparametric Causal Inference for Right-Censored Outcomes with Many\n  Weak Invalid Instruments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semiparametric Causal Inference for Right-Censored Outcomes with Many\n  Weak Invalid Instruments"
                },
                "summary": "We propose a semiparametric framework for causal inference with\nright-censored survival outcomes and many weak invalid instruments, motivated\nby Mendelian randomization in biobank studies where classical methods may fail.\nWe adopt an accelerated failure time model and construct a moment condition\nbased on augmented inverse probability of censoring weighting, incorporating\nboth uncensored and censored observations. Under a heteroscedasticity-based\ncondition on the treatment model, we establish point identification of the\ncausal effect despite censoring and invalid instruments. We propose GEL-NOW\n(Generalized Empirical Likelihood with Non-Orthogonal and Weak moments) for\nvalid inference under these conditions. A divergent number of Neyman orthogonal\nnuisance functions is estimated using deep neural networks. A key challenge is\nthat the conditional censoring distribution is a non-Neyman orthogonal\nnuisance, contributing to the first-order asymptotics of the estimator for the\ntarget causal effect parameter. We derive the asymptotic distribution and\nexplicitly incorporate this additional uncertainty into the asymptotic variance\nformula. We also introduce a censoring-adjusted over-identification test that\naccounts for this variance component. Simulation studies and UK Biobank\napplications demonstrate the method's robustness and practical utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a semiparametric framework for causal inference with\nright-censored survival outcomes and many weak invalid instruments, motivated\nby Mendelian randomization in biobank studies where classical methods may fail.\nWe adopt an accelerated failure time model and construct a moment condition\nbased on augmented inverse probability of censoring weighting, incorporating\nboth uncensored and censored observations. Under a heteroscedasticity-based\ncondition on the treatment model, we establish point identification of the\ncausal effect despite censoring and invalid instruments. We propose GEL-NOW\n(Generalized Empirical Likelihood with Non-Orthogonal and Weak moments) for\nvalid inference under these conditions. A divergent number of Neyman orthogonal\nnuisance functions is estimated using deep neural networks. A key challenge is\nthat the conditional censoring distribution is a non-Neyman orthogonal\nnuisance, contributing to the first-order asymptotics of the estimator for the\ntarget causal effect parameter. We derive the asymptotic distribution and\nexplicitly incorporate this additional uncertainty into the asymptotic variance\nformula. We also introduce a censoring-adjusted over-identification test that\naccounts for this variance component. Simulation studies and UK Biobank\napplications demonstrate the method's robustness and practical utility."
                },
                "authors": [
                    {
                        "name": "Qiushi Bu"
                    },
                    {
                        "name": "Wen Su"
                    },
                    {
                        "name": "Xingqiu Zhao"
                    },
                    {
                        "name": "Zhonghua Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhonghua Liu"
                },
                "author": "Zhonghua Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04852v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04852v2",
                "updated": "2025-09-16T15:29:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    29,
                    33,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-05T07:06:56Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    6,
                    56,
                    4,
                    248,
                    0
                ],
                "title": "Any-Step Density Ratio Estimation via Interval-Annealed Secant Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Any-Step Density Ratio Estimation via Interval-Annealed Secant Alignment"
                },
                "summary": "Estimating density ratios is a fundamental problem in machine learning, but\nexisting methods often trade off accuracy for efficiency. We propose\n\\textit{Interval-annealed Secant Alignment Density Ratio Estimation (ISA-DRE)},\na framework that enables accurate, any-step estimation without numerical\nintegration.\n  Instead of modeling infinitesimal tangents as in prior methods, ISA-DRE\nlearns a global secant function, defined as the expectation of all tangents\nover an interval, with provably lower variance, making it more suitable for\nneural approximation. This is made possible by the \\emph{Secant Alignment\nIdentity}, a self-consistency condition that formally connects the secant with\nits underlying tangent representations.\n  To mitigate instability during early training, we introduce \\emph{Contraction\nInterval Annealing}, a curriculum strategy that gradually expands the alignment\ninterval during training. This process induces a contraction mapping, which\nimproves convergence and training stability.\n  Empirically, ISA-DRE achieves competitive accuracy with significantly fewer\nfunction evaluations compared to prior methods, resulting in much faster\ninference and making it well suited for real-time and interactive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating density ratios is a fundamental problem in machine learning, but\nexisting methods often trade off accuracy for efficiency. We propose\n\\textit{Interval-annealed Secant Alignment Density Ratio Estimation (ISA-DRE)},\na framework that enables accurate, any-step estimation without numerical\nintegration.\n  Instead of modeling infinitesimal tangents as in prior methods, ISA-DRE\nlearns a global secant function, defined as the expectation of all tangents\nover an interval, with provably lower variance, making it more suitable for\nneural approximation. This is made possible by the \\emph{Secant Alignment\nIdentity}, a self-consistency condition that formally connects the secant with\nits underlying tangent representations.\n  To mitigate instability during early training, we introduce \\emph{Contraction\nInterval Annealing}, a curriculum strategy that gradually expands the alignment\ninterval during training. This process induces a contraction mapping, which\nimproves convergence and training stability.\n  Empirically, ISA-DRE achieves competitive accuracy with significantly fewer\nfunction evaluations compared to prior methods, resulting in much faster\ninference and making it well suited for real-time and interactive applications."
                },
                "authors": [
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Shigui Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Zhiqi Lin"
                    },
                    {
                        "name": "Junmei Yang"
                    },
                    {
                        "name": "Delu Zeng"
                    },
                    {
                        "name": "John Paisley"
                    },
                    {
                        "name": "Qibin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Qibin Zhao"
                },
                "author": "Qibin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04852v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04852v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09001v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09001v3",
                "updated": "2025-09-16T15:28:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    28,
                    23,
                    1,
                    259,
                    0
                ],
                "published": "2025-05-13T22:36:07Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    22,
                    36,
                    7,
                    1,
                    133,
                    0
                ],
                "title": "Causal Feedback Discovery using Convergence Cross Mapping on Sea Ice\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Feedback Discovery using Convergence Cross Mapping on Sea Ice\n  Data"
                },
                "summary": "Identifying causal relationships in climate systems remains challenging due\nto nonlinear, coupled dynamics that limit the effectiveness of linear and\nstochastic causal discovery approaches. This study benchmarks Convergence Cross\nMapping (CCM) against Granger causality, PCMCI, and VarLiNGAM using both\nsynthetic datasets with ground truth causal links and 41 years of Arctic\nclimate data (1979--2021). Unlike stochastic models that rely on autoregressive\nresidual dependence, CCM leverages Takens' state-space reconstruction and\ndelay-embedding to reconstruct attractor manifolds from time series. Cross\nmapping between reconstructed manifolds exploits deterministic signatures of\ncausation, enabling the detection of weak and bidirectional causal links that\nlinear models fail to resolve. Results demonstrate that CCM achieves higher\nspecificity and fewer false positives on synthetic benchmarks, while\nmaintaining robustness under observational noise and limited sample lengths. On\nArctic data, CCM reveals significant causal interactions between sea ice extent\nand atmospheric variables like specific humidity, longwave radiation, and\nsurface temperature with a $p$-value of $0.009$, supporting ice-albedo\nfeedbacks and moisture-radiation couplings central to Arctic amplification. In\ncontrast, stochastic approaches miss these nonlinear dependencies or infer\nspurious causal relations. This work establishes CCM as a robust causal\ninference tool for nonlinear climate dynamics and provides the first systematic\nbenchmarking framework for method selection in climate research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying causal relationships in climate systems remains challenging due\nto nonlinear, coupled dynamics that limit the effectiveness of linear and\nstochastic causal discovery approaches. This study benchmarks Convergence Cross\nMapping (CCM) against Granger causality, PCMCI, and VarLiNGAM using both\nsynthetic datasets with ground truth causal links and 41 years of Arctic\nclimate data (1979--2021). Unlike stochastic models that rely on autoregressive\nresidual dependence, CCM leverages Takens' state-space reconstruction and\ndelay-embedding to reconstruct attractor manifolds from time series. Cross\nmapping between reconstructed manifolds exploits deterministic signatures of\ncausation, enabling the detection of weak and bidirectional causal links that\nlinear models fail to resolve. Results demonstrate that CCM achieves higher\nspecificity and fewer false positives on synthetic benchmarks, while\nmaintaining robustness under observational noise and limited sample lengths. On\nArctic data, CCM reveals significant causal interactions between sea ice extent\nand atmospheric variables like specific humidity, longwave radiation, and\nsurface temperature with a $p$-value of $0.009$, supporting ice-albedo\nfeedbacks and moisture-radiation couplings central to Arctic amplification. In\ncontrast, stochastic approaches miss these nonlinear dependencies or infer\nspurious causal relations. This work establishes CCM as a robust causal\ninference tool for nonlinear climate dynamics and provides the first systematic\nbenchmarking framework for method selection in climate research."
                },
                "authors": [
                    {
                        "name": "Francis Nji"
                    },
                    {
                        "name": "Seraj Al Mahmud Mostafa"
                    },
                    {
                        "name": "Jianwu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianwu Wang"
                },
                "author": "Jianwu Wang",
                "arxiv_comment": "In ACM Sigspatial Conference Workshop, 8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09001v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09001v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13175v1",
                "updated": "2025-09-16T15:27:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    27,
                    14,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T15:27:14Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    27,
                    14,
                    1,
                    259,
                    0
                ],
                "title": "More performant and scalable: Rethinking contrastive vision-language\n  pre-training of radiology in the LLM era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More performant and scalable: Rethinking contrastive vision-language\n  pre-training of radiology in the LLM era"
                },
                "summary": "The emergence of Large Language Models (LLMs) presents unprecedented\nopportunities to revolutionize medical contrastive vision-language\npre-training. In this paper, we show how LLMs can facilitate large-scale\nsupervised pre-training, thereby advancing vision-language alignment. We begin\nby demonstrate that modern LLMs can automatically extract diagnostic labels\nfrom radiology reports with remarkable precision (>96\\% AUC in our experiments)\nwithout complex prompt engineering, enabling the creation of large-scale\n\"silver-standard\" datasets at a minimal cost (~\\$3 for 50k CT image-report\npairs). Further, we find that vision encoder trained on this \"silver-standard\"\ndataset achieves performance comparable to those trained on labels extracted by\nspecialized BERT-based models, thereby democratizing the access to large-scale\nsupervised pre-training. Building on this foundation, we proceed to reveal that\nsupervised pre-training fundamentally improves contrastive vision-language\nalignment. Our approach achieves state-of-the-art performance using only a 3D\nResNet-18 with vanilla CLIP training, including 83.8\\% AUC for zero-shot\ndiagnosis on CT-RATE, 77.3\\% AUC on RAD-ChestCT, and substantial improvements\nin cross-modal retrieval (MAP@50=53.7\\% for image-image, Recall@100=52.2\\% for\nreport-image). These results demonstrate the potential of utilizing LLMs to\nfacilitate {\\bf more performant and scalable} medical AI systems. Our code is\navaiable at https://github.com/SadVoxel/More-performant-and-scalable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Large Language Models (LLMs) presents unprecedented\nopportunities to revolutionize medical contrastive vision-language\npre-training. In this paper, we show how LLMs can facilitate large-scale\nsupervised pre-training, thereby advancing vision-language alignment. We begin\nby demonstrate that modern LLMs can automatically extract diagnostic labels\nfrom radiology reports with remarkable precision (>96\\% AUC in our experiments)\nwithout complex prompt engineering, enabling the creation of large-scale\n\"silver-standard\" datasets at a minimal cost (~\\$3 for 50k CT image-report\npairs). Further, we find that vision encoder trained on this \"silver-standard\"\ndataset achieves performance comparable to those trained on labels extracted by\nspecialized BERT-based models, thereby democratizing the access to large-scale\nsupervised pre-training. Building on this foundation, we proceed to reveal that\nsupervised pre-training fundamentally improves contrastive vision-language\nalignment. Our approach achieves state-of-the-art performance using only a 3D\nResNet-18 with vanilla CLIP training, including 83.8\\% AUC for zero-shot\ndiagnosis on CT-RATE, 77.3\\% AUC on RAD-ChestCT, and substantial improvements\nin cross-modal retrieval (MAP@50=53.7\\% for image-image, Recall@100=52.2\\% for\nreport-image). These results demonstrate the potential of utilizing LLMs to\nfacilitate {\\bf more performant and scalable} medical AI systems. Our code is\navaiable at https://github.com/SadVoxel/More-performant-and-scalable."
                },
                "authors": [
                    {
                        "name": "Yingtai Li"
                    },
                    {
                        "name": "Haoran Lai"
                    },
                    {
                        "name": "Xiaoqian Zhou"
                    },
                    {
                        "name": "Shuai Ming"
                    },
                    {
                        "name": "Wenxin Ma"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Shaohua Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Shaohua Kevin Zhou"
                },
                "author": "Shaohua Kevin Zhou",
                "arxiv_comment": "MICCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13169v1",
                "updated": "2025-09-16T15:20:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    20,
                    54,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T15:20:54Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    20,
                    54,
                    1,
                    259,
                    0
                ],
                "title": "Robust Sensitivity Analysis via Augmented Percentile Bootstrap under\n  Simultaneous Violations of Unconfoundedness and Overlap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Sensitivity Analysis via Augmented Percentile Bootstrap under\n  Simultaneous Violations of Unconfoundedness and Overlap"
                },
                "summary": "The identification of causal effects in observational studies typically\nrelies on two standard assumptions: unconfoundedness and overlap. However, both\nassumptions are often questionable in practice: unconfoundedness is inherently\nuntestable, and overlap may fail in the presence of extreme unmeasured\nconfounding. While various approaches have been developed to address unmeasured\nconfounding and extreme propensity scores separately, few methods accommodate\nsimultaneous violations of both assumptions. In this paper, we propose a\nsensitivity analysis framework that relaxes both unconfoundedness and overlap,\nbuilding upon the marginal sensitivity model. Specifically, we allow the bound\non unmeasured confounding to hold for only a subset of the population, thereby\naccommodating heterogeneity in confounding and allowing treatment probabilities\nto be zero or one. Moreover, unlike prior work, our approach does not require\nbounded outcomes and focuses on overlap-weighted average treatment effects,\nwhich are both practically meaningful and robust to non-overlap. We develop\ncomputationally efficient methods to obtain worst-case bounds via linear\nprogramming, and introduce a novel augmented percentile bootstrap procedure for\nstatistical inference. This bootstrap method handles parameters defined through\nover-identified estimating equations involving unobserved variables and may be\nof independent interest. Our work provides a unified and flexible framework for\nsensitivity analysis under violations of both unconfoundedness and overlap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The identification of causal effects in observational studies typically\nrelies on two standard assumptions: unconfoundedness and overlap. However, both\nassumptions are often questionable in practice: unconfoundedness is inherently\nuntestable, and overlap may fail in the presence of extreme unmeasured\nconfounding. While various approaches have been developed to address unmeasured\nconfounding and extreme propensity scores separately, few methods accommodate\nsimultaneous violations of both assumptions. In this paper, we propose a\nsensitivity analysis framework that relaxes both unconfoundedness and overlap,\nbuilding upon the marginal sensitivity model. Specifically, we allow the bound\non unmeasured confounding to hold for only a subset of the population, thereby\naccommodating heterogeneity in confounding and allowing treatment probabilities\nto be zero or one. Moreover, unlike prior work, our approach does not require\nbounded outcomes and focuses on overlap-weighted average treatment effects,\nwhich are both practically meaningful and robust to non-overlap. We develop\ncomputationally efficient methods to obtain worst-case bounds via linear\nprogramming, and introduce a novel augmented percentile bootstrap procedure for\nstatistical inference. This bootstrap method handles parameters defined through\nover-identified estimating equations involving unobserved variables and may be\nof independent interest. Our work provides a unified and flexible framework for\nsensitivity analysis under violations of both unconfoundedness and overlap."
                },
                "authors": [
                    {
                        "name": "Han Cui"
                    },
                    {
                        "name": "Xinran Li"
                    }
                ],
                "author_detail": {
                    "name": "Xinran Li"
                },
                "author": "Xinran Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11683v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11683v5",
                "updated": "2025-09-16T15:19:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    19,
                    32,
                    1,
                    259,
                    0
                ],
                "published": "2024-11-18T16:09:26Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    9,
                    26,
                    0,
                    323,
                    0
                ],
                "title": "TrojanRobot: Physical-world Backdoor Attacks Against VLM-based Robotic\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrojanRobot: Physical-world Backdoor Attacks Against VLM-based Robotic\n  Manipulation"
                },
                "summary": "Robotic manipulation in the physical world is increasingly empowered by\n\\textit{large language models} (LLMs) and \\textit{vision-language models}\n(VLMs), leveraging their understanding and perception capabilities. Recently,\nvarious attacks against such robotic policies have been proposed, with backdoor\nattacks drawing considerable attention for their high stealth and strong\npersistence capabilities. However, existing backdoor efforts are limited to\nsimulators and suffer from physical-world realization. To address this, we\npropose \\textit{TrojanRobot}, a highly stealthy and broadly effective robotic\nbackdoor attack in the physical world. Specifically, we introduce a\nmodule-poisoning approach by embedding a backdoor module into the modular\nrobotic policy, enabling backdoor control over the policy's visual perception\nmodule thereby backdooring the entire robotic policy. Our vanilla\nimplementation leverages a backdoor-finetuned VLM to serve as the backdoor\nmodule. To enhance its generalization in physical environments, we propose a\nprime implementation, leveraging the LVLM-as-a-backdoor paradigm and developing\nthree types of prime attacks, \\ie, \\textit{permutation}, \\textit{stagnation},\nand \\textit{intentional} attacks, thus achieving finer-grained backdoors.\nExtensive experiments on the UR3e manipulator with 18 task instructions using\nrobotic policies based on four VLMs demonstrate the broad effectiveness and\nphysical-world stealth of TrojanRobot. Our attack's video demonstrations are\navailable via a github link https://trojanrobot.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic manipulation in the physical world is increasingly empowered by\n\\textit{large language models} (LLMs) and \\textit{vision-language models}\n(VLMs), leveraging their understanding and perception capabilities. Recently,\nvarious attacks against such robotic policies have been proposed, with backdoor\nattacks drawing considerable attention for their high stealth and strong\npersistence capabilities. However, existing backdoor efforts are limited to\nsimulators and suffer from physical-world realization. To address this, we\npropose \\textit{TrojanRobot}, a highly stealthy and broadly effective robotic\nbackdoor attack in the physical world. Specifically, we introduce a\nmodule-poisoning approach by embedding a backdoor module into the modular\nrobotic policy, enabling backdoor control over the policy's visual perception\nmodule thereby backdooring the entire robotic policy. Our vanilla\nimplementation leverages a backdoor-finetuned VLM to serve as the backdoor\nmodule. To enhance its generalization in physical environments, we propose a\nprime implementation, leveraging the LVLM-as-a-backdoor paradigm and developing\nthree types of prime attacks, \\ie, \\textit{permutation}, \\textit{stagnation},\nand \\textit{intentional} attacks, thus achieving finer-grained backdoors.\nExtensive experiments on the UR3e manipulator with 18 task instructions using\nrobotic policies based on four VLMs demonstrate the broad effectiveness and\nphysical-world stealth of TrojanRobot. Our attack's video demonstrations are\navailable via a github link https://trojanrobot.github.io."
                },
                "authors": [
                    {
                        "name": "Xianlong Wang"
                    },
                    {
                        "name": "Hewen Pan"
                    },
                    {
                        "name": "Hangtao Zhang"
                    },
                    {
                        "name": "Minghui Li"
                    },
                    {
                        "name": "Shengshan Hu"
                    },
                    {
                        "name": "Ziqi Zhou"
                    },
                    {
                        "name": "Lulu Xue"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Yunpeng Jiang"
                    },
                    {
                        "name": "Leo Yu Zhang"
                    },
                    {
                        "name": "Xiaohua Jia"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohua Jia"
                },
                "author": "Xiaohua Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11683v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11683v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13165v1",
                "updated": "2025-09-16T15:17:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    17,
                    13,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T15:17:13Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    17,
                    13,
                    1,
                    259,
                    0
                ],
                "title": "On the Correlation between Individual Fairness and Predictive Accuracy\n  in Probabilistic Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Correlation between Individual Fairness and Predictive Accuracy\n  in Probabilistic Models"
                },
                "summary": "We investigate individual fairness in generative probabilistic classifiers by\nanalysing the robustness of posterior inferences to perturbations in private\nfeatures. Building on established results in robustness analysis, we\nhypothesise a correlation between robustness and predictive accuracy,\nspecifically, instances exhibiting greater robustness are more likely to be\nclassified accurately. We empirically assess this hypothesis using a benchmark\nof fourteen datasets with fairness concerns, employing Bayesian networks as the\nunderlying generative models. To address the computational complexity\nassociated with robustness analysis over multiple private features with\nBayesian networks, we reformulate the problem as a most probable explanation\ntask in an auxiliary Markov random field. Our experiments confirm the\nhypothesis about the correlation, suggesting novel directions to mitigate the\ntraditional trade-off between fairness and accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate individual fairness in generative probabilistic classifiers by\nanalysing the robustness of posterior inferences to perturbations in private\nfeatures. Building on established results in robustness analysis, we\nhypothesise a correlation between robustness and predictive accuracy,\nspecifically, instances exhibiting greater robustness are more likely to be\nclassified accurately. We empirically assess this hypothesis using a benchmark\nof fourteen datasets with fairness concerns, employing Bayesian networks as the\nunderlying generative models. To address the computational complexity\nassociated with robustness analysis over multiple private features with\nBayesian networks, we reformulate the problem as a most probable explanation\ntask in an auxiliary Markov random field. Our experiments confirm the\nhypothesis about the correlation, suggesting novel directions to mitigate the\ntraditional trade-off between fairness and accuracy."
                },
                "authors": [
                    {
                        "name": "Alessandro Antonucci"
                    },
                    {
                        "name": "Eric Rossetto"
                    },
                    {
                        "name": "Ivan Duvnjak"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Duvnjak"
                },
                "author": "Ivan Duvnjak",
                "arxiv_comment": "15 pages, 9 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13160v1",
                "updated": "2025-09-16T15:13:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    13,
                    13,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T15:13:13Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    13,
                    13,
                    1,
                    259,
                    0
                ],
                "title": "FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial\n  Search and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial\n  Search and Reasoning"
                },
                "summary": "Search has emerged as core infrastructure for LLM-based agents and is widely\nviewed as critical on the path toward more general intelligence. Finance is a\nparticularly demanding proving ground: analysts routinely conduct complex,\nmulti-step searches over time-sensitive, domain-specific data, making it ideal\nfor assessing both search proficiency and knowledge-grounded reasoning. Yet no\nexisting open financial datasets evaluate data searching capability of\nend-to-end agents, largely because constructing realistic, complicated tasks\nrequires deep financial expertise and time-sensitive data is hard to evaluate.\nWe present FinSearchComp, the first fully open-source agent benchmark for\nrealistic, open-domain financial search and reasoning. FinSearchComp comprises\nthree tasks -- Time-Sensitive Data Fetching, Simple Historical Lookup, and\nComplex Historical Investigation -- closely reproduce real-world financial\nanalyst workflows. To ensure difficulty and reliability, we engage 70\nprofessional financial experts for annotation and implement a rigorous\nmulti-stage quality-assurance pipeline. The benchmark includes 635 questions\nspanning global and Greater China markets, and we evaluate 21 models (products)\non it. Grok 4 (web) tops the global subset, approaching expert-level accuracy.\nDouBao (web) leads on the Greater China subset. Experimental analyses show that\nequipping agents with web search and financial plugins substantially improves\nresults on FinSearchComp, and the country origin of models and tools impact\nperformance significantly.By aligning with realistic analyst tasks and\nproviding end-to-end evaluation, FinSearchComp offers a professional,\nhigh-difficulty testbed for complex financial search and reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search has emerged as core infrastructure for LLM-based agents and is widely\nviewed as critical on the path toward more general intelligence. Finance is a\nparticularly demanding proving ground: analysts routinely conduct complex,\nmulti-step searches over time-sensitive, domain-specific data, making it ideal\nfor assessing both search proficiency and knowledge-grounded reasoning. Yet no\nexisting open financial datasets evaluate data searching capability of\nend-to-end agents, largely because constructing realistic, complicated tasks\nrequires deep financial expertise and time-sensitive data is hard to evaluate.\nWe present FinSearchComp, the first fully open-source agent benchmark for\nrealistic, open-domain financial search and reasoning. FinSearchComp comprises\nthree tasks -- Time-Sensitive Data Fetching, Simple Historical Lookup, and\nComplex Historical Investigation -- closely reproduce real-world financial\nanalyst workflows. To ensure difficulty and reliability, we engage 70\nprofessional financial experts for annotation and implement a rigorous\nmulti-stage quality-assurance pipeline. The benchmark includes 635 questions\nspanning global and Greater China markets, and we evaluate 21 models (products)\non it. Grok 4 (web) tops the global subset, approaching expert-level accuracy.\nDouBao (web) leads on the Greater China subset. Experimental analyses show that\nequipping agents with web search and financial plugins substantially improves\nresults on FinSearchComp, and the country origin of models and tools impact\nperformance significantly.By aligning with realistic analyst tasks and\nproviding end-to-end evaluation, FinSearchComp offers a professional,\nhigh-difficulty testbed for complex financial search and reasoning."
                },
                "authors": [
                    {
                        "name": "Liang Hu"
                    },
                    {
                        "name": "Jianpeng Jiao"
                    },
                    {
                        "name": "Jiashuo Liu"
                    },
                    {
                        "name": "Yanle Ren"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Kaiyuan Zhang"
                    },
                    {
                        "name": "Xuanliang Zhang"
                    },
                    {
                        "name": "Xiang Gao"
                    },
                    {
                        "name": "Tianci He"
                    },
                    {
                        "name": "Fei Hu"
                    },
                    {
                        "name": "Yali Liao"
                    },
                    {
                        "name": "Zaiyuan Wang"
                    },
                    {
                        "name": "Chenghao Yang"
                    },
                    {
                        "name": "Qianyu Yang"
                    },
                    {
                        "name": "Mingren Yin"
                    },
                    {
                        "name": "Zhiyuan Zeng"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Xinyi Zhang"
                    },
                    {
                        "name": "Xiying Zhao"
                    },
                    {
                        "name": "Zhenwei Zhu"
                    },
                    {
                        "name": "Hongseok Namkoong"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Yuwen Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yuwen Tang"
                },
                "author": "Yuwen Tang",
                "arxiv_comment": "29 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14335v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14335v3",
                "updated": "2025-09-16T15:11:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    11,
                    20,
                    1,
                    259,
                    0
                ],
                "published": "2025-06-17T09:17:41Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    9,
                    17,
                    41,
                    1,
                    168,
                    0
                ],
                "title": "References Matter: Investigating the Impact of Reference Set Variation\n  on Summarization Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "References Matter: Investigating the Impact of Reference Set Variation\n  on Summarization Evaluation"
                },
                "summary": "Human language production exhibits remarkable richness and variation,\nreflecting diverse communication styles and intents. However, this variation is\noften overlooked in summarization evaluation. While having multiple reference\nsummaries is known to improve correlation with human judgments, the impact of\nthe reference set on reference-based metrics has not been systematically\ninvestigated. This work examines the sensitivity of widely used reference-based\nmetrics in relation to the choice of reference sets, analyzing three diverse\nmulti-reference summarization datasets: SummEval, GUMSum, and DUC2004. We\ndemonstrate that many popular metrics exhibit significant instability. This\ninstability is particularly concerning for n-gram-based metrics like ROUGE,\nwhere model rankings vary depending on the reference sets, undermining the\nreliability of model comparisons. We also collect human judgments on LLM\noutputs for genre-diverse data and examine their correlation with metrics to\nsupplement existing findings beyond newswire summaries, finding weak-to-no\ncorrelation. Taken together, we recommend incorporating reference set variation\ninto summarization evaluation to enhance consistency alongside correlation with\nhuman judgments, especially when evaluating LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human language production exhibits remarkable richness and variation,\nreflecting diverse communication styles and intents. However, this variation is\noften overlooked in summarization evaluation. While having multiple reference\nsummaries is known to improve correlation with human judgments, the impact of\nthe reference set on reference-based metrics has not been systematically\ninvestigated. This work examines the sensitivity of widely used reference-based\nmetrics in relation to the choice of reference sets, analyzing three diverse\nmulti-reference summarization datasets: SummEval, GUMSum, and DUC2004. We\ndemonstrate that many popular metrics exhibit significant instability. This\ninstability is particularly concerning for n-gram-based metrics like ROUGE,\nwhere model rankings vary depending on the reference sets, undermining the\nreliability of model comparisons. We also collect human judgments on LLM\noutputs for genre-diverse data and examine their correlation with metrics to\nsupplement existing findings beyond newswire summaries, finding weak-to-no\ncorrelation. Taken together, we recommend incorporating reference set variation\ninto summarization evaluation to enhance consistency alongside correlation with\nhuman judgments, especially when evaluating LLMs."
                },
                "authors": [
                    {
                        "name": "Silvia Casola"
                    },
                    {
                        "name": "Yang Janet Liu"
                    },
                    {
                        "name": "Siyao Peng"
                    },
                    {
                        "name": "Oliver Kraus"
                    },
                    {
                        "name": "Albert Gatt"
                    },
                    {
                        "name": "Barbara Plank"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Plank"
                },
                "author": "Barbara Plank",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14335v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14335v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13154v1",
                "updated": "2025-09-16T15:08:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    8,
                    19,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T15:08:19Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    8,
                    19,
                    1,
                    259,
                    0
                ],
                "title": "LLM Hallucination Detection: A Fast Fourier Transform Method Based on\n  Hidden Layer Temporal Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Hallucination Detection: A Fast Fourier Transform Method Based on\n  Hidden Layer Temporal Signals"
                },
                "summary": "Hallucination remains a critical barrier for deploying large language models\n(LLMs) in reliability-sensitive applications. Existing detection methods\nlargely fall into two categories: factuality checking, which is fundamentally\nconstrained by external knowledge coverage, and static hidden-state analysis,\nthat fails to capture deviations in reasoning dynamics. As a result, their\neffectiveness and robustness remain limited. We propose HSAD (Hidden Signal\nAnalysis-based Detection), a novel hallucination detection framework that\nmodels the temporal dynamics of hidden representations during autoregressive\ngeneration. HSAD constructs hidden-layer signals by sampling activations across\nlayers, applies Fast Fourier Transform (FFT) to obtain frequency-domain\nrepresentations, and extracts the strongest non-DC frequency component as\nspectral features. Furthermore, by leveraging the autoregressive nature of\nLLMs, HSAD identifies optimal observation points for effective and reliable\ndetection. Across multiple benchmarks, including TruthfulQA, HSAD achieves over\n10 percentage points improvement compared to prior state-of-the-art methods. By\nintegrating reasoning-process modeling with frequency-domain analysis, HSAD\nestablishes a new paradigm for robust hallucination detection in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination remains a critical barrier for deploying large language models\n(LLMs) in reliability-sensitive applications. Existing detection methods\nlargely fall into two categories: factuality checking, which is fundamentally\nconstrained by external knowledge coverage, and static hidden-state analysis,\nthat fails to capture deviations in reasoning dynamics. As a result, their\neffectiveness and robustness remain limited. We propose HSAD (Hidden Signal\nAnalysis-based Detection), a novel hallucination detection framework that\nmodels the temporal dynamics of hidden representations during autoregressive\ngeneration. HSAD constructs hidden-layer signals by sampling activations across\nlayers, applies Fast Fourier Transform (FFT) to obtain frequency-domain\nrepresentations, and extracts the strongest non-DC frequency component as\nspectral features. Furthermore, by leveraging the autoregressive nature of\nLLMs, HSAD identifies optimal observation points for effective and reliable\ndetection. Across multiple benchmarks, including TruthfulQA, HSAD achieves over\n10 percentage points improvement compared to prior state-of-the-art methods. By\nintegrating reasoning-process modeling with frequency-domain analysis, HSAD\nestablishes a new paradigm for robust hallucination detection in LLMs."
                },
                "authors": [
                    {
                        "name": "Jinxin Li"
                    },
                    {
                        "name": "Gang Tu"
                    },
                    {
                        "name": "ShengYu Cheng"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Jinting Wang"
                    },
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Zhilong Zhou"
                    },
                    {
                        "name": "Dongbo Shan"
                    }
                ],
                "author_detail": {
                    "name": "Dongbo Shan"
                },
                "author": "Dongbo Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13149v1",
                "updated": "2025-09-16T15:05:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    5,
                    11,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T15:05:11Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    5,
                    11,
                    1,
                    259,
                    0
                ],
                "title": "MSDNet: Efficient 4D Radar Super-Resolution via Multi-Stage Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSDNet: Efficient 4D Radar Super-Resolution via Multi-Stage Distillation"
                },
                "summary": "4D radar super-resolution, which aims to reconstruct sparse and noisy point\nclouds into dense and geometrically consistent representations, is a\nfoundational problem in autonomous perception. However, existing methods often\nsuffer from high training cost or rely on complex diffusion-based sampling,\nresulting in high inference latency and poor generalization, making it\ndifficult to balance accuracy and efficiency. To address these limitations, we\npropose MSDNet, a multi-stage distillation framework that efficiently transfers\ndense LiDAR priors to 4D radar features to achieve both high reconstruction\nquality and computational efficiency. The first stage performs\nreconstruction-guided feature distillation, aligning and densifying the\nstudent's features through feature reconstruction. In the second stage, we\npropose diffusion-guided feature distillation, which treats the stage-one\ndistilled features as a noisy version of the teacher's representations and\nrefines them via a lightweight diffusion network. Furthermore, we introduce a\nnoise adapter that adaptively aligns the noise level of the feature with a\npredefined diffusion timestep, enabling a more precise denoising. Extensive\nexperiments on the VoD and in-house datasets demonstrate that MSDNet achieves\nboth high-fidelity reconstruction and low-latency inference in the task of 4D\nradar point cloud super-resolution, and consistently improves performance on\ndownstream tasks. The code will be publicly available upon publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "4D radar super-resolution, which aims to reconstruct sparse and noisy point\nclouds into dense and geometrically consistent representations, is a\nfoundational problem in autonomous perception. However, existing methods often\nsuffer from high training cost or rely on complex diffusion-based sampling,\nresulting in high inference latency and poor generalization, making it\ndifficult to balance accuracy and efficiency. To address these limitations, we\npropose MSDNet, a multi-stage distillation framework that efficiently transfers\ndense LiDAR priors to 4D radar features to achieve both high reconstruction\nquality and computational efficiency. The first stage performs\nreconstruction-guided feature distillation, aligning and densifying the\nstudent's features through feature reconstruction. In the second stage, we\npropose diffusion-guided feature distillation, which treats the stage-one\ndistilled features as a noisy version of the teacher's representations and\nrefines them via a lightweight diffusion network. Furthermore, we introduce a\nnoise adapter that adaptively aligns the noise level of the feature with a\npredefined diffusion timestep, enabling a more precise denoising. Extensive\nexperiments on the VoD and in-house datasets demonstrate that MSDNet achieves\nboth high-fidelity reconstruction and low-latency inference in the task of 4D\nradar point cloud super-resolution, and consistently improves performance on\ndownstream tasks. The code will be publicly available upon publication."
                },
                "authors": [
                    {
                        "name": "Minqing Huang"
                    },
                    {
                        "name": "Shouyi Lu"
                    },
                    {
                        "name": "Boyuan Zheng"
                    },
                    {
                        "name": "Ziyao Li"
                    },
                    {
                        "name": "Xiao Tang"
                    },
                    {
                        "name": "Guirong Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Guirong Zhuo"
                },
                "author": "Guirong Zhuo",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13144v1",
                "updated": "2025-09-16T15:01:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    1,
                    23,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T15:01:23Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    1,
                    23,
                    1,
                    259,
                    0
                ],
                "title": "Towards the Next Generation of Software: Insights from Grey Literature\n  on AI-Native Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards the Next Generation of Software: Insights from Grey Literature\n  on AI-Native Applications"
                },
                "summary": "Background: The rapid advancement of large language models (LLMs) has given\nrise to AI-native applications, a new paradigm in software engineering that\nfundamentally redefines how software is designed, developed, and evolved.\nDespite their growing prominence, AI-native applications still lack a unified\nengineering definition and architectural blueprint, leaving practitioners\nwithout systematic guidance for system design, quality assurance, and\ntechnology selection.\n  Objective: This study seeks to establish a comprehensive understanding of\nAI-native applications by identifying their defining characteristics, key\nquality attributes, and typical technology stacks, as well as by clarifying the\nopportunities and challenges they present.\n  Method: We conducted a grey literature review, integrating conceptual\nperspectives retrieved from targeted Google and Bing searches with practical\ninsights derived from leading open-source projects on GitHub. A structured\nprotocol encompassing source selection, quality assessment, and thematic\nanalysis was applied to synthesize findings across heterogeneous sources.\n  Results: We finally identified 106 studies based on the selection criteria.\nThe analysis reveals that AI-native applications are distinguished by two core\npillars: the central role of AI as the system's intelligence paradigm and their\ninherently probabilistic, non-deterministic nature. Critical quality attributes\ninclude reliability, usability, performance efficiency, and AI-specific\nobservability. In addition, a typical technology stack has begun to emerge,\ncomprising LLM orchestration frameworks, vector databases, and AI-native\nobservability platforms. These systems emphasize response quality,\ncost-effectiveness, and outcome predictability, setting them apart from\nconventional software systems.\n  Conclusion: This study is the first to propose a dual-layered engineering\nblueprint...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: The rapid advancement of large language models (LLMs) has given\nrise to AI-native applications, a new paradigm in software engineering that\nfundamentally redefines how software is designed, developed, and evolved.\nDespite their growing prominence, AI-native applications still lack a unified\nengineering definition and architectural blueprint, leaving practitioners\nwithout systematic guidance for system design, quality assurance, and\ntechnology selection.\n  Objective: This study seeks to establish a comprehensive understanding of\nAI-native applications by identifying their defining characteristics, key\nquality attributes, and typical technology stacks, as well as by clarifying the\nopportunities and challenges they present.\n  Method: We conducted a grey literature review, integrating conceptual\nperspectives retrieved from targeted Google and Bing searches with practical\ninsights derived from leading open-source projects on GitHub. A structured\nprotocol encompassing source selection, quality assessment, and thematic\nanalysis was applied to synthesize findings across heterogeneous sources.\n  Results: We finally identified 106 studies based on the selection criteria.\nThe analysis reveals that AI-native applications are distinguished by two core\npillars: the central role of AI as the system's intelligence paradigm and their\ninherently probabilistic, non-deterministic nature. Critical quality attributes\ninclude reliability, usability, performance efficiency, and AI-specific\nobservability. In addition, a typical technology stack has begun to emerge,\ncomprising LLM orchestration frameworks, vector databases, and AI-native\nobservability platforms. These systems emphasize response quality,\ncost-effectiveness, and outcome predictability, setting them apart from\nconventional software systems.\n  Conclusion: This study is the first to propose a dual-layered engineering\nblueprint..."
                },
                "authors": [
                    {
                        "name": "Lingli Cao"
                    },
                    {
                        "name": "Shanshan Li"
                    },
                    {
                        "name": "Ying Fan"
                    },
                    {
                        "name": "Danyang Li"
                    },
                    {
                        "name": "Chenxing Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Chenxing Zhong"
                },
                "author": "Chenxing Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13136v1",
                "updated": "2025-09-16T14:53:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    14,
                    53,
                    44,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T14:53:44Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    14,
                    53,
                    44,
                    1,
                    259,
                    0
                ],
                "title": "Discovering Mathematical Equations with Diffusion Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering Mathematical Equations with Diffusion Language Model"
                },
                "summary": "Discovering valid and meaningful mathematical equations from observed data\nplays a crucial role in scientific discovery. While this task, symbolic\nregression, remains challenging due to the vast search space and the trade-off\nbetween accuracy and complexity. In this paper, we introduce DiffuSR, a\npre-training framework for symbolic regression built upon a continuous-state\ndiffusion language model. DiffuSR employs a trainable embedding layer within\nthe diffusion process to map discrete mathematical symbols into a continuous\nlatent space, modeling equation distributions effectively. Through iterative\ndenoising, DiffuSR converts an initial noisy sequence into a symbolic equation,\nguided by numerical data injected via a cross-attention mechanism. We also\ndesign an effective inference strategy to enhance the accuracy of the\ndiffusion-based equation generator, which injects logit priors into genetic\nprogramming. Experimental results on standard symbolic regression benchmarks\ndemonstrate that DiffuSR achieves competitive performance with state-of-the-art\nautoregressive methods and generates more interpretable and diverse\nmathematical expressions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering valid and meaningful mathematical equations from observed data\nplays a crucial role in scientific discovery. While this task, symbolic\nregression, remains challenging due to the vast search space and the trade-off\nbetween accuracy and complexity. In this paper, we introduce DiffuSR, a\npre-training framework for symbolic regression built upon a continuous-state\ndiffusion language model. DiffuSR employs a trainable embedding layer within\nthe diffusion process to map discrete mathematical symbols into a continuous\nlatent space, modeling equation distributions effectively. Through iterative\ndenoising, DiffuSR converts an initial noisy sequence into a symbolic equation,\nguided by numerical data injected via a cross-attention mechanism. We also\ndesign an effective inference strategy to enhance the accuracy of the\ndiffusion-based equation generator, which injects logit priors into genetic\nprogramming. Experimental results on standard symbolic regression benchmarks\ndemonstrate that DiffuSR achieves competitive performance with state-of-the-art\nautoregressive methods and generates more interpretable and diverse\nmathematical expressions."
                },
                "authors": [
                    {
                        "name": "Xiaoxu Han"
                    },
                    {
                        "name": "Chengzhen Ning"
                    },
                    {
                        "name": "Jinghui Zhong"
                    },
                    {
                        "name": "Fubiao Yang"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Xin Mu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Mu"
                },
                "author": "Xin Mu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13131v1",
                "updated": "2025-09-16T14:48:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    14,
                    48,
                    46,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T14:48:46Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    14,
                    48,
                    46,
                    1,
                    259,
                    0
                ],
                "title": "Reasoning with Preference Constraints: A Benchmark for Language Models\n  in Many-to-One Matching Markets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning with Preference Constraints: A Benchmark for Language Models\n  in Many-to-One Matching Markets"
                },
                "summary": "Recent advances in reasoning with large language models (LLMs) have\ndemonstrated strong performance on complex mathematical tasks, including\ncombinatorial optimization. Techniques such as Chain-of-Thought and In-Context\nLearning have further enhanced this capability, making LLMs both powerful and\naccessible tools for a wide range of users, including non-experts. However,\napplying LLMs to matching problems, which require reasoning under preferential\nand structural constraints, remains underexplored. To address this gap, we\nintroduce a novel benchmark of 369 instances of the College Admission Problem,\na canonical example of a matching problem with preferences, to evaluate LLMs\nacross key dimensions: feasibility, stability, and optimality. We employ this\nbenchmark to assess the performance of several open-weight LLMs. Our results\nfirst reveal that while LLMs can satisfy certain constraints, they struggle to\nmeet all evaluation criteria consistently. They also show that reasoning LLMs,\nlike QwQ and GPT-oss, significantly outperform traditional models such as\nLlama, Qwen or Mistral, defined here as models used without any dedicated\nreasoning mechanisms. Moreover, we observed that LLMs reacted differently to\nthe various prompting strategies tested, which include Chain-of-Thought,\nIn-Context Learning and role-based prompting, with no prompt consistently\noffering the best performance. Finally, we report the performances from\niterative prompting with auto-generated feedback and show that they are not\nmonotonic; they can peak early and then significantly decline in later\nattempts. Overall, this work offers a new perspective on model reasoning\nperformance and the effectiveness of prompting strategies in combinatorial\noptimization problems with preferential constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reasoning with large language models (LLMs) have\ndemonstrated strong performance on complex mathematical tasks, including\ncombinatorial optimization. Techniques such as Chain-of-Thought and In-Context\nLearning have further enhanced this capability, making LLMs both powerful and\naccessible tools for a wide range of users, including non-experts. However,\napplying LLMs to matching problems, which require reasoning under preferential\nand structural constraints, remains underexplored. To address this gap, we\nintroduce a novel benchmark of 369 instances of the College Admission Problem,\na canonical example of a matching problem with preferences, to evaluate LLMs\nacross key dimensions: feasibility, stability, and optimality. We employ this\nbenchmark to assess the performance of several open-weight LLMs. Our results\nfirst reveal that while LLMs can satisfy certain constraints, they struggle to\nmeet all evaluation criteria consistently. They also show that reasoning LLMs,\nlike QwQ and GPT-oss, significantly outperform traditional models such as\nLlama, Qwen or Mistral, defined here as models used without any dedicated\nreasoning mechanisms. Moreover, we observed that LLMs reacted differently to\nthe various prompting strategies tested, which include Chain-of-Thought,\nIn-Context Learning and role-based prompting, with no prompt consistently\noffering the best performance. Finally, we report the performances from\niterative prompting with auto-generated feedback and show that they are not\nmonotonic; they can peak early and then significantly decline in later\nattempts. Overall, this work offers a new perspective on model reasoning\nperformance and the effectiveness of prompting strategies in combinatorial\noptimization problems with preferential constraints."
                },
                "authors": [
                    {
                        "name": "Marylou Fauchard"
                    },
                    {
                        "name": "Florian Carichon"
                    },
                    {
                        "name": "Margarida Carvalho"
                    },
                    {
                        "name": "Golnoosh Farnadi"
                    }
                ],
                "author_detail": {
                    "name": "Golnoosh Farnadi"
                },
                "author": "Golnoosh Farnadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23506v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23506v3",
                "updated": "2025-09-16T14:40:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    14,
                    40,
                    13,
                    1,
                    259,
                    0
                ],
                "published": "2024-10-30T23:26:06Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    23,
                    26,
                    6,
                    2,
                    304,
                    0
                ],
                "title": "The Belief State Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Belief State Transformer"
                },
                "summary": "We introduce the \"Belief State Transformer\", a next-token predictor that\ntakes both a prefix and suffix as inputs, with a novel objective of predicting\nboth the next token for the prefix and the previous token for the suffix. The\nBelief State Transformer effectively learns to solve challenging problems that\nconventional forward-only transformers struggle with, in a domain-independent\nfashion. Key to this success is learning a compact belief state that captures\nall relevant information necessary for accurate predictions. Empirical\nablations show that each component of the model is essential in difficult\nscenarios where standard Transformers fall short. For the task of story writing\nwith known prefixes and suffixes, our approach outperforms the\nFill-in-the-Middle method for reaching known goals and demonstrates improved\nperformance even when the goals are unknown. Altogether, the Belief State\nTransformer enables more efficient goal-conditioned decoding, better test-time\ninference, and high-quality text representations on small scale problems.\nWebsite: https://edwhu.github.io/bst-website",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the \"Belief State Transformer\", a next-token predictor that\ntakes both a prefix and suffix as inputs, with a novel objective of predicting\nboth the next token for the prefix and the previous token for the suffix. The\nBelief State Transformer effectively learns to solve challenging problems that\nconventional forward-only transformers struggle with, in a domain-independent\nfashion. Key to this success is learning a compact belief state that captures\nall relevant information necessary for accurate predictions. Empirical\nablations show that each component of the model is essential in difficult\nscenarios where standard Transformers fall short. For the task of story writing\nwith known prefixes and suffixes, our approach outperforms the\nFill-in-the-Middle method for reaching known goals and demonstrates improved\nperformance even when the goals are unknown. Altogether, the Belief State\nTransformer enables more efficient goal-conditioned decoding, better test-time\ninference, and high-quality text representations on small scale problems.\nWebsite: https://edwhu.github.io/bst-website"
                },
                "authors": [
                    {
                        "name": "Edward S. Hu"
                    },
                    {
                        "name": "Kwangjun Ahn"
                    },
                    {
                        "name": "Qinghua Liu"
                    },
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Manan Tomar"
                    },
                    {
                        "name": "Ada Langford"
                    },
                    {
                        "name": "Jayden Teoh"
                    },
                    {
                        "name": "Bryon Xu"
                    },
                    {
                        "name": "David Yan"
                    },
                    {
                        "name": "Dinesh Jayaraman"
                    },
                    {
                        "name": "Alex Lamb"
                    },
                    {
                        "name": "John Langford"
                    }
                ],
                "author_detail": {
                    "name": "John Langford"
                },
                "author": "John Langford",
                "arxiv_comment": "Updated report with new improvements and authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23506v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23506v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13127v1",
                "updated": "2025-09-16T14:36:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    14,
                    36,
                    30,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T14:36:30Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    14,
                    36,
                    30,
                    1,
                    259,
                    0
                ],
                "title": "Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon\n  Planning"
                },
                "summary": "Recent advancements in Large Language Models(LLMs) have led to the\ndevelopment of LLM-based AI agents. A key challenge is the creation of agents\nthat can effectively ground themselves in complex, adversarial long-horizon\nenvironments. Existing methods mainly focus on (1) using LLMs as policies to\ninteract with the environment through generating low-level feasible actions,\nand (2) utilizing LLMs to generate high-level tasks or language guides to\nstimulate action generation. However, the former struggles to generate reliable\nactions, while the latter relies heavily on expert experience to translate\nhigh-level tasks into specific action sequences. To address these challenges,\nwe introduce the Plan with Language, Act with Parameter (PLAP) planning\nframework that facilitates the grounding of LLM-based agents in long-horizon\nenvironments. The PLAP method comprises three key components: (1) a skill\nlibrary containing environment-specific parameterized skills, (2) a skill\nplanner powered by LLMs, and (3) a skill executor converting the parameterized\nskills into executable action sequences. We implement PLAP in MicroRTS, a\nlong-horizon real-time strategy game that provides an unfamiliar and\nchallenging environment for LLMs. The experimental results demonstrate the\neffectiveness of PLAP. In particular, GPT-4o-driven PLAP in a zero-shot setting\noutperforms 80% of baseline agents, and Qwen2-72B-driven PLAP, with carefully\ncrafted few-shot examples, surpasses the top-tier scripted agent, CoacAI.\nAdditionally, we design comprehensive evaluation metrics and test 6\nclosed-source and 2 open-source LLMs within the PLAP framework, ultimately\nreleasing an LLM leaderboard ranking long-horizon skill planning ability. Our\ncode is available at https://github.com/AI-Research-TeamX/PLAP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models(LLMs) have led to the\ndevelopment of LLM-based AI agents. A key challenge is the creation of agents\nthat can effectively ground themselves in complex, adversarial long-horizon\nenvironments. Existing methods mainly focus on (1) using LLMs as policies to\ninteract with the environment through generating low-level feasible actions,\nand (2) utilizing LLMs to generate high-level tasks or language guides to\nstimulate action generation. However, the former struggles to generate reliable\nactions, while the latter relies heavily on expert experience to translate\nhigh-level tasks into specific action sequences. To address these challenges,\nwe introduce the Plan with Language, Act with Parameter (PLAP) planning\nframework that facilitates the grounding of LLM-based agents in long-horizon\nenvironments. The PLAP method comprises three key components: (1) a skill\nlibrary containing environment-specific parameterized skills, (2) a skill\nplanner powered by LLMs, and (3) a skill executor converting the parameterized\nskills into executable action sequences. We implement PLAP in MicroRTS, a\nlong-horizon real-time strategy game that provides an unfamiliar and\nchallenging environment for LLMs. The experimental results demonstrate the\neffectiveness of PLAP. In particular, GPT-4o-driven PLAP in a zero-shot setting\noutperforms 80% of baseline agents, and Qwen2-72B-driven PLAP, with carefully\ncrafted few-shot examples, surpasses the top-tier scripted agent, CoacAI.\nAdditionally, we design comprehensive evaluation metrics and test 6\nclosed-source and 2 open-source LLMs within the PLAP framework, ultimately\nreleasing an LLM leaderboard ranking long-horizon skill planning ability. Our\ncode is available at https://github.com/AI-Research-TeamX/PLAP."
                },
                "authors": [
                    {
                        "name": "Sijia Cui"
                    },
                    {
                        "name": "Shuai Xu"
                    },
                    {
                        "name": "Aiyao He"
                    },
                    {
                        "name": "Yanna Wang"
                    },
                    {
                        "name": "Bo Xu"
                    }
                ],
                "author_detail": {
                    "name": "Bo Xu"
                },
                "author": "Bo Xu",
                "arxiv_comment": "Accepted to IJCNN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18436v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18436v3",
                "updated": "2025-09-16T14:24:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    14,
                    24,
                    5,
                    1,
                    259,
                    0
                ],
                "published": "2024-10-24T05:14:03Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    14,
                    3,
                    3,
                    298,
                    0
                ],
                "title": "Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case\n  Study on English-Korean Code-Switching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case\n  Study on English-Korean Code-Switching"
                },
                "summary": "Recent large language models (LLMs) demonstrate multilingual abilities, yet\nthey are English-centric due to dominance of English in training corpora. The\nlimited resource for low-resource languages remains a crucial challenge.\nCode-switching (CS), a phenomenon where multilingual speakers alternate between\nlanguages in a discourse, can convey subtle cultural and linguistic nuances\nthat can be otherwise lost in translation and elicits language-specific\nknowledge in human communications. In light of this, we investigate whether\ncode-switching can activate, or identify and leverage knowledge for reasoning\nwhen LLMs solve low-resource language tasks. To facilitate the research, we\nfirst present EnKoQA, a synthetic English-Korean CS question-answering dataset.\nWe provide comprehensive analysis on a variety of multilingual LLMs by\nsubdividing activation process into knowledge identification and knowledge\nleveraging. Our results demonstrate that compared to English text, CS can\nfaithfully activate knowledge inside LLMs especially on language-specific\ndomains, suggesting the potential of code-switching on low-resource language\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) demonstrate multilingual abilities, yet\nthey are English-centric due to dominance of English in training corpora. The\nlimited resource for low-resource languages remains a crucial challenge.\nCode-switching (CS), a phenomenon where multilingual speakers alternate between\nlanguages in a discourse, can convey subtle cultural and linguistic nuances\nthat can be otherwise lost in translation and elicits language-specific\nknowledge in human communications. In light of this, we investigate whether\ncode-switching can activate, or identify and leverage knowledge for reasoning\nwhen LLMs solve low-resource language tasks. To facilitate the research, we\nfirst present EnKoQA, a synthetic English-Korean CS question-answering dataset.\nWe provide comprehensive analysis on a variety of multilingual LLMs by\nsubdividing activation process into knowledge identification and knowledge\nleveraging. Our results demonstrate that compared to English text, CS can\nfaithfully activate knowledge inside LLMs especially on language-specific\ndomains, suggesting the potential of code-switching on low-resource language\ntasks."
                },
                "authors": [
                    {
                        "name": "Seoyeon Kim"
                    },
                    {
                        "name": "Huiseo Kim"
                    },
                    {
                        "name": "Chanjun Park"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18436v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18436v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13103v1",
                "updated": "2025-09-16T14:01:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    14,
                    1,
                    44,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T14:01:44Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    14,
                    1,
                    44,
                    1,
                    259,
                    0
                ],
                "title": "Accelerating Discovery: Rapid Literature Screening with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Discovery: Rapid Literature Screening with LLMs"
                },
                "summary": "Background: Conducting Multi Vocal Literature Reviews (MVLRs) is often time\nand effort-intensive. Researchers must review and filter a large number of\nunstructured sources, which frequently contain sparse information and are\nunlikely to be included in the final study. Our experience conducting an MVLR\non Context-Aware Software Systems (CASS) Testing in the avionics domain\nexemplified this challenge, with over 8,000 highly heterogeneous documents\nrequiring review. Therefore, we developed a Large Language Model (LLM)\nassistant to support the search and filtering of documents. Aims: To develop\nand validate an LLM based tool that can support researchers in performing the\nsearch and filtering of documents for an MVLR without compromising the rigor of\nthe research protocol. Method: We applied sound engineering practices to\ndevelop an on-premises LLM-based tool incorporating Retrieval Augmented\nGeneration (RAG) to process candidate sources. Progress towards the aim was\nquantified using the Positive Percent Agreement (PPA) as the primary metric to\nensure the performance of the LLM based tool. Convenience sampling, supported\nby human judgment and statistical sampling, were used to verify and validate\nthe tool's quality-in-use. Results: The tool currently demonstrates a PPA\nagreement with human researchers of 90% for sources that are not relevant to\nthe study. Development details are shared to support domain-specific adaptation\nof the tool. Conclusions: Using LLM-based tools to support academic researchers\nin rigorous MVLR is feasible. These tools can free valuable time for\nhigher-level, abstract tasks. However, researcher participation remains\nessential to ensure that the tool supports thorough research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Conducting Multi Vocal Literature Reviews (MVLRs) is often time\nand effort-intensive. Researchers must review and filter a large number of\nunstructured sources, which frequently contain sparse information and are\nunlikely to be included in the final study. Our experience conducting an MVLR\non Context-Aware Software Systems (CASS) Testing in the avionics domain\nexemplified this challenge, with over 8,000 highly heterogeneous documents\nrequiring review. Therefore, we developed a Large Language Model (LLM)\nassistant to support the search and filtering of documents. Aims: To develop\nand validate an LLM based tool that can support researchers in performing the\nsearch and filtering of documents for an MVLR without compromising the rigor of\nthe research protocol. Method: We applied sound engineering practices to\ndevelop an on-premises LLM-based tool incorporating Retrieval Augmented\nGeneration (RAG) to process candidate sources. Progress towards the aim was\nquantified using the Positive Percent Agreement (PPA) as the primary metric to\nensure the performance of the LLM based tool. Convenience sampling, supported\nby human judgment and statistical sampling, were used to verify and validate\nthe tool's quality-in-use. Results: The tool currently demonstrates a PPA\nagreement with human researchers of 90% for sources that are not relevant to\nthe study. Development details are shared to support domain-specific adaptation\nof the tool. Conclusions: Using LLM-based tools to support academic researchers\nin rigorous MVLR is feasible. These tools can free valuable time for\nhigher-level, abstract tasks. However, researcher participation remains\nessential to ensure that the tool supports thorough research."
                },
                "authors": [
                    {
                        "name": "Santiago Matalonga"
                    },
                    {
                        "name": "Domenico Amalfitano"
                    },
                    {
                        "name": "Jean Carlo Rossa Hauck"
                    },
                    {
                        "name": "MartÃ­n Solari"
                    },
                    {
                        "name": "Guilherme H. Travassos"
                    }
                ],
                "author_detail": {
                    "name": "Guilherme H. Travassos"
                },
                "author": "Guilherme H. Travassos",
                "arxiv_comment": "This version of the manuscript has been submitted to Empirical\n  Software Engieering Journal for consideration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21740v2",
                "updated": "2025-09-16T13:55:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    55,
                    55,
                    1,
                    259,
                    0
                ],
                "published": "2025-05-27T20:29:50Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    20,
                    29,
                    50,
                    1,
                    147,
                    0
                ],
                "title": "Counterfactual Simulatability of LLM Explanations for Generation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual Simulatability of LLM Explanations for Generation Tasks"
                },
                "summary": "LLMs can be unpredictable, as even slight alterations to the prompt can cause\nthe output to change in unexpected ways. Thus, the ability of models to\naccurately explain their behavior is critical, especially in high-stakes\nsettings. One approach for evaluating explanations is counterfactual\nsimulatability, how well an explanation allows users to infer the model's\noutput on related counterfactuals. Counterfactual simulatability has been\npreviously studied for yes/no question answering tasks. We provide a general\nframework for extending this method to generation tasks, using news\nsummarization and medical suggestion as example use cases. We find that while\nLLM explanations do enable users to better predict LLM outputs on\ncounterfactuals in the summarization setting, there is significant room for\nimprovement for medical suggestion. Furthermore, our results suggest that the\nevaluation for counterfactual simulatability may be more appropriate for\nskill-based tasks as opposed to knowledge-based tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can be unpredictable, as even slight alterations to the prompt can cause\nthe output to change in unexpected ways. Thus, the ability of models to\naccurately explain their behavior is critical, especially in high-stakes\nsettings. One approach for evaluating explanations is counterfactual\nsimulatability, how well an explanation allows users to infer the model's\noutput on related counterfactuals. Counterfactual simulatability has been\npreviously studied for yes/no question answering tasks. We provide a general\nframework for extending this method to generation tasks, using news\nsummarization and medical suggestion as example use cases. We find that while\nLLM explanations do enable users to better predict LLM outputs on\ncounterfactuals in the summarization setting, there is significant room for\nimprovement for medical suggestion. Furthermore, our results suggest that the\nevaluation for counterfactual simulatability may be more appropriate for\nskill-based tasks as opposed to knowledge-based tasks."
                },
                "authors": [
                    {
                        "name": "Marvin Limpijankit"
                    },
                    {
                        "name": "Yanda Chen"
                    },
                    {
                        "name": "Melanie Subbiah"
                    },
                    {
                        "name": "Nicholas Deas"
                    },
                    {
                        "name": "Kathleen McKeown"
                    }
                ],
                "author_detail": {
                    "name": "Kathleen McKeown"
                },
                "author": "Kathleen McKeown",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13081v1",
                "updated": "2025-09-16T13:39:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    39,
                    29,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T13:39:29Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    39,
                    29,
                    1,
                    259,
                    0
                ],
                "title": "Shaping Explanations: Semantic Reward Modeling with Encoder-Only\n  Transformers for GRPO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shaping Explanations: Semantic Reward Modeling with Encoder-Only\n  Transformers for GRPO"
                },
                "summary": "While Large Language Models (LLMs) excel at generating human-like text,\naligning their outputs with complex, qualitative goals like pedagogical\nsoundness remains a significant challenge. Standard reinforcement learning\ntechniques often rely on slow and expensive LLM-as-a-judge evaluations or on\nbrittle, keyword-based metrics like ROUGE, which fail to capture the semantic\nessence of a high-quality explanation. In this work, we introduce a novel\napproach to reward shaping within the Group Relative Policy Optimisation (GRPO)\nframework. Our central contribution is the use of a small, efficient\nencoder-only transformer as a semantic reward model. This model provides a\ndense, semantically rich reward signal based on the cosine similarity between a\ngenerated explanation and a ground-truth reference, guiding the policy towards\nexplanations that are not just factually correct but also structurally and\nconceptually aligned with expert reasoning. We apply this method to the task of\ntraining a model for the Italian medical-school entrance examinations,\nfollowing standard domain-adaptive continued pre-training (CPT) and supervised\nfine-tuning (SFT). Our results demonstrate that GRPO with our proposed semantic\nreward significantly improves explanation faithfulness and clarity over a\nstrong SFT baseline, showcasing the power of using lightweight encoder models\nfor nuanced reward shaping in complex generation tasks",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) excel at generating human-like text,\naligning their outputs with complex, qualitative goals like pedagogical\nsoundness remains a significant challenge. Standard reinforcement learning\ntechniques often rely on slow and expensive LLM-as-a-judge evaluations or on\nbrittle, keyword-based metrics like ROUGE, which fail to capture the semantic\nessence of a high-quality explanation. In this work, we introduce a novel\napproach to reward shaping within the Group Relative Policy Optimisation (GRPO)\nframework. Our central contribution is the use of a small, efficient\nencoder-only transformer as a semantic reward model. This model provides a\ndense, semantically rich reward signal based on the cosine similarity between a\ngenerated explanation and a ground-truth reference, guiding the policy towards\nexplanations that are not just factually correct but also structurally and\nconceptually aligned with expert reasoning. We apply this method to the task of\ntraining a model for the Italian medical-school entrance examinations,\nfollowing standard domain-adaptive continued pre-training (CPT) and supervised\nfine-tuning (SFT). Our results demonstrate that GRPO with our proposed semantic\nreward significantly improves explanation faithfulness and clarity over a\nstrong SFT baseline, showcasing the power of using lightweight encoder models\nfor nuanced reward shaping in complex generation tasks"
                },
                "authors": [
                    {
                        "name": "Francesco Pappone"
                    },
                    {
                        "name": "Ruggero Marino Lazzaroni"
                    },
                    {
                        "name": "Federico Califano"
                    },
                    {
                        "name": "NiccolÃ² Gentile"
                    },
                    {
                        "name": "Roberto Marras"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Marras"
                },
                "author": "Roberto Marras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20409v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20409v3",
                "updated": "2025-09-16T13:36:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    36,
                    33,
                    1,
                    259,
                    0
                ],
                "published": "2025-06-25T13:24:46Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    24,
                    46,
                    2,
                    176,
                    0
                ],
                "title": "TAPS: Tool-Augmented Personalisation via Structured Tagging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAPS: Tool-Augmented Personalisation via Structured Tagging"
                },
                "summary": "Recent advancements in tool-augmented large language models have enabled them\nto interact with external tools, enhancing their ability to perform complex\nuser tasks. However, existing approaches overlook the role of personalisation\nin guiding tool use. This work investigates how user preferences can be\neffectively integrated into goal-oriented dialogue agents. Through extensive\nanalysis, we identify key weaknesses in the ability of LLMs to personalise tool\nuse. To this end, we introduce TAPS, a novel solution that enhances\npersonalised tool use by leveraging a structured tagging tool and an\nuncertainty-based tool detector. TAPS significantly improves the ability of\nLLMs to incorporate user preferences, achieving the new state-of-the-art for\nopen source models on the NLSI task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in tool-augmented large language models have enabled them\nto interact with external tools, enhancing their ability to perform complex\nuser tasks. However, existing approaches overlook the role of personalisation\nin guiding tool use. This work investigates how user preferences can be\neffectively integrated into goal-oriented dialogue agents. Through extensive\nanalysis, we identify key weaknesses in the ability of LLMs to personalise tool\nuse. To this end, we introduce TAPS, a novel solution that enhances\npersonalised tool use by leveraging a structured tagging tool and an\nuncertainty-based tool detector. TAPS significantly improves the ability of\nLLMs to incorporate user preferences, achieving the new state-of-the-art for\nopen source models on the NLSI task."
                },
                "authors": [
                    {
                        "name": "Ekaterina Taktasheva"
                    },
                    {
                        "name": "Jeff Dalton"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Dalton"
                },
                "author": "Jeff Dalton",
                "arxiv_comment": "Accepted to EMNLP 2026 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20409v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20409v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.10307v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.10307v3",
                "updated": "2025-09-16T13:35:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    35,
                    35,
                    1,
                    259,
                    0
                ],
                "published": "2024-03-15T13:47:44Z",
                "published_parsed": [
                    2024,
                    3,
                    15,
                    13,
                    47,
                    44,
                    4,
                    75,
                    0
                ],
                "title": "Chernoff Information as a Privacy Constraint for Adversarial\n  Classification and Membership Advantage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chernoff Information as a Privacy Constraint for Adversarial\n  Classification and Membership Advantage"
                },
                "summary": "This work inspects a privacy metric based on Chernoff information, namely\nChernoff differential privacy, due to its significance in characterization of\nthe optimal classifier's performance. Adversarial classification, as any other\nclassification problem is built around minimization of the (average or correct\ndetection) probability of error in deciding on either of the classes in the\ncase of binary classification. Unlike the classical hypothesis testing problem,\nwhere the false alarm and mis-detection probabilities are handled separately\nresulting in an asymmetric behavior of the best error exponent, in this work,\nwe characterize the relationship between $\\varepsilon\\textrm{-}$differential\nprivacy, the best error exponent of one of the errors (when the other is fixed)\nand the best average error exponent. Accordingly, we re-derive Chernoff\ndifferential privacy in connection with $\\varepsilon\\textrm{-}$differential\nprivacy using the Radon-Nikodym derivative, and prove its relation with\nKullback-Leibler (KL) differential privacy. Subsequently, we present numerical\nevaluation results, which demonstrates that Chernoff information outperforms\nKullback-Leibler divergence as a function of the privacy parameter\n$\\varepsilon$ and the impact of the adversary's attack in Laplace mechanisms.\nLastly, we introduce a new upper bound on adversary's membership advantage in\nmembership inference attacks using Chernoff DP and numerically compare its\nperformance with existing alternatives based on $(\\varepsilon,\n\\delta)\\textrm{-}$differential privacy in the literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work inspects a privacy metric based on Chernoff information, namely\nChernoff differential privacy, due to its significance in characterization of\nthe optimal classifier's performance. Adversarial classification, as any other\nclassification problem is built around minimization of the (average or correct\ndetection) probability of error in deciding on either of the classes in the\ncase of binary classification. Unlike the classical hypothesis testing problem,\nwhere the false alarm and mis-detection probabilities are handled separately\nresulting in an asymmetric behavior of the best error exponent, in this work,\nwe characterize the relationship between $\\varepsilon\\textrm{-}$differential\nprivacy, the best error exponent of one of the errors (when the other is fixed)\nand the best average error exponent. Accordingly, we re-derive Chernoff\ndifferential privacy in connection with $\\varepsilon\\textrm{-}$differential\nprivacy using the Radon-Nikodym derivative, and prove its relation with\nKullback-Leibler (KL) differential privacy. Subsequently, we present numerical\nevaluation results, which demonstrates that Chernoff information outperforms\nKullback-Leibler divergence as a function of the privacy parameter\n$\\varepsilon$ and the impact of the adversary's attack in Laplace mechanisms.\nLastly, we introduce a new upper bound on adversary's membership advantage in\nmembership inference attacks using Chernoff DP and numerically compare its\nperformance with existing alternatives based on $(\\varepsilon,\n\\delta)\\textrm{-}$differential privacy in the literature."
                },
                "authors": [
                    {
                        "name": "AyÅe Ãnsal"
                    },
                    {
                        "name": "Melek Ãnen"
                    }
                ],
                "author_detail": {
                    "name": "Melek Ãnen"
                },
                "author": "Melek Ãnen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.10307v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.10307v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13745v2",
                "updated": "2025-09-16T13:30:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    30,
                    5,
                    1,
                    259,
                    0
                ],
                "published": "2024-09-11T01:56:35Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    1,
                    56,
                    35,
                    2,
                    255,
                    0
                ],
                "title": "Context-Aware Membership Inference Attacks against Pre-trained Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Aware Membership Inference Attacks against Pre-trained Large\n  Language Models"
                },
                "summary": "Membership Inference Attacks (MIAs) on pre-trained Large Language Models\n(LLMs) aim at determining if a data point was part of the model's training set.\nPrior MIAs that are built for classification models fail at LLMs, due to\nignoring the generative nature of LLMs across token sequences. In this paper,\nwe present a novel attack on pre-trained LLMs that adapts MIA statistical tests\nto the perplexity dynamics of subsequences within a data point. Our method\nsignificantly outperforms prior approaches, revealing context-dependent\nmemorization patterns in pre-trained LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership Inference Attacks (MIAs) on pre-trained Large Language Models\n(LLMs) aim at determining if a data point was part of the model's training set.\nPrior MIAs that are built for classification models fail at LLMs, due to\nignoring the generative nature of LLMs across token sequences. In this paper,\nwe present a novel attack on pre-trained LLMs that adapts MIA statistical tests\nto the perplexity dynamics of subsequences within a data point. Our method\nsignificantly outperforms prior approaches, revealing context-dependent\nmemorization patterns in pre-trained LLMs."
                },
                "authors": [
                    {
                        "name": "Hongyan Chang"
                    },
                    {
                        "name": "Ali Shahin Shamsabadi"
                    },
                    {
                        "name": "Kleomenis Katevas"
                    },
                    {
                        "name": "Hamed Haddadi"
                    },
                    {
                        "name": "Reza Shokri"
                    }
                ],
                "author_detail": {
                    "name": "Reza Shokri"
                },
                "author": "Reza Shokri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07047v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07047v2",
                "updated": "2025-09-16T13:25:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    25,
                    21,
                    1,
                    259,
                    0
                ],
                "published": "2025-02-10T21:30:24Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    21,
                    30,
                    24,
                    0,
                    41,
                    0
                ],
                "title": "A Closed-Form Transition Density Expansion for Elliptic and\n  Hypo-Elliptic SDEs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Closed-Form Transition Density Expansion for Elliptic and\n  Hypo-Elliptic SDEs"
                },
                "summary": "We introduce a closed-form expansion for the transition density of elliptic\nand hypo-elliptic multivariate Stochastic Differential Equations (SDEs), over a\nperiod $\\Delta\\in (0,1)$, in terms of powers of $\\Delta^{j/2}$, $j\\ge 0$. Our\nmethodology provides approximations of the transition density, easily evaluated\nvia any software that performs symbolic calculations. A major part of the paper\nis devoted to an analytical control of the remainder in our expansion for fixed\n$\\Delta\\in(0,1)$. The obtained error bounds validate theoretically the\nmethodology, by characterising the size of the distance from the true value. It\nis the first time that such a closed-form expansion becomes available for the\nimportant class of hypo-elliptic SDEs, to the best of our knowledge. For\nelliptic SDEs, closed-form expansions are available, with some works\nidentifying the size of the error for fixed $\\Delta$, as per our contribution.\nOur methodology allows for a uniform treatment of elliptic and hypo-elliptic\nSDEs, when earlier works are intrinsically restricted to an elliptic setting.\nWe show numerical applications highlighting the effectiveness of our method, by\ncarrying out parameter inference for hypo-elliptic SDEs that do not satisfy\nstated conditions. The latter are sufficient for controlling the remainder\nterms, but the closed-form expansion itself is applicable in general settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a closed-form expansion for the transition density of elliptic\nand hypo-elliptic multivariate Stochastic Differential Equations (SDEs), over a\nperiod $\\Delta\\in (0,1)$, in terms of powers of $\\Delta^{j/2}$, $j\\ge 0$. Our\nmethodology provides approximations of the transition density, easily evaluated\nvia any software that performs symbolic calculations. A major part of the paper\nis devoted to an analytical control of the remainder in our expansion for fixed\n$\\Delta\\in(0,1)$. The obtained error bounds validate theoretically the\nmethodology, by characterising the size of the distance from the true value. It\nis the first time that such a closed-form expansion becomes available for the\nimportant class of hypo-elliptic SDEs, to the best of our knowledge. For\nelliptic SDEs, closed-form expansions are available, with some works\nidentifying the size of the error for fixed $\\Delta$, as per our contribution.\nOur methodology allows for a uniform treatment of elliptic and hypo-elliptic\nSDEs, when earlier works are intrinsically restricted to an elliptic setting.\nWe show numerical applications highlighting the effectiveness of our method, by\ncarrying out parameter inference for hypo-elliptic SDEs that do not satisfy\nstated conditions. The latter are sufficient for controlling the remainder\nterms, but the closed-form expansion itself is applicable in general settings."
                },
                "authors": [
                    {
                        "name": "Yuga Iguchi"
                    },
                    {
                        "name": "Alexandros Beskos"
                    }
                ],
                "author_detail": {
                    "name": "Alexandros Beskos"
                },
                "author": "Alexandros Beskos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07047v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07047v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13067v1",
                "updated": "2025-09-16T13:22:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    22,
                    8,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T13:22:08Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    22,
                    8,
                    1,
                    259,
                    0
                ],
                "title": "HERO: Rethinking Visual Token Early Dropping in High-Resolution Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERO: Rethinking Visual Token Early Dropping in High-Resolution Large\n  Vision-Language Models"
                },
                "summary": "By cropping high-resolution images into local tiles and encoding them\nindependently, High-Resolution Large Vision-Language Models (HR-LVLMs) have\ndemonstrated remarkable fine-grained visual understanding capabilities.\nHowever, this divide-and-conquer paradigm significantly increases the number of\nvisual tokens, resulting in substantial computational and memory overhead. To\nbetter understand and address this challenge, we empirically investigate visual\ntoken utilization in HR-LVLMs and uncover three key findings: (1) the local\ntiles have varying importance, jointly determined by visual saliency and task\nrelevance; (2) the CLS token in CLIP-based vision encoders exhibits a two-stage\nattention pattern across layers, with each stage attending to different types\nof visual tokens; (3) the visual tokens emphasized at different stages encode\ninformation at varying levels of granularity, playing complementary roles\nwithin LVLMs. Building on these insights, we propose HERO, a High-resolution\nvisual token early dropping framework that integrates content-adaptive token\nbudget allocation with function-aware token selection. By accurately estimating\ntile-level importance and selectively retaining visual tokens with\ncomplementary roles, HERO achieves superior efficiency-accuracy trade-offs\nacross diverse benchmarks and model scales, all in a training-free manner. This\nstudy provides both empirical insights and practical solutions toward efficient\ninference in HR-LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By cropping high-resolution images into local tiles and encoding them\nindependently, High-Resolution Large Vision-Language Models (HR-LVLMs) have\ndemonstrated remarkable fine-grained visual understanding capabilities.\nHowever, this divide-and-conquer paradigm significantly increases the number of\nvisual tokens, resulting in substantial computational and memory overhead. To\nbetter understand and address this challenge, we empirically investigate visual\ntoken utilization in HR-LVLMs and uncover three key findings: (1) the local\ntiles have varying importance, jointly determined by visual saliency and task\nrelevance; (2) the CLS token in CLIP-based vision encoders exhibits a two-stage\nattention pattern across layers, with each stage attending to different types\nof visual tokens; (3) the visual tokens emphasized at different stages encode\ninformation at varying levels of granularity, playing complementary roles\nwithin LVLMs. Building on these insights, we propose HERO, a High-resolution\nvisual token early dropping framework that integrates content-adaptive token\nbudget allocation with function-aware token selection. By accurately estimating\ntile-level importance and selectively retaining visual tokens with\ncomplementary roles, HERO achieves superior efficiency-accuracy trade-offs\nacross diverse benchmarks and model scales, all in a training-free manner. This\nstudy provides both empirical insights and practical solutions toward efficient\ninference in HR-LVLMs."
                },
                "authors": [
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yuxuan Liang"
                    },
                    {
                        "name": "Xiaolei Chen"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "Haotian Chen"
                    },
                    {
                        "name": "Bin Li"
                    },
                    {
                        "name": "Xiangyang Xue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Xue"
                },
                "author": "Xiangyang Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08375v2",
                "updated": "2025-09-16T13:19:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    19,
                    41,
                    1,
                    259,
                    0
                ],
                "published": "2025-06-10T02:39:55Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    39,
                    55,
                    1,
                    161,
                    0
                ],
                "title": "EIFBENCH: Extremely Complex Instruction Following Benchmark for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EIFBENCH: Extremely Complex Instruction Following Benchmark for Large\n  Language Models"
                },
                "summary": "With the development and widespread application of large language models\n(LLMs), the new paradigm of \"Model as Product\" is rapidly evolving, and demands\nhigher capabilities to address complex user needs, often requiring precise\nworkflow execution which involves the accurate understanding of multiple tasks.\nHowever, existing benchmarks focusing on single-task environments with limited\nconstraints lack the complexity required to fully reflect real-world scenarios.\nTo bridge this gap, we present the Extremely Complex Instruction Following\nBenchmark (EIFBENCH), meticulously crafted to facilitate a more realistic and\nrobust evaluation of LLMs. EIFBENCH not only includes multi-task scenarios that\nenable comprehensive assessment across diverse task types concurrently, but\nalso integrates a variety of constraints, replicating complex operational\nenvironments. Furthermore, we propose the Segment Policy Optimization (SegPO)\nalgorithm to enhance the LLM's ability to accurately fulfill multi-task\nworkflow. Evaluations on EIFBENCH have unveiled considerable performance\ndiscrepancies in existing LLMs when challenged with these extremely complex\ninstructions. This finding underscores the necessity for ongoing optimization\nto navigate the intricate challenges posed by LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development and widespread application of large language models\n(LLMs), the new paradigm of \"Model as Product\" is rapidly evolving, and demands\nhigher capabilities to address complex user needs, often requiring precise\nworkflow execution which involves the accurate understanding of multiple tasks.\nHowever, existing benchmarks focusing on single-task environments with limited\nconstraints lack the complexity required to fully reflect real-world scenarios.\nTo bridge this gap, we present the Extremely Complex Instruction Following\nBenchmark (EIFBENCH), meticulously crafted to facilitate a more realistic and\nrobust evaluation of LLMs. EIFBENCH not only includes multi-task scenarios that\nenable comprehensive assessment across diverse task types concurrently, but\nalso integrates a variety of constraints, replicating complex operational\nenvironments. Furthermore, we propose the Segment Policy Optimization (SegPO)\nalgorithm to enhance the LLM's ability to accurately fulfill multi-task\nworkflow. Evaluations on EIFBENCH have unveiled considerable performance\ndiscrepancies in existing LLMs when challenged with these extremely complex\ninstructions. This finding underscores the necessity for ongoing optimization\nto navigate the intricate challenges posed by LLM applications."
                },
                "authors": [
                    {
                        "name": "Tao Zou"
                    },
                    {
                        "name": "Xinghua Zhang"
                    },
                    {
                        "name": "Haiyang Yu"
                    },
                    {
                        "name": "Minzheng Wang"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "arxiv_comment": "Accepted by EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13055v1",
                "updated": "2025-09-16T13:12:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    12,
                    11,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T13:12:11Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    12,
                    11,
                    1,
                    259,
                    0
                ],
                "title": "Automating Code Generation for Semiconductor Equipment Control from\n  Developer Utterances with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Code Generation for Semiconductor Equipment Control from\n  Developer Utterances with LLMs"
                },
                "summary": "Semiconductors form the backbone of modern electronics, with their\nmanufacturing and testing relying on highly specialized equipment and\ndomain-specific programming languages. Equipment languages such as the\nAlgorithmic Pattern Generator (ALPG) are critical for precise hardware control\nbut are challenging to program due to their low-level syntax and steep learning\ncurve. While large language models (LLMs) have shown promise in generating\nhigh-level code from natural language, their effectiveness on low-level\nequipment languages remains limited. To address this, we propose Progressive\nKnowledge Enhancement (PKE), a novel multi-stage prompting framework that\nprogressively extracts and activates the latent knowledge within LLMs, guiding\nthem from simple to complex examples without extensive fine-tuning. Empirical\nevaluation on an industrial ALPG dataset shows that PKE significantly\noutperforms standard prompting and surpasses state-of-the-art methods in\ngenerating correct ALPG code, achieving 11.1\\% and 15.2\\% higher exact match\nscores compared to the second-best technique. Further analysis of individual\ncomponents confirms that progressive knowledge extraction based on difficulty\nenhances accuracy. Our study offer a practical approach to boosting LLM\ncapabilities for specialized low-level programming, supporting greater\nproductivity in semiconductor software development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semiconductors form the backbone of modern electronics, with their\nmanufacturing and testing relying on highly specialized equipment and\ndomain-specific programming languages. Equipment languages such as the\nAlgorithmic Pattern Generator (ALPG) are critical for precise hardware control\nbut are challenging to program due to their low-level syntax and steep learning\ncurve. While large language models (LLMs) have shown promise in generating\nhigh-level code from natural language, their effectiveness on low-level\nequipment languages remains limited. To address this, we propose Progressive\nKnowledge Enhancement (PKE), a novel multi-stage prompting framework that\nprogressively extracts and activates the latent knowledge within LLMs, guiding\nthem from simple to complex examples without extensive fine-tuning. Empirical\nevaluation on an industrial ALPG dataset shows that PKE significantly\noutperforms standard prompting and surpasses state-of-the-art methods in\ngenerating correct ALPG code, achieving 11.1\\% and 15.2\\% higher exact match\nscores compared to the second-best technique. Further analysis of individual\ncomponents confirms that progressive knowledge extraction based on difficulty\nenhances accuracy. Our study offer a practical approach to boosting LLM\ncapabilities for specialized low-level programming, supporting greater\nproductivity in semiconductor software development."
                },
                "authors": [
                    {
                        "name": "Youngkyoung Kim"
                    },
                    {
                        "name": "Sanghyeok Park"
                    },
                    {
                        "name": "Misoo Kim"
                    },
                    {
                        "name": "Gangho Yoon"
                    },
                    {
                        "name": "Eunseok Lee"
                    },
                    {
                        "name": "Simon S. Woo"
                    }
                ],
                "author_detail": {
                    "name": "Simon S. Woo"
                },
                "author": "Simon S. Woo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03107v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03107v2",
                "updated": "2025-09-16T13:11:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    11,
                    17,
                    1,
                    259,
                    0
                ],
                "published": "2024-12-04T08:13:29Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    13,
                    29,
                    2,
                    339,
                    0
                ],
                "title": "CredID: Credible Multi-Bit Watermark for Large Language Models\n  Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CredID: Credible Multi-Bit Watermark for Large Language Models\n  Identification"
                },
                "summary": "Large Language Models (LLMs) are widely used in complex natural language\nprocessing tasks but raise privacy and security concerns due to the lack of\nidentity recognition. This paper proposes a multi-party credible watermarking\nframework (CredID) involving a trusted third party (TTP) and multiple LLM\nvendors to address these issues. In the watermark embedding stage, vendors\nrequest a seed from the TTP to generate watermarked text without sending the\nuser's prompt. In the extraction stage, the TTP coordinates each vendor to\nextract and verify the watermark from the text. This provides a credible\nwatermarking scheme while preserving vendor privacy. Furthermore, current\nwatermarking algorithms struggle with text quality, information capacity, and\nrobustness, making it challenging to meet the diverse identification needs of\nLLMs. Thus, we propose a novel multi-bit watermarking algorithm and an\nopen-source toolkit to facilitate research. Experiments show our CredID\nenhances watermark credibility and efficiency without compromising text\nquality. Additionally, we successfully utilized this framework to achieve\nhighly accurate identification among multiple LLM vendors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used in complex natural language\nprocessing tasks but raise privacy and security concerns due to the lack of\nidentity recognition. This paper proposes a multi-party credible watermarking\nframework (CredID) involving a trusted third party (TTP) and multiple LLM\nvendors to address these issues. In the watermark embedding stage, vendors\nrequest a seed from the TTP to generate watermarked text without sending the\nuser's prompt. In the extraction stage, the TTP coordinates each vendor to\nextract and verify the watermark from the text. This provides a credible\nwatermarking scheme while preserving vendor privacy. Furthermore, current\nwatermarking algorithms struggle with text quality, information capacity, and\nrobustness, making it challenging to meet the diverse identification needs of\nLLMs. Thus, we propose a novel multi-bit watermarking algorithm and an\nopen-source toolkit to facilitate research. Experiments show our CredID\nenhances watermark credibility and efficiency without compromising text\nquality. Additionally, we successfully utilized this framework to achieve\nhighly accurate identification among multiple LLM vendors."
                },
                "authors": [
                    {
                        "name": "Haoyu Jiang"
                    },
                    {
                        "name": "Xuhong Wang"
                    },
                    {
                        "name": "Ping Yi"
                    },
                    {
                        "name": "Shanzhe Lei"
                    },
                    {
                        "name": "Yilun Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yilun Lin"
                },
                "author": "Yilun Lin",
                "arxiv_comment": "v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03107v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03107v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08045v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08045v3",
                "updated": "2025-09-16T13:10:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    10,
                    21,
                    1,
                    259,
                    0
                ],
                "published": "2025-02-12T01:04:13Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    1,
                    4,
                    13,
                    2,
                    43,
                    0
                ],
                "title": "Break the Checkbox: Challenging Closed-Style Evaluations of Cultural\n  Alignment in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Break the Checkbox: Challenging Closed-Style Evaluations of Cultural\n  Alignment in LLMs"
                },
                "summary": "A large number of studies rely on closed-style multiple-choice surveys to\nevaluate cultural alignment in Large Language Models (LLMs). In this work, we\nchallenge this constrained evaluation paradigm and explore more realistic,\nunconstrained approaches. Using the World Values Survey (WVS) and Hofstede\nCultural Dimensions as case studies, we demonstrate that LLMs exhibit stronger\ncultural alignment in less constrained settings, where responses are not\nforced. Additionally, we show that even minor changes, such as reordering\nsurvey choices, lead to inconsistent outputs, exposing the limitations of\nclosed-style evaluations. Our findings advocate for more robust and flexible\nevaluation frameworks that focus on specific cultural proxies, encouraging more\nnuanced and accurate assessments of cultural alignment in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A large number of studies rely on closed-style multiple-choice surveys to\nevaluate cultural alignment in Large Language Models (LLMs). In this work, we\nchallenge this constrained evaluation paradigm and explore more realistic,\nunconstrained approaches. Using the World Values Survey (WVS) and Hofstede\nCultural Dimensions as case studies, we demonstrate that LLMs exhibit stronger\ncultural alignment in less constrained settings, where responses are not\nforced. Additionally, we show that even minor changes, such as reordering\nsurvey choices, lead to inconsistent outputs, exposing the limitations of\nclosed-style evaluations. Our findings advocate for more robust and flexible\nevaluation frameworks that focus on specific cultural proxies, encouraging more\nnuanced and accurate assessments of cultural alignment in LLMs."
                },
                "authors": [
                    {
                        "name": "Mohsinul Kabir"
                    },
                    {
                        "name": "Ajwad Abrar"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    }
                ],
                "author_detail": {
                    "name": "Sophia Ananiadou"
                },
                "author": "Sophia Ananiadou",
                "arxiv_comment": "Accepted at EMNLP 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08045v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08045v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03592v2",
                "updated": "2025-09-16T13:10:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    10,
                    4,
                    1,
                    259,
                    0
                ],
                "published": "2025-06-04T05:46:40Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    5,
                    46,
                    40,
                    2,
                    155,
                    0
                ],
                "title": "From Understanding to Generation: An Efficient Shortcut for Evaluating\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Understanding to Generation: An Efficient Shortcut for Evaluating\n  Language Models"
                },
                "summary": "Iterative evaluation of LLMs during training is essential to ensure expected\ncapability development, but can be time- and compute-intensive. While NLU\ntasks, where the model selects from fixed answer choices, are cheap to\nevaluate, essential capabilities like reasoning and code generation rely on the\nmore time-consuming NLG (token-by-token generation) format. In this work, our\naim is to decrease the computational burden of NLG benchmarks in order to\nenable monitoring crucial LLM capabilities during model training. We\nreformulate generative tasks into computationally cheaper NLU alternatives. We\ntest the performance correlation between the original and reformulated tasks\nusing 8 LMs of various sizes and 4 capabilities: mathematical reasoning, code\ngeneration, factual knowledge and reading comprehension. Our results show a\nstrong correlation between task formats, supporting capability assessment via\ncheaper alternatives and achieving over 35x average reduction in evaluation\ntime. Our project is available at:\nhttps://github.com/Fraunhofer-IIS/EvalShortcut",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative evaluation of LLMs during training is essential to ensure expected\ncapability development, but can be time- and compute-intensive. While NLU\ntasks, where the model selects from fixed answer choices, are cheap to\nevaluate, essential capabilities like reasoning and code generation rely on the\nmore time-consuming NLG (token-by-token generation) format. In this work, our\naim is to decrease the computational burden of NLG benchmarks in order to\nenable monitoring crucial LLM capabilities during model training. We\nreformulate generative tasks into computationally cheaper NLU alternatives. We\ntest the performance correlation between the original and reformulated tasks\nusing 8 LMs of various sizes and 4 capabilities: mathematical reasoning, code\ngeneration, factual knowledge and reading comprehension. Our results show a\nstrong correlation between task formats, supporting capability assessment via\ncheaper alternatives and achieving over 35x average reduction in evaluation\ntime. Our project is available at:\nhttps://github.com/Fraunhofer-IIS/EvalShortcut"
                },
                "authors": [
                    {
                        "name": "Viktor Hangya"
                    },
                    {
                        "name": "Fabian KÃ¼ch"
                    },
                    {
                        "name": "Darina Gold"
                    }
                ],
                "author_detail": {
                    "name": "Darina Gold"
                },
                "author": "Darina Gold",
                "arxiv_comment": "Accepted to EMNLP 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13047v1",
                "updated": "2025-09-16T13:04:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    4,
                    48,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T13:04:48Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    4,
                    48,
                    1,
                    259,
                    0
                ],
                "title": "Multi-Model Synthetic Training for Mission-Critical Small Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Model Synthetic Training for Mission-Critical Small Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nmany domains, yet their application to specialized fields remains constrained\nby the scarcity and complexity of domain-specific training data. We present a\nnovel approach that achieves a 261x cost reduction for maritime intelligence by\nusing LLMs as one-time teachers rather than using them directly for inference.\nOur method transforms 3.2 billion Automatic Identification System (AIS) vessel\ntracking records into 21,543 synthetic question and answer pairs through\nmulti-model generation (GPT-4o and o3-mini), preventing overfitting and\nensuring accurate reasoning. The resulting fine-tuned Qwen2.5-7B model achieves\n75% accuracy on maritime tasks, while being substantially cheaper than using a\nlarger model for inference. We show that smaller, cheaper models -- when fine\ntuned properly -- can provide similar accuracy compared to larger models that\nare prohibitively expensive. Our work contributes to the growing field of\nsynthetic dataset generation for specialized AI applications and presents a\nhighly reproducible framework for domains where manual annotation is\ninfeasible. Beyond expanding research in the growing field of specialized small\nlanguage models, our approach has immediate applications in maritime safety,\nsecurity operations, and vessel traffic management systems in various\nindustries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nmany domains, yet their application to specialized fields remains constrained\nby the scarcity and complexity of domain-specific training data. We present a\nnovel approach that achieves a 261x cost reduction for maritime intelligence by\nusing LLMs as one-time teachers rather than using them directly for inference.\nOur method transforms 3.2 billion Automatic Identification System (AIS) vessel\ntracking records into 21,543 synthetic question and answer pairs through\nmulti-model generation (GPT-4o and o3-mini), preventing overfitting and\nensuring accurate reasoning. The resulting fine-tuned Qwen2.5-7B model achieves\n75% accuracy on maritime tasks, while being substantially cheaper than using a\nlarger model for inference. We show that smaller, cheaper models -- when fine\ntuned properly -- can provide similar accuracy compared to larger models that\nare prohibitively expensive. Our work contributes to the growing field of\nsynthetic dataset generation for specialized AI applications and presents a\nhighly reproducible framework for domains where manual annotation is\ninfeasible. Beyond expanding research in the growing field of specialized small\nlanguage models, our approach has immediate applications in maritime safety,\nsecurity operations, and vessel traffic management systems in various\nindustries."
                },
                "authors": [
                    {
                        "name": "Nolan Platt"
                    },
                    {
                        "name": "Pragyansmita Nayak"
                    }
                ],
                "author_detail": {
                    "name": "Pragyansmita Nayak"
                },
                "author": "Pragyansmita Nayak",
                "arxiv_comment": "8 pages. Accepted as a full paper to the 3rd International Conference\n  on Foundation and Large Language Models (IEEE FLLM) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13046v1",
                "updated": "2025-09-16T13:03:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    3,
                    54,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T13:03:54Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    3,
                    54,
                    1,
                    259,
                    0
                ],
                "title": "MIA-EPT: Membership Inference Attack via Error Prediction for Tabular\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIA-EPT: Membership Inference Attack via Error Prediction for Tabular\n  Data"
                },
                "summary": "Synthetic data generation plays an important role in enabling data sharing,\nparticularly in sensitive domains like healthcare and finance. Recent advances\nin diffusion models have made it possible to generate realistic, high-quality\ntabular data, but they may also memorize training records and leak sensitive\ninformation. Membership inference attacks (MIAs) exploit this vulnerability by\ndetermining whether a record was used in training. While MIAs have been studied\nin images and text, their use against tabular diffusion models remains\nunderexplored despite the unique risks of structured attributes and limited\nrecord diversity. In this paper, we introduce MIAEPT, Membership Inference\nAttack via Error Prediction for Tabular Data, a novel black-box attack\nspecifically designed to target tabular diffusion models. MIA-EPT constructs\nerrorbased feature vectors by masking and reconstructing attributes of target\nrecords, disclosing membership signals based on how well these attributes are\npredicted. MIA-EPT operates without access to the internal components of the\ngenerative model, relying only on its synthetic data output, and was shown to\ngeneralize across multiple state-of-the-art diffusion models. We validate\nMIA-EPT on three diffusion-based synthesizers, achieving AUC-ROC scores of up\nto 0.599 and TPR@10% FPR values of 22.0% in our internal tests. Under the MIDST\n2025 competition conditions, MIA-EPT achieved second place in the Black-box\nMulti-Table track (TPR@10% FPR = 20.0%). These results demonstrate that our\nmethod can uncover substantial membership leakage in synthetic tabular data,\nchallenging the assumption that synthetic data is inherently\nprivacy-preserving. Our code is publicly available at\nhttps://github.com/eyalgerman/MIA-EPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic data generation plays an important role in enabling data sharing,\nparticularly in sensitive domains like healthcare and finance. Recent advances\nin diffusion models have made it possible to generate realistic, high-quality\ntabular data, but they may also memorize training records and leak sensitive\ninformation. Membership inference attacks (MIAs) exploit this vulnerability by\ndetermining whether a record was used in training. While MIAs have been studied\nin images and text, their use against tabular diffusion models remains\nunderexplored despite the unique risks of structured attributes and limited\nrecord diversity. In this paper, we introduce MIAEPT, Membership Inference\nAttack via Error Prediction for Tabular Data, a novel black-box attack\nspecifically designed to target tabular diffusion models. MIA-EPT constructs\nerrorbased feature vectors by masking and reconstructing attributes of target\nrecords, disclosing membership signals based on how well these attributes are\npredicted. MIA-EPT operates without access to the internal components of the\ngenerative model, relying only on its synthetic data output, and was shown to\ngeneralize across multiple state-of-the-art diffusion models. We validate\nMIA-EPT on three diffusion-based synthesizers, achieving AUC-ROC scores of up\nto 0.599 and TPR@10% FPR values of 22.0% in our internal tests. Under the MIDST\n2025 competition conditions, MIA-EPT achieved second place in the Black-box\nMulti-Table track (TPR@10% FPR = 20.0%). These results demonstrate that our\nmethod can uncover substantial membership leakage in synthetic tabular data,\nchallenging the assumption that synthetic data is inherently\nprivacy-preserving. Our code is publicly available at\nhttps://github.com/eyalgerman/MIA-EPT."
                },
                "authors": [
                    {
                        "name": "Eyal German"
                    },
                    {
                        "name": "Daniel Samira"
                    },
                    {
                        "name": "Yuval Elovici"
                    },
                    {
                        "name": "Asaf Shabtai"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Shabtai"
                },
                "author": "Asaf Shabtai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16467v2",
                "updated": "2025-09-16T12:56:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    56,
                    24,
                    1,
                    259,
                    0
                ],
                "published": "2025-05-22T09:48:51Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    9,
                    48,
                    51,
                    3,
                    142,
                    0
                ],
                "title": "Reading Between the Prompts: How Stereotypes Shape LLM's Implicit\n  Personalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reading Between the Prompts: How Stereotypes Shape LLM's Implicit\n  Personalization"
                },
                "summary": "Generative Large Language Models (LLMs) infer user's demographic information\nfrom subtle cues in the conversation -- a phenomenon called implicit\npersonalization. Prior work has shown that such inferences can lead to lower\nquality responses for users assumed to be from minority groups, even when no\ndemographic information is explicitly provided. In this work, we systematically\nexplore how LLMs respond to stereotypical cues using controlled synthetic\nconversations, by analyzing the models' latent user representations through\nboth model internals and generated answers to targeted user questions. Our\nfindings reveal that LLMs do infer demographic attributes based on these\nstereotypical signals, which for a number of groups even persists when the user\nexplicitly identifies with a different demographic group. Finally, we show that\nthis form of stereotype-driven implicit personalization can be effectively\nmitigated by intervening on the model's internal representations using a\ntrained linear probe to steer them toward the explicitly stated identity. Our\nresults highlight the need for greater transparency and control in how LLMs\nrepresent user identity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Large Language Models (LLMs) infer user's demographic information\nfrom subtle cues in the conversation -- a phenomenon called implicit\npersonalization. Prior work has shown that such inferences can lead to lower\nquality responses for users assumed to be from minority groups, even when no\ndemographic information is explicitly provided. In this work, we systematically\nexplore how LLMs respond to stereotypical cues using controlled synthetic\nconversations, by analyzing the models' latent user representations through\nboth model internals and generated answers to targeted user questions. Our\nfindings reveal that LLMs do infer demographic attributes based on these\nstereotypical signals, which for a number of groups even persists when the user\nexplicitly identifies with a different demographic group. Finally, we show that\nthis form of stereotype-driven implicit personalization can be effectively\nmitigated by intervening on the model's internal representations using a\ntrained linear probe to steer them toward the explicitly stated identity. Our\nresults highlight the need for greater transparency and control in how LLMs\nrepresent user identity."
                },
                "authors": [
                    {
                        "name": "Vera Neplenbroek"
                    },
                    {
                        "name": "Arianna Bisazza"
                    },
                    {
                        "name": "Raquel FernÃ¡ndez"
                    }
                ],
                "author_detail": {
                    "name": "Raquel FernÃ¡ndez"
                },
                "author": "Raquel FernÃ¡ndez",
                "arxiv_comment": "Accepted at EMNLP Main 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13031v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13031v1",
                "updated": "2025-09-16T12:51:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    51,
                    11,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T12:51:11Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    51,
                    11,
                    1,
                    259,
                    0
                ],
                "title": "Perception Before Reasoning: Two-Stage Reinforcement Learning for Visual\n  Reasoning in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception Before Reasoning: Two-Stage Reinforcement Learning for Visual\n  Reasoning in Vision-Language Models"
                },
                "summary": "Reinforcement learning (RL) has proven highly effective in eliciting the\nreasoning capabilities of large language models (LLMs). Inspired by this\nsuccess, recent studies have explored applying similar techniques to\nvision-language models (VLMs), aiming to enhance their reasoning performance.\nHowever, directly transplanting RL methods from LLMs to VLMs is suboptimal, as\nthe tasks faced by VLMs are inherently more complex. Specifically, VLMs must\nfirst accurately perceive and understand visual inputs before reasoning can be\neffectively performed. To address this challenge, we propose a two-stage\nreinforcement learning framework designed to jointly enhance both the\nperceptual and reasoning capabilities of VLMs. To mitigate the vanishing\nadvantage issue commonly observed in RL training, we first perform\ndataset-level sampling to selectively strengthen specific capabilities using\ndistinct data sources. During training, the first stage focuses on improving\nthe model's visual perception through coarse- and fine-grained visual\nunderstanding, while the second stage targets the enhancement of reasoning\nabilities. After the proposed two-stage reinforcement learning process, we\nobtain PeBR-R1, a vision-language model with significantly enhanced perceptual\nand reasoning capabilities. Experimental results on seven benchmark datasets\ndemonstrate the effectiveness of our approach and validate the superior\nperformance of PeBR-R1 across diverse visual reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has proven highly effective in eliciting the\nreasoning capabilities of large language models (LLMs). Inspired by this\nsuccess, recent studies have explored applying similar techniques to\nvision-language models (VLMs), aiming to enhance their reasoning performance.\nHowever, directly transplanting RL methods from LLMs to VLMs is suboptimal, as\nthe tasks faced by VLMs are inherently more complex. Specifically, VLMs must\nfirst accurately perceive and understand visual inputs before reasoning can be\neffectively performed. To address this challenge, we propose a two-stage\nreinforcement learning framework designed to jointly enhance both the\nperceptual and reasoning capabilities of VLMs. To mitigate the vanishing\nadvantage issue commonly observed in RL training, we first perform\ndataset-level sampling to selectively strengthen specific capabilities using\ndistinct data sources. During training, the first stage focuses on improving\nthe model's visual perception through coarse- and fine-grained visual\nunderstanding, while the second stage targets the enhancement of reasoning\nabilities. After the proposed two-stage reinforcement learning process, we\nobtain PeBR-R1, a vision-language model with significantly enhanced perceptual\nand reasoning capabilities. Experimental results on seven benchmark datasets\ndemonstrate the effectiveness of our approach and validate the superior\nperformance of PeBR-R1 across diverse visual reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Yan Chen"
                    },
                    {
                        "name": "Long Li"
                    },
                    {
                        "name": "Teng Xi"
                    },
                    {
                        "name": "Long Zeng"
                    },
                    {
                        "name": "Jingdong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingdong Wang"
                },
                "author": "Jingdong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13031v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13025v1",
                "updated": "2025-09-16T12:46:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    46,
                    39,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T12:46:39Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    46,
                    39,
                    1,
                    259,
                    0
                ],
                "title": "GView: A Survey of Binary Forensics via Visual, Semantic, and\n  AI-Enhanced Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GView: A Survey of Binary Forensics via Visual, Semantic, and\n  AI-Enhanced Analysis"
                },
                "summary": "Cybersecurity threats continue to become more sophisticated and diverse in\ntheir artifacts, boosting both their volume and complexity. To overcome those\nchallenges, we present GView, an open-source forensic analysis framework with\nvisual and AI-enhanced reasoning. It started with focus on the practical\ncybersecurity industry. It has evolved significantly, incorporating large\nlanguage models (LLMs) to dynamically enhance reasoning and ease the forensic\nworkflows. This paper surveys both the current state of GView with its\npublished papers alongside those that are in the publishing process. It also\nincludes its innovative use of logical inference through predicates and\ninference rules for both the analyzed documents and the user's actions for\nbetter suggestions. We highlight the extensible architecture, showcasing its\npotential as a bridge between the practical forensics worlds with the academic\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cybersecurity threats continue to become more sophisticated and diverse in\ntheir artifacts, boosting both their volume and complexity. To overcome those\nchallenges, we present GView, an open-source forensic analysis framework with\nvisual and AI-enhanced reasoning. It started with focus on the practical\ncybersecurity industry. It has evolved significantly, incorporating large\nlanguage models (LLMs) to dynamically enhance reasoning and ease the forensic\nworkflows. This paper surveys both the current state of GView with its\npublished papers alongside those that are in the publishing process. It also\nincludes its innovative use of logical inference through predicates and\ninference rules for both the analyzed documents and the user's actions for\nbetter suggestions. We highlight the extensible architecture, showcasing its\npotential as a bridge between the practical forensics worlds with the academic\nresearch."
                },
                "authors": [
                    {
                        "name": "Raul Zaharia"
                    },
                    {
                        "name": "DragoÅ GavriluÅ£"
                    },
                    {
                        "name": "GheorghiÅ£Ä Mutu"
                    }
                ],
                "author_detail": {
                    "name": "GheorghiÅ£Ä Mutu"
                },
                "arxiv_affiliation": "Al. I. Cuza University & Bitdefender",
                "author": "GheorghiÅ£Ä Mutu",
                "arxiv_doi": "10.4204/EPTCS.427.9",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4204/EPTCS.427.9",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.13025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings FROM 2025, arXiv:2509.11877",
                "arxiv_journal_ref": "EPTCS 427, 2025, pp. 134-140",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13023v1",
                "updated": "2025-09-16T12:46:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    46,
                    11,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T12:46:11Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    46,
                    11,
                    1,
                    259,
                    0
                ],
                "title": "Validating Solidity Code Defects using Symbolic and Concrete Execution\n  powered by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Validating Solidity Code Defects using Symbolic and Concrete Execution\n  powered by Large Language Models"
                },
                "summary": "The high rate of false alarms from static analysis tools and Large Language\nModels (LLMs) complicates vulnerability detection in Solidity Smart Contracts,\ndemanding methods that can formally or empirically prove the presence of\ndefects. This paper introduces a novel detection pipeline that integrates\ncustom Slither-based detectors, LLMs, Kontrol, and Forge. Our approach is\ndesigned to reliably detect defects and generate proofs. We currently perform\nexperiments with promising results for seven types of critical defects. We\ndemonstrate the pipeline's efficacy by presenting our findings for three\nvulnerabilities -- Reentrancy, Complex Fallback, and Faulty Access Control\nPolicies -- that are challenging for current verification solutions, which\noften generate false alarms or fail to detect them entirely. We highlight the\npotential of either symbolic or concrete execution in correctly classifying\nsuch code faults. By chaining these instruments, our method effectively\nvalidates true positives, significantly reducing the manual verification\nburden. Although we identify potential limitations, such as the inconsistency\nand the cost of LLMs, our findings establish a robust framework for combining\nheuristic analysis with formal verification to achieve more reliable and\nautomated smart contract auditing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The high rate of false alarms from static analysis tools and Large Language\nModels (LLMs) complicates vulnerability detection in Solidity Smart Contracts,\ndemanding methods that can formally or empirically prove the presence of\ndefects. This paper introduces a novel detection pipeline that integrates\ncustom Slither-based detectors, LLMs, Kontrol, and Forge. Our approach is\ndesigned to reliably detect defects and generate proofs. We currently perform\nexperiments with promising results for seven types of critical defects. We\ndemonstrate the pipeline's efficacy by presenting our findings for three\nvulnerabilities -- Reentrancy, Complex Fallback, and Faulty Access Control\nPolicies -- that are challenging for current verification solutions, which\noften generate false alarms or fail to detect them entirely. We highlight the\npotential of either symbolic or concrete execution in correctly classifying\nsuch code faults. By chaining these instruments, our method effectively\nvalidates true positives, significantly reducing the manual verification\nburden. Although we identify potential limitations, such as the inconsistency\nand the cost of LLMs, our findings establish a robust framework for combining\nheuristic analysis with formal verification to achieve more reliable and\nautomated smart contract auditing."
                },
                "authors": [
                    {
                        "name": "Åtefan-Claudiu Susan"
                    },
                    {
                        "name": "Andrei Arusoaie"
                    },
                    {
                        "name": "Dorel Lucanu"
                    }
                ],
                "author_detail": {
                    "name": "Dorel Lucanu"
                },
                "arxiv_affiliation": "\"Alexandru Ioan Cuza\", University of IaÅi, Department of Computer Science",
                "author": "Dorel Lucanu",
                "arxiv_doi": "10.4204/EPTCS.427.7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4204/EPTCS.427.7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.13023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings FROM 2025, arXiv:2509.11877",
                "arxiv_journal_ref": "EPTCS 427, 2025, pp. 98-116",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.2;D.2.5;D.2.4;D.4.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13022v1",
                "updated": "2025-09-16T12:45:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    45,
                    57,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T12:45:57Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    45,
                    57,
                    1,
                    259,
                    0
                ],
                "title": "Navigating the Python Type Jungle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating the Python Type Jungle"
                },
                "summary": "Python's typing system has evolved pragmatically into a powerful but\ntheoretically fragmented system, with scattered specifications. This paper\nproposes a formalization to address this fragmentation. The central\ncontribution is a formal foundation that uses concepts from type theory to\ndemonstrate that Python's type system can be elegantly described. This work\naims to serve as a crucial first step toward the future development of type\ninference tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Python's typing system has evolved pragmatically into a powerful but\ntheoretically fragmented system, with scattered specifications. This paper\nproposes a formalization to address this fragmentation. The central\ncontribution is a formal foundation that uses concepts from type theory to\ndemonstrate that Python's type system can be elegantly described. This work\naims to serve as a crucial first step toward the future development of type\ninference tools."
                },
                "authors": [
                    {
                        "name": "Andrei Nacu"
                    },
                    {
                        "name": "Dorel Lucanu"
                    }
                ],
                "author_detail": {
                    "name": "Dorel Lucanu"
                },
                "arxiv_affiliation": "Faculty of Computer Science, Alexandru Ioan Cuza University, IaÅi",
                "author": "Dorel Lucanu",
                "arxiv_doi": "10.4204/EPTCS.427.6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4204/EPTCS.427.6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.13022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings FROM 2025, arXiv:2509.11877",
                "arxiv_journal_ref": "EPTCS 427, 2025, pp. 79-97",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13021v1",
                "updated": "2025-09-16T12:45:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    45,
                    45,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T12:45:45Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    45,
                    45,
                    1,
                    259,
                    0
                ],
                "title": "xOffense: An AI-driven autonomous penetration testing framework with\n  offensive knowledge-enhanced LLMs and multi agent systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xOffense: An AI-driven autonomous penetration testing framework with\n  offensive knowledge-enhanced LLMs and multi agent systems"
                },
                "summary": "This work introduces xOffense, an AI-driven, multi-agent penetration testing\nframework that shifts the process from labor-intensive, expert-driven manual\nefforts to fully automated, machine-executable workflows capable of scaling\nseamlessly with computational infrastructure. At its core, xOffense leverages a\nfine-tuned, mid-scale open-source LLM (Qwen3-32B) to drive reasoning and\ndecision-making in penetration testing. The framework assigns specialized\nagents to reconnaissance, vulnerability scanning, and exploitation, with an\norchestration layer ensuring seamless coordination across phases. Fine-tuning\non Chain-of-Thought penetration testing data further enables the model to\ngenerate precise tool commands and perform consistent multi-step reasoning. We\nevaluate xOffense on two rigorous benchmarks: AutoPenBench and\nAI-Pentest-Benchmark. The results demonstrate that xOffense consistently\noutperforms contemporary methods, achieving a sub-task completion rate of\n79.17%, decisively surpassing leading systems such as VulnBot and PentestGPT.\nThese findings highlight the potential of domain-adapted mid-scale LLMs, when\nembedded within structured multi-agent orchestration, to deliver superior,\ncost-efficient, and reproducible solutions for autonomous penetration testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces xOffense, an AI-driven, multi-agent penetration testing\nframework that shifts the process from labor-intensive, expert-driven manual\nefforts to fully automated, machine-executable workflows capable of scaling\nseamlessly with computational infrastructure. At its core, xOffense leverages a\nfine-tuned, mid-scale open-source LLM (Qwen3-32B) to drive reasoning and\ndecision-making in penetration testing. The framework assigns specialized\nagents to reconnaissance, vulnerability scanning, and exploitation, with an\norchestration layer ensuring seamless coordination across phases. Fine-tuning\non Chain-of-Thought penetration testing data further enables the model to\ngenerate precise tool commands and perform consistent multi-step reasoning. We\nevaluate xOffense on two rigorous benchmarks: AutoPenBench and\nAI-Pentest-Benchmark. The results demonstrate that xOffense consistently\noutperforms contemporary methods, achieving a sub-task completion rate of\n79.17%, decisively surpassing leading systems such as VulnBot and PentestGPT.\nThese findings highlight the potential of domain-adapted mid-scale LLMs, when\nembedded within structured multi-agent orchestration, to deliver superior,\ncost-efficient, and reproducible solutions for autonomous penetration testing."
                },
                "authors": [
                    {
                        "name": "Phung Duc Luong"
                    },
                    {
                        "name": "Le Tran Gia Bao"
                    },
                    {
                        "name": "Nguyen Vu Khai Tam"
                    },
                    {
                        "name": "Dong Huu Nguyen Khoa"
                    },
                    {
                        "name": "Nguyen Huu Quyen"
                    },
                    {
                        "name": "Van-Hau Pham"
                    },
                    {
                        "name": "Phan The Duy"
                    }
                ],
                "author_detail": {
                    "name": "Phan The Duy"
                },
                "author": "Phan The Duy",
                "arxiv_comment": "17 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11552v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11552v2",
                "updated": "2025-09-16T12:36:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    36,
                    35,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-15T03:32:50Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    3,
                    32,
                    50,
                    0,
                    258,
                    0
                ],
                "title": "HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with\n  Hierarchical Chunking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with\n  Hierarchical Chunking"
                },
                "summary": "Retrieval-Augmented Generation (RAG) enhances the response capabilities of\nlanguage models by integrating external knowledge sources. However, document\nchunking as an important part of RAG system often lacks effective evaluation\ntools. This paper first analyzes why existing RAG evaluation benchmarks are\ninadequate for assessing document chunking quality, specifically due to\nevidence sparsity. Based on this conclusion, we propose HiCBench, which\nincludes manually annotated multi-level document chunking points, synthesized\nevidence-dense quetion answer(QA) pairs, and their corresponding evidence\nsources. Additionally, we introduce the HiChunk framework, a multi-level\ndocument structuring framework based on fine-tuned LLMs, combined with the\nAuto-Merge retrieval algorithm to improve retrieval quality. Experiments\ndemonstrate that HiCBench effectively evaluates the impact of different\nchunking methods across the entire RAG pipeline. Moreover, HiChunk achieves\nbetter chunking quality within reasonable time consumption, thereby enhancing\nthe overall performance of RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) enhances the response capabilities of\nlanguage models by integrating external knowledge sources. However, document\nchunking as an important part of RAG system often lacks effective evaluation\ntools. This paper first analyzes why existing RAG evaluation benchmarks are\ninadequate for assessing document chunking quality, specifically due to\nevidence sparsity. Based on this conclusion, we propose HiCBench, which\nincludes manually annotated multi-level document chunking points, synthesized\nevidence-dense quetion answer(QA) pairs, and their corresponding evidence\nsources. Additionally, we introduce the HiChunk framework, a multi-level\ndocument structuring framework based on fine-tuned LLMs, combined with the\nAuto-Merge retrieval algorithm to improve retrieval quality. Experiments\ndemonstrate that HiCBench effectively evaluates the impact of different\nchunking methods across the entire RAG pipeline. Moreover, HiChunk achieves\nbetter chunking quality within reasonable time consumption, thereby enhancing\nthe overall performance of RAG systems."
                },
                "authors": [
                    {
                        "name": "Wensheng Lu"
                    },
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Ruizhi Qiao"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "17 pages, 5 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11552v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11552v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13011v1",
                "updated": "2025-09-16T12:33:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    33,
                    54,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T12:33:54Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    33,
                    54,
                    1,
                    259,
                    0
                ],
                "title": "A Visualized Framework for Event Cooperation with Generative Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Visualized Framework for Event Cooperation with Generative Agents"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the simulation of agent\nsocieties, enabling autonomous planning, memory formation, and social\ninteractions. However, existing frameworks often overlook systematic\nevaluations for event organization and lack visualized integration with\nphysically grounded environments, limiting agents' ability to navigate spaces\nand interact with items realistically. We develop MiniAgentPro, a visualization\nplatform featuring an intuitive map editor for customizing environments and a\nsimulation player with smooth animations. Based on this tool, we introduce a\ncomprehensive test set comprising eight diverse event scenarios with basic and\nhard variants to assess agents' ability. Evaluations using GPT-4o demonstrate\nstrong performance in basic settings but highlight coordination challenges in\nhard variants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the simulation of agent\nsocieties, enabling autonomous planning, memory formation, and social\ninteractions. However, existing frameworks often overlook systematic\nevaluations for event organization and lack visualized integration with\nphysically grounded environments, limiting agents' ability to navigate spaces\nand interact with items realistically. We develop MiniAgentPro, a visualization\nplatform featuring an intuitive map editor for customizing environments and a\nsimulation player with smooth animations. Based on this tool, we introduce a\ncomprehensive test set comprising eight diverse event scenarios with basic and\nhard variants to assess agents' ability. Evaluations using GPT-4o demonstrate\nstrong performance in basic settings but highlight coordination challenges in\nhard variants."
                },
                "authors": [
                    {
                        "name": "Yuyang Tian"
                    },
                    {
                        "name": "Shunqiang Mao"
                    },
                    {
                        "name": "Wenchang Gao"
                    },
                    {
                        "name": "Lanlan Qiu"
                    },
                    {
                        "name": "Tianxing He"
                    }
                ],
                "author_detail": {
                    "name": "Tianxing He"
                },
                "author": "Tianxing He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19894v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19894v4",
                "updated": "2025-09-17T05:54:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    5,
                    54,
                    41,
                    2,
                    260,
                    0
                ],
                "published": "2024-09-30T02:53:03Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    2,
                    53,
                    3,
                    0,
                    274,
                    0
                ],
                "title": "Semantic Alignment-Enhanced Code Translation via an LLM-Based\n  Multi-Agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Alignment-Enhanced Code Translation via an LLM-Based\n  Multi-Agent System"
                },
                "summary": "Code translation converts code from one programming language to another while\nmaintaining its original functionality, which is crucial for software\nmigration, system refactoring, and cross-platform development. Traditional\nrule-based methods rely on manually-written rules, which can be time-consuming\nand often result in less readable code. To overcome this, learning-based\nmethods have been developed, leveraging parallel data to train models for\nautomated code translation. More recently, the advance of Large Language Models\n(LLMs) further boosts learning-based code translation. Although promising,\nLLM-translated program still suffers from diverse quality issues (e.g., syntax\nerrors and semantic errors). In particular, it can be challenging for LLMs to\nself-debug these errors when simply provided with the corresponding error\nmessages.\n  In this work, we propose a novel LLM-based multi-agent system TRANSAGENT,\nwhich enhances LLM-based code translation by fixing the syntax errors and\nsemantic errors with the synergy between four LLM-based agents, including\nInitial Code Translator, Syntax Error Fixer, Code Aligner, and Semantic Error\nFixer. The main insight of TRANSAGENT is to first localize the error code block\nin the target program based on the execution alignment between the target and\nsource program, which can narrow down the fixing space and thus lower down the\nfixing difficulties. To evaluate TRANSAGENT, we first construct a new benchmark\nfrom recent programming tasks to mitigate the potential data leakage issue. On\nour benchmark, TRANSAGENT outperforms the latest LLM-based code translation\ntechnique UniTrans in both translation effectiveness and efficiency;\nadditionally, our evaluation on different LLMs show the generalization of\nTRANSAGENT and our ablation study shows the contribution of each agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code translation converts code from one programming language to another while\nmaintaining its original functionality, which is crucial for software\nmigration, system refactoring, and cross-platform development. Traditional\nrule-based methods rely on manually-written rules, which can be time-consuming\nand often result in less readable code. To overcome this, learning-based\nmethods have been developed, leveraging parallel data to train models for\nautomated code translation. More recently, the advance of Large Language Models\n(LLMs) further boosts learning-based code translation. Although promising,\nLLM-translated program still suffers from diverse quality issues (e.g., syntax\nerrors and semantic errors). In particular, it can be challenging for LLMs to\nself-debug these errors when simply provided with the corresponding error\nmessages.\n  In this work, we propose a novel LLM-based multi-agent system TRANSAGENT,\nwhich enhances LLM-based code translation by fixing the syntax errors and\nsemantic errors with the synergy between four LLM-based agents, including\nInitial Code Translator, Syntax Error Fixer, Code Aligner, and Semantic Error\nFixer. The main insight of TRANSAGENT is to first localize the error code block\nin the target program based on the execution alignment between the target and\nsource program, which can narrow down the fixing space and thus lower down the\nfixing difficulties. To evaluate TRANSAGENT, we first construct a new benchmark\nfrom recent programming tasks to mitigate the potential data leakage issue. On\nour benchmark, TRANSAGENT outperforms the latest LLM-based code translation\ntechnique UniTrans in both translation effectiveness and efficiency;\nadditionally, our evaluation on different LLMs show the generalization of\nTRANSAGENT and our ablation study shows the contribution of each agent."
                },
                "authors": [
                    {
                        "name": "Zhiqiang Yuan"
                    },
                    {
                        "name": "Weitong Chen"
                    },
                    {
                        "name": "Hanlin Wang"
                    },
                    {
                        "name": "Kai Yu"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Yiling Lou"
                    }
                ],
                "author_detail": {
                    "name": "Yiling Lou"
                },
                "author": "Yiling Lou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19894v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19894v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08679v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08679v2",
                "updated": "2025-09-16T12:31:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    31,
                    6,
                    1,
                    259,
                    0
                ],
                "published": "2025-07-11T15:21:49Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    21,
                    49,
                    4,
                    192,
                    0
                ],
                "title": "ByDeWay: Boost Your multimodal LLM with DEpth prompting in a\n  Training-Free Way",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ByDeWay: Boost Your multimodal LLM with DEpth prompting in a\n  Training-Free Way"
                },
                "summary": "We introduce ByDeWay, a training-free framework designed to enhance the\nperformance of Multimodal Large Language Models (MLLMs). ByDeWay uses a novel\nprompting strategy called Layered-Depth-Based Prompting (LDP), which improves\nspatial reasoning and grounding without modifying any model parameters. It\nsegments the scene into closest, mid-range, and farthest layers using monocular\ndepth estimation, then generates region-specific captions with a grounded\nvision-language model. These structured, depth-aware captions are appended to\nthe image-question prompt, enriching it with spatial context. This guides MLLMs\nto produce more grounded and less hallucinated responses. Our method is\nlightweight, modular, and compatible with black-box MLLMs. Experiments on\nhallucination-sensitive (POPE) and reasoning-intensive (GQA) benchmarks show\nconsistent improvements across multiple MLLMs, validating the effectiveness of\ndepth-aware prompting in a zero-training setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ByDeWay, a training-free framework designed to enhance the\nperformance of Multimodal Large Language Models (MLLMs). ByDeWay uses a novel\nprompting strategy called Layered-Depth-Based Prompting (LDP), which improves\nspatial reasoning and grounding without modifying any model parameters. It\nsegments the scene into closest, mid-range, and farthest layers using monocular\ndepth estimation, then generates region-specific captions with a grounded\nvision-language model. These structured, depth-aware captions are appended to\nthe image-question prompt, enriching it with spatial context. This guides MLLMs\nto produce more grounded and less hallucinated responses. Our method is\nlightweight, modular, and compatible with black-box MLLMs. Experiments on\nhallucination-sensitive (POPE) and reasoning-intensive (GQA) benchmarks show\nconsistent improvements across multiple MLLMs, validating the effectiveness of\ndepth-aware prompting in a zero-training setting."
                },
                "authors": [
                    {
                        "name": "Rajarshi Roy"
                    },
                    {
                        "name": "Devleena Das"
                    },
                    {
                        "name": "Ankesh Banerjee"
                    },
                    {
                        "name": "Arjya Bhattacharjee"
                    },
                    {
                        "name": "Kousik Dasgupta"
                    },
                    {
                        "name": "Subarna Tripathi"
                    }
                ],
                "author_detail": {
                    "name": "Subarna Tripathi"
                },
                "author": "Subarna Tripathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08679v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08679v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13009v1",
                "updated": "2025-09-16T12:29:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    29,
                    0,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T12:29:00Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    29,
                    0,
                    1,
                    259,
                    0
                ],
                "title": "Binary imposters: Mergers in massive hierarchical triple stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary imposters: Mergers in massive hierarchical triple stars"
                },
                "summary": "Massive stars are often born in triples, where gravitational dynamics and\nstellar interactions play a crucial role in shaping their evolution. One such\npathway includes the merger of the inner binary, transforming the system to a\nbinary with a distinct formation history. Therefore, the interpretation of\nobserved binary properties and their inferred formation history may require the\nconsideration of a potential triple origin. We aim to investigate the\npopulation of stellar mergers in massive hierarchical triples. Specifically, we\nassess how frequently mergers occur, and characterise the properties of the\npost-merger binaries and their subsequent evolution. We combine the triple\npopulation synthesis code TRES, which self-consistently models stellar\nevolution, binary interaction, and gravitational dynamics, with the binary\npopulation synthesis code SeBa to simulate 10^5 dynamically stable, massive\ntriples from the zero-age main sequence through merger and post-merger\nevolution. We explore the effects of a range of physical models for the initial\nstellar properties, mass transfer, and merger. We find that stellar mergers are\na common outcome, occurring in 20-32% of massive triples. Most mergers happen\nrelatively early in the evolution of the system and involve two main-sequence\n(MS) stars, producing rejuvenated merger remnants that can appear significantly\nyounger than their tertiary companions. Consequently, we predict that 2-10% of\nall wide MS+MS binaries (P>100 days) have a measurable age discrepancy, and\nserve as a promising way to identify merged stars. The post-merger systems\npreferentially evolve into wide, eccentric binaries, with ~80% avoiding further\ninteraction. However, a notable fraction (16-22%) undergoes a second\nmass-transfer phase, which may result in the formation of high-mass X-ray\nbinaries or mergers of compact objects that spiral in via gravitational-wave\nemission.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive stars are often born in triples, where gravitational dynamics and\nstellar interactions play a crucial role in shaping their evolution. One such\npathway includes the merger of the inner binary, transforming the system to a\nbinary with a distinct formation history. Therefore, the interpretation of\nobserved binary properties and their inferred formation history may require the\nconsideration of a potential triple origin. We aim to investigate the\npopulation of stellar mergers in massive hierarchical triples. Specifically, we\nassess how frequently mergers occur, and characterise the properties of the\npost-merger binaries and their subsequent evolution. We combine the triple\npopulation synthesis code TRES, which self-consistently models stellar\nevolution, binary interaction, and gravitational dynamics, with the binary\npopulation synthesis code SeBa to simulate 10^5 dynamically stable, massive\ntriples from the zero-age main sequence through merger and post-merger\nevolution. We explore the effects of a range of physical models for the initial\nstellar properties, mass transfer, and merger. We find that stellar mergers are\na common outcome, occurring in 20-32% of massive triples. Most mergers happen\nrelatively early in the evolution of the system and involve two main-sequence\n(MS) stars, producing rejuvenated merger remnants that can appear significantly\nyounger than their tertiary companions. Consequently, we predict that 2-10% of\nall wide MS+MS binaries (P>100 days) have a measurable age discrepancy, and\nserve as a promising way to identify merged stars. The post-merger systems\npreferentially evolve into wide, eccentric binaries, with ~80% avoiding further\ninteraction. However, a notable fraction (16-22%) undergoes a second\nmass-transfer phase, which may result in the formation of high-mass X-ray\nbinaries or mergers of compact objects that spiral in via gravitational-wave\nemission."
                },
                "authors": [
                    {
                        "name": "F. Kummer"
                    },
                    {
                        "name": "G. Simion"
                    },
                    {
                        "name": "S. Toonen"
                    },
                    {
                        "name": "A. de Koter"
                    }
                ],
                "author_detail": {
                    "name": "A. de Koter"
                },
                "author": "A. de Koter",
                "arxiv_comment": "14 pages, 8 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12067v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12067v3",
                "updated": "2025-09-16T12:21:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    21,
                    22,
                    1,
                    259,
                    0
                ],
                "published": "2025-02-17T17:37:26Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    17,
                    37,
                    26,
                    0,
                    48,
                    0
                ],
                "title": "TokenSkip: Controllable Chain-of-Thought Compression in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSkip: Controllable Chain-of-Thought Compression in LLMs"
                },
                "summary": "Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Recent advancements, such as\nOpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT\nsequences during inference could further boost LLM reasoning performance.\nHowever, due to the autoregressive nature of LLM decoding, longer CoT outputs\nlead to a linear increase in inference latency, adversely affecting user\nexperience, particularly when the CoT exceeds 10,000 tokens. To address this\nlimitation, we analyze the semantic importance of tokens within CoT outputs and\nreveal that their contributions to reasoning vary. Building on this insight, we\npropose TokenSkip, a simple yet effective approach that enables LLMs to\nselectively skip less important tokens, allowing for controllable CoT\ncompression. Extensive experiments across various models and tasks demonstrate\nthe effectiveness of TokenSkip in reducing CoT token usage while preserving\nstrong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct,\nTokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less\nthan a 0.4% performance drop. We release our code and checkpoints in\nhttps://github.com/hemingkx/TokenSkip.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Recent advancements, such as\nOpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT\nsequences during inference could further boost LLM reasoning performance.\nHowever, due to the autoregressive nature of LLM decoding, longer CoT outputs\nlead to a linear increase in inference latency, adversely affecting user\nexperience, particularly when the CoT exceeds 10,000 tokens. To address this\nlimitation, we analyze the semantic importance of tokens within CoT outputs and\nreveal that their contributions to reasoning vary. Building on this insight, we\npropose TokenSkip, a simple yet effective approach that enables LLMs to\nselectively skip less important tokens, allowing for controllable CoT\ncompression. Extensive experiments across various models and tasks demonstrate\nthe effectiveness of TokenSkip in reducing CoT token usage while preserving\nstrong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct,\nTokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less\nthan a 0.4% performance drop. We release our code and checkpoints in\nhttps://github.com/hemingkx/TokenSkip."
                },
                "authors": [
                    {
                        "name": "Heming Xia"
                    },
                    {
                        "name": "Chak Tou Leong"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Wenjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Li"
                },
                "author": "Wenjie Li",
                "arxiv_comment": "EMNLP 2025 (Long Paper), camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12067v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12067v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12994v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12994v1",
                "updated": "2025-09-16T12:06:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    6,
                    5,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T12:06:05Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    6,
                    5,
                    1,
                    259,
                    0
                ],
                "title": "SitLLM: Large Language Models for Sitting Posture Health Understanding\n  via Pressure Sensor Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SitLLM: Large Language Models for Sitting Posture Health Understanding\n  via Pressure Sensor Data"
                },
                "summary": "Poor sitting posture is a critical yet often overlooked factor contributing\nto long-term musculoskeletal disorders and physiological dysfunctions. Existing\nsitting posture monitoring systems, although leveraging visual, IMU, or\npressure-based modalities, often suffer from coarse-grained recognition and\nlack the semantic expressiveness necessary for personalized feedback. In this\npaper, we propose \\textbf{SitLLM}, a lightweight multimodal framework that\nintegrates flexible pressure sensing with large language models (LLMs) to\nenable fine-grained posture understanding and personalized health-oriented\nresponse generation. SitLLM comprises three key components: (1) a\n\\textit{Gaussian-Robust Sensor Embedding Module} that partitions pressure maps\ninto spatial patches and injects local noise perturbations for robust feature\nextraction; (2) a \\textit{Prompt-Driven Cross-Modal Alignment Module} that\nreprograms sensor embeddings into the LLM's semantic space via multi-head\ncross-attention using the pre-trained vocabulary embeddings; and (3) a\n\\textit{Multi-Context Prompt Module} that fuses feature-level, structure-level,\nstatistical-level, and semantic-level contextual information to guide\ninstruction comprehension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Poor sitting posture is a critical yet often overlooked factor contributing\nto long-term musculoskeletal disorders and physiological dysfunctions. Existing\nsitting posture monitoring systems, although leveraging visual, IMU, or\npressure-based modalities, often suffer from coarse-grained recognition and\nlack the semantic expressiveness necessary for personalized feedback. In this\npaper, we propose \\textbf{SitLLM}, a lightweight multimodal framework that\nintegrates flexible pressure sensing with large language models (LLMs) to\nenable fine-grained posture understanding and personalized health-oriented\nresponse generation. SitLLM comprises three key components: (1) a\n\\textit{Gaussian-Robust Sensor Embedding Module} that partitions pressure maps\ninto spatial patches and injects local noise perturbations for robust feature\nextraction; (2) a \\textit{Prompt-Driven Cross-Modal Alignment Module} that\nreprograms sensor embeddings into the LLM's semantic space via multi-head\ncross-attention using the pre-trained vocabulary embeddings; and (3) a\n\\textit{Multi-Context Prompt Module} that fuses feature-level, structure-level,\nstatistical-level, and semantic-level contextual information to guide\ninstruction comprehension."
                },
                "authors": [
                    {
                        "name": "Jian Gao"
                    },
                    {
                        "name": "Fufangchen Zhao"
                    },
                    {
                        "name": "Yiyang Zhang"
                    },
                    {
                        "name": "Danfeng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Danfeng Yan"
                },
                "author": "Danfeng Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12994v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12994v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12993v1",
                "updated": "2025-09-16T12:04:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    4,
                    0,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T12:04:00Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    4,
                    0,
                    1,
                    259,
                    0
                ],
                "title": "HPIM: Heterogeneous Processing-In-Memory-based Accelerator for Large\n  Language Models Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPIM: Heterogeneous Processing-In-Memory-based Accelerator for Large\n  Language Models Inference"
                },
                "summary": "The deployment of large language models (LLMs) presents significant\nchallenges due to their enormous memory footprints, low arithmetic intensity,\nand stringent latency requirements, particularly during the autoregressive\ndecoding stage. Traditional compute-centric accelerators, such as GPUs, suffer\nfrom severe resource underutilization and memory bandwidth bottlenecks in these\nmemory-bound workloads. To overcome these fundamental limitations, we propose\nHPIM, the first memory-centric heterogeneous Processing-In-Memory (PIM)\naccelerator that integrates SRAM-PIM and HBM-PIM subsystems designed\nspecifically for LLM inference. HPIM employs a software-hardware co-design\napproach that combines a specialized compiler framework with a heterogeneous\nhardware architecture. It intelligently partitions workloads based on their\ncharacteristics: latency-critical attention operations are mapped to the\nSRAM-PIM subsystem to exploit its ultra-low latency and high computational\nflexibility, while weight-intensive GEMV computations are assigned to the\nHBM-PIM subsystem to leverage its high internal bandwidth and large storage\ncapacity. Furthermore, HPIM introduces a tightly coupled pipeline strategy\nacross SRAM-PIM and HBM-PIM subsystems to maximize intra-token parallelism,\nthereby significantly mitigating serial dependency of the autoregressive\ndecoding stage. Comprehensive evaluations using a cycle-accurate simulator\ndemonstrate that HPIM significantly outperforms state-of-the-art accelerators,\nachieving a peak speedup of up to 22.8x compared to the NVIDIA A100 GPU.\nMoreover, HPIM exhibits superior performance over contemporary PIM-based\naccelerators, highlighting its potential as a highly practical and scalable\nsolution for accelerating large-scale LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) presents significant\nchallenges due to their enormous memory footprints, low arithmetic intensity,\nand stringent latency requirements, particularly during the autoregressive\ndecoding stage. Traditional compute-centric accelerators, such as GPUs, suffer\nfrom severe resource underutilization and memory bandwidth bottlenecks in these\nmemory-bound workloads. To overcome these fundamental limitations, we propose\nHPIM, the first memory-centric heterogeneous Processing-In-Memory (PIM)\naccelerator that integrates SRAM-PIM and HBM-PIM subsystems designed\nspecifically for LLM inference. HPIM employs a software-hardware co-design\napproach that combines a specialized compiler framework with a heterogeneous\nhardware architecture. It intelligently partitions workloads based on their\ncharacteristics: latency-critical attention operations are mapped to the\nSRAM-PIM subsystem to exploit its ultra-low latency and high computational\nflexibility, while weight-intensive GEMV computations are assigned to the\nHBM-PIM subsystem to leverage its high internal bandwidth and large storage\ncapacity. Furthermore, HPIM introduces a tightly coupled pipeline strategy\nacross SRAM-PIM and HBM-PIM subsystems to maximize intra-token parallelism,\nthereby significantly mitigating serial dependency of the autoregressive\ndecoding stage. Comprehensive evaluations using a cycle-accurate simulator\ndemonstrate that HPIM significantly outperforms state-of-the-art accelerators,\nachieving a peak speedup of up to 22.8x compared to the NVIDIA A100 GPU.\nMoreover, HPIM exhibits superior performance over contemporary PIM-based\naccelerators, highlighting its potential as a highly practical and scalable\nsolution for accelerating large-scale LLM inference."
                },
                "authors": [
                    {
                        "name": "Cenlin Duan"
                    },
                    {
                        "name": "Jianlei Yang"
                    },
                    {
                        "name": "Rubing Yang"
                    },
                    {
                        "name": "Yikun Wang"
                    },
                    {
                        "name": "Yiou Wang"
                    },
                    {
                        "name": "Lingkun Long"
                    },
                    {
                        "name": "Yingjie Qi"
                    },
                    {
                        "name": "Xiaolin He"
                    },
                    {
                        "name": "Ao Zhou"
                    },
                    {
                        "name": "Xueyan Wang"
                    },
                    {
                        "name": "Weisheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Weisheng Zhao"
                },
                "author": "Weisheng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16408v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16408v2",
                "updated": "2025-09-16T11:55:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    55,
                    23,
                    1,
                    259,
                    0
                ],
                "published": "2025-05-22T09:00:01Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    9,
                    0,
                    1,
                    3,
                    142,
                    0
                ],
                "title": "From Surveys to Narratives: Rethinking Cultural Value Adaptation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Surveys to Narratives: Rethinking Cultural Value Adaptation in LLMs"
                },
                "summary": "Adapting cultural values in Large Language Models (LLMs) presents significant\nchallenges, particularly due to biases and limited training data. Prior work\nprimarily aligns LLMs with different cultural values using World Values Survey\n(WVS) data. However, it remains unclear whether this approach effectively\ncaptures cultural nuances or produces distinct cultural representations for\nvarious downstream tasks. In this paper, we systematically investigate\nWVS-based training for cultural value adaptation and find that relying solely\non survey data can homogenize cultural norms and interfere with factual\nknowledge. To investigate these issues, we augment WVS with encyclopedic and\nscenario-based cultural narratives from Wikipedia and NormAd. While these\nnarratives may have variable effects on downstream tasks, they consistently\nimprove cultural distinctiveness than survey data alone. Our work highlights\nthe inherent complexity of aligning cultural values with the goal of guiding\ntask-specific behavior. We release our code at\nhttps://github.com/faridlazuarda/from-surveys-to-narratives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting cultural values in Large Language Models (LLMs) presents significant\nchallenges, particularly due to biases and limited training data. Prior work\nprimarily aligns LLMs with different cultural values using World Values Survey\n(WVS) data. However, it remains unclear whether this approach effectively\ncaptures cultural nuances or produces distinct cultural representations for\nvarious downstream tasks. In this paper, we systematically investigate\nWVS-based training for cultural value adaptation and find that relying solely\non survey data can homogenize cultural norms and interfere with factual\nknowledge. To investigate these issues, we augment WVS with encyclopedic and\nscenario-based cultural narratives from Wikipedia and NormAd. While these\nnarratives may have variable effects on downstream tasks, they consistently\nimprove cultural distinctiveness than survey data alone. Our work highlights\nthe inherent complexity of aligning cultural values with the goal of guiding\ntask-specific behavior. We release our code at\nhttps://github.com/faridlazuarda/from-surveys-to-narratives."
                },
                "authors": [
                    {
                        "name": "Muhammad Farid Adilazuarda"
                    },
                    {
                        "name": "Chen Cecilia Liu"
                    },
                    {
                        "name": "Iryna Gurevych"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    }
                ],
                "author_detail": {
                    "name": "Alham Fikri Aji"
                },
                "author": "Alham Fikri Aji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16408v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16408v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12987v1",
                "updated": "2025-09-16T11:51:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    51,
                    7,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T11:51:07Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    51,
                    7,
                    1,
                    259,
                    0
                ],
                "title": "Toward PDDL Planning Copilot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward PDDL Planning Copilot"
                },
                "summary": "Large Language Models (LLMs) are increasingly being used as autonomous agents\ncapable of performing complicated tasks. However, they lack the ability to\nperform reliable long-horizon planning on their own. This paper bridges this\ngap by introducing the Planning Copilot, a chatbot that integrates multiple\nplanning tools and allows users to invoke them through instructions in natural\nlanguage. The Planning Copilot leverages the Model Context Protocol (MCP), a\nrecently developed standard for connecting LLMs with external tools and\nsystems. This approach allows using any LLM that supports MCP without\ndomain-specific fine-tuning. Our Planning Copilot supports common planning\ntasks such as checking the syntax of planning problems, selecting an\nappropriate planner, calling it, validating the plan it generates, and\nsimulating their execution. We empirically evaluate the ability of our Planning\nCopilot to perform these tasks using three open-source LLMs. The results show\nthat the Planning Copilot highly outperforms using the same LLMs without the\nplanning tools. We also conducted a limited qualitative comparison of our tool\nagainst Chat GPT-5, a very recent commercial LLM. Our results shows that our\nPlanning Copilot significantly outperforms GPT-5 despite relying on a much\nsmaller LLM. This suggests dedicated planning tools may be an effective way to\nenable LLMs to perform planning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being used as autonomous agents\ncapable of performing complicated tasks. However, they lack the ability to\nperform reliable long-horizon planning on their own. This paper bridges this\ngap by introducing the Planning Copilot, a chatbot that integrates multiple\nplanning tools and allows users to invoke them through instructions in natural\nlanguage. The Planning Copilot leverages the Model Context Protocol (MCP), a\nrecently developed standard for connecting LLMs with external tools and\nsystems. This approach allows using any LLM that supports MCP without\ndomain-specific fine-tuning. Our Planning Copilot supports common planning\ntasks such as checking the syntax of planning problems, selecting an\nappropriate planner, calling it, validating the plan it generates, and\nsimulating their execution. We empirically evaluate the ability of our Planning\nCopilot to perform these tasks using three open-source LLMs. The results show\nthat the Planning Copilot highly outperforms using the same LLMs without the\nplanning tools. We also conducted a limited qualitative comparison of our tool\nagainst Chat GPT-5, a very recent commercial LLM. Our results shows that our\nPlanning Copilot significantly outperforms GPT-5 despite relying on a much\nsmaller LLM. This suggests dedicated planning tools may be an effective way to\nenable LLMs to perform planning tasks."
                },
                "authors": [
                    {
                        "name": "Yarin Benyamin"
                    },
                    {
                        "name": "Argaman Mordoch"
                    },
                    {
                        "name": "Shahaf S. Shperberg"
                    },
                    {
                        "name": "Roni Stern"
                    }
                ],
                "author_detail": {
                    "name": "Roni Stern"
                },
                "author": "Roni Stern",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21188v2",
                "updated": "2025-09-16T11:50:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    50,
                    27,
                    1,
                    259,
                    0
                ],
                "published": "2025-05-27T13:38:59Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    13,
                    38,
                    59,
                    1,
                    147,
                    0
                ],
                "title": "Optimized quantum sensor networks for ultralight dark matter detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimized quantum sensor networks for ultralight dark matter detection"
                },
                "summary": "Dark matter (DM) remains one of the most compelling unresolved problems in\nfundamental physics, motivating the search for new detection approaches. We\npropose a network-based quantum sensor architecture to enhance sensitivity to\nultralight DM fields. Each node in the network is a superconducting qubit,\ninterconnected via controlled-Z gates in symmetric topologies such as line,\nring, star, and fully connected graphs. We investigate four- and nine-qubit\nsystems, optimizing both state preparation and measurement using a variational\nquantum metrology framework. This approach minimizes the quantum and classical\nCram\\'er-Rao bounds to identify optimal configurations. Bayesian inference is\nemployed to extract the DM-induced phase shift from measurement outcomes. Our\nresults show that optimized network configurations significantly outperform\nconventional GHZ-based protocols while maintaining shallow circuit depths\ncompatible with noisy intermediate-scale quantum hardware. Sensitivity remains\nrobust under local dephasing noise. These findings highlight the importance of\nnetwork structure in quantum sensing and point toward scalable strategies for\nquantum-enhanced DM detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dark matter (DM) remains one of the most compelling unresolved problems in\nfundamental physics, motivating the search for new detection approaches. We\npropose a network-based quantum sensor architecture to enhance sensitivity to\nultralight DM fields. Each node in the network is a superconducting qubit,\ninterconnected via controlled-Z gates in symmetric topologies such as line,\nring, star, and fully connected graphs. We investigate four- and nine-qubit\nsystems, optimizing both state preparation and measurement using a variational\nquantum metrology framework. This approach minimizes the quantum and classical\nCram\\'er-Rao bounds to identify optimal configurations. Bayesian inference is\nemployed to extract the DM-induced phase shift from measurement outcomes. Our\nresults show that optimized network configurations significantly outperform\nconventional GHZ-based protocols while maintaining shallow circuit depths\ncompatible with noisy intermediate-scale quantum hardware. Sensitivity remains\nrobust under local dephasing noise. These findings highlight the importance of\nnetwork structure in quantum sensing and point toward scalable strategies for\nquantum-enhanced DM detection."
                },
                "authors": [
                    {
                        "name": "Adriel I. Santoso"
                    },
                    {
                        "name": "Le Bin Ho"
                    }
                ],
                "author_detail": {
                    "name": "Le Bin Ho"
                },
                "author": "Le Bin Ho",
                "arxiv_comment": "7 pages, 7 figures",
                "arxiv_journal_ref": "Physical Review D (Letter) (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12985v1",
                "updated": "2025-09-16T11:47:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    47,
                    59,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T11:47:59Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    47,
                    59,
                    1,
                    259,
                    0
                ],
                "title": "Dynamic Local Average Treatment Effects in Time Series",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Local Average Treatment Effects in Time Series"
                },
                "summary": "This paper discusses identification, estimation, and inference on dynamic\nlocal average treatment effects (LATEs) in instrumental variables (IVs)\nsettings. First, we show that compliers--observations whose treatment status is\naffected by the instrument--can be identified individually in time series data\nusing smoothness assumptions and local comparisons of treatment assignments.\nSecond, we show that this result enables not only better interpretability of IV\nestimates but also direct testing of the exclusion restriction by comparing\noutcomes among identified non-compliers across instrument values. Third, we\ndocument pervasive weak identification in applied work using IVs with time\nseries data by surveying recent publications in leading economics journals.\nHowever, we find that strong identification often holds in large subsamples for\nwhich the instrument induces changes in the treatment. Motivated by this, we\nintroduce a method based on dynamic programming to detect the most\nstrongly-identified subsample and show how to use this subsample to improve\nestimation and inference. We also develop new identification-robust inference\nprocedures that focus on the most strongly-identified subsample, offering\nefficiency gains relative to existing full sample identification-robust\ninference when identification fails over parts of the sample. Finally, we apply\nour results to heteroskedasticity-based identification of monetary policy\neffects. We find that about 75% of observations are compliers (i.e., cases\nwhere the variance of the policy shifts up on FOMC announcement days), and we\nfail to reject the exclusion restriction. Estimation using the most\nstrongly-identified subsample helps reconcile conflicting IV and GMM estimates\nin the literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper discusses identification, estimation, and inference on dynamic\nlocal average treatment effects (LATEs) in instrumental variables (IVs)\nsettings. First, we show that compliers--observations whose treatment status is\naffected by the instrument--can be identified individually in time series data\nusing smoothness assumptions and local comparisons of treatment assignments.\nSecond, we show that this result enables not only better interpretability of IV\nestimates but also direct testing of the exclusion restriction by comparing\noutcomes among identified non-compliers across instrument values. Third, we\ndocument pervasive weak identification in applied work using IVs with time\nseries data by surveying recent publications in leading economics journals.\nHowever, we find that strong identification often holds in large subsamples for\nwhich the instrument induces changes in the treatment. Motivated by this, we\nintroduce a method based on dynamic programming to detect the most\nstrongly-identified subsample and show how to use this subsample to improve\nestimation and inference. We also develop new identification-robust inference\nprocedures that focus on the most strongly-identified subsample, offering\nefficiency gains relative to existing full sample identification-robust\ninference when identification fails over parts of the sample. Finally, we apply\nour results to heteroskedasticity-based identification of monetary policy\neffects. We find that about 75% of observations are compliers (i.e., cases\nwhere the variance of the policy shifts up on FOMC announcement days), and we\nfail to reject the exclusion restriction. Estimation using the most\nstrongly-identified subsample helps reconcile conflicting IV and GMM estimates\nin the literature."
                },
                "authors": [
                    {
                        "name": "Alessandro Casini"
                    },
                    {
                        "name": "Adam McCloskey"
                    },
                    {
                        "name": "Luca Rolla"
                    },
                    {
                        "name": "Raimondo Pala"
                    }
                ],
                "author_detail": {
                    "name": "Raimondo Pala"
                },
                "author": "Raimondo Pala",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12973v1",
                "updated": "2025-09-16T11:30:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    30,
                    10,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T11:30:10Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    30,
                    10,
                    1,
                    259,
                    0
                ],
                "title": "Evaluating Large Language Models for Code Translation: Effects of Prompt\n  Language and Prompt Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models for Code Translation: Effects of Prompt\n  Language and Prompt Design"
                },
                "summary": "Large language models (LLMs) have shown promise for automated source-code\ntranslation, a capability critical to software migration, maintenance, and\ninteroperability. Yet comparative evidence on how model choice, prompt design,\nand prompt language shape translation quality across multiple programming\nlanguages remains limited. This study conducts a systematic empirical\nassessment of state-of-the-art LLMs for code translation among C++, Java,\nPython, and C#, alongside a traditional baseline (TransCoder). Using BLEU and\nCodeBLEU, we quantify syntactic fidelity and structural correctness under two\nprompt styles (concise instruction and detailed specification) and two prompt\nlanguages (English and Arabic), with direction-aware evaluation across language\npairs. Experiments show that detailed prompts deliver consistent gains across\nmodels and translation directions, and English prompts outperform Arabic by\n13-15%. The top-performing model attains the highest CodeBLEU on challenging\npairs such as Java to C# and Python to C++. Our evaluation shows that each LLM\noutperforms TransCoder across the benchmark. These results demonstrate the\nvalue of careful prompt engineering and prompt language choice, and provide\npractical guidance for software modernization and cross-language\ninteroperability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promise for automated source-code\ntranslation, a capability critical to software migration, maintenance, and\ninteroperability. Yet comparative evidence on how model choice, prompt design,\nand prompt language shape translation quality across multiple programming\nlanguages remains limited. This study conducts a systematic empirical\nassessment of state-of-the-art LLMs for code translation among C++, Java,\nPython, and C#, alongside a traditional baseline (TransCoder). Using BLEU and\nCodeBLEU, we quantify syntactic fidelity and structural correctness under two\nprompt styles (concise instruction and detailed specification) and two prompt\nlanguages (English and Arabic), with direction-aware evaluation across language\npairs. Experiments show that detailed prompts deliver consistent gains across\nmodels and translation directions, and English prompts outperform Arabic by\n13-15%. The top-performing model attains the highest CodeBLEU on challenging\npairs such as Java to C# and Python to C++. Our evaluation shows that each LLM\noutperforms TransCoder across the benchmark. These results demonstrate the\nvalue of careful prompt engineering and prompt language choice, and provide\npractical guidance for software modernization and cross-language\ninteroperability."
                },
                "authors": [
                    {
                        "name": "Aamer Aljagthami"
                    },
                    {
                        "name": "Mohammed Banabila"
                    },
                    {
                        "name": "Musab Alshehri"
                    },
                    {
                        "name": "Mohammed Kabini"
                    },
                    {
                        "name": "Mohammad D. Alahmadi"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad D. Alahmadi"
                },
                "author": "Mohammad D. Alahmadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12769v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12769v3",
                "updated": "2025-09-16T11:12:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    12,
                    12,
                    1,
                    259,
                    0
                ],
                "published": "2025-02-18T11:32:43Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    11,
                    32,
                    43,
                    1,
                    49,
                    0
                ],
                "title": "How Much Do LLMs Hallucinate across Languages? On Multilingual\n  Estimation of LLM Hallucination in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Much Do LLMs Hallucinate across Languages? On Multilingual\n  Estimation of LLM Hallucination in the Wild"
                },
                "summary": "In the age of misinformation, hallucination -- the tendency of Large Language\nModels (LLMs) to generate non-factual or unfaithful responses -- represents the\nmain risk for their global utility. Despite LLMs becoming increasingly\nmultilingual, the vast majority of research on detecting and quantifying LLM\nhallucination are (a) English-centric and (b) focus on machine translation (MT)\nand summarization, tasks that are less common ``in the wild'' than open\ninformation seeking. In contrast, we aim to quantify the extent of LLM\nhallucination across languages in knowledge-intensive long-form question\nanswering. To this end, we train a multilingual hallucination detection model\nand conduct a large-scale study across 30 languages and 6 open-source LLM\nfamilies. We start from an English hallucination detection dataset and rely on\nMT to generate (noisy) training data in other languages. We also manually\nannotate gold data for five high-resource languages; we then demonstrate, for\nthese languages, that the estimates of hallucination rates are similar between\nsilver (LLM-generated) and gold test sets, validating the use of silver data\nfor estimating hallucination rates for other languages. For the final rates\nestimation, we build a knowledge-intensive QA dataset for 30 languages with\nLLM-generated prompts and Wikipedia articles as references. We find that, while\nLLMs generate longer responses with more hallucinated tokens for\nhigher-resource languages, there is no correlation between length-normalized\nhallucination rates of languages and their digital representation. Further, we\nfind that smaller LLMs exhibit larger hallucination rates than larger models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the age of misinformation, hallucination -- the tendency of Large Language\nModels (LLMs) to generate non-factual or unfaithful responses -- represents the\nmain risk for their global utility. Despite LLMs becoming increasingly\nmultilingual, the vast majority of research on detecting and quantifying LLM\nhallucination are (a) English-centric and (b) focus on machine translation (MT)\nand summarization, tasks that are less common ``in the wild'' than open\ninformation seeking. In contrast, we aim to quantify the extent of LLM\nhallucination across languages in knowledge-intensive long-form question\nanswering. To this end, we train a multilingual hallucination detection model\nand conduct a large-scale study across 30 languages and 6 open-source LLM\nfamilies. We start from an English hallucination detection dataset and rely on\nMT to generate (noisy) training data in other languages. We also manually\nannotate gold data for five high-resource languages; we then demonstrate, for\nthese languages, that the estimates of hallucination rates are similar between\nsilver (LLM-generated) and gold test sets, validating the use of silver data\nfor estimating hallucination rates for other languages. For the final rates\nestimation, we build a knowledge-intensive QA dataset for 30 languages with\nLLM-generated prompts and Wikipedia articles as references. We find that, while\nLLMs generate longer responses with more hallucinated tokens for\nhigher-resource languages, there is no correlation between length-normalized\nhallucination rates of languages and their digital representation. Further, we\nfind that smaller LLMs exhibit larger hallucination rates than larger models."
                },
                "authors": [
                    {
                        "name": "Saad Obaid ul Islam"
                    },
                    {
                        "name": "Anne Lauscher"
                    },
                    {
                        "name": "Goran GlavaÅ¡"
                    }
                ],
                "author_detail": {
                    "name": "Goran GlavaÅ¡"
                },
                "author": "Goran GlavaÅ¡",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12769v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12769v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12961v1",
                "updated": "2025-09-16T11:10:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    10,
                    30,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T11:10:30Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    10,
                    30,
                    1,
                    259,
                    0
                ],
                "title": "Do LLMs Understand Wine Descriptors Across Cultures? A Benchmark for\n  Cultural Adaptations of Wine Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Understand Wine Descriptors Across Cultures? A Benchmark for\n  Cultural Adaptations of Wine Reviews"
                },
                "summary": "Recent advances in large language models (LLMs) have opened the door to\nculture-aware language tasks. We introduce the novel problem of adapting wine\nreviews across Chinese and English, which goes beyond literal translation by\nincorporating regional taste preferences and culture-specific flavor\ndescriptors. In a case study on cross-cultural wine review adaptation, we\ncompile the first parallel corpus of professional reviews, containing 8k\nChinese and 16k Anglophone reviews. We benchmark both\nneural-machine-translation baselines and state-of-the-art LLMs with automatic\nmetrics and human evaluation. For the latter, we propose three culture-oriented\ncriteria -- Cultural Proximity, Cultural Neutrality, and Cultural Genuineness\n-- to assess how naturally a translated review resonates with target-culture\nreaders. Our analysis shows that current models struggle to capture cultural\nnuances, especially in translating wine descriptions across different cultures.\nThis highlights the challenges and limitations of translation models in\nhandling cultural content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have opened the door to\nculture-aware language tasks. We introduce the novel problem of adapting wine\nreviews across Chinese and English, which goes beyond literal translation by\nincorporating regional taste preferences and culture-specific flavor\ndescriptors. In a case study on cross-cultural wine review adaptation, we\ncompile the first parallel corpus of professional reviews, containing 8k\nChinese and 16k Anglophone reviews. We benchmark both\nneural-machine-translation baselines and state-of-the-art LLMs with automatic\nmetrics and human evaluation. For the latter, we propose three culture-oriented\ncriteria -- Cultural Proximity, Cultural Neutrality, and Cultural Genuineness\n-- to assess how naturally a translated review resonates with target-culture\nreaders. Our analysis shows that current models struggle to capture cultural\nnuances, especially in translating wine descriptions across different cultures.\nThis highlights the challenges and limitations of translation models in\nhandling cultural content."
                },
                "authors": [
                    {
                        "name": "Chenye Zou"
                    },
                    {
                        "name": "Xingyue Wen"
                    },
                    {
                        "name": "Tianyi Hu"
                    },
                    {
                        "name": "Qian Janice Wang"
                    },
                    {
                        "name": "Daniel Hershcovich"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Hershcovich"
                },
                "author": "Daniel Hershcovich",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12960v1",
                "updated": "2025-09-16T11:06:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    6,
                    58,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T11:06:58Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    6,
                    58,
                    1,
                    259,
                    0
                ],
                "title": "Investigating ReLoRA: Effects on the Learning Dynamics of Small Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating ReLoRA: Effects on the Learning Dynamics of Small Language\n  Models"
                },
                "summary": "Parameter-efficient methods such as LoRA have revolutionised the fine-tuning\nof LLMs. Still, their extension to pretraining via ReLoRA is less well\nunderstood, especially for small language models (SLMs), which offer lower\ncomputational and environmental costs. This work is the first systematic study\nof ReLoRA in SLMs (11M-66M parameters), evaluating both performance and\nlearning dynamics. Through ablation experiments, we find that ReLoRA generally\nperforms worse than standard training on loss, Paloma perplexity and BLiMP,\nwith the gap widening for the larger models. Further analysis of the learning\ndynamics of the models indicates that ReLoRA reinforces the rank deficiencies\nfound in smaller models. These results indicate that low-rank update strategies\nmay not transfer easily to SLM pretraining, highlighting the need for more\nresearch in the low-compute regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-efficient methods such as LoRA have revolutionised the fine-tuning\nof LLMs. Still, their extension to pretraining via ReLoRA is less well\nunderstood, especially for small language models (SLMs), which offer lower\ncomputational and environmental costs. This work is the first systematic study\nof ReLoRA in SLMs (11M-66M parameters), evaluating both performance and\nlearning dynamics. Through ablation experiments, we find that ReLoRA generally\nperforms worse than standard training on loss, Paloma perplexity and BLiMP,\nwith the gap widening for the larger models. Further analysis of the learning\ndynamics of the models indicates that ReLoRA reinforces the rank deficiencies\nfound in smaller models. These results indicate that low-rank update strategies\nmay not transfer easily to SLM pretraining, highlighting the need for more\nresearch in the low-compute regime."
                },
                "authors": [
                    {
                        "name": "Yuval Weiss"
                    },
                    {
                        "name": "David Demitri Africa"
                    },
                    {
                        "name": "Paula Buttery"
                    },
                    {
                        "name": "Richard Diehl Martinez"
                    }
                ],
                "author_detail": {
                    "name": "Richard Diehl Martinez"
                },
                "author": "Richard Diehl Martinez",
                "arxiv_comment": "12 Pages, 6 Tables, 8 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12953v1",
                "updated": "2025-09-16T10:57:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    57,
                    51,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T10:57:51Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    57,
                    51,
                    1,
                    259,
                    0
                ],
                "title": "Spatiotemporal graph neural process for reconstruction, extrapolation,\n  and classification of cardiac trajectories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatiotemporal graph neural process for reconstruction, extrapolation,\n  and classification of cardiac trajectories"
                },
                "summary": "We present a probabilistic framework for modeling structured spatiotemporal\ndynamics from sparse observations, focusing on cardiac motion. Our approach\nintegrates neural ordinary differential equations (NODEs), graph neural\nnetworks (GNNs), and neural processes into a unified model that captures\nuncertainty, temporal continuity, and anatomical structure. We represent\ndynamic systems as spatiotemporal multiplex graphs and model their latent\ntrajectories using a GNN-parameterized vector field. Given the sparse context\nobservations at node and edge levels, the model infers a distribution over\nlatent initial states and control variables, enabling both interpolation and\nextrapolation of trajectories. We validate the method on three synthetic\ndynamical systems (coupled pendulum, Lorenz attractor, and Kuramoto\noscillators) and two real-world cardiac imaging datasets - ACDC (N=150) and UK\nBiobank (N=526) - demonstrating accurate reconstruction, extrapolation, and\ndisease classification capabilities. The model accurately reconstructs\ntrajectories and extrapolates future cardiac cycles from a single observed\ncycle. It achieves state-of-the-art results on the ACDC classification task (up\nto 99% accuracy), and detects atrial fibrillation in UK Biobank subjects with\ncompetitive performance (up to 67% accuracy). This work introduces a flexible\napproach for analyzing cardiac motion and offers a foundation for graph-based\nlearning in structured biomedical spatiotemporal time-series data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a probabilistic framework for modeling structured spatiotemporal\ndynamics from sparse observations, focusing on cardiac motion. Our approach\nintegrates neural ordinary differential equations (NODEs), graph neural\nnetworks (GNNs), and neural processes into a unified model that captures\nuncertainty, temporal continuity, and anatomical structure. We represent\ndynamic systems as spatiotemporal multiplex graphs and model their latent\ntrajectories using a GNN-parameterized vector field. Given the sparse context\nobservations at node and edge levels, the model infers a distribution over\nlatent initial states and control variables, enabling both interpolation and\nextrapolation of trajectories. We validate the method on three synthetic\ndynamical systems (coupled pendulum, Lorenz attractor, and Kuramoto\noscillators) and two real-world cardiac imaging datasets - ACDC (N=150) and UK\nBiobank (N=526) - demonstrating accurate reconstruction, extrapolation, and\ndisease classification capabilities. The model accurately reconstructs\ntrajectories and extrapolates future cardiac cycles from a single observed\ncycle. It achieves state-of-the-art results on the ACDC classification task (up\nto 99% accuracy), and detects atrial fibrillation in UK Biobank subjects with\ncompetitive performance (up to 67% accuracy). This work introduces a flexible\napproach for analyzing cardiac motion and offers a foundation for graph-based\nlearning in structured biomedical spatiotemporal time-series data."
                },
                "authors": [
                    {
                        "name": "Jaume Banus"
                    },
                    {
                        "name": "Augustin C. Ogier"
                    },
                    {
                        "name": "Roger Hullin"
                    },
                    {
                        "name": "Philippe Meyer"
                    },
                    {
                        "name": "Ruud B. van Heeswijk"
                    },
                    {
                        "name": "Jonas Richiardi"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Richiardi"
                },
                "author": "Jonas Richiardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12951v1",
                "updated": "2025-09-16T10:55:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    55,
                    50,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T10:55:50Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    55,
                    50,
                    1,
                    259,
                    0
                ],
                "title": "Black-box Model Merging for Language-Model-as-a-Service with Massive\n  Model Repositories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box Model Merging for Language-Model-as-a-Service with Massive\n  Model Repositories"
                },
                "summary": "Model merging refers to the process of integrating multiple distinct models\ninto a unified model that preserves and combines the strengths and capabilities\nof the individual models. Most existing approaches rely on task vectors to\ncombine models, typically under the assumption that model parameters are\naccessible. However, for extremely large language models (LLMs) such as GPT-4,\nwhich are often provided solely as black-box services through API interfaces\n(Language-Model-as-a-Service), model weights are not available to end users.\nThis presents a significant challenge, which we refer to as black-box model\nmerging (BMM) with massive LLMs. To address this challenge, we propose a\nderivative-free optimization framework based on the evolutionary algorithm\n(Evo-Merging) that enables effective model merging using only inference-time\nAPI queries. Our method consists of two key components: (1) sparsity-based\ndenoising, designed to identify and filter out irrelevant or redundant\ninformation across models, and (2) sign-aware scaling, which dynamically\ncomputes optimal combination weights for the relevant models based on their\nperformance. We also provide a formal justification, along with a theoretical\nanalysis, for our asymmetric sparsification. Extensive experimental evaluations\ndemonstrate that our approach achieves state-of-the-art results on a range of\ntasks, significantly outperforming existing strong baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging refers to the process of integrating multiple distinct models\ninto a unified model that preserves and combines the strengths and capabilities\nof the individual models. Most existing approaches rely on task vectors to\ncombine models, typically under the assumption that model parameters are\naccessible. However, for extremely large language models (LLMs) such as GPT-4,\nwhich are often provided solely as black-box services through API interfaces\n(Language-Model-as-a-Service), model weights are not available to end users.\nThis presents a significant challenge, which we refer to as black-box model\nmerging (BMM) with massive LLMs. To address this challenge, we propose a\nderivative-free optimization framework based on the evolutionary algorithm\n(Evo-Merging) that enables effective model merging using only inference-time\nAPI queries. Our method consists of two key components: (1) sparsity-based\ndenoising, designed to identify and filter out irrelevant or redundant\ninformation across models, and (2) sign-aware scaling, which dynamically\ncomputes optimal combination weights for the relevant models based on their\nperformance. We also provide a formal justification, along with a theoretical\nanalysis, for our asymmetric sparsification. Extensive experimental evaluations\ndemonstrate that our approach achieves state-of-the-art results on a range of\ntasks, significantly outperforming existing strong baselines."
                },
                "authors": [
                    {
                        "name": "Shilian Chen"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Tianyu Huai"
                    },
                    {
                        "name": "Yujiang Lu"
                    },
                    {
                        "name": "Junsong Li"
                    },
                    {
                        "name": "Bihao Zhan"
                    },
                    {
                        "name": "Qianjun Pan"
                    },
                    {
                        "name": "Yutao Yang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Qin Chen"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12950v1",
                "updated": "2025-09-16T10:54:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    54,
                    0,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T10:54:00Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    54,
                    0,
                    1,
                    259,
                    0
                ],
                "title": "Protecting participants or population? Comparison of k-anonymous\n  Origin-Destination matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protecting participants or population? Comparison of k-anonymous\n  Origin-Destination matrices"
                },
                "summary": "Origin-Destination (OD) matrices are a core component of research on users'\nmobility and summarize how individuals move between geographical regions. These\nregions should be small enough to be representative of user mobility, without\nincurring substantial privacy risks. There are two added values of the\nNetMob2025 challenge dataset. Firstly, the data is extensive and contains a lot\nof socio-demographic information that can be used to create multiple OD\nmatrices, based on the segments of the population. Secondly, a participant is\nnot merely a record in the data, but a statistically weighted proxy for a\nsegment of the real population. This opens the door to a fundamental shift in\nthe anonymization paradigm. A population-based view of privacy is central to\nour contribution. By adjusting our anonymization framework to account for\nrepresentativeness, we are also protecting the inferred identity of the actual\npopulation, rather than survey participants alone. The challenge addressed in\nthis work is to produce and compare OD matrices that are k-anonymous for survey\nparticipants and for the whole population. We compare several traditional\nmethods of anonymization to k-anonymity by generalizing geographical areas.\nThese include generalization over a hierarchy (ATG and OIGH) and the classical\nMondrian. To this established toolkit, we add a novel method, i.e., ODkAnon, a\ngreedy algorithm aiming at balancing speed and quality. Unlike previous\napproaches, which primarily address the privacy aspects of the given datasets,\nwe aim to contribute to the generation of privacy-preserving OD matrices\nenriched with socio-demographic segmentation that achieves k-anonymity on the\nactual population.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Origin-Destination (OD) matrices are a core component of research on users'\nmobility and summarize how individuals move between geographical regions. These\nregions should be small enough to be representative of user mobility, without\nincurring substantial privacy risks. There are two added values of the\nNetMob2025 challenge dataset. Firstly, the data is extensive and contains a lot\nof socio-demographic information that can be used to create multiple OD\nmatrices, based on the segments of the population. Secondly, a participant is\nnot merely a record in the data, but a statistically weighted proxy for a\nsegment of the real population. This opens the door to a fundamental shift in\nthe anonymization paradigm. A population-based view of privacy is central to\nour contribution. By adjusting our anonymization framework to account for\nrepresentativeness, we are also protecting the inferred identity of the actual\npopulation, rather than survey participants alone. The challenge addressed in\nthis work is to produce and compare OD matrices that are k-anonymous for survey\nparticipants and for the whole population. We compare several traditional\nmethods of anonymization to k-anonymity by generalizing geographical areas.\nThese include generalization over a hierarchy (ATG and OIGH) and the classical\nMondrian. To this established toolkit, we add a novel method, i.e., ODkAnon, a\ngreedy algorithm aiming at balancing speed and quality. Unlike previous\napproaches, which primarily address the privacy aspects of the given datasets,\nwe aim to contribute to the generation of privacy-preserving OD matrices\nenriched with socio-demographic segmentation that achieves k-anonymity on the\nactual population."
                },
                "authors": [
                    {
                        "name": "Pietro Armenante"
                    },
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Nikhil Jha"
                    },
                    {
                        "name": "Luca Vassio"
                    }
                ],
                "author_detail": {
                    "name": "Luca Vassio"
                },
                "author": "Luca Vassio",
                "arxiv_comment": "Accepted at NetMob 2025 Data Challenge (full report)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17356v2",
                "updated": "2025-09-16T10:52:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    52,
                    32,
                    1,
                    259,
                    0
                ],
                "published": "2025-04-24T08:16:36Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    16,
                    36,
                    3,
                    114,
                    0
                ],
                "title": "Comprehend, Divide, and Conquer: Feature Subspace Exploration via\n  Multi-Agent Hierarchical Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehend, Divide, and Conquer: Feature Subspace Exploration via\n  Multi-Agent Hierarchical Reinforcement Learning"
                },
                "summary": "Feature selection aims to preprocess the target dataset, find an optimal and\nmost streamlined feature subset, and enhance the downstream machine learning\ntask. Among filter, wrapper, and embedded-based approaches, the reinforcement\nlearning (RL)-based subspace exploration strategy provides a novel objective\noptimization-directed perspective and promising performance. Nevertheless, even\nwith improved performance, current reinforcement learning approaches face\nchallenges similar to conventional methods when dealing with complex datasets.\nThese challenges stem from the inefficient paradigm of using one agent per\nfeature and the inherent complexities present in the datasets. This observation\nmotivates us to investigate and address the above issue and propose a novel\napproach, namely HRLFS. Our methodology initially employs a Large Language\nModel (LLM)-based hybrid state extractor to capture each feature's mathematical\nand semantic characteristics. Based on this information, features are\nclustered, facilitating the construction of hierarchical agents for each\ncluster and sub-cluster. Extensive experiments demonstrate the efficiency,\nscalability, and robustness of our approach. Compared to contemporary or the\none-feature-one-agent RL-based approaches, HRLFS improves the downstream ML\nperformance with iterative feature subspace exploration while accelerating\ntotal run time by reducing the number of agents involved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature selection aims to preprocess the target dataset, find an optimal and\nmost streamlined feature subset, and enhance the downstream machine learning\ntask. Among filter, wrapper, and embedded-based approaches, the reinforcement\nlearning (RL)-based subspace exploration strategy provides a novel objective\noptimization-directed perspective and promising performance. Nevertheless, even\nwith improved performance, current reinforcement learning approaches face\nchallenges similar to conventional methods when dealing with complex datasets.\nThese challenges stem from the inefficient paradigm of using one agent per\nfeature and the inherent complexities present in the datasets. This observation\nmotivates us to investigate and address the above issue and propose a novel\napproach, namely HRLFS. Our methodology initially employs a Large Language\nModel (LLM)-based hybrid state extractor to capture each feature's mathematical\nand semantic characteristics. Based on this information, features are\nclustered, facilitating the construction of hierarchical agents for each\ncluster and sub-cluster. Extensive experiments demonstrate the efficiency,\nscalability, and robustness of our approach. Compared to contemporary or the\none-feature-one-agent RL-based approaches, HRLFS improves the downstream ML\nperformance with iterative feature subspace exploration while accelerating\ntotal run time by reducing the number of agents involved."
                },
                "authors": [
                    {
                        "name": "Weiliang Zhang"
                    },
                    {
                        "name": "Xiaohan Huang"
                    },
                    {
                        "name": "Yi Du"
                    },
                    {
                        "name": "Ziyue Qiao"
                    },
                    {
                        "name": "Qingqing Long"
                    },
                    {
                        "name": "Zhen Meng"
                    },
                    {
                        "name": "Yuanchun Zhou"
                    },
                    {
                        "name": "Meng Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Meng Xiao"
                },
                "author": "Meng Xiao",
                "arxiv_comment": "20 pages, keywords: Automated Feature Engineering, Tabular Dataset,\n  Multi-Agent Reinforcement Learning, Feature Selection",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12948v1",
                "updated": "2025-09-16T10:52:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    52,
                    3,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T10:52:03Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    52,
                    3,
                    1,
                    259,
                    0
                ],
                "title": "A Learnable Fully Interacted Two-Tower Model for Pre-Ranking System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Learnable Fully Interacted Two-Tower Model for Pre-Ranking System"
                },
                "summary": "Pre-ranking plays a crucial role in large-scale recommender systems by\nsignificantly improving the efficiency and scalability within the constraints\nof providing high-quality candidate sets in real time. The two-tower model is\nwidely used in pre-ranking systems due to a good balance between efficiency and\neffectiveness with decoupled architecture, which independently processes user\nand item inputs before calculating their interaction (e.g. dot product or\nsimilarity measure). However, this independence also leads to the lack of\ninformation interaction between the two towers, resulting in less\neffectiveness. In this paper, a novel architecture named learnable Fully\nInteracted Two-tower Model (FIT) is proposed, which enables rich information\ninteractions while ensuring inference efficiency. FIT mainly consists of two\nparts: Meta Query Module (MQM) and Lightweight Similarity Scorer (LSS).\nSpecifically, MQM introduces a learnable item meta matrix to achieve expressive\nearly interaction between user and item features. Moreover, LSS is designed to\nfurther obtain effective late interaction between the user and item towers.\nFinally, experimental results on several public datasets show that our proposed\nFIT significantly outperforms the state-of-the-art baseline pre-ranking models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-ranking plays a crucial role in large-scale recommender systems by\nsignificantly improving the efficiency and scalability within the constraints\nof providing high-quality candidate sets in real time. The two-tower model is\nwidely used in pre-ranking systems due to a good balance between efficiency and\neffectiveness with decoupled architecture, which independently processes user\nand item inputs before calculating their interaction (e.g. dot product or\nsimilarity measure). However, this independence also leads to the lack of\ninformation interaction between the two towers, resulting in less\neffectiveness. In this paper, a novel architecture named learnable Fully\nInteracted Two-tower Model (FIT) is proposed, which enables rich information\ninteractions while ensuring inference efficiency. FIT mainly consists of two\nparts: Meta Query Module (MQM) and Lightweight Similarity Scorer (LSS).\nSpecifically, MQM introduces a learnable item meta matrix to achieve expressive\nearly interaction between user and item features. Moreover, LSS is designed to\nfurther obtain effective late interaction between the user and item towers.\nFinally, experimental results on several public datasets show that our proposed\nFIT significantly outperforms the state-of-the-art baseline pre-ranking models."
                },
                "authors": [
                    {
                        "name": "Chao Xiong"
                    },
                    {
                        "name": "Xianwen Yu"
                    },
                    {
                        "name": "Wei Xu"
                    },
                    {
                        "name": "Lei Cheng"
                    },
                    {
                        "name": "Chuan Yuan"
                    },
                    {
                        "name": "Linjian Mo"
                    }
                ],
                "author_detail": {
                    "name": "Linjian Mo"
                },
                "author": "Linjian Mo",
                "arxiv_doi": "10.1145/3726302.3729881",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3729881",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.12948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "SIGIR2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12945v1",
                "updated": "2025-09-16T10:50:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    50,
                    29,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T10:50:29Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    50,
                    29,
                    1,
                    259,
                    0
                ],
                "title": "FusionMAE: large-scale pretrained model to optimize and simplify\n  diagnostic and control of fusion plasma",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FusionMAE: large-scale pretrained model to optimize and simplify\n  diagnostic and control of fusion plasma"
                },
                "summary": "In magnetically confined fusion device, the complex, multiscale, and\nnonlinear dynamics of plasmas necessitate the integration of extensive\ndiagnostic systems to effectively monitor and control plasma behaviour. The\ncomplexity and uncertainty arising from these extensive systems and their\ntangled interrelations has long posed a significant obstacle to the\nacceleration of fusion energy development. In this work, a large-scale model,\nfusion masked auto-encoder (FusionMAE) is pre-trained to compress the\ninformation from 88 diagnostic signals into a concrete embedding, to provide a\nunified interface between diagnostic systems and control actuators. Two\nmechanisms are proposed to ensure a meaningful embedding: compression-reduction\nand missing-signal reconstruction. Upon completion of pre-training, the model\nacquires the capability for 'virtual backup diagnosis', enabling the inference\nof missing diagnostic data with 96.7% reliability. Furthermore, the model\ndemonstrates three emergent capabilities: automatic data analysis, universal\ncontrol-diagnosis interface, and enhancement of control performance on multiple\ntasks. This work pioneers large-scale AI model integration in fusion energy,\ndemonstrating how pre-trained embeddings can simplify the system interface,\nreducing necessary diagnostic systems and optimize operation performance for\nfuture fusion reactors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In magnetically confined fusion device, the complex, multiscale, and\nnonlinear dynamics of plasmas necessitate the integration of extensive\ndiagnostic systems to effectively monitor and control plasma behaviour. The\ncomplexity and uncertainty arising from these extensive systems and their\ntangled interrelations has long posed a significant obstacle to the\nacceleration of fusion energy development. In this work, a large-scale model,\nfusion masked auto-encoder (FusionMAE) is pre-trained to compress the\ninformation from 88 diagnostic signals into a concrete embedding, to provide a\nunified interface between diagnostic systems and control actuators. Two\nmechanisms are proposed to ensure a meaningful embedding: compression-reduction\nand missing-signal reconstruction. Upon completion of pre-training, the model\nacquires the capability for 'virtual backup diagnosis', enabling the inference\nof missing diagnostic data with 96.7% reliability. Furthermore, the model\ndemonstrates three emergent capabilities: automatic data analysis, universal\ncontrol-diagnosis interface, and enhancement of control performance on multiple\ntasks. This work pioneers large-scale AI model integration in fusion energy,\ndemonstrating how pre-trained embeddings can simplify the system interface,\nreducing necessary diagnostic systems and optimize operation performance for\nfuture fusion reactors."
                },
                "authors": [
                    {
                        "name": "Zongyu Yang"
                    },
                    {
                        "name": "Zhenghao Yang"
                    },
                    {
                        "name": "Wenjing Tian"
                    },
                    {
                        "name": "Jiyuan Li"
                    },
                    {
                        "name": "Xiang Sun"
                    },
                    {
                        "name": "Guohui Zheng"
                    },
                    {
                        "name": "Songfen Liu"
                    },
                    {
                        "name": "Niannian Wu"
                    },
                    {
                        "name": "Rongpeng Li"
                    },
                    {
                        "name": "Zhaohe Xu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Zhongbing Shi"
                    },
                    {
                        "name": "Zhe Gao"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Xiaoquan Ji"
                    },
                    {
                        "name": "Min Xu"
                    },
                    {
                        "name": "Wulyu Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Wulyu Zhong"
                },
                "author": "Wulyu Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08171v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08171v2",
                "updated": "2025-09-16T10:35:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    35,
                    33,
                    1,
                    259,
                    0
                ],
                "published": "2025-06-09T19:33:30Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    33,
                    30,
                    0,
                    160,
                    0
                ],
                "title": "Worst-Case Symbolic Constraints Analysis and Generalisation with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Worst-Case Symbolic Constraints Analysis and Generalisation with Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated strong performance on coding\ntasks such as generation, completion and repair, but their ability to handle\ncomplex symbolic reasoning over code still remains underexplored. We introduce\nthe task of worst-case symbolic constraints analysis, which requires inferring\nthe symbolic constraints that characterise worst-case program executions; these\nconstraints can be solved to obtain inputs that expose performance bottlenecks\nor denial-of-service vulnerabilities in software systems. We show that even\nstate-of-the-art LLMs (e.g., GPT-5) struggle when applied directly on this\ntask. To address this challenge, we propose WARP, an innovative neurosymbolic\napproach that computes worst-case constraints on smaller concrete input sizes\nusing existing program analysis tools, and then leverages LLMs to generalise\nthese constraints to larger input sizes. Concretely, WARP comprises: (1) an\nincremental strategy for LLM-based worst-case reasoning, (2) a solver-aligned\nneurosymbolic framework that integrates reinforcement learning with SMT\n(Satisfiability Modulo Theories) solving, and (3) a curated dataset of symbolic\nconstraints. Experimental results show that WARP consistently improves\nperformance on worst-case constraint reasoning. Leveraging the curated\nconstraint dataset, we use reinforcement learning to fine-tune a model,\nWARP-1.0-3B, which significantly outperforms size-matched and even larger\nbaselines. These results demonstrate that incremental constraint reasoning\nenhances LLMs' ability to handle symbolic reasoning and highlight the potential\nfor deeper integration between neural learning and formal methods in rigorous\nprogram analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong performance on coding\ntasks such as generation, completion and repair, but their ability to handle\ncomplex symbolic reasoning over code still remains underexplored. We introduce\nthe task of worst-case symbolic constraints analysis, which requires inferring\nthe symbolic constraints that characterise worst-case program executions; these\nconstraints can be solved to obtain inputs that expose performance bottlenecks\nor denial-of-service vulnerabilities in software systems. We show that even\nstate-of-the-art LLMs (e.g., GPT-5) struggle when applied directly on this\ntask. To address this challenge, we propose WARP, an innovative neurosymbolic\napproach that computes worst-case constraints on smaller concrete input sizes\nusing existing program analysis tools, and then leverages LLMs to generalise\nthese constraints to larger input sizes. Concretely, WARP comprises: (1) an\nincremental strategy for LLM-based worst-case reasoning, (2) a solver-aligned\nneurosymbolic framework that integrates reinforcement learning with SMT\n(Satisfiability Modulo Theories) solving, and (3) a curated dataset of symbolic\nconstraints. Experimental results show that WARP consistently improves\nperformance on worst-case constraint reasoning. Leveraging the curated\nconstraint dataset, we use reinforcement learning to fine-tune a model,\nWARP-1.0-3B, which significantly outperforms size-matched and even larger\nbaselines. These results demonstrate that incremental constraint reasoning\nenhances LLMs' ability to handle symbolic reasoning and highlight the potential\nfor deeper integration between neural learning and formal methods in rigorous\nprogram analysis."
                },
                "authors": [
                    {
                        "name": "Daniel Koh"
                    },
                    {
                        "name": "Yannic Noller"
                    },
                    {
                        "name": "Corina S. Pasareanu"
                    },
                    {
                        "name": "Adrians Skapars"
                    },
                    {
                        "name": "Youcheng Sun"
                    }
                ],
                "author_detail": {
                    "name": "Youcheng Sun"
                },
                "author": "Youcheng Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08171v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08171v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12937v1",
                "updated": "2025-09-16T10:34:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    34,
                    26,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T10:34:26Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    34,
                    26,
                    1,
                    259,
                    0
                ],
                "title": "Jailbreaking Large Language Models Through Content Concretization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking Large Language Models Through Content Concretization"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed for task automation\nand content generation, yet their safety mechanisms remain vulnerable to\ncircumvention through different jailbreaking techniques. In this paper, we\nintroduce \\textit{Content Concretization} (CC), a novel jailbreaking technique\nthat iteratively transforms abstract malicious requests into concrete,\nexecutable implementations. CC is a two-stage process: first, generating\ninitial LLM responses using lower-tier, less constrained safety filters models,\nthen refining them through higher-tier models that process both the preliminary\noutput and original prompt. We evaluate our technique using 350\ncybersecurity-specific prompts, demonstrating substantial improvements in\njailbreak Success Rates (SRs), increasing from 7\\% (no refinements) to 62\\%\nafter three refinement iterations, while maintaining a cost of 7.5\\textcent~per\nprompt. Comparative A/B testing across nine different LLM evaluators confirms\nthat outputs from additional refinement steps are consistently rated as more\nmalicious and technically superior. Moreover, manual code analysis reveals that\ngenerated outputs execute with minimal modification, although optimal\ndeployment typically requires target-specific fine-tuning. With eventual\nimproved harmful code generation, these results highlight critical\nvulnerabilities in current LLM safety frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed for task automation\nand content generation, yet their safety mechanisms remain vulnerable to\ncircumvention through different jailbreaking techniques. In this paper, we\nintroduce \\textit{Content Concretization} (CC), a novel jailbreaking technique\nthat iteratively transforms abstract malicious requests into concrete,\nexecutable implementations. CC is a two-stage process: first, generating\ninitial LLM responses using lower-tier, less constrained safety filters models,\nthen refining them through higher-tier models that process both the preliminary\noutput and original prompt. We evaluate our technique using 350\ncybersecurity-specific prompts, demonstrating substantial improvements in\njailbreak Success Rates (SRs), increasing from 7\\% (no refinements) to 62\\%\nafter three refinement iterations, while maintaining a cost of 7.5\\textcent~per\nprompt. Comparative A/B testing across nine different LLM evaluators confirms\nthat outputs from additional refinement steps are consistently rated as more\nmalicious and technically superior. Moreover, manual code analysis reveals that\ngenerated outputs execute with minimal modification, although optimal\ndeployment typically requires target-specific fine-tuning. With eventual\nimproved harmful code generation, these results highlight critical\nvulnerabilities in current LLM safety frameworks."
                },
                "authors": [
                    {
                        "name": "Johan WahrÃ©us"
                    },
                    {
                        "name": "Ahmed Hussain"
                    },
                    {
                        "name": "Panos Papadimitratos"
                    }
                ],
                "author_detail": {
                    "name": "Panos Papadimitratos"
                },
                "author": "Panos Papadimitratos",
                "arxiv_comment": "Accepted for presentation in the Conference on Game Theory and AI for\n  Security (GameSec) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v3",
                "updated": "2025-09-16T10:33:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    33,
                    29,
                    1,
                    259,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_doi": "10.1145/3744916.3764523",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3744916.3764523",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08523v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ICSE '26 (The 48th IEEE/ACM International Conference on\n  Software Engineering)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12936v1",
                "updated": "2025-09-16T10:32:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    32,
                    59,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T10:32:59Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    32,
                    59,
                    1,
                    259,
                    0
                ],
                "title": "Rethinking the Evaluation of Alignment Methods: Insights into Diversity,\n  Generalisation, and Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking the Evaluation of Alignment Methods: Insights into Diversity,\n  Generalisation, and Safety"
                },
                "summary": "Large language models (LLMs) require careful alignment to balance competing\nobjectives - factuality, safety, conciseness, proactivity, and diversity.\nExisting studies focus on individual techniques or specific dimensions, lacking\na holistic assessment of the inherent trade-offs. We propose a unified\nevaluation framework that compares LLM alignment methods (PPO, DPO, ORPO, KTO)\nacross these five axes, using both in-distribution and out-of-distribution\ndatasets. Leveraging a specialized LLM-as-Judge prompt, validated through human\nstudies, we reveal that DPO and KTO excel in factual accuracy, PPO and DPO lead\nin safety, and PPO best balances conciseness with proactivity. Our findings\nprovide insights into trade-offs of common alignment methods, guiding the\ndevelopment of more balanced and reliable LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) require careful alignment to balance competing\nobjectives - factuality, safety, conciseness, proactivity, and diversity.\nExisting studies focus on individual techniques or specific dimensions, lacking\na holistic assessment of the inherent trade-offs. We propose a unified\nevaluation framework that compares LLM alignment methods (PPO, DPO, ORPO, KTO)\nacross these five axes, using both in-distribution and out-of-distribution\ndatasets. Leveraging a specialized LLM-as-Judge prompt, validated through human\nstudies, we reveal that DPO and KTO excel in factual accuracy, PPO and DPO lead\nin safety, and PPO best balances conciseness with proactivity. Our findings\nprovide insights into trade-offs of common alignment methods, guiding the\ndevelopment of more balanced and reliable LLMs."
                },
                "authors": [
                    {
                        "name": "Denis Janiak"
                    },
                    {
                        "name": "Julia Moska"
                    },
                    {
                        "name": "Dawid Motyka"
                    },
                    {
                        "name": "Karolina Seweryn"
                    },
                    {
                        "name": "PaweÅ Walkowiak"
                    },
                    {
                        "name": "Bartosz Å»uk"
                    },
                    {
                        "name": "Arkadiusz Janz"
                    }
                ],
                "author_detail": {
                    "name": "Arkadiusz Janz"
                },
                "author": "Arkadiusz Janz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09767v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09767v2",
                "updated": "2025-09-16T10:28:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    28,
                    11,
                    1,
                    259,
                    0
                ],
                "published": "2025-08-13T12:52:38Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    52,
                    38,
                    2,
                    225,
                    0
                ],
                "title": "UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in\n  Multilingual Text-to-Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in\n  Multilingual Text-to-Speech"
                },
                "summary": "We propose UtterTune, a lightweight adaptation method that fine-tunes a\nmultilingual text-to-speech (TTS) system based on a large language model (LLM)\narchitecture, designed to enhance the controllability of pronunciation in a\ntarget language while preserving performance in others. While LLM architectures\nhave enabled TTS models to achieve remarkable naturalness, accurately modeling\ngrapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially\nwhen the model omits an explicit G2P module and directly processes minimally\nencoded text (e.g., byte-pair encoding). UtterTune leverages low-rank\nadaptation to enable the control of segmental pronunciation and pitch accent at\nthe phoneme level for Japanese speech, the target language in this paper, while\nmaintaining naturalness and speaker similarity in a zero-shot setting.\nObjective and subjective evaluations confirm its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose UtterTune, a lightweight adaptation method that fine-tunes a\nmultilingual text-to-speech (TTS) system based on a large language model (LLM)\narchitecture, designed to enhance the controllability of pronunciation in a\ntarget language while preserving performance in others. While LLM architectures\nhave enabled TTS models to achieve remarkable naturalness, accurately modeling\ngrapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially\nwhen the model omits an explicit G2P module and directly processes minimally\nencoded text (e.g., byte-pair encoding). UtterTune leverages low-rank\nadaptation to enable the control of segmental pronunciation and pitch accent at\nthe phoneme level for Japanese speech, the target language in this paper, while\nmaintaining naturalness and speaker similarity in a zero-shot setting.\nObjective and subjective evaluations confirm its effectiveness."
                },
                "authors": [
                    {
                        "name": "Shuhei Kato"
                    }
                ],
                "author_detail": {
                    "name": "Shuhei Kato"
                },
                "author": "Shuhei Kato",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09767v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09767v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12927v1",
                "updated": "2025-09-16T10:26:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    26,
                    12,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T10:26:12Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    26,
                    12,
                    1,
                    259,
                    0
                ],
                "title": "HLSMAC: A New StarCraft Multi-Agent Challenge for High-Level Strategic\n  Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HLSMAC: A New StarCraft Multi-Agent Challenge for High-Level Strategic\n  Decision-Making"
                },
                "summary": "Benchmarks are crucial for assessing multi-agent reinforcement learning\n(MARL) algorithms. While StarCraft II-related environments have driven\nsignificant advances in MARL, existing benchmarks like SMAC focus primarily on\nmicromanagement, limiting comprehensive evaluation of high-level strategic\nintelligence. To address this, we introduce HLSMAC, a new cooperative MARL\nbenchmark with 12 carefully designed StarCraft II scenarios based on classical\nstratagems from the Thirty-Six Stratagems. Each scenario corresponds to a\nspecific stratagem and is designed to challenge agents with diverse strategic\nelements, including tactical maneuvering, timing coordination, and deception,\nthereby opening up avenues for evaluating high-level strategic decision-making\ncapabilities. We also propose novel metrics across multiple dimensions beyond\nconventional win rate, such as ability utilization and advancement efficiency,\nto assess agents' overall performance within the HLSMAC environment. We\nintegrate state-of-the-art MARL algorithms and LLM-based agents with our\nbenchmark and conduct comprehensive experiments. The results demonstrate that\nHLSMAC serves as a robust testbed for advancing multi-agent strategic\ndecision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarks are crucial for assessing multi-agent reinforcement learning\n(MARL) algorithms. While StarCraft II-related environments have driven\nsignificant advances in MARL, existing benchmarks like SMAC focus primarily on\nmicromanagement, limiting comprehensive evaluation of high-level strategic\nintelligence. To address this, we introduce HLSMAC, a new cooperative MARL\nbenchmark with 12 carefully designed StarCraft II scenarios based on classical\nstratagems from the Thirty-Six Stratagems. Each scenario corresponds to a\nspecific stratagem and is designed to challenge agents with diverse strategic\nelements, including tactical maneuvering, timing coordination, and deception,\nthereby opening up avenues for evaluating high-level strategic decision-making\ncapabilities. We also propose novel metrics across multiple dimensions beyond\nconventional win rate, such as ability utilization and advancement efficiency,\nto assess agents' overall performance within the HLSMAC environment. We\nintegrate state-of-the-art MARL algorithms and LLM-based agents with our\nbenchmark and conduct comprehensive experiments. The results demonstrate that\nHLSMAC serves as a robust testbed for advancing multi-agent strategic\ndecision-making."
                },
                "authors": [
                    {
                        "name": "Xingxing Hong"
                    },
                    {
                        "name": "Yungong Wang"
                    },
                    {
                        "name": "Dexin Jin"
                    },
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Ximing Huang"
                    },
                    {
                        "name": "Zijian Wu"
                    },
                    {
                        "name": "Wenxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenxin Li"
                },
                "author": "Wenxin Li",
                "arxiv_comment": "30 pages, 13 figures with appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19075v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19075v2",
                "updated": "2025-09-16T10:22:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    22,
                    22,
                    1,
                    259,
                    0
                ],
                "published": "2025-04-27T02:10:12Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    2,
                    10,
                    12,
                    6,
                    117,
                    0
                ],
                "title": "HoloDx: Knowledge- and Data-Driven Multimodal Diagnosis of Alzheimer's\n  Disease",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HoloDx: Knowledge- and Data-Driven Multimodal Diagnosis of Alzheimer's\n  Disease"
                },
                "summary": "Accurate diagnosis of Alzheimer's disease (AD) requires effectively\nintegrating multimodal data and clinical expertise. However, existing methods\noften struggle to fully utilize multimodal information and lack structured\nmechanisms to incorporate dynamic domain knowledge. To address these\nlimitations, we propose HoloDx, a knowledge- and data-driven framework that\nenhances AD diagnosis by aligning domain knowledge with multimodal clinical\ndata. HoloDx incorporates a knowledge injection module with a knowledge-aware\ngated cross-attention, allowing the model to dynamically integrate\ndomain-specific insights from both large language models (LLMs) and clinical\nexpertise. Also, a memory injection module with a designed prototypical memory\nattention enables the model to retain and retrieve subject-specific\ninformation, ensuring consistency in decision-making. By jointly leveraging\nthese mechanisms, HoloDx enhances interpretability, improves robustness, and\neffectively aligns prior knowledge with current subject data. Evaluations on\nfive AD datasets demonstrate that HoloDx outperforms state-of-the-art methods,\nachieving superior diagnostic accuracy and strong generalization across diverse\ncohorts. The source code will be released upon publication acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate diagnosis of Alzheimer's disease (AD) requires effectively\nintegrating multimodal data and clinical expertise. However, existing methods\noften struggle to fully utilize multimodal information and lack structured\nmechanisms to incorporate dynamic domain knowledge. To address these\nlimitations, we propose HoloDx, a knowledge- and data-driven framework that\nenhances AD diagnosis by aligning domain knowledge with multimodal clinical\ndata. HoloDx incorporates a knowledge injection module with a knowledge-aware\ngated cross-attention, allowing the model to dynamically integrate\ndomain-specific insights from both large language models (LLMs) and clinical\nexpertise. Also, a memory injection module with a designed prototypical memory\nattention enables the model to retain and retrieve subject-specific\ninformation, ensuring consistency in decision-making. By jointly leveraging\nthese mechanisms, HoloDx enhances interpretability, improves robustness, and\neffectively aligns prior knowledge with current subject data. Evaluations on\nfive AD datasets demonstrate that HoloDx outperforms state-of-the-art methods,\nachieving superior diagnostic accuracy and strong generalization across diverse\ncohorts. The source code will be released upon publication acceptance."
                },
                "authors": [
                    {
                        "name": "Qiuhui Chen"
                    },
                    {
                        "name": "Jintao Wang"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Yi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Yi Hong"
                },
                "author": "Yi Hong",
                "arxiv_comment": "Accepted by IEEE Transactions on Medical Imaging (TMI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19075v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19075v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12922v1",
                "updated": "2025-09-16T10:19:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    19,
                    37,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T10:19:37Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    19,
                    37,
                    1,
                    259,
                    0
                ],
                "title": "Evolution of the infrared luminosity function and its corresponding\n  dust-obscured star formation rate density out to z~6",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolution of the infrared luminosity function and its corresponding\n  dust-obscured star formation rate density out to z~6"
                },
                "summary": "We present a new determination of the evolving far-infrared galaxy luminosity\nfunction (FIR LF) and the resulting inferred evolution of dust-obscured\nstar-formation rate density (SFRD) out to redshift z~6. To establish the\nevolving co-moving number density of FIR-bright objects, we make use of the\nhigh-resolution ALMA follow-up study (AS2UDS), of the JCMT SCUBA-2 Cosmology\nLegacy Survey (S2CLS) sub-mm imaging in the UKIDSS UDS survey field. In order\nto estimate the contributions of faint/low-mass sources we implement a method\nin which the faint-end of the IR LF is inferred by stacking (in stellar mass\nand redshift bins) the optical/near-infrared samples of star-forming galaxies\ninto the appropriate FIR Herschel and sub-mm JCMT maps. Using this information\nwe determine the faint-end slope of the FIR LF in two intermediate redshift\nbins (where it can be robustly established) and then adopt this result at all\nother redshifts. The evolution of the characteristic luminosity of the galaxy\nFIR LF, L*, is found to be increase monotonically with redshift, evolving as\nz^1.38+-0.07, while the characteristic number density is well fitted by double\npower-law function, constant at z<2.24 and declining as z^-4.95+-0.73 at higher\nredshifts. The evolution of the corresponding dust-obscured star-formation rate\ndensity was then calculated and is here compared with the results from a number\nof recent studies in the literature. Our analysis confirms that dust-obscured\nstar-formation activity dominates SFRD at cosmic noon, but then becomes\nprogressively less important with increasing redshift: while dusty star-forming\ngalaxies are still found out to the highest redshifts explored here, UV-visible\nstar formation dominates at z>4, and dust-obscured activity contributes <25% of\nSFRD by z~6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a new determination of the evolving far-infrared galaxy luminosity\nfunction (FIR LF) and the resulting inferred evolution of dust-obscured\nstar-formation rate density (SFRD) out to redshift z~6. To establish the\nevolving co-moving number density of FIR-bright objects, we make use of the\nhigh-resolution ALMA follow-up study (AS2UDS), of the JCMT SCUBA-2 Cosmology\nLegacy Survey (S2CLS) sub-mm imaging in the UKIDSS UDS survey field. In order\nto estimate the contributions of faint/low-mass sources we implement a method\nin which the faint-end of the IR LF is inferred by stacking (in stellar mass\nand redshift bins) the optical/near-infrared samples of star-forming galaxies\ninto the appropriate FIR Herschel and sub-mm JCMT maps. Using this information\nwe determine the faint-end slope of the FIR LF in two intermediate redshift\nbins (where it can be robustly established) and then adopt this result at all\nother redshifts. The evolution of the characteristic luminosity of the galaxy\nFIR LF, L*, is found to be increase monotonically with redshift, evolving as\nz^1.38+-0.07, while the characteristic number density is well fitted by double\npower-law function, constant at z<2.24 and declining as z^-4.95+-0.73 at higher\nredshifts. The evolution of the corresponding dust-obscured star-formation rate\ndensity was then calculated and is here compared with the results from a number\nof recent studies in the literature. Our analysis confirms that dust-obscured\nstar-formation activity dominates SFRD at cosmic noon, but then becomes\nprogressively less important with increasing redshift: while dusty star-forming\ngalaxies are still found out to the highest redshifts explored here, UV-visible\nstar formation dominates at z>4, and dust-obscured activity contributes <25% of\nSFRD by z~6."
                },
                "authors": [
                    {
                        "name": "M. P. Koprowski"
                    },
                    {
                        "name": "J. V. Wijesekera"
                    },
                    {
                        "name": "J. S. Dunlop"
                    },
                    {
                        "name": "K. Lisiecki"
                    },
                    {
                        "name": "D. J. McLeod"
                    },
                    {
                        "name": "R. J. McLure"
                    },
                    {
                        "name": "M. J. MichaÅowski"
                    },
                    {
                        "name": "M. Solar"
                    }
                ],
                "author_detail": {
                    "name": "M. Solar"
                },
                "author": "M. Solar",
                "arxiv_comment": "10 pages, 7 figures, 5 tables, submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12918v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12918v2",
                "updated": "2025-09-17T12:59:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    12,
                    59,
                    25,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-16T10:11:59Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    11,
                    59,
                    1,
                    259,
                    0
                ],
                "title": "A Novel Compression Framework for YOLOv8: Achieving Real-Time Aerial\n  Object Detection on Edge Devices via Structured Pruning and Channel-Wise\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Compression Framework for YOLOv8: Achieving Real-Time Aerial\n  Object Detection on Edge Devices via Structured Pruning and Channel-Wise\n  Distillation"
                },
                "summary": "Efficient deployment of deep learning models for aerial object detection on\nresource-constrained devices requires significant compression without\ncom-promising performance. In this study, we propose a novel three-stage\ncompression pipeline for the YOLOv8 object detection model, integrating\nsparsity-aware training, structured channel pruning, and Channel-Wise Knowledge\nDistillation (CWD). First, sparsity-aware training introduces dynamic sparsity\nduring model optimization, effectively balancing parameter reduction and\ndetection accuracy. Second, we apply structured channel pruning by leveraging\nbatch normalization scaling factors to eliminate redundant channels,\nsignificantly reducing model size and computational complexity. Finally, to\nmitigate the accuracy drop caused by pruning, we employ CWD to transfer\nknowledge from the original model, using an adjustable temperature and loss\nweighting scheme tailored for small and medium object detection. Extensive\nexperiments on the VisDrone dataset demonstrate the effectiveness of our\napproach across multiple YOLOv8 variants. For YOLOv8m, our method reduces model\nparameters from 25.85M to 6.85M (a 73.51% reduction), FLOPs from 49.6G to\n13.3G, and MACs from 101G to 34.5G, while reducing AP50 by only 2.7%. The\nresulting compressed model achieves 47.9 AP50 and boosts inference speed from\n26 FPS (YOLOv8m baseline) to 45 FPS, enabling real-time deployment on edge\ndevices. We further apply TensorRT as a lightweight optimization step. While\nthis introduces a minor drop in AP50 (from 47.9 to 47.6), it significantly\nimproves inference speed from 45 to 68 FPS, demonstrating the practicality of\nour approach for high-throughput, re-source-constrained scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient deployment of deep learning models for aerial object detection on\nresource-constrained devices requires significant compression without\ncom-promising performance. In this study, we propose a novel three-stage\ncompression pipeline for the YOLOv8 object detection model, integrating\nsparsity-aware training, structured channel pruning, and Channel-Wise Knowledge\nDistillation (CWD). First, sparsity-aware training introduces dynamic sparsity\nduring model optimization, effectively balancing parameter reduction and\ndetection accuracy. Second, we apply structured channel pruning by leveraging\nbatch normalization scaling factors to eliminate redundant channels,\nsignificantly reducing model size and computational complexity. Finally, to\nmitigate the accuracy drop caused by pruning, we employ CWD to transfer\nknowledge from the original model, using an adjustable temperature and loss\nweighting scheme tailored for small and medium object detection. Extensive\nexperiments on the VisDrone dataset demonstrate the effectiveness of our\napproach across multiple YOLOv8 variants. For YOLOv8m, our method reduces model\nparameters from 25.85M to 6.85M (a 73.51% reduction), FLOPs from 49.6G to\n13.3G, and MACs from 101G to 34.5G, while reducing AP50 by only 2.7%. The\nresulting compressed model achieves 47.9 AP50 and boosts inference speed from\n26 FPS (YOLOv8m baseline) to 45 FPS, enabling real-time deployment on edge\ndevices. We further apply TensorRT as a lightweight optimization step. While\nthis introduces a minor drop in AP50 (from 47.9 to 47.6), it significantly\nimproves inference speed from 45 to 68 FPS, demonstrating the practicality of\nour approach for high-throughput, re-source-constrained scenarios."
                },
                "authors": [
                    {
                        "name": "Melika Sabaghian"
                    },
                    {
                        "name": "Mohammad Ali Keyvanrad"
                    },
                    {
                        "name": "Seyyedeh Mahila Moghadami"
                    }
                ],
                "author_detail": {
                    "name": "Seyyedeh Mahila Moghadami"
                },
                "author": "Seyyedeh Mahila Moghadami",
                "arxiv_comment": "28 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12918v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12918v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12914v1",
                "updated": "2025-09-16T10:09:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    9,
                    0,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T10:09:00Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    9,
                    0,
                    1,
                    259,
                    0
                ],
                "title": "Stochastic Streets: A Walk Through Random LLM Address Generation in four\n  European Cities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Streets: A Walk Through Random LLM Address Generation in four\n  European Cities"
                },
                "summary": "Large Language Models (LLMs) are capable of solving complex math problems or\nanswer difficult questions on almost any topic, but can they generate random\nstreet addresses for European cities?",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are capable of solving complex math problems or\nanswer difficult questions on almost any topic, but can they generate random\nstreet addresses for European cities?"
                },
                "authors": [
                    {
                        "name": "Tairan Fu"
                    },
                    {
                        "name": "David Campo-Nazareno"
                    },
                    {
                        "name": "Javier Coronado-BlÃ¡zquez"
                    },
                    {
                        "name": "Javier Conde"
                    },
                    {
                        "name": "Pedro Reviriego"
                    },
                    {
                        "name": "Fabrizio Lombardi"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Lombardi"
                },
                "author": "Fabrizio Lombardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12908v1",
                "updated": "2025-09-16T10:02:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    2,
                    52,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T10:02:52Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    2,
                    52,
                    1,
                    259,
                    0
                ],
                "title": "All Roads Lead to Rome: Graph-Based Confidence Estimation for Large\n  Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All Roads Lead to Rome: Graph-Based Confidence Estimation for Large\n  Language Model Reasoning"
                },
                "summary": "Confidence estimation is essential for the reliable deployment of large\nlanguage models (LLMs). Existing methods are primarily designed for factual QA\ntasks and often fail to generalize to reasoning tasks. To address this gap, we\npropose a set of training-free, graph-based confidence estimation methods\ntailored to reasoning tasks. Our approach models reasoning paths as directed\ngraphs and estimates confidence by exploiting graph properties such as\ncentrality, path convergence, and path weighting. Experiments with two LLMs on\nthree reasoning datasets demonstrate improved confidence estimation and\nenhanced performance on two downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence estimation is essential for the reliable deployment of large\nlanguage models (LLMs). Existing methods are primarily designed for factual QA\ntasks and often fail to generalize to reasoning tasks. To address this gap, we\npropose a set of training-free, graph-based confidence estimation methods\ntailored to reasoning tasks. Our approach models reasoning paths as directed\ngraphs and estimates confidence by exploiting graph properties such as\ncentrality, path convergence, and path weighting. Experiments with two LLMs on\nthree reasoning datasets demonstrate improved confidence estimation and\nenhanced performance on two downstream tasks."
                },
                "authors": [
                    {
                        "name": "Caiqi Zhang"
                    },
                    {
                        "name": "Chang Shu"
                    },
                    {
                        "name": "Ehsan Shareghi"
                    },
                    {
                        "name": "Nigel Collier"
                    }
                ],
                "author_detail": {
                    "name": "Nigel Collier"
                },
                "author": "Nigel Collier",
                "arxiv_comment": "EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12894v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12894v1",
                "updated": "2025-09-16T09:49:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    49,
                    34,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:49:34Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    49,
                    34,
                    1,
                    259,
                    0
                ],
                "title": "DialNav: Multi-turn Dialog Navigation with a Remote Guide",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DialNav: Multi-turn Dialog Navigation with a Remote Guide"
                },
                "summary": "We introduce DialNav, a novel collaborative embodied dialog task, where a\nnavigation agent (Navigator) and a remote guide (Guide) engage in multi-turn\ndialog to reach a goal location. Unlike prior work, DialNav aims for holistic\nevaluation and requires the Guide to infer the Navigator's location, making\ncommunication essential for task success. To support this task, we collect and\nrelease the Remote Assistance in Navigation (RAIN) dataset, human-human dialog\npaired with navigation trajectories in photorealistic environments. We design a\ncomprehensive benchmark to evaluate both navigation and dialog, and conduct\nextensive experiments analyzing the impact of different Navigator and Guide\nmodels. We highlight key challenges and publicly release the dataset, code, and\nevaluation framework to foster future research in embodied dialog.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DialNav, a novel collaborative embodied dialog task, where a\nnavigation agent (Navigator) and a remote guide (Guide) engage in multi-turn\ndialog to reach a goal location. Unlike prior work, DialNav aims for holistic\nevaluation and requires the Guide to infer the Navigator's location, making\ncommunication essential for task success. To support this task, we collect and\nrelease the Remote Assistance in Navigation (RAIN) dataset, human-human dialog\npaired with navigation trajectories in photorealistic environments. We design a\ncomprehensive benchmark to evaluate both navigation and dialog, and conduct\nextensive experiments analyzing the impact of different Navigator and Guide\nmodels. We highlight key challenges and publicly release the dataset, code, and\nevaluation framework to foster future research in embodied dialog."
                },
                "authors": [
                    {
                        "name": "Leekyeung Han"
                    },
                    {
                        "name": "Hyunji Min"
                    },
                    {
                        "name": "Gyeom Hwangbo"
                    },
                    {
                        "name": "Jonghyun Choi"
                    },
                    {
                        "name": "Paul Hongsuck Seo"
                    }
                ],
                "author_detail": {
                    "name": "Paul Hongsuck Seo"
                },
                "author": "Paul Hongsuck Seo",
                "arxiv_comment": "18 pages, 8 figures, ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12894v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12894v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.13316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13316v1",
                "updated": "2025-09-16T17:59:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    59,
                    4,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T17:59:04Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    59,
                    4,
                    1,
                    259,
                    0
                ],
                "title": "Do Natural Language Descriptions of Model Activations Convey Privileged\n  Information?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Natural Language Descriptions of Model Activations Convey Privileged\n  Information?"
                },
                "summary": "Recent interpretability methods have proposed to translate LLM internal\nrepresentations into natural language descriptions using a second verbalizer\nLLM. This is intended to illuminate how the target model represents and\noperates on inputs. But do such activation verbalization approaches actually\nprovide privileged knowledge about the internal workings of the target model,\nor do they merely convey information about its inputs? We critically evaluate\npopular verbalization methods across datasets used in prior work and find that\nthey succeed at benchmarks without any access to target model internals,\nsuggesting that these datasets are not ideal for evaluating verbalization\nmethods. We then run controlled experiments which reveal that verbalizations\noften reflect the parametric knowledge of the verbalizer LLM which generated\nthem, rather than the activations of the target LLM being decoded. Taken\ntogether, our results indicate a need for targeted benchmarks and experimental\ncontrols to rigorously assess whether verbalization methods provide meaningful\ninsights into the operations of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent interpretability methods have proposed to translate LLM internal\nrepresentations into natural language descriptions using a second verbalizer\nLLM. This is intended to illuminate how the target model represents and\noperates on inputs. But do such activation verbalization approaches actually\nprovide privileged knowledge about the internal workings of the target model,\nor do they merely convey information about its inputs? We critically evaluate\npopular verbalization methods across datasets used in prior work and find that\nthey succeed at benchmarks without any access to target model internals,\nsuggesting that these datasets are not ideal for evaluating verbalization\nmethods. We then run controlled experiments which reveal that verbalizations\noften reflect the parametric knowledge of the verbalizer LLM which generated\nthem, rather than the activations of the target LLM being decoded. Taken\ntogether, our results indicate a need for targeted benchmarks and experimental\ncontrols to rigorously assess whether verbalization methods provide meaningful\ninsights into the operations of LLMs."
                },
                "authors": [
                    {
                        "name": "Millicent Li"
                    },
                    {
                        "name": "Alberto Mario Ceballos Arroyo"
                    },
                    {
                        "name": "Giordano Rogers"
                    },
                    {
                        "name": "Naomi Saphra"
                    },
                    {
                        "name": "Byron C. Wallace"
                    }
                ],
                "author_detail": {
                    "name": "Byron C. Wallace"
                },
                "author": "Byron C. Wallace",
                "arxiv_comment": "34 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13313v1",
                "updated": "2025-09-16T17:57:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    57,
                    22,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T17:57:22Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    57,
                    22,
                    1,
                    259,
                    0
                ],
                "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context\n  Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReSum: Unlocking Long-Horizon Search Intelligence via Context\n  Summarization"
                },
                "summary": "Large Language Model (LLM)-based web agents demonstrate strong performance on\nknowledge-intensive tasks but are hindered by context window limitations in\nparadigms like ReAct. Complex queries involving multiple entities, intertwined\nrelationships, and high uncertainty demand extensive search cycles that rapidly\nexhaust context budgets before reaching complete solutions. To overcome this\nchallenge, we introduce ReSum, a novel paradigm that enables indefinite\nexploration through periodic context summarization. ReSum converts growing\ninteraction histories into compact reasoning states, maintaining awareness of\nprior discoveries while bypassing context constraints. For paradigm adaptation,\nwe propose ReSum-GRPO, integrating GRPO with segmented trajectory training and\nadvantage broadcasting to familiarize agents with summary-conditioned\nreasoning. Extensive experiments on web agents of varying scales across three\nbenchmarks demonstrate that ReSum delivers an average absolute improvement of\n4.5\\% over ReAct, with further gains of up to 8.2\\% following ReSum-GRPO\ntraining. Notably, with only 1K training samples, our WebResummer-30B (a\nReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\\% Pass@1 on\nBrowseComp-zh and 18.3\\% on BrowseComp-en, surpassing existing open-source web\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based web agents demonstrate strong performance on\nknowledge-intensive tasks but are hindered by context window limitations in\nparadigms like ReAct. Complex queries involving multiple entities, intertwined\nrelationships, and high uncertainty demand extensive search cycles that rapidly\nexhaust context budgets before reaching complete solutions. To overcome this\nchallenge, we introduce ReSum, a novel paradigm that enables indefinite\nexploration through periodic context summarization. ReSum converts growing\ninteraction histories into compact reasoning states, maintaining awareness of\nprior discoveries while bypassing context constraints. For paradigm adaptation,\nwe propose ReSum-GRPO, integrating GRPO with segmented trajectory training and\nadvantage broadcasting to familiarize agents with summary-conditioned\nreasoning. Extensive experiments on web agents of varying scales across three\nbenchmarks demonstrate that ReSum delivers an average absolute improvement of\n4.5\\% over ReAct, with further gains of up to 8.2\\% following ReSum-GRPO\ntraining. Notably, with only 1K training samples, our WebResummer-30B (a\nReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\\% Pass@1 on\nBrowseComp-zh and 18.3\\% on BrowseComp-en, surpassing existing open-source web\nagents."
                },
                "authors": [
                    {
                        "name": "Xixi Wu"
                    },
                    {
                        "name": "Kuan Li"
                    },
                    {
                        "name": "Yida Zhao"
                    },
                    {
                        "name": "Liwen Zhang"
                    },
                    {
                        "name": "Litu Ou"
                    },
                    {
                        "name": "Huifeng Yin"
                    },
                    {
                        "name": "Zhongwang Zhang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Minhao Cheng"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Hong Cheng"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13310v1",
                "updated": "2025-09-16T17:57:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    57,
                    19,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T17:57:19Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    57,
                    19,
                    1,
                    259,
                    0
                ],
                "title": "Scaling Agents via Continual Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Agents via Continual Pre-training"
                },
                "summary": "Large language models (LLMs) have evolved into agentic systems capable of\nautonomous tool use and multi-step reasoning for complex problem-solving.\nHowever, post-training approaches building upon general-purpose foundation\nmodels consistently underperform in agentic tasks, particularly in open-source\nimplementations. We identify the root cause: the absence of robust agentic\nfoundation models forces models during post-training to simultaneously learn\ndiverse agentic behaviors while aligning them to expert demonstrations, thereby\ncreating fundamental optimization tensions. To this end, we are the first to\npropose incorporating Agentic Continual Pre-training (Agentic CPT) into the\ndeep research agents training pipeline to build powerful agentic foundational\nmodels. Based on this approach, we develop a deep research agent model named\nAgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve\nstate-of-the-art performance while retains strong tool-use ability, notably\n39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have evolved into agentic systems capable of\nautonomous tool use and multi-step reasoning for complex problem-solving.\nHowever, post-training approaches building upon general-purpose foundation\nmodels consistently underperform in agentic tasks, particularly in open-source\nimplementations. We identify the root cause: the absence of robust agentic\nfoundation models forces models during post-training to simultaneously learn\ndiverse agentic behaviors while aligning them to expert demonstrations, thereby\ncreating fundamental optimization tensions. To this end, we are the first to\npropose incorporating Agentic Continual Pre-training (Agentic CPT) into the\ndeep research agents training pipeline to build powerful agentic foundational\nmodels. Based on this approach, we develop a deep research agent model named\nAgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve\nstate-of-the-art performance while retains strong tool-use ability, notably\n39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE."
                },
                "authors": [
                    {
                        "name": "Liangcai Su"
                    },
                    {
                        "name": "Zhen Zhang"
                    },
                    {
                        "name": "Guangyu Li"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Maojia Song"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Kuan Li"
                    },
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Xuanzhong Chen"
                    },
                    {
                        "name": "Zile Qiao"
                    },
                    {
                        "name": "Zhongwang Zhang"
                    },
                    {
                        "name": "Huifeng Yin"
                    },
                    {
                        "name": "Shihao Cai"
                    },
                    {
                        "name": "Runnan Fang"
                    },
                    {
                        "name": "Zhengwei Tao"
                    },
                    {
                        "name": "Wenbiao Yin"
                    },
                    {
                        "name": "Chenxiong Qian"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13305v1",
                "updated": "2025-09-16T17:57:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    57,
                    3,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T17:57:03Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    57,
                    3,
                    1,
                    259,
                    0
                ],
                "title": "WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic\n  Data and Scalable Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic\n  Data and Scalable Reinforcement Learning"
                },
                "summary": "Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all open-source agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all open-source agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap."
                },
                "authors": [
                    {
                        "name": "Kuan Li"
                    },
                    {
                        "name": "Zhongwang Zhang"
                    },
                    {
                        "name": "Huifeng Yin"
                    },
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Yida Zhao"
                    },
                    {
                        "name": "Liwen Zhang"
                    },
                    {
                        "name": "Litu Ou"
                    },
                    {
                        "name": "Dingchu Zhang"
                    },
                    {
                        "name": "Xixi Wu"
                    },
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Zile Qiao"
                    },
                    {
                        "name": "Zhen Zhang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20404v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20404v3",
                "updated": "2025-09-16T17:48:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    48,
                    53,
                    1,
                    259,
                    0
                ],
                "published": "2024-05-30T18:16:41Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    18,
                    16,
                    41,
                    3,
                    151,
                    0
                ],
                "title": "JoPA:Explaining Large Language Model's Generation via Joint Prompt\n  Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JoPA:Explaining Large Language Model's Generation via Joint Prompt\n  Attribution"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performances in\ncomplex text generation tasks. However, the contribution of the input prompt to\nthe generated content still remains obscure to humans, underscoring the\nnecessity of understanding the causality between input and output pairs.\nExisting works for providing prompt-specific explanation often confine model\noutput to be classification or next-word prediction. Few initial attempts\naiming to explain the entire language generation often treat input prompt texts\nindependently, ignoring their combinatorial effects on the follow-up\ngeneration. In this study, we introduce a counterfactual explanation framework\nbased on Joint Prompt Attribution, JoPA, which aims to explain how a few prompt\ntexts collaboratively influences the LLM's complete generation. Particularly,\nwe formulate the task of prompt attribution for generation interpretation as a\ncombinatorial optimization problem, and introduce a probabilistic algorithm to\nsearch for the casual input combination in the discrete space. We define and\nutilize multiple metrics to evaluate the produced explanations, demonstrating\nboth the faithfulness and efficiency of our framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performances in\ncomplex text generation tasks. However, the contribution of the input prompt to\nthe generated content still remains obscure to humans, underscoring the\nnecessity of understanding the causality between input and output pairs.\nExisting works for providing prompt-specific explanation often confine model\noutput to be classification or next-word prediction. Few initial attempts\naiming to explain the entire language generation often treat input prompt texts\nindependently, ignoring their combinatorial effects on the follow-up\ngeneration. In this study, we introduce a counterfactual explanation framework\nbased on Joint Prompt Attribution, JoPA, which aims to explain how a few prompt\ntexts collaboratively influences the LLM's complete generation. Particularly,\nwe formulate the task of prompt attribution for generation interpretation as a\ncombinatorial optimization problem, and introduce a probabilistic algorithm to\nsearch for the casual input combination in the discrete space. We define and\nutilize multiple metrics to evaluate the produced explanations, demonstrating\nboth the faithfulness and efficiency of our framework."
                },
                "authors": [
                    {
                        "name": "Yurui Chang"
                    },
                    {
                        "name": "Bochuan Cao"
                    },
                    {
                        "name": "Yujia Wang"
                    },
                    {
                        "name": "Jinghui Chen"
                    },
                    {
                        "name": "Lu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lu Lin"
                },
                "author": "Lu Lin",
                "arxiv_doi": "10.18653/v1/2025.acl-long.1074",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2025.acl-long.1074",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.20404v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20404v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to ACL 2025 (Main)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13281v1",
                "updated": "2025-09-16T17:35:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    35,
                    36,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T17:35:36Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    35,
                    36,
                    1,
                    259,
                    0
                ],
                "title": "RepIt: Representing Isolated Targets to Steer Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepIt: Representing Isolated Targets to Steer Language Models"
                },
                "summary": "While activation steering in large language models (LLMs) is a growing area\nof research, methods can often incur broader effects than desired. This\nmotivates isolation of purer concept vectors to enable targeted interventions\nand understand LLM behavior at a more granular level. We present RepIt, a\nsimple and data-efficient framework for isolating concept-specific\nrepresentations. Across five frontier LLMs, RepIt enables precise\ninterventions: it selectively suppresses refusal on targeted concepts while\npreserving refusal elsewhere, producing models that answer WMD-related\nquestions while still scoring as safe on standard benchmarks. We further show\nthat the corrective signal localizes to just 100-200 neurons and that robust\ntarget representations can be extracted from as few as a dozen examples on a\nsingle A6000. This efficiency raises a dual concern: manipulations can be\nperformed with modest compute and data to extend to underrepresented\ndata-scarce topics while evading existing benchmarks. By disentangling refusal\nvectors with RepIt, this work demonstrates that targeted interventions can\ncounteract overgeneralization, laying the foundation for more granular control\nof model behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While activation steering in large language models (LLMs) is a growing area\nof research, methods can often incur broader effects than desired. This\nmotivates isolation of purer concept vectors to enable targeted interventions\nand understand LLM behavior at a more granular level. We present RepIt, a\nsimple and data-efficient framework for isolating concept-specific\nrepresentations. Across five frontier LLMs, RepIt enables precise\ninterventions: it selectively suppresses refusal on targeted concepts while\npreserving refusal elsewhere, producing models that answer WMD-related\nquestions while still scoring as safe on standard benchmarks. We further show\nthat the corrective signal localizes to just 100-200 neurons and that robust\ntarget representations can be extracted from as few as a dozen examples on a\nsingle A6000. This efficiency raises a dual concern: manipulations can be\nperformed with modest compute and data to extend to underrepresented\ndata-scarce topics while evading existing benchmarks. By disentangling refusal\nvectors with RepIt, this work demonstrates that targeted interventions can\ncounteract overgeneralization, laying the foundation for more granular control\nof model behavior."
                },
                "authors": [
                    {
                        "name": "Vincent Siu"
                    },
                    {
                        "name": "Nathan W. Henry"
                    },
                    {
                        "name": "Nicholas Crispino"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Chenguang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chenguang Wang"
                },
                "author": "Chenguang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13268v1",
                "updated": "2025-09-16T17:26:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    26,
                    17,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T17:26:17Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    26,
                    17,
                    1,
                    259,
                    0
                ],
                "title": "LLMs for energy and macronutrients estimation using only text data from\n  24-hour dietary recalls: a parameter-efficient fine-tuning experiment using a\n  10-shot prompt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for energy and macronutrients estimation using only text data from\n  24-hour dietary recalls: a parameter-efficient fine-tuning experiment using a\n  10-shot prompt"
                },
                "summary": "BACKGROUND: Most artificial intelligence tools used to estimate nutritional\ncontent rely on image input. However, whether large language models (LLMs) can\naccurately predict nutritional values based solely on text descriptions of\nfoods consumed remains unknown. If effective, this approach could enable\nsimpler dietary monitoring without the need for photographs. METHODS: We used\n24-hour dietary recalls from adolescents aged 12-19 years in the National\nHealth and Nutrition Examination Survey (NHANES). An open-source quantized LLM\nwas prompted using a 10-shot, chain-of-thought approach to estimate energy and\nfive macronutrients based solely on text strings listing foods and their\nquantities. We then applied parameter-efficient fine-tuning (PEFT) to evaluate\nwhether predictive accuracy improved. NHANES-calculated values served as the\nground truth for energy, proteins, carbohydrates, total sugar, dietary fiber\nand total fat. RESULTS: In a pooled dataset of 11,281 adolescents (49.9% male,\nmean age 15.4 years), the vanilla LLM yielded poor predictions. The mean\nabsolute error (MAE) was 652.08 for energy and the Lin's CCC <0.46 across\nendpoints. In contrast, the fine-tuned model performed substantially better,\nwith energy MAEs ranging from 171.34 to 190.90 across subsets, and Lin's CCC\nexceeding 0.89 for all outcomes. CONCLUSIONS: When prompted using a\nchain-of-thought approach and fine-tuned with PEFT, open-source LLMs exposed\nsolely to text input can accurately predict energy and macronutrient values\nfrom 24-hour dietary recalls. This approach holds promise for low-burden,\ntext-based dietary monitoring tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BACKGROUND: Most artificial intelligence tools used to estimate nutritional\ncontent rely on image input. However, whether large language models (LLMs) can\naccurately predict nutritional values based solely on text descriptions of\nfoods consumed remains unknown. If effective, this approach could enable\nsimpler dietary monitoring without the need for photographs. METHODS: We used\n24-hour dietary recalls from adolescents aged 12-19 years in the National\nHealth and Nutrition Examination Survey (NHANES). An open-source quantized LLM\nwas prompted using a 10-shot, chain-of-thought approach to estimate energy and\nfive macronutrients based solely on text strings listing foods and their\nquantities. We then applied parameter-efficient fine-tuning (PEFT) to evaluate\nwhether predictive accuracy improved. NHANES-calculated values served as the\nground truth for energy, proteins, carbohydrates, total sugar, dietary fiber\nand total fat. RESULTS: In a pooled dataset of 11,281 adolescents (49.9% male,\nmean age 15.4 years), the vanilla LLM yielded poor predictions. The mean\nabsolute error (MAE) was 652.08 for energy and the Lin's CCC <0.46 across\nendpoints. In contrast, the fine-tuned model performed substantially better,\nwith energy MAEs ranging from 171.34 to 190.90 across subsets, and Lin's CCC\nexceeding 0.89 for all outcomes. CONCLUSIONS: When prompted using a\nchain-of-thought approach and fine-tuned with PEFT, open-source LLMs exposed\nsolely to text input can accurately predict energy and macronutrient values\nfrom 24-hour dietary recalls. This approach holds promise for low-burden,\ntext-based dietary monitoring tools."
                },
                "authors": [
                    {
                        "name": "Rodrigo M Carrillo-Larco"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo M Carrillo-Larco"
                },
                "author": "Rodrigo M Carrillo-Larco",
                "arxiv_comment": "https://github.com/rodrigo-carrillo/LLMs-Macronutrient-Estimation-NHANES-Adolescents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13265v1",
                "updated": "2025-09-16T17:22:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    22,
                    0,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T17:22:00Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    22,
                    0,
                    1,
                    259,
                    0
                ],
                "title": "Beyond Private or Public: Large Language Models as Quasi-Public Goods in\n  the AI Economy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Private or Public: Large Language Models as Quasi-Public Goods in\n  the AI Economy"
                },
                "summary": "This paper conceptualizes Large Language Models (LLMs) as a form of mixed\npublic goods within digital infrastructure, analyzing their economic properties\nthrough a comprehensive theoretical framework. We develop mathematical models\nto quantify the non-rivalry characteristics, partial excludability, and\npositive externalities of LLMs. Through comparative analysis of open-source and\nclosed-source development paths, we identify systematic differences in resource\nallocation efficiency, innovation trajectories, and access equity. Our\nempirical research evaluates the spillover effects and network externalities of\nLLMs across different domains, including knowledge diffusion, innovation\nacceleration, and industry transformation. Based on these findings, we propose\npolicy recommendations for balancing innovation incentives with equitable\naccess, including public-private partnership mechanisms, computational resource\ndemocratization, and governance structures that optimize social welfare. This\ninterdisciplinary approach contributes to understanding the economic nature of\nfoundation AI models and provides policy guidance for their development as\ncritical digital infrastructure",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper conceptualizes Large Language Models (LLMs) as a form of mixed\npublic goods within digital infrastructure, analyzing their economic properties\nthrough a comprehensive theoretical framework. We develop mathematical models\nto quantify the non-rivalry characteristics, partial excludability, and\npositive externalities of LLMs. Through comparative analysis of open-source and\nclosed-source development paths, we identify systematic differences in resource\nallocation efficiency, innovation trajectories, and access equity. Our\nempirical research evaluates the spillover effects and network externalities of\nLLMs across different domains, including knowledge diffusion, innovation\nacceleration, and industry transformation. Based on these findings, we propose\npolicy recommendations for balancing innovation incentives with equitable\naccess, including public-private partnership mechanisms, computational resource\ndemocratization, and governance structures that optimize social welfare. This\ninterdisciplinary approach contributes to understanding the economic nature of\nfoundation AI models and provides policy guidance for their development as\ncritical digital infrastructure"
                },
                "authors": [
                    {
                        "name": "Yukun Zhang"
                    },
                    {
                        "name": "TianYang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "TianYang Zhang"
                },
                "author": "TianYang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.01754v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.01754v4",
                "updated": "2025-09-16T17:04:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    4,
                    19,
                    1,
                    259,
                    0
                ],
                "published": "2022-09-05T04:19:16Z",
                "published_parsed": [
                    2022,
                    9,
                    5,
                    4,
                    19,
                    16,
                    0,
                    248,
                    0
                ],
                "title": "Learning from a Biased Sample",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from a Biased Sample"
                },
                "summary": "The empirical risk minimization approach to data-driven decision making\nrequires access to training data drawn under the same conditions as those that\nwill be faced when the decision rule is deployed. However, in a number of\nsettings, we may be concerned that our training sample is biased in the sense\nthat some groups (characterized by either observable or unobservable\nattributes) may be under- or over-represented relative to the general\npopulation; and in this setting empirical risk minimization over the training\nset may fail to yield rules that perform well at deployment. We propose a model\nof sampling bias called conditional $\\Gamma$-biased sampling, where observed\ncovariates can affect the probability of sample selection arbitrarily much but\nthe amount of unexplained variation in the probability of sample selection is\nbounded by a constant factor. Applying the distributionally robust optimization\nframework, we propose a method for learning a decision rule that minimizes the\nworst-case risk incurred under a family of test distributions that can generate\nthe training distribution under $\\Gamma$-biased sampling. We apply a result of\nRockafellar and Uryasev to show that this problem is equivalent to an augmented\nconvex risk minimization problem. We give statistical guarantees for learning a\nmodel that is robust to sampling bias via the method of sieves, and propose a\ndeep learning algorithm whose loss function captures our robust learning\ntarget. We empirically validate our proposed method in a case study on\nprediction of mental health scores from health survey data and a case study on\nICU length of stay prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The empirical risk minimization approach to data-driven decision making\nrequires access to training data drawn under the same conditions as those that\nwill be faced when the decision rule is deployed. However, in a number of\nsettings, we may be concerned that our training sample is biased in the sense\nthat some groups (characterized by either observable or unobservable\nattributes) may be under- or over-represented relative to the general\npopulation; and in this setting empirical risk minimization over the training\nset may fail to yield rules that perform well at deployment. We propose a model\nof sampling bias called conditional $\\Gamma$-biased sampling, where observed\ncovariates can affect the probability of sample selection arbitrarily much but\nthe amount of unexplained variation in the probability of sample selection is\nbounded by a constant factor. Applying the distributionally robust optimization\nframework, we propose a method for learning a decision rule that minimizes the\nworst-case risk incurred under a family of test distributions that can generate\nthe training distribution under $\\Gamma$-biased sampling. We apply a result of\nRockafellar and Uryasev to show that this problem is equivalent to an augmented\nconvex risk minimization problem. We give statistical guarantees for learning a\nmodel that is robust to sampling bias via the method of sieves, and propose a\ndeep learning algorithm whose loss function captures our robust learning\ntarget. We empirically validate our proposed method in a case study on\nprediction of mental health scores from health survey data and a case study on\nICU length of stay prediction."
                },
                "authors": [
                    {
                        "name": "Roshni Sahoo"
                    },
                    {
                        "name": "Lihua Lei"
                    },
                    {
                        "name": "Stefan Wager"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wager"
                },
                "author": "Stefan Wager",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2209.01754v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.01754v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13251v1",
                "updated": "2025-09-16T17:02:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    2,
                    24,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T17:02:24Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    2,
                    24,
                    1,
                    259,
                    0
                ],
                "title": "Large Language Model-assisted Meta-optimizer for Automated Design of\n  Constrained Evolutionary Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-assisted Meta-optimizer for Automated Design of\n  Constrained Evolutionary Algorithm"
                },
                "summary": "Meta-black-box optimization has been significantly advanced through the use\nof large language models (LLMs), yet in fancy on constrained evolutionary\noptimization. In this work, AwesomeDE is proposed that leverages LLMs as the\nstrategy of meta-optimizer to generate update rules for constrained\nevolutionary algorithm without human intervention. On the meanwhile, $RTO^2H$\nframework is introduced for standardize prompt design of LLMs. The\nmeta-optimizer is trained on a diverse set of constrained optimization\nproblems. Key components, including prompt design and iterative refinement, are\nsystematically analyzed to determine their impact on design quality.\nExperimental results demonstrate that the proposed approach outperforms\nexisting methods in terms of computational efficiency and solution accuracy.\nFurthermore, AwesomeDE is shown to generalize well across distinct problem\ndomains, suggesting its potential for broad applicability. This research\ncontributes to the field by providing a scalable and data-driven methodology\nfor automated constrained algorithm design, while also highlighting limitations\nand directions for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-black-box optimization has been significantly advanced through the use\nof large language models (LLMs), yet in fancy on constrained evolutionary\noptimization. In this work, AwesomeDE is proposed that leverages LLMs as the\nstrategy of meta-optimizer to generate update rules for constrained\nevolutionary algorithm without human intervention. On the meanwhile, $RTO^2H$\nframework is introduced for standardize prompt design of LLMs. The\nmeta-optimizer is trained on a diverse set of constrained optimization\nproblems. Key components, including prompt design and iterative refinement, are\nsystematically analyzed to determine their impact on design quality.\nExperimental results demonstrate that the proposed approach outperforms\nexisting methods in terms of computational efficiency and solution accuracy.\nFurthermore, AwesomeDE is shown to generalize well across distinct problem\ndomains, suggesting its potential for broad applicability. This research\ncontributes to the field by providing a scalable and data-driven methodology\nfor automated constrained algorithm design, while also highlighting limitations\nand directions for future work."
                },
                "authors": [
                    {
                        "name": "Xu Yang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Kaiwen Li"
                    },
                    {
                        "name": "Wenhua Li"
                    },
                    {
                        "name": "Weixiong Huang"
                    }
                ],
                "author_detail": {
                    "name": "Weixiong Huang"
                },
                "author": "Weixiong Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13244v1",
                "updated": "2025-09-16T16:54:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    54,
                    35,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T16:54:35Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    54,
                    35,
                    1,
                    259,
                    0
                ],
                "title": "Evaluating LLM Alignment on Personality Inference from Real-World\n  Interview Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLM Alignment on Personality Inference from Real-World\n  Interview Data"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in roles requiring\nnuanced psychological understanding, such as emotional support agents,\ncounselors, and decision-making assistants. However, their ability to interpret\nhuman personality traits, a critical aspect of such applications, remains\nunexplored, particularly in ecologically valid conversational settings. While\nprior work has simulated LLM \"personas\" using discrete Big Five labels on\nsocial media data, the alignment of LLMs with continuous, ground-truth\npersonality assessments derived from natural interactions is largely\nunexamined. To address this gap, we introduce a novel benchmark comprising\nsemi-structured interview transcripts paired with validated continuous Big Five\ntrait scores. Using this dataset, we systematically evaluate LLM performance\nacross three paradigms: (1) zero-shot and chain-of-thought prompting with\nGPT-4.1 Mini, (2) LoRA-based fine-tuning applied to both RoBERTa and Meta-LLaMA\narchitectures, and (3) regression using static embeddings from pretrained BERT\nand OpenAI's text-embedding-3-small. Our results reveal that all Pearson\ncorrelations between model predictions and ground-truth personality traits\nremain below 0.26, highlighting the limited alignment of current LLMs with\nvalidated psychological constructs. Chain-of-thought prompting offers minimal\ngains over zero-shot, suggesting that personality inference relies more on\nlatent semantic representation than explicit reasoning. These findings\nunderscore the challenges of aligning LLMs with complex human attributes and\nmotivate future work on trait-specific prompting, context-aware modeling, and\nalignment-oriented fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in roles requiring\nnuanced psychological understanding, such as emotional support agents,\ncounselors, and decision-making assistants. However, their ability to interpret\nhuman personality traits, a critical aspect of such applications, remains\nunexplored, particularly in ecologically valid conversational settings. While\nprior work has simulated LLM \"personas\" using discrete Big Five labels on\nsocial media data, the alignment of LLMs with continuous, ground-truth\npersonality assessments derived from natural interactions is largely\nunexamined. To address this gap, we introduce a novel benchmark comprising\nsemi-structured interview transcripts paired with validated continuous Big Five\ntrait scores. Using this dataset, we systematically evaluate LLM performance\nacross three paradigms: (1) zero-shot and chain-of-thought prompting with\nGPT-4.1 Mini, (2) LoRA-based fine-tuning applied to both RoBERTa and Meta-LLaMA\narchitectures, and (3) regression using static embeddings from pretrained BERT\nand OpenAI's text-embedding-3-small. Our results reveal that all Pearson\ncorrelations between model predictions and ground-truth personality traits\nremain below 0.26, highlighting the limited alignment of current LLMs with\nvalidated psychological constructs. Chain-of-thought prompting offers minimal\ngains over zero-shot, suggesting that personality inference relies more on\nlatent semantic representation than explicit reasoning. These findings\nunderscore the challenges of aligning LLMs with complex human attributes and\nmotivate future work on trait-specific prompting, context-aware modeling, and\nalignment-oriented fine-tuning."
                },
                "authors": [
                    {
                        "name": "Jianfeng Zhu"
                    },
                    {
                        "name": "Julina Maharjan"
                    },
                    {
                        "name": "Xinyu Li"
                    },
                    {
                        "name": "Karin G. Coifman"
                    },
                    {
                        "name": "Ruoming Jin"
                    }
                ],
                "author_detail": {
                    "name": "Ruoming Jin"
                },
                "author": "Ruoming Jin",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19668v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19668v3",
                "updated": "2025-09-16T16:49:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    49,
                    11,
                    1,
                    259,
                    0
                ],
                "published": "2025-02-27T01:29:51Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    1,
                    29,
                    51,
                    3,
                    58,
                    0
                ],
                "title": "SuPreME: A Supervised Pre-training Framework for Multimodal ECG\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuPreME: A Supervised Pre-training Framework for Multimodal ECG\n  Representation Learning"
                },
                "summary": "Cardiovascular diseases are a leading cause of death and disability\nworldwide. Electrocardiogram (ECG) is critical for diagnosing and monitoring\ncardiac health, but obtaining large-scale annotated ECG datasets is\nlabor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL)\nmethods mitigate this by learning features without extensive labels but fail to\ncapture fine-grained clinical semantics and require extensive task-specific\nfine-tuning. To address these challenges, we propose $\\textbf{SuPreME}$, a\n$\\textbf{Su}$pervised $\\textbf{Pre}$-training framework for\n$\\textbf{M}$ultimodal $\\textbf{E}$CG representation learning. SuPreME is\npre-trained using structured diagnostic labels derived from ECG report entities\nthrough a one-time offline extraction with Large Language Models (LLMs), which\nhelp denoise, standardize cardiac concepts, and improve clinical representation\nlearning. By fusing ECG signals with textual cardiac queries instead of fixed\nlabels, SuPreME enables zero-shot classification of unseen conditions without\nfurther fine-tuning. We evaluate SuPreME on six downstream datasets covering\n106 cardiac conditions, achieving superior zero-shot AUC performance of\n$77.20\\%$, surpassing state-of-the-art eSSLs by $4.98\\%$. Results demonstrate\nSuPreME's effectiveness in leveraging structured, clinically relevant knowledge\nfor high-quality ECG representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cardiovascular diseases are a leading cause of death and disability\nworldwide. Electrocardiogram (ECG) is critical for diagnosing and monitoring\ncardiac health, but obtaining large-scale annotated ECG datasets is\nlabor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL)\nmethods mitigate this by learning features without extensive labels but fail to\ncapture fine-grained clinical semantics and require extensive task-specific\nfine-tuning. To address these challenges, we propose $\\textbf{SuPreME}$, a\n$\\textbf{Su}$pervised $\\textbf{Pre}$-training framework for\n$\\textbf{M}$ultimodal $\\textbf{E}$CG representation learning. SuPreME is\npre-trained using structured diagnostic labels derived from ECG report entities\nthrough a one-time offline extraction with Large Language Models (LLMs), which\nhelp denoise, standardize cardiac concepts, and improve clinical representation\nlearning. By fusing ECG signals with textual cardiac queries instead of fixed\nlabels, SuPreME enables zero-shot classification of unseen conditions without\nfurther fine-tuning. We evaluate SuPreME on six downstream datasets covering\n106 cardiac conditions, achieving superior zero-shot AUC performance of\n$77.20\\%$, surpassing state-of-the-art eSSLs by $4.98\\%$. Results demonstrate\nSuPreME's effectiveness in leveraging structured, clinically relevant knowledge\nfor high-quality ECG representations."
                },
                "authors": [
                    {
                        "name": "Mingsheng Cai"
                    },
                    {
                        "name": "Jiuming Jiang"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Rossella Arcucci"
                    }
                ],
                "author_detail": {
                    "name": "Rossella Arcucci"
                },
                "author": "Rossella Arcucci",
                "arxiv_comment": "Findings of The 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19668v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19668v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01132v2",
                "updated": "2025-09-16T16:47:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    47,
                    26,
                    1,
                    259,
                    0
                ],
                "published": "2025-04-01T19:08:24Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    8,
                    24,
                    1,
                    91,
                    0
                ],
                "title": "Is the Top Still Spinning? Evaluating Subjectivity in Narrative\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is the Top Still Spinning? Evaluating Subjectivity in Narrative\n  Understanding"
                },
                "summary": "Determining faithfulness of a claim to a source document is an important\nproblem across many domains. This task is generally treated as a binary\njudgment of whether the claim is supported or unsupported in relation to the\nsource. In many cases, though, whether a claim is supported can be ambiguous.\nFor instance, it may depend on making inferences from given evidence, and\ndifferent people can reasonably interpret the claim as either supported or\nunsupported based on their agreement with those inferences. Forcing binary\nlabels upon such claims lowers the reliability of evaluation. In this work, we\nreframe the task to manage the subjectivity involved with factuality judgments\nof ambiguous claims. We introduce LLM-generated edits of summaries as a method\nof providing a nuanced evaluation of claims: how much does a summary need to be\nedited to be unambiguous? Whether a claim gets rewritten and how much it\nchanges can be used as an automatic evaluation metric, the Ambiguity Rewrite\nMetric (ARM), with a much richer feedback signal than a binary judgment of\nfaithfulness. We focus on the area of narrative summarization as it is\nparticularly rife with ambiguity and subjective interpretation. We show that\nARM produces a 21% absolute improvement in annotator agreement on claim\nfaithfulness, indicating that subjectivity is reduced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Determining faithfulness of a claim to a source document is an important\nproblem across many domains. This task is generally treated as a binary\njudgment of whether the claim is supported or unsupported in relation to the\nsource. In many cases, though, whether a claim is supported can be ambiguous.\nFor instance, it may depend on making inferences from given evidence, and\ndifferent people can reasonably interpret the claim as either supported or\nunsupported based on their agreement with those inferences. Forcing binary\nlabels upon such claims lowers the reliability of evaluation. In this work, we\nreframe the task to manage the subjectivity involved with factuality judgments\nof ambiguous claims. We introduce LLM-generated edits of summaries as a method\nof providing a nuanced evaluation of claims: how much does a summary need to be\nedited to be unambiguous? Whether a claim gets rewritten and how much it\nchanges can be used as an automatic evaluation metric, the Ambiguity Rewrite\nMetric (ARM), with a much richer feedback signal than a binary judgment of\nfaithfulness. We focus on the area of narrative summarization as it is\nparticularly rife with ambiguity and subjective interpretation. We show that\nARM produces a 21% absolute improvement in annotator agreement on claim\nfaithfulness, indicating that subjectivity is reduced."
                },
                "authors": [
                    {
                        "name": "Melanie Subbiah"
                    },
                    {
                        "name": "Akankshya Mishra"
                    },
                    {
                        "name": "Grace Kim"
                    },
                    {
                        "name": "Liyan Tang"
                    },
                    {
                        "name": "Greg Durrett"
                    },
                    {
                        "name": "Kathleen McKeown"
                    }
                ],
                "author_detail": {
                    "name": "Kathleen McKeown"
                },
                "author": "Kathleen McKeown",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13237v1",
                "updated": "2025-09-16T16:44:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    44,
                    26,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T16:44:26Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    44,
                    26,
                    1,
                    259,
                    0
                ],
                "title": "Metacognitive Reuse: Turning Recurring LLM Reasoning Into Concise\n  Behaviors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metacognitive Reuse: Turning Recurring LLM Reasoning Into Concise\n  Behaviors"
                },
                "summary": "Large language models (LLMs) now solve multi-step problems by emitting\nextended chains of thought. During the process, they often re-derive the same\nintermediate steps across problems, inflating token usage and latency. This\nsaturation of the context window leaves less capacity for exploration. We study\na simple mechanism that converts recurring reasoning fragments into concise,\nreusable \"behaviors\" (name + instruction) via the model's own metacognitive\nanalysis of prior traces. These behaviors are stored in a \"behavior handbook\"\nwhich supplies them to the model in-context at inference or distills them into\nparameters via supervised fine-tuning. This approach achieves improved\ntest-time reasoning across three different settings - 1) Behavior-conditioned\ninference: Providing the LLM relevant behaviors in-context during reasoning\nreduces number of reasoning tokens by up to 46% while matching or improving\nbaseline accuracy; 2) Behavior-guided self-improvement: Without any parameter\nupdates, the model improves its own future reasoning by leveraging behaviors\nfrom its own past problem solving attempts. This yields up to 10% higher\naccuracy than a naive critique-and-revise baseline; and 3) Behavior-conditioned\nSFT: SFT on behavior-conditioned reasoning traces is more effective at\nconverting non-reasoning models into reasoning models as compared to vanilla\nSFT. Together, these results indicate that turning slow derivations into fast\nprocedural hints enables LLMs to remember how to reason, not just what to\nconclude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) now solve multi-step problems by emitting\nextended chains of thought. During the process, they often re-derive the same\nintermediate steps across problems, inflating token usage and latency. This\nsaturation of the context window leaves less capacity for exploration. We study\na simple mechanism that converts recurring reasoning fragments into concise,\nreusable \"behaviors\" (name + instruction) via the model's own metacognitive\nanalysis of prior traces. These behaviors are stored in a \"behavior handbook\"\nwhich supplies them to the model in-context at inference or distills them into\nparameters via supervised fine-tuning. This approach achieves improved\ntest-time reasoning across three different settings - 1) Behavior-conditioned\ninference: Providing the LLM relevant behaviors in-context during reasoning\nreduces number of reasoning tokens by up to 46% while matching or improving\nbaseline accuracy; 2) Behavior-guided self-improvement: Without any parameter\nupdates, the model improves its own future reasoning by leveraging behaviors\nfrom its own past problem solving attempts. This yields up to 10% higher\naccuracy than a naive critique-and-revise baseline; and 3) Behavior-conditioned\nSFT: SFT on behavior-conditioned reasoning traces is more effective at\nconverting non-reasoning models into reasoning models as compared to vanilla\nSFT. Together, these results indicate that turning slow derivations into fast\nprocedural hints enables LLMs to remember how to reason, not just what to\nconclude."
                },
                "authors": [
                    {
                        "name": "Aniket Didolkar"
                    },
                    {
                        "name": "Nicolas Ballas"
                    },
                    {
                        "name": "Sanjeev Arora"
                    },
                    {
                        "name": "Anirudh Goyal"
                    }
                ],
                "author_detail": {
                    "name": "Anirudh Goyal"
                },
                "author": "Anirudh Goyal",
                "arxiv_comment": "18 pages, 9 Figures, 5 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13234v1",
                "updated": "2025-09-16T16:42:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    42,
                    19,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T16:42:19Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    42,
                    19,
                    1,
                    259,
                    0
                ],
                "title": "Simulating Clinical AI Assistance using Multimodal LLMs: A Case Study in\n  Diabetic Retinopathy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Clinical AI Assistance using Multimodal LLMs: A Case Study in\n  Diabetic Retinopathy"
                },
                "summary": "Diabetic retinopathy (DR) is a leading cause of blindness worldwide, and AI\nsystems can expand access to fundus photography screening. Current FDA-cleared\nsystems primarily provide binary referral outputs, where this minimal output\nmay limit clinical trust and utility. Yet, determining the most effective\noutput format to enhance clinician-AI performance is an empirical challenge\nthat is difficult to assess at scale. We evaluated multimodal large language\nmodels (MLLMs) for DR detection and their ability to simulate clinical AI\nassistance across different output types. Two models were tested on IDRiD and\nMessidor-2: GPT-4o, a general-purpose MLLM, and MedGemma, an open-source\nmedical model. Experiments included: (1) baseline evaluation, (2) simulated AI\nassistance with synthetic predictions, and (3) actual AI-to-AI collaboration\nwhere GPT-4o incorporated MedGemma outputs. MedGemma outperformed GPT-4o at\nbaseline, achieving higher sensitivity and AUROC, while GPT-4o showed\nnear-perfect specificity but low sensitivity. Both models adjusted predictions\nbased on simulated AI inputs, but GPT-4o's performance collapsed with incorrect\nones, whereas MedGemma remained more stable. In actual collaboration, GPT-4o\nachieved strong results when guided by MedGemma's descriptive outputs, even\nwithout direct image access (AUROC up to 0.96). These findings suggest MLLMs\nmay improve DR screening pipelines and serve as scalable simulators for\nstudying clinical AI assistance across varying output configurations. Open,\nlightweight models such as MedGemma may be especially valuable in low-resource\nsettings, while descriptive outputs could enhance explainability and clinician\ntrust in clinical workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diabetic retinopathy (DR) is a leading cause of blindness worldwide, and AI\nsystems can expand access to fundus photography screening. Current FDA-cleared\nsystems primarily provide binary referral outputs, where this minimal output\nmay limit clinical trust and utility. Yet, determining the most effective\noutput format to enhance clinician-AI performance is an empirical challenge\nthat is difficult to assess at scale. We evaluated multimodal large language\nmodels (MLLMs) for DR detection and their ability to simulate clinical AI\nassistance across different output types. Two models were tested on IDRiD and\nMessidor-2: GPT-4o, a general-purpose MLLM, and MedGemma, an open-source\nmedical model. Experiments included: (1) baseline evaluation, (2) simulated AI\nassistance with synthetic predictions, and (3) actual AI-to-AI collaboration\nwhere GPT-4o incorporated MedGemma outputs. MedGemma outperformed GPT-4o at\nbaseline, achieving higher sensitivity and AUROC, while GPT-4o showed\nnear-perfect specificity but low sensitivity. Both models adjusted predictions\nbased on simulated AI inputs, but GPT-4o's performance collapsed with incorrect\nones, whereas MedGemma remained more stable. In actual collaboration, GPT-4o\nachieved strong results when guided by MedGemma's descriptive outputs, even\nwithout direct image access (AUROC up to 0.96). These findings suggest MLLMs\nmay improve DR screening pipelines and serve as scalable simulators for\nstudying clinical AI assistance across varying output configurations. Open,\nlightweight models such as MedGemma may be especially valuable in low-resource\nsettings, while descriptive outputs could enhance explainability and clinician\ntrust in clinical workflows."
                },
                "authors": [
                    {
                        "name": "Nadim Barakat"
                    },
                    {
                        "name": "William Lotter"
                    }
                ],
                "author_detail": {
                    "name": "William Lotter"
                },
                "author": "William Lotter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13232v1",
                "updated": "2025-09-16T16:39:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    39,
                    11,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T16:39:11Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    39,
                    11,
                    1,
                    259,
                    0
                ],
                "title": "Single-stream Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-stream Policy Optimization"
                },
                "summary": "We revisit policy-gradient optimization for Large Language Models (LLMs) from\na single-stream perspective. Prevailing group-based methods like GRPO reduce\nvariance with on-the-fly baselines but suffer from critical flaws: frequent\ndegenerate groups erase learning signals, and synchronization barriers hinder\nscalability. We introduce Single-stream Policy Optimization (SPO), which\neliminates these issues by design. SPO replaces per-group baselines with a\npersistent, KL-adaptive value tracker and normalizes advantages globally across\nthe batch, providing a stable, low-variance learning signal for every sample.\nBeing group-free, SPO enables higher throughput and scales effectively in\nlong-horizon or tool-integrated settings where generation times vary.\nFurthermore, the persistent value tracker naturally enables an adaptive\ncurriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO\nconverges more smoothly and attains higher accuracy than GRPO, while\neliminating computation wasted on degenerate groups. Ablation studies confirm\nthat SPO's gains stem from its principled approach to baseline estimation and\nadvantage normalization, offering a more robust and efficient path for LLM\nreasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the\naverage maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial\nabsolute point gains on challenging datasets, including +7.3 pp on BRUMO 25,\n+4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain\nin pass@$k$ across the evaluated $k$ values. SPO's success challenges the\nprevailing trend of adding incidental complexity to RL algorithms, highlighting\na path where fundamental principles, not architectural workarounds, drive the\nnext wave of progress in LLM reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We revisit policy-gradient optimization for Large Language Models (LLMs) from\na single-stream perspective. Prevailing group-based methods like GRPO reduce\nvariance with on-the-fly baselines but suffer from critical flaws: frequent\ndegenerate groups erase learning signals, and synchronization barriers hinder\nscalability. We introduce Single-stream Policy Optimization (SPO), which\neliminates these issues by design. SPO replaces per-group baselines with a\npersistent, KL-adaptive value tracker and normalizes advantages globally across\nthe batch, providing a stable, low-variance learning signal for every sample.\nBeing group-free, SPO enables higher throughput and scales effectively in\nlong-horizon or tool-integrated settings where generation times vary.\nFurthermore, the persistent value tracker naturally enables an adaptive\ncurriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO\nconverges more smoothly and attains higher accuracy than GRPO, while\neliminating computation wasted on degenerate groups. Ablation studies confirm\nthat SPO's gains stem from its principled approach to baseline estimation and\nadvantage normalization, offering a more robust and efficient path for LLM\nreasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the\naverage maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial\nabsolute point gains on challenging datasets, including +7.3 pp on BRUMO 25,\n+4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain\nin pass@$k$ across the evaluated $k$ values. SPO's success challenges the\nprevailing trend of adding incidental complexity to RL algorithms, highlighting\na path where fundamental principles, not architectural workarounds, drive the\nnext wave of progress in LLM reasoning."
                },
                "authors": [
                    {
                        "name": "Zhongwen Xu"
                    },
                    {
                        "name": "Zihan Ding"
                    }
                ],
                "author_detail": {
                    "name": "Zihan Ding"
                },
                "author": "Zihan Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13229v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13229v1",
                "updated": "2025-09-16T16:37:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    37,
                    59,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T16:37:59Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    37,
                    59,
                    1,
                    259,
                    0
                ],
                "title": "Curriculum Multi-Task Self-Supervision Improves Lightweight\n  Architectures for Onboard Satellite Hyperspectral Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Curriculum Multi-Task Self-Supervision Improves Lightweight\n  Architectures for Onboard Satellite Hyperspectral Image Segmentation"
                },
                "summary": "Hyperspectral imaging (HSI) captures detailed spectral signatures across\nhundreds of contiguous bands per pixel, being indispensable for remote sensing\napplications such as land-cover classification, change detection, and\nenvironmental monitoring. Due to the high dimensionality of HSI data and the\nslow rate of data transfer in satellite-based systems, compact and efficient\nmodels are required to support onboard processing and minimize the transmission\nof redundant or low-value data, e.g. cloud-covered areas. To this end, we\nintroduce a novel curriculum multi-task self-supervised learning (CMTSSL)\nframework designed for lightweight architectures for HSI analysis. CMTSSL\nintegrates masked image modeling with decoupled spatial and spectral jigsaw\npuzzle solving, guided by a curriculum learning strategy that progressively\nincreases data complexity during self-supervision. This enables the encoder to\njointly capture fine-grained spectral continuity, spatial structure, and global\nsemantic features. Unlike prior dual-task SSL methods, CMTSSL simultaneously\naddresses spatial and spectral reasoning within a unified and computationally\nefficient design, being particularly suitable for training lightweight models\nfor onboard satellite deployment. We validate our approach on four public\nbenchmark datasets, demonstrating consistent gains in downstream segmentation\ntasks, using architectures that are over 16,000x lighter than some\nstate-of-the-art models. These results highlight the potential of CMTSSL in\ngeneralizable representation learning with lightweight architectures for\nreal-world HSI applications. Our code is publicly available at\nhttps://github.com/hugocarlesso/CMTSSL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperspectral imaging (HSI) captures detailed spectral signatures across\nhundreds of contiguous bands per pixel, being indispensable for remote sensing\napplications such as land-cover classification, change detection, and\nenvironmental monitoring. Due to the high dimensionality of HSI data and the\nslow rate of data transfer in satellite-based systems, compact and efficient\nmodels are required to support onboard processing and minimize the transmission\nof redundant or low-value data, e.g. cloud-covered areas. To this end, we\nintroduce a novel curriculum multi-task self-supervised learning (CMTSSL)\nframework designed for lightweight architectures for HSI analysis. CMTSSL\nintegrates masked image modeling with decoupled spatial and spectral jigsaw\npuzzle solving, guided by a curriculum learning strategy that progressively\nincreases data complexity during self-supervision. This enables the encoder to\njointly capture fine-grained spectral continuity, spatial structure, and global\nsemantic features. Unlike prior dual-task SSL methods, CMTSSL simultaneously\naddresses spatial and spectral reasoning within a unified and computationally\nefficient design, being particularly suitable for training lightweight models\nfor onboard satellite deployment. We validate our approach on four public\nbenchmark datasets, demonstrating consistent gains in downstream segmentation\ntasks, using architectures that are over 16,000x lighter than some\nstate-of-the-art models. These results highlight the potential of CMTSSL in\ngeneralizable representation learning with lightweight architectures for\nreal-world HSI applications. Our code is publicly available at\nhttps://github.com/hugocarlesso/CMTSSL."
                },
                "authors": [
                    {
                        "name": "Hugo Carlesso"
                    },
                    {
                        "name": "Josiane Mothe"
                    },
                    {
                        "name": "Radu Tudor Ionescu"
                    }
                ],
                "author_detail": {
                    "name": "Radu Tudor Ionescu"
                },
                "author": "Radu Tudor Ionescu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13229v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13201v1",
                "updated": "2025-09-16T16:06:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    6,
                    33,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T16:06:33Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    6,
                    33,
                    1,
                    259,
                    0
                ],
                "title": "Scaling Up Throughput-oriented LLM Inference Applications on\n  Heterogeneous Opportunistic GPU Clusters with Pervasive Context Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up Throughput-oriented LLM Inference Applications on\n  Heterogeneous Opportunistic GPU Clusters with Pervasive Context Management"
                },
                "summary": "The widespread growth in LLM developments increasingly demands more\ncomputational power from clusters than what they can supply. Traditional LLM\napplications inherently require huge static resource allocations, which force\nusers to either wait in a long job queue and accept progress delay, or buy\nexpensive hardware to fulfill their needs and exacerbate the demand-supply\nproblem. However, not all LLM applications are latency-sensitive and can\ninstead be executed in a throughput-oriented way. This throughput orientation\nallows a dynamic allocation that opportunistically pools available resources\nover time, avoiding both the long queue and expensive GPU purchases.\nEffectively utilizing opportunistic resources brings numerous challenges\nnevertheless. Our solution, pervasive context management, exploits the common\ncomputational context in LLM applications and provides mechanisms and policies\nthat allow seamless context reuse on opportunistic resources. Our evaluation\nshows an LLM application with pervasive context management on opportunistic\nresources reduces its execution time by 98.1%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread growth in LLM developments increasingly demands more\ncomputational power from clusters than what they can supply. Traditional LLM\napplications inherently require huge static resource allocations, which force\nusers to either wait in a long job queue and accept progress delay, or buy\nexpensive hardware to fulfill their needs and exacerbate the demand-supply\nproblem. However, not all LLM applications are latency-sensitive and can\ninstead be executed in a throughput-oriented way. This throughput orientation\nallows a dynamic allocation that opportunistically pools available resources\nover time, avoiding both the long queue and expensive GPU purchases.\nEffectively utilizing opportunistic resources brings numerous challenges\nnevertheless. Our solution, pervasive context management, exploits the common\ncomputational context in LLM applications and provides mechanisms and policies\nthat allow seamless context reuse on opportunistic resources. Our evaluation\nshows an LLM application with pervasive context management on opportunistic\nresources reduces its execution time by 98.1%."
                },
                "authors": [
                    {
                        "name": "Thanh Son Phung"
                    },
                    {
                        "name": "Douglas Thain"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Thain"
                },
                "author": "Douglas Thain",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13196v1",
                "updated": "2025-09-16T16:00:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    0,
                    6,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T16:00:06Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    0,
                    6,
                    1,
                    259,
                    0
                ],
                "title": "The Few-shot Dilemma: Over-prompting Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Few-shot Dilemma: Over-prompting Large Language Models"
                },
                "summary": "Over-prompting, a phenomenon where excessive examples in prompts lead to\ndiminished performance in Large Language Models (LLMs), challenges the\nconventional wisdom about in-context few-shot learning. To investigate this\nfew-shot dilemma, we outline a prompting framework that leverages three\nstandard few-shot selection methods - random sampling, semantic embedding, and\nTF-IDF vectors - and evaluate these methods across multiple LLMs, including\nGPT-4o, GPT-3.5-turbo, DeepSeek-V3, Gemma-3, LLaMA-3.1, LLaMA-3.2, and Mistral.\nOur experimental results reveal that incorporating excessive domain-specific\nexamples into prompts can paradoxically degrade performance in certain LLMs,\nwhich contradicts the prior empirical conclusion that more relevant few-shot\nexamples universally benefit LLMs. Given the trend of LLM-assisted software\nengineering and requirement analysis, we experiment with two real-world\nsoftware requirement classification datasets. By gradually increasing the\nnumber of TF-IDF-selected and stratified few-shot examples, we identify their\noptimal quantity for each LLM. This combined approach achieves superior\nperformance with fewer examples, avoiding the over-prompting problem, thus\nsurpassing the state-of-the-art by 1% in classifying functional and\nnon-functional requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over-prompting, a phenomenon where excessive examples in prompts lead to\ndiminished performance in Large Language Models (LLMs), challenges the\nconventional wisdom about in-context few-shot learning. To investigate this\nfew-shot dilemma, we outline a prompting framework that leverages three\nstandard few-shot selection methods - random sampling, semantic embedding, and\nTF-IDF vectors - and evaluate these methods across multiple LLMs, including\nGPT-4o, GPT-3.5-turbo, DeepSeek-V3, Gemma-3, LLaMA-3.1, LLaMA-3.2, and Mistral.\nOur experimental results reveal that incorporating excessive domain-specific\nexamples into prompts can paradoxically degrade performance in certain LLMs,\nwhich contradicts the prior empirical conclusion that more relevant few-shot\nexamples universally benefit LLMs. Given the trend of LLM-assisted software\nengineering and requirement analysis, we experiment with two real-world\nsoftware requirement classification datasets. By gradually increasing the\nnumber of TF-IDF-selected and stratified few-shot examples, we identify their\noptimal quantity for each LLM. This combined approach achieves superior\nperformance with fewer examples, avoiding the over-prompting problem, thus\nsurpassing the state-of-the-art by 1% in classifying functional and\nnon-functional requirements."
                },
                "authors": [
                    {
                        "name": "Yongjian Tang"
                    },
                    {
                        "name": "Doruk Tuncel"
                    },
                    {
                        "name": "Christian Koerner"
                    },
                    {
                        "name": "Thomas Runkler"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Runkler"
                },
                "author": "Thomas Runkler",
                "arxiv_comment": "accepted for the main track of FLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13187v1",
                "updated": "2025-09-16T15:41:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    41,
                    21,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T15:41:21Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    41,
                    21,
                    1,
                    259,
                    0
                ],
                "title": "Enhancement of torque transmission capability in Magneto-Rheological\n  fluid-based Clutch using novel hybrid corrugated plane transmission surface\n  strategy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancement of torque transmission capability in Magneto-Rheological\n  fluid-based Clutch using novel hybrid corrugated plane transmission surface\n  strategy"
                },
                "summary": "In an increased automated world, miniaturization is the key to widespread\ndeployment of advanced technologies. Enhancing the torque transmissibility by\nabiding to the spatial constraints imposed by radial space availability has\nconsistently remained a hurdle in the implementation of Magneto-Rheological\n(MR) clutches that use shear mode of MR fluid (MRF). This proves the necessity\nof a novel design capable of providing required transmission capability with a\nreduced transmission surface area. The present study analyzes a corrugated\ntransmissible surface design which improves torque transmissibility with the\nhelp of increased transmission area and proper alignment of field lines passing\nthrough the MRF gap. In this paper, the impact of various dimensional\nparameters of a hybrid corrugated plane type MR clutch (MRC) design was studied\nwith the aid of magnetic analysis performed on COMSOL Multiphysics software.\nThe results obtained shows that various parameters in the design of MR\nclutches, such as annular and radial MR gaps, disc width, individual\ncorrugation heights, corrugation width, bobbin thickness and radii of plane\nsurface influences the torque transmission capability of MR clutches. Also, an\noptimization of the hybrid corrugated plane MR Clutch of the chosen geometry\nhas been conducted with the transmission capability increasing by 39.37%\ncompared with the non-optimized geometrical configuration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an increased automated world, miniaturization is the key to widespread\ndeployment of advanced technologies. Enhancing the torque transmissibility by\nabiding to the spatial constraints imposed by radial space availability has\nconsistently remained a hurdle in the implementation of Magneto-Rheological\n(MR) clutches that use shear mode of MR fluid (MRF). This proves the necessity\nof a novel design capable of providing required transmission capability with a\nreduced transmission surface area. The present study analyzes a corrugated\ntransmissible surface design which improves torque transmissibility with the\nhelp of increased transmission area and proper alignment of field lines passing\nthrough the MRF gap. In this paper, the impact of various dimensional\nparameters of a hybrid corrugated plane type MR clutch (MRC) design was studied\nwith the aid of magnetic analysis performed on COMSOL Multiphysics software.\nThe results obtained shows that various parameters in the design of MR\nclutches, such as annular and radial MR gaps, disc width, individual\ncorrugation heights, corrugation width, bobbin thickness and radii of plane\nsurface influences the torque transmission capability of MR clutches. Also, an\noptimization of the hybrid corrugated plane MR Clutch of the chosen geometry\nhas been conducted with the transmission capability increasing by 39.37%\ncompared with the non-optimized geometrical configuration."
                },
                "authors": [
                    {
                        "name": "Jithin Vijaykumar"
                    },
                    {
                        "name": "Loyad Joseph Losan"
                    },
                    {
                        "name": "Saddala Reddy Tharun"
                    },
                    {
                        "name": "Murthi Ram Chandra Reddy"
                    },
                    {
                        "name": "Mood Rahul"
                    },
                    {
                        "name": "Jagadeesha T"
                    }
                ],
                "author_detail": {
                    "name": "Jagadeesha T"
                },
                "author": "Jagadeesha T",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13186v1",
                "updated": "2025-09-16T15:39:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    39,
                    23,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T15:39:23Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    39,
                    23,
                    1,
                    259,
                    0
                ],
                "title": "Characterizing Phishing Pages by JavaScript Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing Phishing Pages by JavaScript Capabilities"
                },
                "summary": "In 2024, the Anti-Phishing Work Group identified over one million phishing\npages. Phishers achieve this scale by using phishing kits -- ready-to-deploy\nphishing websites -- to rapidly deploy phishing campaigns with specific data\nexfiltration, evasion, or mimicry techniques. In contrast, researchers and\ndefenders continue to fight phishing on a page-by-page basis and rely on manual\nanalysis to recognize static features for kit identification.\n  This paper aims to aid researchers and analysts by automatically\ndifferentiating groups of phishing pages based on the underlying kit,\nautomating a previously manual process, and enabling us to measure how popular\ndifferent client-side techniques are across these groups. For kit detection,\nour system has an accuracy of 97% on a ground-truth dataset of 548 kit families\ndeployed across 4,562 phishing URLs. On an unlabeled dataset, we leverage the\ncomplexity of 434,050 phishing pages' JavaScript logic to group them into\n11,377 clusters, annotating the clusters with what phishing techniques they\nemploy. We find that UI interactivity and basic fingerprinting are universal\ntechniques, present in 90% and 80% of the clusters, respectively. On the other\nhand, mouse detection via the browser's mouse API is among the rarest\nbehaviors, despite being used in a deployment of a 7-year-old open-source\nphishing kit. Our methods and findings provide new ways for researchers and\nanalysts to tackle the volume of phishing pages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In 2024, the Anti-Phishing Work Group identified over one million phishing\npages. Phishers achieve this scale by using phishing kits -- ready-to-deploy\nphishing websites -- to rapidly deploy phishing campaigns with specific data\nexfiltration, evasion, or mimicry techniques. In contrast, researchers and\ndefenders continue to fight phishing on a page-by-page basis and rely on manual\nanalysis to recognize static features for kit identification.\n  This paper aims to aid researchers and analysts by automatically\ndifferentiating groups of phishing pages based on the underlying kit,\nautomating a previously manual process, and enabling us to measure how popular\ndifferent client-side techniques are across these groups. For kit detection,\nour system has an accuracy of 97% on a ground-truth dataset of 548 kit families\ndeployed across 4,562 phishing URLs. On an unlabeled dataset, we leverage the\ncomplexity of 434,050 phishing pages' JavaScript logic to group them into\n11,377 clusters, annotating the clusters with what phishing techniques they\nemploy. We find that UI interactivity and basic fingerprinting are universal\ntechniques, present in 90% and 80% of the clusters, respectively. On the other\nhand, mouse detection via the browser's mouse API is among the rarest\nbehaviors, despite being used in a deployment of a 7-year-old open-source\nphishing kit. Our methods and findings provide new ways for researchers and\nanalysts to tackle the volume of phishing pages."
                },
                "authors": [
                    {
                        "name": "Aleksandr Nahapetyan"
                    },
                    {
                        "name": "Kanv Khare"
                    },
                    {
                        "name": "Kevin Schwarz"
                    },
                    {
                        "name": "Bradley Reaves"
                    },
                    {
                        "name": "Alexandros Kapravelos"
                    }
                ],
                "author_detail": {
                    "name": "Alexandros Kapravelos"
                },
                "author": "Alexandros Kapravelos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13181v1",
                "updated": "2025-09-16T15:34:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    34,
                    43,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T15:34:43Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    34,
                    43,
                    1,
                    259,
                    0
                ],
                "title": "Road Obstacle Video Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Road Obstacle Video Segmentation"
                },
                "summary": "With the growing deployment of autonomous driving agents, the detection and\nsegmentation of road obstacles have become critical to ensure safe autonomous\nnavigation. However, existing road-obstacle segmentation methods are applied on\nindividual frames, overlooking the temporal nature of the problem, leading to\ninconsistent prediction maps between consecutive frames. In this work, we\ndemonstrate that the road-obstacle segmentation task is inherently temporal,\nsince the segmentation maps for consecutive frames are strongly correlated. To\naddress this, we curate and adapt four evaluation benchmarks for road-obstacle\nvideo segmentation and evaluate 11 state-of-the-art image- and video-based\nsegmentation methods on these benchmarks. Moreover, we introduce two strong\nbaseline methods based on vision foundation models. Our approach establishes a\nnew state-of-the-art in road-obstacle video segmentation for long-range video\nsequences, providing valuable insights and direction for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing deployment of autonomous driving agents, the detection and\nsegmentation of road obstacles have become critical to ensure safe autonomous\nnavigation. However, existing road-obstacle segmentation methods are applied on\nindividual frames, overlooking the temporal nature of the problem, leading to\ninconsistent prediction maps between consecutive frames. In this work, we\ndemonstrate that the road-obstacle segmentation task is inherently temporal,\nsince the segmentation maps for consecutive frames are strongly correlated. To\naddress this, we curate and adapt four evaluation benchmarks for road-obstacle\nvideo segmentation and evaluate 11 state-of-the-art image- and video-based\nsegmentation methods on these benchmarks. Moreover, we introduce two strong\nbaseline methods based on vision foundation models. Our approach establishes a\nnew state-of-the-art in road-obstacle video segmentation for long-range video\nsequences, providing valuable insights and direction for future research."
                },
                "authors": [
                    {
                        "name": "Shyam Nandan Rai"
                    },
                    {
                        "name": "Shyamgopal Karthik"
                    },
                    {
                        "name": "Mariana-Iuliana Georgescu"
                    },
                    {
                        "name": "Barbara Caputo"
                    },
                    {
                        "name": "Carlo Masone"
                    },
                    {
                        "name": "Zeynep Akata"
                    }
                ],
                "author_detail": {
                    "name": "Zeynep Akata"
                },
                "author": "Zeynep Akata",
                "arxiv_comment": "GCPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13886v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13886v4",
                "updated": "2025-09-16T15:33:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    33,
                    16,
                    1,
                    259,
                    0
                ],
                "published": "2025-05-20T03:47:44Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    3,
                    47,
                    44,
                    1,
                    140,
                    0
                ],
                "title": "Game-RL: Synthesizing Verifiable Game Tasks at Scale to Boost VLMs\n  General Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Game-RL: Synthesizing Verifiable Game Tasks at Scale to Boost VLMs\n  General Reasoning"
                },
                "summary": "Real-world vision language reasoning scenarios often include diverse and\ncomplex tasks. However, vision language reinforcement learning has primarily\nfocused on a narrow set of tasks (e.g. geometry or chart reasoning), limiting\nthe improvement of Vision Language Models' (VLMs) general reasoning. Therefore,\nwe propose a novel Code2Logic approach, using Large Language Models (LLMs) to\nsynthesize verifiable game reasoning tasks at scale via adapting game code.\nUsing the Code2Logic, we developed the GameQA dataset to train and evaluate\nVLMs. GameQA is verifiable and scalable, offers controllable difficulty\ngradation and is diverse with 30 games and 158 tasks. Then we apply Game-RL,\nwhich is simple reinforcement learning on GameQA. Surprisingly, despite\ntraining solely on game tasks, VLMs demonstrated out of domain generalization,\nspecifically Qwen2.5-VL-7B improving performance by 2.33% across 7 diverse\nvision-language benchmarks. Our code, dataset and models are available at the\nGitHub repository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world vision language reasoning scenarios often include diverse and\ncomplex tasks. However, vision language reinforcement learning has primarily\nfocused on a narrow set of tasks (e.g. geometry or chart reasoning), limiting\nthe improvement of Vision Language Models' (VLMs) general reasoning. Therefore,\nwe propose a novel Code2Logic approach, using Large Language Models (LLMs) to\nsynthesize verifiable game reasoning tasks at scale via adapting game code.\nUsing the Code2Logic, we developed the GameQA dataset to train and evaluate\nVLMs. GameQA is verifiable and scalable, offers controllable difficulty\ngradation and is diverse with 30 games and 158 tasks. Then we apply Game-RL,\nwhich is simple reinforcement learning on GameQA. Surprisingly, despite\ntraining solely on game tasks, VLMs demonstrated out of domain generalization,\nspecifically Qwen2.5-VL-7B improving performance by 2.33% across 7 diverse\nvision-language benchmarks. Our code, dataset and models are available at the\nGitHub repository."
                },
                "authors": [
                    {
                        "name": "Jingqi Tong"
                    },
                    {
                        "name": "Jixin Tang"
                    },
                    {
                        "name": "Hangcheng Li"
                    },
                    {
                        "name": "Yurong Mou"
                    },
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Yanbo Wen"
                    },
                    {
                        "name": "Fan Song"
                    },
                    {
                        "name": "Jiahao Zhan"
                    },
                    {
                        "name": "Yuyang Lu"
                    },
                    {
                        "name": "Chaoran Tao"
                    },
                    {
                        "name": "Zhiyuan Guo"
                    },
                    {
                        "name": "Jizhou Yu"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Changhao Jiang"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Tao Liang"
                    },
                    {
                        "name": "Zhihui Fei"
                    },
                    {
                        "name": "Mingyang Wan"
                    },
                    {
                        "name": "Guojun Ma"
                    },
                    {
                        "name": "Weifeng Ge"
                    },
                    {
                        "name": "Guanhua Chen"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "63 pages, 23 figures, submitted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13886v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13886v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13179v1",
                "updated": "2025-09-16T15:32:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    32,
                    51,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T15:32:51Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    32,
                    51,
                    1,
                    259,
                    0
                ],
                "title": "Efficient Cold-Start Recommendation via BPE Token-Level Embedding\n  Initialization with LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Cold-Start Recommendation via BPE Token-Level Embedding\n  Initialization with LLM"
                },
                "summary": "The cold-start issue is the challenge when we talk about recommender systems,\nespecially in the case when we do not have the past interaction data of new\nusers or new items. Content-based features or hybrid solutions are common as\nconventional solutions, but they can only work in a sparse metadata environment\nwith shallow patterns. In this paper, the efficient cold-start recommendation\nstrategy is presented, which is based on the sub word-level representations by\napplying Byte Pair Encoding (BPE) tokenization and pre-trained Large Language\nModel (LLM) embedding in the initialization procedure. We obtain fine-grained\ntoken-level vectors that are aligned with the BPE vocabulary as opposed to\nusing coarse-grained sentence embeddings. Together, these token embeddings can\nbe used as dense semantic priors on unseen entities, making immediate\nrecommendation performance possible without user-item interaction history. Our\nmechanism can be compared to collaborative filtering systems and tested over\nbenchmark datasets with stringent cold-start assumptions. Experimental findings\nshow that the given BPE-LLM method achieves higher Recall@k, NDCG@k, and Hit\nRate measurements compared to the standard baseline and displays the same\ncapability of sufficient computational performance. Furthermore, we demonstrate\nthat using subword-aware embeddings yields better generalizability and is more\ninterpretable, especially within a multilingual and sparse input setting. The\npractical application of token-level semantic initialization as a lightweight,\nbut nevertheless effective extension to modern recommender systems in the\nzero-shot setting is indicated within this work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cold-start issue is the challenge when we talk about recommender systems,\nespecially in the case when we do not have the past interaction data of new\nusers or new items. Content-based features or hybrid solutions are common as\nconventional solutions, but they can only work in a sparse metadata environment\nwith shallow patterns. In this paper, the efficient cold-start recommendation\nstrategy is presented, which is based on the sub word-level representations by\napplying Byte Pair Encoding (BPE) tokenization and pre-trained Large Language\nModel (LLM) embedding in the initialization procedure. We obtain fine-grained\ntoken-level vectors that are aligned with the BPE vocabulary as opposed to\nusing coarse-grained sentence embeddings. Together, these token embeddings can\nbe used as dense semantic priors on unseen entities, making immediate\nrecommendation performance possible without user-item interaction history. Our\nmechanism can be compared to collaborative filtering systems and tested over\nbenchmark datasets with stringent cold-start assumptions. Experimental findings\nshow that the given BPE-LLM method achieves higher Recall@k, NDCG@k, and Hit\nRate measurements compared to the standard baseline and displays the same\ncapability of sufficient computational performance. Furthermore, we demonstrate\nthat using subword-aware embeddings yields better generalizability and is more\ninterpretable, especially within a multilingual and sparse input setting. The\npractical application of token-level semantic initialization as a lightweight,\nbut nevertheless effective extension to modern recommender systems in the\nzero-shot setting is indicated within this work."
                },
                "authors": [
                    {
                        "name": "Yushang Zhao"
                    },
                    {
                        "name": "Xinyue Han"
                    },
                    {
                        "name": "Qian Leng"
                    },
                    {
                        "name": "Qianyi Sun"
                    },
                    {
                        "name": "Haotian Lyu"
                    },
                    {
                        "name": "Chengrui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Chengrui Zhou"
                },
                "author": "Chengrui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13175v1",
                "updated": "2025-09-16T15:27:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    27,
                    14,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T15:27:14Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    27,
                    14,
                    1,
                    259,
                    0
                ],
                "title": "More performant and scalable: Rethinking contrastive vision-language\n  pre-training of radiology in the LLM era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More performant and scalable: Rethinking contrastive vision-language\n  pre-training of radiology in the LLM era"
                },
                "summary": "The emergence of Large Language Models (LLMs) presents unprecedented\nopportunities to revolutionize medical contrastive vision-language\npre-training. In this paper, we show how LLMs can facilitate large-scale\nsupervised pre-training, thereby advancing vision-language alignment. We begin\nby demonstrate that modern LLMs can automatically extract diagnostic labels\nfrom radiology reports with remarkable precision (>96\\% AUC in our experiments)\nwithout complex prompt engineering, enabling the creation of large-scale\n\"silver-standard\" datasets at a minimal cost (~\\$3 for 50k CT image-report\npairs). Further, we find that vision encoder trained on this \"silver-standard\"\ndataset achieves performance comparable to those trained on labels extracted by\nspecialized BERT-based models, thereby democratizing the access to large-scale\nsupervised pre-training. Building on this foundation, we proceed to reveal that\nsupervised pre-training fundamentally improves contrastive vision-language\nalignment. Our approach achieves state-of-the-art performance using only a 3D\nResNet-18 with vanilla CLIP training, including 83.8\\% AUC for zero-shot\ndiagnosis on CT-RATE, 77.3\\% AUC on RAD-ChestCT, and substantial improvements\nin cross-modal retrieval (MAP@50=53.7\\% for image-image, Recall@100=52.2\\% for\nreport-image). These results demonstrate the potential of utilizing LLMs to\nfacilitate {\\bf more performant and scalable} medical AI systems. Our code is\navaiable at https://github.com/SadVoxel/More-performant-and-scalable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Large Language Models (LLMs) presents unprecedented\nopportunities to revolutionize medical contrastive vision-language\npre-training. In this paper, we show how LLMs can facilitate large-scale\nsupervised pre-training, thereby advancing vision-language alignment. We begin\nby demonstrate that modern LLMs can automatically extract diagnostic labels\nfrom radiology reports with remarkable precision (>96\\% AUC in our experiments)\nwithout complex prompt engineering, enabling the creation of large-scale\n\"silver-standard\" datasets at a minimal cost (~\\$3 for 50k CT image-report\npairs). Further, we find that vision encoder trained on this \"silver-standard\"\ndataset achieves performance comparable to those trained on labels extracted by\nspecialized BERT-based models, thereby democratizing the access to large-scale\nsupervised pre-training. Building on this foundation, we proceed to reveal that\nsupervised pre-training fundamentally improves contrastive vision-language\nalignment. Our approach achieves state-of-the-art performance using only a 3D\nResNet-18 with vanilla CLIP training, including 83.8\\% AUC for zero-shot\ndiagnosis on CT-RATE, 77.3\\% AUC on RAD-ChestCT, and substantial improvements\nin cross-modal retrieval (MAP@50=53.7\\% for image-image, Recall@100=52.2\\% for\nreport-image). These results demonstrate the potential of utilizing LLMs to\nfacilitate {\\bf more performant and scalable} medical AI systems. Our code is\navaiable at https://github.com/SadVoxel/More-performant-and-scalable."
                },
                "authors": [
                    {
                        "name": "Yingtai Li"
                    },
                    {
                        "name": "Haoran Lai"
                    },
                    {
                        "name": "Xiaoqian Zhou"
                    },
                    {
                        "name": "Shuai Ming"
                    },
                    {
                        "name": "Wenxin Ma"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Shaohua Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Shaohua Kevin Zhou"
                },
                "author": "Shaohua Kevin Zhou",
                "arxiv_comment": "MICCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10407v2",
                "updated": "2025-09-16T15:24:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    24,
                    32,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-12T16:58:20Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    58,
                    20,
                    4,
                    255,
                    0
                ],
                "title": "Compressed Video Quality Enhancement: Classifying and Benchmarking over\n  Standards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressed Video Quality Enhancement: Classifying and Benchmarking over\n  Standards"
                },
                "summary": "Compressed video quality enhancement (CVQE) is crucial for improving user\nexperience with lossy video codecs like H.264/AVC, H.265/HEVC, and H.266/VVC.\nWhile deep learning based CVQE has driven significant progress, existing\nsurveys still suffer from limitations: lack of systematic classification\nlinking methods to specific standards and artifacts, insufficient comparative\nanalysis of architectural paradigms across coding types, and underdeveloped\nbenchmarking practices. To address these gaps, this paper presents three key\ncontributions. First, it introduces a novel taxonomy classifying CVQE methods\nacross architectural paradigms, coding standards, and compressed-domain feature\nutilization. Second, it proposes a unified benchmarking framework integrating\nmodern compression protocols and standard test sequences for fair\nmulti-criteria evaluation. Third, it provides a systematic analysis of the\ncritical trade-offs between reconstruction performance and computational\ncomplexity observed in state-of-the-art methods and highlighting promising\ndirections for future research. This comprehensive review aims to establish a\nfoundation for consistent assessment and informed model selection in CVQE\nresearch and deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressed video quality enhancement (CVQE) is crucial for improving user\nexperience with lossy video codecs like H.264/AVC, H.265/HEVC, and H.266/VVC.\nWhile deep learning based CVQE has driven significant progress, existing\nsurveys still suffer from limitations: lack of systematic classification\nlinking methods to specific standards and artifacts, insufficient comparative\nanalysis of architectural paradigms across coding types, and underdeveloped\nbenchmarking practices. To address these gaps, this paper presents three key\ncontributions. First, it introduces a novel taxonomy classifying CVQE methods\nacross architectural paradigms, coding standards, and compressed-domain feature\nutilization. Second, it proposes a unified benchmarking framework integrating\nmodern compression protocols and standard test sequences for fair\nmulti-criteria evaluation. Third, it provides a systematic analysis of the\ncritical trade-offs between reconstruction performance and computational\ncomplexity observed in state-of-the-art methods and highlighting promising\ndirections for future research. This comprehensive review aims to establish a\nfoundation for consistent assessment and informed model selection in CVQE\nresearch and deployment."
                },
                "authors": [
                    {
                        "name": "Xiem HoangVan"
                    },
                    {
                        "name": "Dang BuiDinh"
                    },
                    {
                        "name": "Sang NguyenQuang"
                    },
                    {
                        "name": "Wen-Hsiao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Hsiao Peng"
                },
                "author": "Wen-Hsiao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13172v1",
                "updated": "2025-09-16T15:23:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    23,
                    40,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T15:23:40Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    23,
                    40,
                    1,
                    259,
                    0
                ],
                "title": "WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory"
                },
                "summary": "Street trees are vital to urban livability, providing ecological and social\nbenefits. Establishing a detailed, accurate, and dynamically updated street\ntree inventory has become essential for optimizing these multifunctional assets\nwithin space-constrained urban environments. Given that traditional field\nsurveys are time-consuming and labor-intensive, automated surveys utilizing\nMobile Mapping Systems (MMS) offer a more efficient solution. However, existing\nMMS-acquired tree datasets are limited by small-scale scene, limited\nannotation, or single modality, restricting their utility for comprehensive\nanalysis. To address these limitations, we introduce WHU-STree, a cross-city,\nrichly annotated, and multi-modal urban street tree dataset. Collected across\ntwo distinct cities, WHU-STree integrates synchronized point clouds and\nhigh-resolution images, encompassing 21,007 annotated tree instances across 50\nspecies and 2 morphological parameters. Leveraging the unique characteristics,\nWHU-STree concurrently supports over 10 tasks related to street tree inventory.\nWe benchmark representative baselines for two key tasks--tree species\nclassification and individual tree segmentation. Extensive experiments and\nin-depth analysis demonstrate the significant potential of multi-modal data\nfusion and underscore cross-domain applicability as a critical prerequisite for\npractical algorithm deployment. In particular, we identify key challenges and\noutline potential future works for fully exploiting WHU-STree, encompassing\nmulti-modal fusion, multi-task collaboration, cross-domain generalization,\nspatial pattern learning, and Multi-modal Large Language Model for street tree\nasset management. The WHU-STree dataset is accessible at:\nhttps://github.com/WHU-USI3DV/WHU-STree.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Street trees are vital to urban livability, providing ecological and social\nbenefits. Establishing a detailed, accurate, and dynamically updated street\ntree inventory has become essential for optimizing these multifunctional assets\nwithin space-constrained urban environments. Given that traditional field\nsurveys are time-consuming and labor-intensive, automated surveys utilizing\nMobile Mapping Systems (MMS) offer a more efficient solution. However, existing\nMMS-acquired tree datasets are limited by small-scale scene, limited\nannotation, or single modality, restricting their utility for comprehensive\nanalysis. To address these limitations, we introduce WHU-STree, a cross-city,\nrichly annotated, and multi-modal urban street tree dataset. Collected across\ntwo distinct cities, WHU-STree integrates synchronized point clouds and\nhigh-resolution images, encompassing 21,007 annotated tree instances across 50\nspecies and 2 morphological parameters. Leveraging the unique characteristics,\nWHU-STree concurrently supports over 10 tasks related to street tree inventory.\nWe benchmark representative baselines for two key tasks--tree species\nclassification and individual tree segmentation. Extensive experiments and\nin-depth analysis demonstrate the significant potential of multi-modal data\nfusion and underscore cross-domain applicability as a critical prerequisite for\npractical algorithm deployment. In particular, we identify key challenges and\noutline potential future works for fully exploiting WHU-STree, encompassing\nmulti-modal fusion, multi-task collaboration, cross-domain generalization,\nspatial pattern learning, and Multi-modal Large Language Model for street tree\nasset management. The WHU-STree dataset is accessible at:\nhttps://github.com/WHU-USI3DV/WHU-STree."
                },
                "authors": [
                    {
                        "name": "Ruifei Ding"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Wen Fan"
                    },
                    {
                        "name": "Chen Long"
                    },
                    {
                        "name": "Huijuan Xiao"
                    },
                    {
                        "name": "Yelu Zeng"
                    },
                    {
                        "name": "Zhen Dong"
                    },
                    {
                        "name": "Bisheng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Bisheng Yang"
                },
                "author": "Bisheng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11683v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11683v5",
                "updated": "2025-09-16T15:19:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    19,
                    32,
                    1,
                    259,
                    0
                ],
                "published": "2024-11-18T16:09:26Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    9,
                    26,
                    0,
                    323,
                    0
                ],
                "title": "TrojanRobot: Physical-world Backdoor Attacks Against VLM-based Robotic\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrojanRobot: Physical-world Backdoor Attacks Against VLM-based Robotic\n  Manipulation"
                },
                "summary": "Robotic manipulation in the physical world is increasingly empowered by\n\\textit{large language models} (LLMs) and \\textit{vision-language models}\n(VLMs), leveraging their understanding and perception capabilities. Recently,\nvarious attacks against such robotic policies have been proposed, with backdoor\nattacks drawing considerable attention for their high stealth and strong\npersistence capabilities. However, existing backdoor efforts are limited to\nsimulators and suffer from physical-world realization. To address this, we\npropose \\textit{TrojanRobot}, a highly stealthy and broadly effective robotic\nbackdoor attack in the physical world. Specifically, we introduce a\nmodule-poisoning approach by embedding a backdoor module into the modular\nrobotic policy, enabling backdoor control over the policy's visual perception\nmodule thereby backdooring the entire robotic policy. Our vanilla\nimplementation leverages a backdoor-finetuned VLM to serve as the backdoor\nmodule. To enhance its generalization in physical environments, we propose a\nprime implementation, leveraging the LVLM-as-a-backdoor paradigm and developing\nthree types of prime attacks, \\ie, \\textit{permutation}, \\textit{stagnation},\nand \\textit{intentional} attacks, thus achieving finer-grained backdoors.\nExtensive experiments on the UR3e manipulator with 18 task instructions using\nrobotic policies based on four VLMs demonstrate the broad effectiveness and\nphysical-world stealth of TrojanRobot. Our attack's video demonstrations are\navailable via a github link https://trojanrobot.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic manipulation in the physical world is increasingly empowered by\n\\textit{large language models} (LLMs) and \\textit{vision-language models}\n(VLMs), leveraging their understanding and perception capabilities. Recently,\nvarious attacks against such robotic policies have been proposed, with backdoor\nattacks drawing considerable attention for their high stealth and strong\npersistence capabilities. However, existing backdoor efforts are limited to\nsimulators and suffer from physical-world realization. To address this, we\npropose \\textit{TrojanRobot}, a highly stealthy and broadly effective robotic\nbackdoor attack in the physical world. Specifically, we introduce a\nmodule-poisoning approach by embedding a backdoor module into the modular\nrobotic policy, enabling backdoor control over the policy's visual perception\nmodule thereby backdooring the entire robotic policy. Our vanilla\nimplementation leverages a backdoor-finetuned VLM to serve as the backdoor\nmodule. To enhance its generalization in physical environments, we propose a\nprime implementation, leveraging the LVLM-as-a-backdoor paradigm and developing\nthree types of prime attacks, \\ie, \\textit{permutation}, \\textit{stagnation},\nand \\textit{intentional} attacks, thus achieving finer-grained backdoors.\nExtensive experiments on the UR3e manipulator with 18 task instructions using\nrobotic policies based on four VLMs demonstrate the broad effectiveness and\nphysical-world stealth of TrojanRobot. Our attack's video demonstrations are\navailable via a github link https://trojanrobot.github.io."
                },
                "authors": [
                    {
                        "name": "Xianlong Wang"
                    },
                    {
                        "name": "Hewen Pan"
                    },
                    {
                        "name": "Hangtao Zhang"
                    },
                    {
                        "name": "Minghui Li"
                    },
                    {
                        "name": "Shengshan Hu"
                    },
                    {
                        "name": "Ziqi Zhou"
                    },
                    {
                        "name": "Lulu Xue"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Yunpeng Jiang"
                    },
                    {
                        "name": "Leo Yu Zhang"
                    },
                    {
                        "name": "Xiaohua Jia"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohua Jia"
                },
                "author": "Xiaohua Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11683v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11683v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13164v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13164v2",
                "updated": "2025-09-17T04:40:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    4,
                    40,
                    30,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-16T15:14:44Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    14,
                    44,
                    1,
                    259,
                    0
                ],
                "title": "TeraSim-World: Worldwide Safety-Critical Data Synthesis for End-to-End\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeraSim-World: Worldwide Safety-Critical Data Synthesis for End-to-End\n  Autonomous Driving"
                },
                "summary": "Safe and scalable deployment of end-to-end (E2E) autonomous driving requires\nextensive and diverse data, particularly safety-critical events. Existing data\nare mostly generated from simulators with a significant sim-to-real gap or\ncollected from on-road testing that is costly and unsafe. This paper presents\nTeraSim-World, an automated pipeline that synthesizes realistic and\ngeographically diverse safety-critical data for E2E autonomous driving at\nanywhere in the world. Starting from an arbitrary location, TeraSim-World\nretrieves real-world maps and traffic demand from geospatial data sources.\nThen, it simulates agent behaviors from naturalistic driving datasets, and\norchestrates diverse adversities to create corner cases. Informed by street\nviews of the same location, it achieves photorealistic, geographically grounded\nsensor rendering via the frontier video generation model Cosmos-Drive. By\nbridging agent and sensor simulations, TeraSim-World provides a scalable and\ncritical data synthesis framework for training and evaluation of E2E autonomous\ndriving systems. Codes and videos are available at\nhttps://wjiawei.com/terasim-world-web/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe and scalable deployment of end-to-end (E2E) autonomous driving requires\nextensive and diverse data, particularly safety-critical events. Existing data\nare mostly generated from simulators with a significant sim-to-real gap or\ncollected from on-road testing that is costly and unsafe. This paper presents\nTeraSim-World, an automated pipeline that synthesizes realistic and\ngeographically diverse safety-critical data for E2E autonomous driving at\nanywhere in the world. Starting from an arbitrary location, TeraSim-World\nretrieves real-world maps and traffic demand from geospatial data sources.\nThen, it simulates agent behaviors from naturalistic driving datasets, and\norchestrates diverse adversities to create corner cases. Informed by street\nviews of the same location, it achieves photorealistic, geographically grounded\nsensor rendering via the frontier video generation model Cosmos-Drive. By\nbridging agent and sensor simulations, TeraSim-World provides a scalable and\ncritical data synthesis framework for training and evaluation of E2E autonomous\ndriving systems. Codes and videos are available at\nhttps://wjiawei.com/terasim-world-web/ ."
                },
                "authors": [
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Haowei Sun"
                    },
                    {
                        "name": "Xintao Yan"
                    },
                    {
                        "name": "Shuo Feng"
                    },
                    {
                        "name": "Jun Gao"
                    },
                    {
                        "name": "Henry X. Liu"
                    }
                ],
                "author_detail": {
                    "name": "Henry X. Liu"
                },
                "author": "Henry X. Liu",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13164v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13164v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13160v1",
                "updated": "2025-09-16T15:13:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    13,
                    13,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T15:13:13Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    13,
                    13,
                    1,
                    259,
                    0
                ],
                "title": "FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial\n  Search and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial\n  Search and Reasoning"
                },
                "summary": "Search has emerged as core infrastructure for LLM-based agents and is widely\nviewed as critical on the path toward more general intelligence. Finance is a\nparticularly demanding proving ground: analysts routinely conduct complex,\nmulti-step searches over time-sensitive, domain-specific data, making it ideal\nfor assessing both search proficiency and knowledge-grounded reasoning. Yet no\nexisting open financial datasets evaluate data searching capability of\nend-to-end agents, largely because constructing realistic, complicated tasks\nrequires deep financial expertise and time-sensitive data is hard to evaluate.\nWe present FinSearchComp, the first fully open-source agent benchmark for\nrealistic, open-domain financial search and reasoning. FinSearchComp comprises\nthree tasks -- Time-Sensitive Data Fetching, Simple Historical Lookup, and\nComplex Historical Investigation -- closely reproduce real-world financial\nanalyst workflows. To ensure difficulty and reliability, we engage 70\nprofessional financial experts for annotation and implement a rigorous\nmulti-stage quality-assurance pipeline. The benchmark includes 635 questions\nspanning global and Greater China markets, and we evaluate 21 models (products)\non it. Grok 4 (web) tops the global subset, approaching expert-level accuracy.\nDouBao (web) leads on the Greater China subset. Experimental analyses show that\nequipping agents with web search and financial plugins substantially improves\nresults on FinSearchComp, and the country origin of models and tools impact\nperformance significantly.By aligning with realistic analyst tasks and\nproviding end-to-end evaluation, FinSearchComp offers a professional,\nhigh-difficulty testbed for complex financial search and reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search has emerged as core infrastructure for LLM-based agents and is widely\nviewed as critical on the path toward more general intelligence. Finance is a\nparticularly demanding proving ground: analysts routinely conduct complex,\nmulti-step searches over time-sensitive, domain-specific data, making it ideal\nfor assessing both search proficiency and knowledge-grounded reasoning. Yet no\nexisting open financial datasets evaluate data searching capability of\nend-to-end agents, largely because constructing realistic, complicated tasks\nrequires deep financial expertise and time-sensitive data is hard to evaluate.\nWe present FinSearchComp, the first fully open-source agent benchmark for\nrealistic, open-domain financial search and reasoning. FinSearchComp comprises\nthree tasks -- Time-Sensitive Data Fetching, Simple Historical Lookup, and\nComplex Historical Investigation -- closely reproduce real-world financial\nanalyst workflows. To ensure difficulty and reliability, we engage 70\nprofessional financial experts for annotation and implement a rigorous\nmulti-stage quality-assurance pipeline. The benchmark includes 635 questions\nspanning global and Greater China markets, and we evaluate 21 models (products)\non it. Grok 4 (web) tops the global subset, approaching expert-level accuracy.\nDouBao (web) leads on the Greater China subset. Experimental analyses show that\nequipping agents with web search and financial plugins substantially improves\nresults on FinSearchComp, and the country origin of models and tools impact\nperformance significantly.By aligning with realistic analyst tasks and\nproviding end-to-end evaluation, FinSearchComp offers a professional,\nhigh-difficulty testbed for complex financial search and reasoning."
                },
                "authors": [
                    {
                        "name": "Liang Hu"
                    },
                    {
                        "name": "Jianpeng Jiao"
                    },
                    {
                        "name": "Jiashuo Liu"
                    },
                    {
                        "name": "Yanle Ren"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Kaiyuan Zhang"
                    },
                    {
                        "name": "Xuanliang Zhang"
                    },
                    {
                        "name": "Xiang Gao"
                    },
                    {
                        "name": "Tianci He"
                    },
                    {
                        "name": "Fei Hu"
                    },
                    {
                        "name": "Yali Liao"
                    },
                    {
                        "name": "Zaiyuan Wang"
                    },
                    {
                        "name": "Chenghao Yang"
                    },
                    {
                        "name": "Qianyu Yang"
                    },
                    {
                        "name": "Mingren Yin"
                    },
                    {
                        "name": "Zhiyuan Zeng"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Xinyi Zhang"
                    },
                    {
                        "name": "Xiying Zhao"
                    },
                    {
                        "name": "Zhenwei Zhu"
                    },
                    {
                        "name": "Hongseok Namkoong"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Yuwen Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yuwen Tang"
                },
                "author": "Yuwen Tang",
                "arxiv_comment": "29 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14335v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14335v3",
                "updated": "2025-09-16T15:11:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    11,
                    20,
                    1,
                    259,
                    0
                ],
                "published": "2025-06-17T09:17:41Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    9,
                    17,
                    41,
                    1,
                    168,
                    0
                ],
                "title": "References Matter: Investigating the Impact of Reference Set Variation\n  on Summarization Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "References Matter: Investigating the Impact of Reference Set Variation\n  on Summarization Evaluation"
                },
                "summary": "Human language production exhibits remarkable richness and variation,\nreflecting diverse communication styles and intents. However, this variation is\noften overlooked in summarization evaluation. While having multiple reference\nsummaries is known to improve correlation with human judgments, the impact of\nthe reference set on reference-based metrics has not been systematically\ninvestigated. This work examines the sensitivity of widely used reference-based\nmetrics in relation to the choice of reference sets, analyzing three diverse\nmulti-reference summarization datasets: SummEval, GUMSum, and DUC2004. We\ndemonstrate that many popular metrics exhibit significant instability. This\ninstability is particularly concerning for n-gram-based metrics like ROUGE,\nwhere model rankings vary depending on the reference sets, undermining the\nreliability of model comparisons. We also collect human judgments on LLM\noutputs for genre-diverse data and examine their correlation with metrics to\nsupplement existing findings beyond newswire summaries, finding weak-to-no\ncorrelation. Taken together, we recommend incorporating reference set variation\ninto summarization evaluation to enhance consistency alongside correlation with\nhuman judgments, especially when evaluating LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human language production exhibits remarkable richness and variation,\nreflecting diverse communication styles and intents. However, this variation is\noften overlooked in summarization evaluation. While having multiple reference\nsummaries is known to improve correlation with human judgments, the impact of\nthe reference set on reference-based metrics has not been systematically\ninvestigated. This work examines the sensitivity of widely used reference-based\nmetrics in relation to the choice of reference sets, analyzing three diverse\nmulti-reference summarization datasets: SummEval, GUMSum, and DUC2004. We\ndemonstrate that many popular metrics exhibit significant instability. This\ninstability is particularly concerning for n-gram-based metrics like ROUGE,\nwhere model rankings vary depending on the reference sets, undermining the\nreliability of model comparisons. We also collect human judgments on LLM\noutputs for genre-diverse data and examine their correlation with metrics to\nsupplement existing findings beyond newswire summaries, finding weak-to-no\ncorrelation. Taken together, we recommend incorporating reference set variation\ninto summarization evaluation to enhance consistency alongside correlation with\nhuman judgments, especially when evaluating LLMs."
                },
                "authors": [
                    {
                        "name": "Silvia Casola"
                    },
                    {
                        "name": "Yang Janet Liu"
                    },
                    {
                        "name": "Siyao Peng"
                    },
                    {
                        "name": "Oliver Kraus"
                    },
                    {
                        "name": "Albert Gatt"
                    },
                    {
                        "name": "Barbara Plank"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Plank"
                },
                "author": "Barbara Plank",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14335v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14335v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13154v1",
                "updated": "2025-09-16T15:08:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    8,
                    19,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T15:08:19Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    8,
                    19,
                    1,
                    259,
                    0
                ],
                "title": "LLM Hallucination Detection: A Fast Fourier Transform Method Based on\n  Hidden Layer Temporal Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Hallucination Detection: A Fast Fourier Transform Method Based on\n  Hidden Layer Temporal Signals"
                },
                "summary": "Hallucination remains a critical barrier for deploying large language models\n(LLMs) in reliability-sensitive applications. Existing detection methods\nlargely fall into two categories: factuality checking, which is fundamentally\nconstrained by external knowledge coverage, and static hidden-state analysis,\nthat fails to capture deviations in reasoning dynamics. As a result, their\neffectiveness and robustness remain limited. We propose HSAD (Hidden Signal\nAnalysis-based Detection), a novel hallucination detection framework that\nmodels the temporal dynamics of hidden representations during autoregressive\ngeneration. HSAD constructs hidden-layer signals by sampling activations across\nlayers, applies Fast Fourier Transform (FFT) to obtain frequency-domain\nrepresentations, and extracts the strongest non-DC frequency component as\nspectral features. Furthermore, by leveraging the autoregressive nature of\nLLMs, HSAD identifies optimal observation points for effective and reliable\ndetection. Across multiple benchmarks, including TruthfulQA, HSAD achieves over\n10 percentage points improvement compared to prior state-of-the-art methods. By\nintegrating reasoning-process modeling with frequency-domain analysis, HSAD\nestablishes a new paradigm for robust hallucination detection in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination remains a critical barrier for deploying large language models\n(LLMs) in reliability-sensitive applications. Existing detection methods\nlargely fall into two categories: factuality checking, which is fundamentally\nconstrained by external knowledge coverage, and static hidden-state analysis,\nthat fails to capture deviations in reasoning dynamics. As a result, their\neffectiveness and robustness remain limited. We propose HSAD (Hidden Signal\nAnalysis-based Detection), a novel hallucination detection framework that\nmodels the temporal dynamics of hidden representations during autoregressive\ngeneration. HSAD constructs hidden-layer signals by sampling activations across\nlayers, applies Fast Fourier Transform (FFT) to obtain frequency-domain\nrepresentations, and extracts the strongest non-DC frequency component as\nspectral features. Furthermore, by leveraging the autoregressive nature of\nLLMs, HSAD identifies optimal observation points for effective and reliable\ndetection. Across multiple benchmarks, including TruthfulQA, HSAD achieves over\n10 percentage points improvement compared to prior state-of-the-art methods. By\nintegrating reasoning-process modeling with frequency-domain analysis, HSAD\nestablishes a new paradigm for robust hallucination detection in LLMs."
                },
                "authors": [
                    {
                        "name": "Jinxin Li"
                    },
                    {
                        "name": "Gang Tu"
                    },
                    {
                        "name": "ShengYu Cheng"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Jinting Wang"
                    },
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Zhilong Zhou"
                    },
                    {
                        "name": "Dongbo Shan"
                    }
                ],
                "author_detail": {
                    "name": "Dongbo Shan"
                },
                "author": "Dongbo Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13144v1",
                "updated": "2025-09-16T15:01:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    1,
                    23,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T15:01:23Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    15,
                    1,
                    23,
                    1,
                    259,
                    0
                ],
                "title": "Towards the Next Generation of Software: Insights from Grey Literature\n  on AI-Native Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards the Next Generation of Software: Insights from Grey Literature\n  on AI-Native Applications"
                },
                "summary": "Background: The rapid advancement of large language models (LLMs) has given\nrise to AI-native applications, a new paradigm in software engineering that\nfundamentally redefines how software is designed, developed, and evolved.\nDespite their growing prominence, AI-native applications still lack a unified\nengineering definition and architectural blueprint, leaving practitioners\nwithout systematic guidance for system design, quality assurance, and\ntechnology selection.\n  Objective: This study seeks to establish a comprehensive understanding of\nAI-native applications by identifying their defining characteristics, key\nquality attributes, and typical technology stacks, as well as by clarifying the\nopportunities and challenges they present.\n  Method: We conducted a grey literature review, integrating conceptual\nperspectives retrieved from targeted Google and Bing searches with practical\ninsights derived from leading open-source projects on GitHub. A structured\nprotocol encompassing source selection, quality assessment, and thematic\nanalysis was applied to synthesize findings across heterogeneous sources.\n  Results: We finally identified 106 studies based on the selection criteria.\nThe analysis reveals that AI-native applications are distinguished by two core\npillars: the central role of AI as the system's intelligence paradigm and their\ninherently probabilistic, non-deterministic nature. Critical quality attributes\ninclude reliability, usability, performance efficiency, and AI-specific\nobservability. In addition, a typical technology stack has begun to emerge,\ncomprising LLM orchestration frameworks, vector databases, and AI-native\nobservability platforms. These systems emphasize response quality,\ncost-effectiveness, and outcome predictability, setting them apart from\nconventional software systems.\n  Conclusion: This study is the first to propose a dual-layered engineering\nblueprint...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: The rapid advancement of large language models (LLMs) has given\nrise to AI-native applications, a new paradigm in software engineering that\nfundamentally redefines how software is designed, developed, and evolved.\nDespite their growing prominence, AI-native applications still lack a unified\nengineering definition and architectural blueprint, leaving practitioners\nwithout systematic guidance for system design, quality assurance, and\ntechnology selection.\n  Objective: This study seeks to establish a comprehensive understanding of\nAI-native applications by identifying their defining characteristics, key\nquality attributes, and typical technology stacks, as well as by clarifying the\nopportunities and challenges they present.\n  Method: We conducted a grey literature review, integrating conceptual\nperspectives retrieved from targeted Google and Bing searches with practical\ninsights derived from leading open-source projects on GitHub. A structured\nprotocol encompassing source selection, quality assessment, and thematic\nanalysis was applied to synthesize findings across heterogeneous sources.\n  Results: We finally identified 106 studies based on the selection criteria.\nThe analysis reveals that AI-native applications are distinguished by two core\npillars: the central role of AI as the system's intelligence paradigm and their\ninherently probabilistic, non-deterministic nature. Critical quality attributes\ninclude reliability, usability, performance efficiency, and AI-specific\nobservability. In addition, a typical technology stack has begun to emerge,\ncomprising LLM orchestration frameworks, vector databases, and AI-native\nobservability platforms. These systems emphasize response quality,\ncost-effectiveness, and outcome predictability, setting them apart from\nconventional software systems.\n  Conclusion: This study is the first to propose a dual-layered engineering\nblueprint..."
                },
                "authors": [
                    {
                        "name": "Lingli Cao"
                    },
                    {
                        "name": "Shanshan Li"
                    },
                    {
                        "name": "Ying Fan"
                    },
                    {
                        "name": "Danyang Li"
                    },
                    {
                        "name": "Chenxing Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Chenxing Zhong"
                },
                "author": "Chenxing Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13137v1",
                "updated": "2025-09-16T14:53:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    14,
                    53,
                    51,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T14:53:51Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    14,
                    53,
                    51,
                    1,
                    259,
                    0
                ],
                "title": "Agentic AI for Financial Crime Compliance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI for Financial Crime Compliance"
                },
                "summary": "The cost and complexity of financial crime compliance (FCC) continue to rise,\noften without measurable improvements in effectiveness. While AI offers\npotential, most solutions remain opaque and poorly aligned with regulatory\nexpectations. This paper presents the design and deployment of an agentic AI\nsystem for FCC in digitally native financial platforms. Developed through an\nAction Design Research (ADR) process with a fintech firm and regulatory\nstakeholders, the system automates onboarding, monitoring, investigation, and\nreporting, emphasizing explainability, traceability, and compliance-by-design.\nUsing artifact-centric modeling, it assigns clearly bounded roles to autonomous\nagents and enables task-specific model routing and audit logging. The\ncontribution includes a reference architecture, a real-world prototype, and\ninsights into how Agentic AI can reconfigure FCC workflows under regulatory\nconstraints. Our findings extend IS literature on AI-enabled compliance by\ndemonstrating how automation, when embedded within accountable governance\nstructures, can support transparency and institutional trust in high-stakes,\nregulated environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cost and complexity of financial crime compliance (FCC) continue to rise,\noften without measurable improvements in effectiveness. While AI offers\npotential, most solutions remain opaque and poorly aligned with regulatory\nexpectations. This paper presents the design and deployment of an agentic AI\nsystem for FCC in digitally native financial platforms. Developed through an\nAction Design Research (ADR) process with a fintech firm and regulatory\nstakeholders, the system automates onboarding, monitoring, investigation, and\nreporting, emphasizing explainability, traceability, and compliance-by-design.\nUsing artifact-centric modeling, it assigns clearly bounded roles to autonomous\nagents and enables task-specific model routing and audit logging. The\ncontribution includes a reference architecture, a real-world prototype, and\ninsights into how Agentic AI can reconfigure FCC workflows under regulatory\nconstraints. Our findings extend IS literature on AI-enabled compliance by\ndemonstrating how automation, when embedded within accountable governance\nstructures, can support transparency and institutional trust in high-stakes,\nregulated environments."
                },
                "authors": [
                    {
                        "name": "Henrik Axelsen"
                    },
                    {
                        "name": "Valdemar Licht"
                    },
                    {
                        "name": "Jan Damsgaard"
                    }
                ],
                "author_detail": {
                    "name": "Jan Damsgaard"
                },
                "author": "Jan Damsgaard",
                "arxiv_comment": "Accepted for presentation at HICSS-59 (2026), forthcoming in\n  Proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.4.4; K.6.5; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13131v1",
                "updated": "2025-09-16T14:48:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    14,
                    48,
                    46,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T14:48:46Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    14,
                    48,
                    46,
                    1,
                    259,
                    0
                ],
                "title": "Reasoning with Preference Constraints: A Benchmark for Language Models\n  in Many-to-One Matching Markets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning with Preference Constraints: A Benchmark for Language Models\n  in Many-to-One Matching Markets"
                },
                "summary": "Recent advances in reasoning with large language models (LLMs) have\ndemonstrated strong performance on complex mathematical tasks, including\ncombinatorial optimization. Techniques such as Chain-of-Thought and In-Context\nLearning have further enhanced this capability, making LLMs both powerful and\naccessible tools for a wide range of users, including non-experts. However,\napplying LLMs to matching problems, which require reasoning under preferential\nand structural constraints, remains underexplored. To address this gap, we\nintroduce a novel benchmark of 369 instances of the College Admission Problem,\na canonical example of a matching problem with preferences, to evaluate LLMs\nacross key dimensions: feasibility, stability, and optimality. We employ this\nbenchmark to assess the performance of several open-weight LLMs. Our results\nfirst reveal that while LLMs can satisfy certain constraints, they struggle to\nmeet all evaluation criteria consistently. They also show that reasoning LLMs,\nlike QwQ and GPT-oss, significantly outperform traditional models such as\nLlama, Qwen or Mistral, defined here as models used without any dedicated\nreasoning mechanisms. Moreover, we observed that LLMs reacted differently to\nthe various prompting strategies tested, which include Chain-of-Thought,\nIn-Context Learning and role-based prompting, with no prompt consistently\noffering the best performance. Finally, we report the performances from\niterative prompting with auto-generated feedback and show that they are not\nmonotonic; they can peak early and then significantly decline in later\nattempts. Overall, this work offers a new perspective on model reasoning\nperformance and the effectiveness of prompting strategies in combinatorial\noptimization problems with preferential constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reasoning with large language models (LLMs) have\ndemonstrated strong performance on complex mathematical tasks, including\ncombinatorial optimization. Techniques such as Chain-of-Thought and In-Context\nLearning have further enhanced this capability, making LLMs both powerful and\naccessible tools for a wide range of users, including non-experts. However,\napplying LLMs to matching problems, which require reasoning under preferential\nand structural constraints, remains underexplored. To address this gap, we\nintroduce a novel benchmark of 369 instances of the College Admission Problem,\na canonical example of a matching problem with preferences, to evaluate LLMs\nacross key dimensions: feasibility, stability, and optimality. We employ this\nbenchmark to assess the performance of several open-weight LLMs. Our results\nfirst reveal that while LLMs can satisfy certain constraints, they struggle to\nmeet all evaluation criteria consistently. They also show that reasoning LLMs,\nlike QwQ and GPT-oss, significantly outperform traditional models such as\nLlama, Qwen or Mistral, defined here as models used without any dedicated\nreasoning mechanisms. Moreover, we observed that LLMs reacted differently to\nthe various prompting strategies tested, which include Chain-of-Thought,\nIn-Context Learning and role-based prompting, with no prompt consistently\noffering the best performance. Finally, we report the performances from\niterative prompting with auto-generated feedback and show that they are not\nmonotonic; they can peak early and then significantly decline in later\nattempts. Overall, this work offers a new perspective on model reasoning\nperformance and the effectiveness of prompting strategies in combinatorial\noptimization problems with preferential constraints."
                },
                "authors": [
                    {
                        "name": "Marylou Fauchard"
                    },
                    {
                        "name": "Florian Carichon"
                    },
                    {
                        "name": "Margarida Carvalho"
                    },
                    {
                        "name": "Golnoosh Farnadi"
                    }
                ],
                "author_detail": {
                    "name": "Golnoosh Farnadi"
                },
                "author": "Golnoosh Farnadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11867v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11867v2",
                "updated": "2025-09-16T14:40:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    14,
                    40,
                    49,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-15T12:39:51Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    39,
                    51,
                    0,
                    258,
                    0
                ],
                "title": "Letter of Intent: AICE -- 100m Atom Interferometer Experiment at CERN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Letter of Intent: AICE -- 100m Atom Interferometer Experiment at CERN"
                },
                "summary": "We propose an O(100)m Atom Interferometer (AI) experiment -- AICE -- to be\ninstalled against a wall of the PX46 access shaft to the LHC. This experiment\nwould probe unexplored ranges of the possible couplings of bosonic ultralight\ndark matter (ULDM) to atomic constituents and undertake a pioneering search for\ngravitational waves (GWs) at frequencies intermediate between those to which\nexisting and planned experiments are sensitive, among other fundamental physics\nstudies. A conceptual feasibility study showed that this AI experiment could be\nisolated from the LHC by installing a shielding wall in the TX46 gallery, and\nsurveyed issues related to the proximity of the LHC machine, finding no\ntechnical obstacles. A detailed technical implementation study has shown that\nthe preparatory civil-engineering work, installation of bespoke radiation\nshielding, deployment of access-control systems and safety alarms, and\ninstallation of an elevator platform could be carried out during LS3, allowing\ninstallation and operation of the AICE detector to proceed during Run 4 without\nimpacting HL-LHC operation. These studies have established that PX46 is a\nuniquely promising location for an AI experiment. We foresee that, if the CERN\nmanagement encourages this Letter of Intent, a significant fraction of the\nTerrestrial Very Long Baseline Atom Interferometer (TVLBAI) Proto-Collaboration\nmay wish to contribute to AICE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an O(100)m Atom Interferometer (AI) experiment -- AICE -- to be\ninstalled against a wall of the PX46 access shaft to the LHC. This experiment\nwould probe unexplored ranges of the possible couplings of bosonic ultralight\ndark matter (ULDM) to atomic constituents and undertake a pioneering search for\ngravitational waves (GWs) at frequencies intermediate between those to which\nexisting and planned experiments are sensitive, among other fundamental physics\nstudies. A conceptual feasibility study showed that this AI experiment could be\nisolated from the LHC by installing a shielding wall in the TX46 gallery, and\nsurveyed issues related to the proximity of the LHC machine, finding no\ntechnical obstacles. A detailed technical implementation study has shown that\nthe preparatory civil-engineering work, installation of bespoke radiation\nshielding, deployment of access-control systems and safety alarms, and\ninstallation of an elevator platform could be carried out during LS3, allowing\ninstallation and operation of the AICE detector to proceed during Run 4 without\nimpacting HL-LHC operation. These studies have established that PX46 is a\nuniquely promising location for an AI experiment. We foresee that, if the CERN\nmanagement encourages this Letter of Intent, a significant fraction of the\nTerrestrial Very Long Baseline Atom Interferometer (TVLBAI) Proto-Collaboration\nmay wish to contribute to AICE."
                },
                "authors": [
                    {
                        "name": "Charles Baynham"
                    },
                    {
                        "name": "Andrea Bertoldi"
                    },
                    {
                        "name": "Diego Blas"
                    },
                    {
                        "name": "Oliver Buchmueller"
                    },
                    {
                        "name": "Sergio Calatroni"
                    },
                    {
                        "name": "Vassilis Charmandaris"
                    },
                    {
                        "name": "Maria Luisa Chiofalo"
                    },
                    {
                        "name": "Pierre CladÃ©"
                    },
                    {
                        "name": "Jonathon Coleman"
                    },
                    {
                        "name": "Fabio Di Pumpo"
                    },
                    {
                        "name": "John Ellis"
                    },
                    {
                        "name": "Naceur Gaaloul"
                    },
                    {
                        "name": "SaÃ¯da Guellati-Khelifa"
                    },
                    {
                        "name": "Tiffany Harte"
                    },
                    {
                        "name": "Richard Hobson"
                    },
                    {
                        "name": "Michael Holynski"
                    },
                    {
                        "name": "Samuel Lellouch"
                    },
                    {
                        "name": "Lucas Lombriser"
                    },
                    {
                        "name": "Elias Lopez Asamar"
                    },
                    {
                        "name": "Michele Maggiore"
                    },
                    {
                        "name": "Christopher McCabe"
                    },
                    {
                        "name": "Jeremiah Mitchell"
                    },
                    {
                        "name": "Ernst M. Rasel"
                    },
                    {
                        "name": "Federico Sanchez Nieto"
                    },
                    {
                        "name": "Wolfgang Schleich"
                    },
                    {
                        "name": "Dennis Schlippert"
                    },
                    {
                        "name": "Ulrich Schneider"
                    },
                    {
                        "name": "Steven Schramm"
                    },
                    {
                        "name": "Marcelle Soares-Santos"
                    },
                    {
                        "name": "Guglielmo M. Tino"
                    },
                    {
                        "name": "Jonathan N. Tinsley"
                    },
                    {
                        "name": "Tristan Valenzuela"
                    },
                    {
                        "name": "Maurits van der Grinten"
                    },
                    {
                        "name": "Wolf von Klitzing"
                    }
                ],
                "author_detail": {
                    "name": "Wolf von Klitzing"
                },
                "author": "Wolf von Klitzing",
                "arxiv_comment": "16 pages, including figures and appendices. Submitted to CERN LHCC as\n  Letter of Intent. arXiv admin note: substantial text overlap with\n  arXiv:2304.00614",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11867v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11867v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13127v1",
                "updated": "2025-09-16T14:36:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    14,
                    36,
                    30,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T14:36:30Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    14,
                    36,
                    30,
                    1,
                    259,
                    0
                ],
                "title": "Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon\n  Planning"
                },
                "summary": "Recent advancements in Large Language Models(LLMs) have led to the\ndevelopment of LLM-based AI agents. A key challenge is the creation of agents\nthat can effectively ground themselves in complex, adversarial long-horizon\nenvironments. Existing methods mainly focus on (1) using LLMs as policies to\ninteract with the environment through generating low-level feasible actions,\nand (2) utilizing LLMs to generate high-level tasks or language guides to\nstimulate action generation. However, the former struggles to generate reliable\nactions, while the latter relies heavily on expert experience to translate\nhigh-level tasks into specific action sequences. To address these challenges,\nwe introduce the Plan with Language, Act with Parameter (PLAP) planning\nframework that facilitates the grounding of LLM-based agents in long-horizon\nenvironments. The PLAP method comprises three key components: (1) a skill\nlibrary containing environment-specific parameterized skills, (2) a skill\nplanner powered by LLMs, and (3) a skill executor converting the parameterized\nskills into executable action sequences. We implement PLAP in MicroRTS, a\nlong-horizon real-time strategy game that provides an unfamiliar and\nchallenging environment for LLMs. The experimental results demonstrate the\neffectiveness of PLAP. In particular, GPT-4o-driven PLAP in a zero-shot setting\noutperforms 80% of baseline agents, and Qwen2-72B-driven PLAP, with carefully\ncrafted few-shot examples, surpasses the top-tier scripted agent, CoacAI.\nAdditionally, we design comprehensive evaluation metrics and test 6\nclosed-source and 2 open-source LLMs within the PLAP framework, ultimately\nreleasing an LLM leaderboard ranking long-horizon skill planning ability. Our\ncode is available at https://github.com/AI-Research-TeamX/PLAP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models(LLMs) have led to the\ndevelopment of LLM-based AI agents. A key challenge is the creation of agents\nthat can effectively ground themselves in complex, adversarial long-horizon\nenvironments. Existing methods mainly focus on (1) using LLMs as policies to\ninteract with the environment through generating low-level feasible actions,\nand (2) utilizing LLMs to generate high-level tasks or language guides to\nstimulate action generation. However, the former struggles to generate reliable\nactions, while the latter relies heavily on expert experience to translate\nhigh-level tasks into specific action sequences. To address these challenges,\nwe introduce the Plan with Language, Act with Parameter (PLAP) planning\nframework that facilitates the grounding of LLM-based agents in long-horizon\nenvironments. The PLAP method comprises three key components: (1) a skill\nlibrary containing environment-specific parameterized skills, (2) a skill\nplanner powered by LLMs, and (3) a skill executor converting the parameterized\nskills into executable action sequences. We implement PLAP in MicroRTS, a\nlong-horizon real-time strategy game that provides an unfamiliar and\nchallenging environment for LLMs. The experimental results demonstrate the\neffectiveness of PLAP. In particular, GPT-4o-driven PLAP in a zero-shot setting\noutperforms 80% of baseline agents, and Qwen2-72B-driven PLAP, with carefully\ncrafted few-shot examples, surpasses the top-tier scripted agent, CoacAI.\nAdditionally, we design comprehensive evaluation metrics and test 6\nclosed-source and 2 open-source LLMs within the PLAP framework, ultimately\nreleasing an LLM leaderboard ranking long-horizon skill planning ability. Our\ncode is available at https://github.com/AI-Research-TeamX/PLAP."
                },
                "authors": [
                    {
                        "name": "Sijia Cui"
                    },
                    {
                        "name": "Shuai Xu"
                    },
                    {
                        "name": "Aiyao He"
                    },
                    {
                        "name": "Yanna Wang"
                    },
                    {
                        "name": "Bo Xu"
                    }
                ],
                "author_detail": {
                    "name": "Bo Xu"
                },
                "author": "Bo Xu",
                "arxiv_comment": "Accepted to IJCNN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18436v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18436v3",
                "updated": "2025-09-16T14:24:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    14,
                    24,
                    5,
                    1,
                    259,
                    0
                ],
                "published": "2024-10-24T05:14:03Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    14,
                    3,
                    3,
                    298,
                    0
                ],
                "title": "Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case\n  Study on English-Korean Code-Switching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case\n  Study on English-Korean Code-Switching"
                },
                "summary": "Recent large language models (LLMs) demonstrate multilingual abilities, yet\nthey are English-centric due to dominance of English in training corpora. The\nlimited resource for low-resource languages remains a crucial challenge.\nCode-switching (CS), a phenomenon where multilingual speakers alternate between\nlanguages in a discourse, can convey subtle cultural and linguistic nuances\nthat can be otherwise lost in translation and elicits language-specific\nknowledge in human communications. In light of this, we investigate whether\ncode-switching can activate, or identify and leverage knowledge for reasoning\nwhen LLMs solve low-resource language tasks. To facilitate the research, we\nfirst present EnKoQA, a synthetic English-Korean CS question-answering dataset.\nWe provide comprehensive analysis on a variety of multilingual LLMs by\nsubdividing activation process into knowledge identification and knowledge\nleveraging. Our results demonstrate that compared to English text, CS can\nfaithfully activate knowledge inside LLMs especially on language-specific\ndomains, suggesting the potential of code-switching on low-resource language\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) demonstrate multilingual abilities, yet\nthey are English-centric due to dominance of English in training corpora. The\nlimited resource for low-resource languages remains a crucial challenge.\nCode-switching (CS), a phenomenon where multilingual speakers alternate between\nlanguages in a discourse, can convey subtle cultural and linguistic nuances\nthat can be otherwise lost in translation and elicits language-specific\nknowledge in human communications. In light of this, we investigate whether\ncode-switching can activate, or identify and leverage knowledge for reasoning\nwhen LLMs solve low-resource language tasks. To facilitate the research, we\nfirst present EnKoQA, a synthetic English-Korean CS question-answering dataset.\nWe provide comprehensive analysis on a variety of multilingual LLMs by\nsubdividing activation process into knowledge identification and knowledge\nleveraging. Our results demonstrate that compared to English text, CS can\nfaithfully activate knowledge inside LLMs especially on language-specific\ndomains, suggesting the potential of code-switching on low-resource language\ntasks."
                },
                "authors": [
                    {
                        "name": "Seoyeon Kim"
                    },
                    {
                        "name": "Huiseo Kim"
                    },
                    {
                        "name": "Chanjun Park"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18436v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18436v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13103v1",
                "updated": "2025-09-16T14:01:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    14,
                    1,
                    44,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T14:01:44Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    14,
                    1,
                    44,
                    1,
                    259,
                    0
                ],
                "title": "Accelerating Discovery: Rapid Literature Screening with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Discovery: Rapid Literature Screening with LLMs"
                },
                "summary": "Background: Conducting Multi Vocal Literature Reviews (MVLRs) is often time\nand effort-intensive. Researchers must review and filter a large number of\nunstructured sources, which frequently contain sparse information and are\nunlikely to be included in the final study. Our experience conducting an MVLR\non Context-Aware Software Systems (CASS) Testing in the avionics domain\nexemplified this challenge, with over 8,000 highly heterogeneous documents\nrequiring review. Therefore, we developed a Large Language Model (LLM)\nassistant to support the search and filtering of documents. Aims: To develop\nand validate an LLM based tool that can support researchers in performing the\nsearch and filtering of documents for an MVLR without compromising the rigor of\nthe research protocol. Method: We applied sound engineering practices to\ndevelop an on-premises LLM-based tool incorporating Retrieval Augmented\nGeneration (RAG) to process candidate sources. Progress towards the aim was\nquantified using the Positive Percent Agreement (PPA) as the primary metric to\nensure the performance of the LLM based tool. Convenience sampling, supported\nby human judgment and statistical sampling, were used to verify and validate\nthe tool's quality-in-use. Results: The tool currently demonstrates a PPA\nagreement with human researchers of 90% for sources that are not relevant to\nthe study. Development details are shared to support domain-specific adaptation\nof the tool. Conclusions: Using LLM-based tools to support academic researchers\nin rigorous MVLR is feasible. These tools can free valuable time for\nhigher-level, abstract tasks. However, researcher participation remains\nessential to ensure that the tool supports thorough research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Conducting Multi Vocal Literature Reviews (MVLRs) is often time\nand effort-intensive. Researchers must review and filter a large number of\nunstructured sources, which frequently contain sparse information and are\nunlikely to be included in the final study. Our experience conducting an MVLR\non Context-Aware Software Systems (CASS) Testing in the avionics domain\nexemplified this challenge, with over 8,000 highly heterogeneous documents\nrequiring review. Therefore, we developed a Large Language Model (LLM)\nassistant to support the search and filtering of documents. Aims: To develop\nand validate an LLM based tool that can support researchers in performing the\nsearch and filtering of documents for an MVLR without compromising the rigor of\nthe research protocol. Method: We applied sound engineering practices to\ndevelop an on-premises LLM-based tool incorporating Retrieval Augmented\nGeneration (RAG) to process candidate sources. Progress towards the aim was\nquantified using the Positive Percent Agreement (PPA) as the primary metric to\nensure the performance of the LLM based tool. Convenience sampling, supported\nby human judgment and statistical sampling, were used to verify and validate\nthe tool's quality-in-use. Results: The tool currently demonstrates a PPA\nagreement with human researchers of 90% for sources that are not relevant to\nthe study. Development details are shared to support domain-specific adaptation\nof the tool. Conclusions: Using LLM-based tools to support academic researchers\nin rigorous MVLR is feasible. These tools can free valuable time for\nhigher-level, abstract tasks. However, researcher participation remains\nessential to ensure that the tool supports thorough research."
                },
                "authors": [
                    {
                        "name": "Santiago Matalonga"
                    },
                    {
                        "name": "Domenico Amalfitano"
                    },
                    {
                        "name": "Jean Carlo Rossa Hauck"
                    },
                    {
                        "name": "MartÃ­n Solari"
                    },
                    {
                        "name": "Guilherme H. Travassos"
                    }
                ],
                "author_detail": {
                    "name": "Guilherme H. Travassos"
                },
                "author": "Guilherme H. Travassos",
                "arxiv_comment": "This version of the manuscript has been submitted to Empirical\n  Software Engieering Journal for consideration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10493v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10493v2",
                "updated": "2025-09-16T13:58:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    58,
                    52,
                    1,
                    259,
                    0
                ],
                "published": "2025-08-31T12:36:33Z",
                "published_parsed": [
                    2025,
                    8,
                    31,
                    12,
                    36,
                    33,
                    6,
                    243,
                    0
                ],
                "title": "Online Learning Based Efficient Resource Allocation for LoRaWAN Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Learning Based Efficient Resource Allocation for LoRaWAN Network"
                },
                "summary": "The deployment of large-scale LoRaWAN networks requires jointly optimizing\nconflicting metrics like Packet Delivery Ratio (PDR) and Energy Efficiency (EE)\nby dynamically allocating transmission parameters, including Carrier Frequency,\nSpreading Factor, and Transmission Power. Existing methods often oversimplify\nthis challenge, focusing on a single metric or lacking the adaptability needed\nfor dynamic channel environments, leading to suboptimal performance. To address\nthis, we propose two online learning-based resource allocation frameworks that\nintelligently navigate the PDR-EE trade-off. Our foundational proposal, D-LoRa,\nis a fully distributed framework that models the problem as a Combinatorial\nMulti-Armed Bandit. By decomposing the joint parameter selection and employing\nspecialized, disaggregated reward functions, D-LoRa dramatically reduces\nlearning complexity and enables nodes to autonomously adapt to network\ndynamics. To further enhance performance in LoRaWAN networks, we introduce\nCD-LoRa, a hybrid framework that integrates a lightweight, centralized\ninitialization phase to perform a one-time, quasi-optimal channel assignment\nand action space pruning, thereby accelerating subsequent distributed learning.\nExtensive simulations and real-world field experiments demonstrate the\nsuperiority of our frameworks, showing that D-LoRa excels in non-stationary\nenvironments while CD-LoRa achieves the fastest convergence in stationary\nconditions. In physical deployments, our methods outperform state-of-the-art\nbaselines, improving PDR by up to 10.8% and EE by 26.1%, confirming their\npractical effectiveness for scalable and efficient LoRaWAN networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large-scale LoRaWAN networks requires jointly optimizing\nconflicting metrics like Packet Delivery Ratio (PDR) and Energy Efficiency (EE)\nby dynamically allocating transmission parameters, including Carrier Frequency,\nSpreading Factor, and Transmission Power. Existing methods often oversimplify\nthis challenge, focusing on a single metric or lacking the adaptability needed\nfor dynamic channel environments, leading to suboptimal performance. To address\nthis, we propose two online learning-based resource allocation frameworks that\nintelligently navigate the PDR-EE trade-off. Our foundational proposal, D-LoRa,\nis a fully distributed framework that models the problem as a Combinatorial\nMulti-Armed Bandit. By decomposing the joint parameter selection and employing\nspecialized, disaggregated reward functions, D-LoRa dramatically reduces\nlearning complexity and enables nodes to autonomously adapt to network\ndynamics. To further enhance performance in LoRaWAN networks, we introduce\nCD-LoRa, a hybrid framework that integrates a lightweight, centralized\ninitialization phase to perform a one-time, quasi-optimal channel assignment\nand action space pruning, thereby accelerating subsequent distributed learning.\nExtensive simulations and real-world field experiments demonstrate the\nsuperiority of our frameworks, showing that D-LoRa excels in non-stationary\nenvironments while CD-LoRa achieves the fastest convergence in stationary\nconditions. In physical deployments, our methods outperform state-of-the-art\nbaselines, improving PDR by up to 10.8% and EE by 26.1%, confirming their\npractical effectiveness for scalable and efficient LoRaWAN networks."
                },
                "authors": [
                    {
                        "name": "Ruiqi Wang"
                    },
                    {
                        "name": "Wenjun Li"
                    },
                    {
                        "name": "Jing Ren"
                    },
                    {
                        "name": "Tongyu Song"
                    },
                    {
                        "name": "Xiong Wang"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Shizhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Shizhong Xu"
                },
                "author": "Shizhong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10493v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10493v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21740v2",
                "updated": "2025-09-16T13:55:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    55,
                    55,
                    1,
                    259,
                    0
                ],
                "published": "2025-05-27T20:29:50Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    20,
                    29,
                    50,
                    1,
                    147,
                    0
                ],
                "title": "Counterfactual Simulatability of LLM Explanations for Generation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual Simulatability of LLM Explanations for Generation Tasks"
                },
                "summary": "LLMs can be unpredictable, as even slight alterations to the prompt can cause\nthe output to change in unexpected ways. Thus, the ability of models to\naccurately explain their behavior is critical, especially in high-stakes\nsettings. One approach for evaluating explanations is counterfactual\nsimulatability, how well an explanation allows users to infer the model's\noutput on related counterfactuals. Counterfactual simulatability has been\npreviously studied for yes/no question answering tasks. We provide a general\nframework for extending this method to generation tasks, using news\nsummarization and medical suggestion as example use cases. We find that while\nLLM explanations do enable users to better predict LLM outputs on\ncounterfactuals in the summarization setting, there is significant room for\nimprovement for medical suggestion. Furthermore, our results suggest that the\nevaluation for counterfactual simulatability may be more appropriate for\nskill-based tasks as opposed to knowledge-based tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can be unpredictable, as even slight alterations to the prompt can cause\nthe output to change in unexpected ways. Thus, the ability of models to\naccurately explain their behavior is critical, especially in high-stakes\nsettings. One approach for evaluating explanations is counterfactual\nsimulatability, how well an explanation allows users to infer the model's\noutput on related counterfactuals. Counterfactual simulatability has been\npreviously studied for yes/no question answering tasks. We provide a general\nframework for extending this method to generation tasks, using news\nsummarization and medical suggestion as example use cases. We find that while\nLLM explanations do enable users to better predict LLM outputs on\ncounterfactuals in the summarization setting, there is significant room for\nimprovement for medical suggestion. Furthermore, our results suggest that the\nevaluation for counterfactual simulatability may be more appropriate for\nskill-based tasks as opposed to knowledge-based tasks."
                },
                "authors": [
                    {
                        "name": "Marvin Limpijankit"
                    },
                    {
                        "name": "Yanda Chen"
                    },
                    {
                        "name": "Melanie Subbiah"
                    },
                    {
                        "name": "Nicholas Deas"
                    },
                    {
                        "name": "Kathleen McKeown"
                    }
                ],
                "author_detail": {
                    "name": "Kathleen McKeown"
                },
                "author": "Kathleen McKeown",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13081v1",
                "updated": "2025-09-16T13:39:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    39,
                    29,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T13:39:29Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    39,
                    29,
                    1,
                    259,
                    0
                ],
                "title": "Shaping Explanations: Semantic Reward Modeling with Encoder-Only\n  Transformers for GRPO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shaping Explanations: Semantic Reward Modeling with Encoder-Only\n  Transformers for GRPO"
                },
                "summary": "While Large Language Models (LLMs) excel at generating human-like text,\naligning their outputs with complex, qualitative goals like pedagogical\nsoundness remains a significant challenge. Standard reinforcement learning\ntechniques often rely on slow and expensive LLM-as-a-judge evaluations or on\nbrittle, keyword-based metrics like ROUGE, which fail to capture the semantic\nessence of a high-quality explanation. In this work, we introduce a novel\napproach to reward shaping within the Group Relative Policy Optimisation (GRPO)\nframework. Our central contribution is the use of a small, efficient\nencoder-only transformer as a semantic reward model. This model provides a\ndense, semantically rich reward signal based on the cosine similarity between a\ngenerated explanation and a ground-truth reference, guiding the policy towards\nexplanations that are not just factually correct but also structurally and\nconceptually aligned with expert reasoning. We apply this method to the task of\ntraining a model for the Italian medical-school entrance examinations,\nfollowing standard domain-adaptive continued pre-training (CPT) and supervised\nfine-tuning (SFT). Our results demonstrate that GRPO with our proposed semantic\nreward significantly improves explanation faithfulness and clarity over a\nstrong SFT baseline, showcasing the power of using lightweight encoder models\nfor nuanced reward shaping in complex generation tasks",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) excel at generating human-like text,\naligning their outputs with complex, qualitative goals like pedagogical\nsoundness remains a significant challenge. Standard reinforcement learning\ntechniques often rely on slow and expensive LLM-as-a-judge evaluations or on\nbrittle, keyword-based metrics like ROUGE, which fail to capture the semantic\nessence of a high-quality explanation. In this work, we introduce a novel\napproach to reward shaping within the Group Relative Policy Optimisation (GRPO)\nframework. Our central contribution is the use of a small, efficient\nencoder-only transformer as a semantic reward model. This model provides a\ndense, semantically rich reward signal based on the cosine similarity between a\ngenerated explanation and a ground-truth reference, guiding the policy towards\nexplanations that are not just factually correct but also structurally and\nconceptually aligned with expert reasoning. We apply this method to the task of\ntraining a model for the Italian medical-school entrance examinations,\nfollowing standard domain-adaptive continued pre-training (CPT) and supervised\nfine-tuning (SFT). Our results demonstrate that GRPO with our proposed semantic\nreward significantly improves explanation faithfulness and clarity over a\nstrong SFT baseline, showcasing the power of using lightweight encoder models\nfor nuanced reward shaping in complex generation tasks"
                },
                "authors": [
                    {
                        "name": "Francesco Pappone"
                    },
                    {
                        "name": "Ruggero Marino Lazzaroni"
                    },
                    {
                        "name": "Federico Califano"
                    },
                    {
                        "name": "NiccolÃ² Gentile"
                    },
                    {
                        "name": "Roberto Marras"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Marras"
                },
                "author": "Roberto Marras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20409v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20409v3",
                "updated": "2025-09-16T13:36:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    36,
                    33,
                    1,
                    259,
                    0
                ],
                "published": "2025-06-25T13:24:46Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    24,
                    46,
                    2,
                    176,
                    0
                ],
                "title": "TAPS: Tool-Augmented Personalisation via Structured Tagging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAPS: Tool-Augmented Personalisation via Structured Tagging"
                },
                "summary": "Recent advancements in tool-augmented large language models have enabled them\nto interact with external tools, enhancing their ability to perform complex\nuser tasks. However, existing approaches overlook the role of personalisation\nin guiding tool use. This work investigates how user preferences can be\neffectively integrated into goal-oriented dialogue agents. Through extensive\nanalysis, we identify key weaknesses in the ability of LLMs to personalise tool\nuse. To this end, we introduce TAPS, a novel solution that enhances\npersonalised tool use by leveraging a structured tagging tool and an\nuncertainty-based tool detector. TAPS significantly improves the ability of\nLLMs to incorporate user preferences, achieving the new state-of-the-art for\nopen source models on the NLSI task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in tool-augmented large language models have enabled them\nto interact with external tools, enhancing their ability to perform complex\nuser tasks. However, existing approaches overlook the role of personalisation\nin guiding tool use. This work investigates how user preferences can be\neffectively integrated into goal-oriented dialogue agents. Through extensive\nanalysis, we identify key weaknesses in the ability of LLMs to personalise tool\nuse. To this end, we introduce TAPS, a novel solution that enhances\npersonalised tool use by leveraging a structured tagging tool and an\nuncertainty-based tool detector. TAPS significantly improves the ability of\nLLMs to incorporate user preferences, achieving the new state-of-the-art for\nopen source models on the NLSI task."
                },
                "authors": [
                    {
                        "name": "Ekaterina Taktasheva"
                    },
                    {
                        "name": "Jeff Dalton"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Dalton"
                },
                "author": "Jeff Dalton",
                "arxiv_comment": "Accepted to EMNLP 2026 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20409v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20409v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13745v2",
                "updated": "2025-09-16T13:30:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    30,
                    5,
                    1,
                    259,
                    0
                ],
                "published": "2024-09-11T01:56:35Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    1,
                    56,
                    35,
                    2,
                    255,
                    0
                ],
                "title": "Context-Aware Membership Inference Attacks against Pre-trained Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Aware Membership Inference Attacks against Pre-trained Large\n  Language Models"
                },
                "summary": "Membership Inference Attacks (MIAs) on pre-trained Large Language Models\n(LLMs) aim at determining if a data point was part of the model's training set.\nPrior MIAs that are built for classification models fail at LLMs, due to\nignoring the generative nature of LLMs across token sequences. In this paper,\nwe present a novel attack on pre-trained LLMs that adapts MIA statistical tests\nto the perplexity dynamics of subsequences within a data point. Our method\nsignificantly outperforms prior approaches, revealing context-dependent\nmemorization patterns in pre-trained LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership Inference Attacks (MIAs) on pre-trained Large Language Models\n(LLMs) aim at determining if a data point was part of the model's training set.\nPrior MIAs that are built for classification models fail at LLMs, due to\nignoring the generative nature of LLMs across token sequences. In this paper,\nwe present a novel attack on pre-trained LLMs that adapts MIA statistical tests\nto the perplexity dynamics of subsequences within a data point. Our method\nsignificantly outperforms prior approaches, revealing context-dependent\nmemorization patterns in pre-trained LLMs."
                },
                "authors": [
                    {
                        "name": "Hongyan Chang"
                    },
                    {
                        "name": "Ali Shahin Shamsabadi"
                    },
                    {
                        "name": "Kleomenis Katevas"
                    },
                    {
                        "name": "Hamed Haddadi"
                    },
                    {
                        "name": "Reza Shokri"
                    }
                ],
                "author_detail": {
                    "name": "Reza Shokri"
                },
                "author": "Reza Shokri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08375v2",
                "updated": "2025-09-16T13:19:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    19,
                    41,
                    1,
                    259,
                    0
                ],
                "published": "2025-06-10T02:39:55Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    39,
                    55,
                    1,
                    161,
                    0
                ],
                "title": "EIFBENCH: Extremely Complex Instruction Following Benchmark for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EIFBENCH: Extremely Complex Instruction Following Benchmark for Large\n  Language Models"
                },
                "summary": "With the development and widespread application of large language models\n(LLMs), the new paradigm of \"Model as Product\" is rapidly evolving, and demands\nhigher capabilities to address complex user needs, often requiring precise\nworkflow execution which involves the accurate understanding of multiple tasks.\nHowever, existing benchmarks focusing on single-task environments with limited\nconstraints lack the complexity required to fully reflect real-world scenarios.\nTo bridge this gap, we present the Extremely Complex Instruction Following\nBenchmark (EIFBENCH), meticulously crafted to facilitate a more realistic and\nrobust evaluation of LLMs. EIFBENCH not only includes multi-task scenarios that\nenable comprehensive assessment across diverse task types concurrently, but\nalso integrates a variety of constraints, replicating complex operational\nenvironments. Furthermore, we propose the Segment Policy Optimization (SegPO)\nalgorithm to enhance the LLM's ability to accurately fulfill multi-task\nworkflow. Evaluations on EIFBENCH have unveiled considerable performance\ndiscrepancies in existing LLMs when challenged with these extremely complex\ninstructions. This finding underscores the necessity for ongoing optimization\nto navigate the intricate challenges posed by LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development and widespread application of large language models\n(LLMs), the new paradigm of \"Model as Product\" is rapidly evolving, and demands\nhigher capabilities to address complex user needs, often requiring precise\nworkflow execution which involves the accurate understanding of multiple tasks.\nHowever, existing benchmarks focusing on single-task environments with limited\nconstraints lack the complexity required to fully reflect real-world scenarios.\nTo bridge this gap, we present the Extremely Complex Instruction Following\nBenchmark (EIFBENCH), meticulously crafted to facilitate a more realistic and\nrobust evaluation of LLMs. EIFBENCH not only includes multi-task scenarios that\nenable comprehensive assessment across diverse task types concurrently, but\nalso integrates a variety of constraints, replicating complex operational\nenvironments. Furthermore, we propose the Segment Policy Optimization (SegPO)\nalgorithm to enhance the LLM's ability to accurately fulfill multi-task\nworkflow. Evaluations on EIFBENCH have unveiled considerable performance\ndiscrepancies in existing LLMs when challenged with these extremely complex\ninstructions. This finding underscores the necessity for ongoing optimization\nto navigate the intricate challenges posed by LLM applications."
                },
                "authors": [
                    {
                        "name": "Tao Zou"
                    },
                    {
                        "name": "Xinghua Zhang"
                    },
                    {
                        "name": "Haiyang Yu"
                    },
                    {
                        "name": "Minzheng Wang"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "arxiv_comment": "Accepted by EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13055v1",
                "updated": "2025-09-16T13:12:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    12,
                    11,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T13:12:11Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    12,
                    11,
                    1,
                    259,
                    0
                ],
                "title": "Automating Code Generation for Semiconductor Equipment Control from\n  Developer Utterances with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Code Generation for Semiconductor Equipment Control from\n  Developer Utterances with LLMs"
                },
                "summary": "Semiconductors form the backbone of modern electronics, with their\nmanufacturing and testing relying on highly specialized equipment and\ndomain-specific programming languages. Equipment languages such as the\nAlgorithmic Pattern Generator (ALPG) are critical for precise hardware control\nbut are challenging to program due to their low-level syntax and steep learning\ncurve. While large language models (LLMs) have shown promise in generating\nhigh-level code from natural language, their effectiveness on low-level\nequipment languages remains limited. To address this, we propose Progressive\nKnowledge Enhancement (PKE), a novel multi-stage prompting framework that\nprogressively extracts and activates the latent knowledge within LLMs, guiding\nthem from simple to complex examples without extensive fine-tuning. Empirical\nevaluation on an industrial ALPG dataset shows that PKE significantly\noutperforms standard prompting and surpasses state-of-the-art methods in\ngenerating correct ALPG code, achieving 11.1\\% and 15.2\\% higher exact match\nscores compared to the second-best technique. Further analysis of individual\ncomponents confirms that progressive knowledge extraction based on difficulty\nenhances accuracy. Our study offer a practical approach to boosting LLM\ncapabilities for specialized low-level programming, supporting greater\nproductivity in semiconductor software development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semiconductors form the backbone of modern electronics, with their\nmanufacturing and testing relying on highly specialized equipment and\ndomain-specific programming languages. Equipment languages such as the\nAlgorithmic Pattern Generator (ALPG) are critical for precise hardware control\nbut are challenging to program due to their low-level syntax and steep learning\ncurve. While large language models (LLMs) have shown promise in generating\nhigh-level code from natural language, their effectiveness on low-level\nequipment languages remains limited. To address this, we propose Progressive\nKnowledge Enhancement (PKE), a novel multi-stage prompting framework that\nprogressively extracts and activates the latent knowledge within LLMs, guiding\nthem from simple to complex examples without extensive fine-tuning. Empirical\nevaluation on an industrial ALPG dataset shows that PKE significantly\noutperforms standard prompting and surpasses state-of-the-art methods in\ngenerating correct ALPG code, achieving 11.1\\% and 15.2\\% higher exact match\nscores compared to the second-best technique. Further analysis of individual\ncomponents confirms that progressive knowledge extraction based on difficulty\nenhances accuracy. Our study offer a practical approach to boosting LLM\ncapabilities for specialized low-level programming, supporting greater\nproductivity in semiconductor software development."
                },
                "authors": [
                    {
                        "name": "Youngkyoung Kim"
                    },
                    {
                        "name": "Sanghyeok Park"
                    },
                    {
                        "name": "Misoo Kim"
                    },
                    {
                        "name": "Gangho Yoon"
                    },
                    {
                        "name": "Eunseok Lee"
                    },
                    {
                        "name": "Simon S. Woo"
                    }
                ],
                "author_detail": {
                    "name": "Simon S. Woo"
                },
                "author": "Simon S. Woo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03107v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03107v2",
                "updated": "2025-09-16T13:11:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    11,
                    17,
                    1,
                    259,
                    0
                ],
                "published": "2024-12-04T08:13:29Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    13,
                    29,
                    2,
                    339,
                    0
                ],
                "title": "CredID: Credible Multi-Bit Watermark for Large Language Models\n  Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CredID: Credible Multi-Bit Watermark for Large Language Models\n  Identification"
                },
                "summary": "Large Language Models (LLMs) are widely used in complex natural language\nprocessing tasks but raise privacy and security concerns due to the lack of\nidentity recognition. This paper proposes a multi-party credible watermarking\nframework (CredID) involving a trusted third party (TTP) and multiple LLM\nvendors to address these issues. In the watermark embedding stage, vendors\nrequest a seed from the TTP to generate watermarked text without sending the\nuser's prompt. In the extraction stage, the TTP coordinates each vendor to\nextract and verify the watermark from the text. This provides a credible\nwatermarking scheme while preserving vendor privacy. Furthermore, current\nwatermarking algorithms struggle with text quality, information capacity, and\nrobustness, making it challenging to meet the diverse identification needs of\nLLMs. Thus, we propose a novel multi-bit watermarking algorithm and an\nopen-source toolkit to facilitate research. Experiments show our CredID\nenhances watermark credibility and efficiency without compromising text\nquality. Additionally, we successfully utilized this framework to achieve\nhighly accurate identification among multiple LLM vendors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used in complex natural language\nprocessing tasks but raise privacy and security concerns due to the lack of\nidentity recognition. This paper proposes a multi-party credible watermarking\nframework (CredID) involving a trusted third party (TTP) and multiple LLM\nvendors to address these issues. In the watermark embedding stage, vendors\nrequest a seed from the TTP to generate watermarked text without sending the\nuser's prompt. In the extraction stage, the TTP coordinates each vendor to\nextract and verify the watermark from the text. This provides a credible\nwatermarking scheme while preserving vendor privacy. Furthermore, current\nwatermarking algorithms struggle with text quality, information capacity, and\nrobustness, making it challenging to meet the diverse identification needs of\nLLMs. Thus, we propose a novel multi-bit watermarking algorithm and an\nopen-source toolkit to facilitate research. Experiments show our CredID\nenhances watermark credibility and efficiency without compromising text\nquality. Additionally, we successfully utilized this framework to achieve\nhighly accurate identification among multiple LLM vendors."
                },
                "authors": [
                    {
                        "name": "Haoyu Jiang"
                    },
                    {
                        "name": "Xuhong Wang"
                    },
                    {
                        "name": "Ping Yi"
                    },
                    {
                        "name": "Shanzhe Lei"
                    },
                    {
                        "name": "Yilun Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yilun Lin"
                },
                "author": "Yilun Lin",
                "arxiv_comment": "v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03107v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03107v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08045v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08045v3",
                "updated": "2025-09-16T13:10:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    10,
                    21,
                    1,
                    259,
                    0
                ],
                "published": "2025-02-12T01:04:13Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    1,
                    4,
                    13,
                    2,
                    43,
                    0
                ],
                "title": "Break the Checkbox: Challenging Closed-Style Evaluations of Cultural\n  Alignment in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Break the Checkbox: Challenging Closed-Style Evaluations of Cultural\n  Alignment in LLMs"
                },
                "summary": "A large number of studies rely on closed-style multiple-choice surveys to\nevaluate cultural alignment in Large Language Models (LLMs). In this work, we\nchallenge this constrained evaluation paradigm and explore more realistic,\nunconstrained approaches. Using the World Values Survey (WVS) and Hofstede\nCultural Dimensions as case studies, we demonstrate that LLMs exhibit stronger\ncultural alignment in less constrained settings, where responses are not\nforced. Additionally, we show that even minor changes, such as reordering\nsurvey choices, lead to inconsistent outputs, exposing the limitations of\nclosed-style evaluations. Our findings advocate for more robust and flexible\nevaluation frameworks that focus on specific cultural proxies, encouraging more\nnuanced and accurate assessments of cultural alignment in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A large number of studies rely on closed-style multiple-choice surveys to\nevaluate cultural alignment in Large Language Models (LLMs). In this work, we\nchallenge this constrained evaluation paradigm and explore more realistic,\nunconstrained approaches. Using the World Values Survey (WVS) and Hofstede\nCultural Dimensions as case studies, we demonstrate that LLMs exhibit stronger\ncultural alignment in less constrained settings, where responses are not\nforced. Additionally, we show that even minor changes, such as reordering\nsurvey choices, lead to inconsistent outputs, exposing the limitations of\nclosed-style evaluations. Our findings advocate for more robust and flexible\nevaluation frameworks that focus on specific cultural proxies, encouraging more\nnuanced and accurate assessments of cultural alignment in LLMs."
                },
                "authors": [
                    {
                        "name": "Mohsinul Kabir"
                    },
                    {
                        "name": "Ajwad Abrar"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    }
                ],
                "author_detail": {
                    "name": "Sophia Ananiadou"
                },
                "author": "Sophia Ananiadou",
                "arxiv_comment": "Accepted at EMNLP 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08045v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08045v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03592v2",
                "updated": "2025-09-16T13:10:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    10,
                    4,
                    1,
                    259,
                    0
                ],
                "published": "2025-06-04T05:46:40Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    5,
                    46,
                    40,
                    2,
                    155,
                    0
                ],
                "title": "From Understanding to Generation: An Efficient Shortcut for Evaluating\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Understanding to Generation: An Efficient Shortcut for Evaluating\n  Language Models"
                },
                "summary": "Iterative evaluation of LLMs during training is essential to ensure expected\ncapability development, but can be time- and compute-intensive. While NLU\ntasks, where the model selects from fixed answer choices, are cheap to\nevaluate, essential capabilities like reasoning and code generation rely on the\nmore time-consuming NLG (token-by-token generation) format. In this work, our\naim is to decrease the computational burden of NLG benchmarks in order to\nenable monitoring crucial LLM capabilities during model training. We\nreformulate generative tasks into computationally cheaper NLU alternatives. We\ntest the performance correlation between the original and reformulated tasks\nusing 8 LMs of various sizes and 4 capabilities: mathematical reasoning, code\ngeneration, factual knowledge and reading comprehension. Our results show a\nstrong correlation between task formats, supporting capability assessment via\ncheaper alternatives and achieving over 35x average reduction in evaluation\ntime. Our project is available at:\nhttps://github.com/Fraunhofer-IIS/EvalShortcut",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative evaluation of LLMs during training is essential to ensure expected\ncapability development, but can be time- and compute-intensive. While NLU\ntasks, where the model selects from fixed answer choices, are cheap to\nevaluate, essential capabilities like reasoning and code generation rely on the\nmore time-consuming NLG (token-by-token generation) format. In this work, our\naim is to decrease the computational burden of NLG benchmarks in order to\nenable monitoring crucial LLM capabilities during model training. We\nreformulate generative tasks into computationally cheaper NLU alternatives. We\ntest the performance correlation between the original and reformulated tasks\nusing 8 LMs of various sizes and 4 capabilities: mathematical reasoning, code\ngeneration, factual knowledge and reading comprehension. Our results show a\nstrong correlation between task formats, supporting capability assessment via\ncheaper alternatives and achieving over 35x average reduction in evaluation\ntime. Our project is available at:\nhttps://github.com/Fraunhofer-IIS/EvalShortcut"
                },
                "authors": [
                    {
                        "name": "Viktor Hangya"
                    },
                    {
                        "name": "Fabian KÃ¼ch"
                    },
                    {
                        "name": "Darina Gold"
                    }
                ],
                "author_detail": {
                    "name": "Darina Gold"
                },
                "author": "Darina Gold",
                "arxiv_comment": "Accepted to EMNLP 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13049v1",
                "updated": "2025-09-16T13:09:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    9,
                    13,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T13:09:13Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    9,
                    13,
                    1,
                    259,
                    0
                ],
                "title": "Spiking Vocos: An Energy-Efficient Neural Vocoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Vocos: An Energy-Efficient Neural Vocoder"
                },
                "summary": "Despite the remarkable progress in the synthesis speed and fidelity of neural\nvocoders, their high energy consumption remains a critical barrier to practical\ndeployment on computationally restricted edge devices. Spiking Neural Networks\n(SNNs), widely recognized for their high energy efficiency due to their\nevent-driven nature, offer a promising solution for low-resource scenarios. In\nthis paper, we propose Spiking Vocos, a novel spiking neural vocoder with\nultra-low energy consumption, built upon the efficient Vocos framework. To\nmitigate the inherent information bottleneck in SNNs, we design a Spiking\nConvNeXt module to reduce Multiply-Accumulate (MAC) operations and incorporate\nan amplitude shortcut path to preserve crucial signal dynamics. Furthermore, to\nbridge the performance gap with its Artificial Neural Network (ANN)\ncounterpart, we introduce a self-architectural distillation strategy to\neffectively transfer knowledge. A lightweight Temporal Shift Module is also\nintegrated to enhance the model's ability to fuse information across the\ntemporal dimension with negligible computational overhead. Experiments\ndemonstrate that our model achieves performance comparable to its ANN\ncounterpart, with UTMOS and PESQ scores of 3.74 and 3.45 respectively, while\nconsuming only 14.7% of the energy. The source code is available at\nhttps://github.com/pymaster17/Spiking-Vocos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable progress in the synthesis speed and fidelity of neural\nvocoders, their high energy consumption remains a critical barrier to practical\ndeployment on computationally restricted edge devices. Spiking Neural Networks\n(SNNs), widely recognized for their high energy efficiency due to their\nevent-driven nature, offer a promising solution for low-resource scenarios. In\nthis paper, we propose Spiking Vocos, a novel spiking neural vocoder with\nultra-low energy consumption, built upon the efficient Vocos framework. To\nmitigate the inherent information bottleneck in SNNs, we design a Spiking\nConvNeXt module to reduce Multiply-Accumulate (MAC) operations and incorporate\nan amplitude shortcut path to preserve crucial signal dynamics. Furthermore, to\nbridge the performance gap with its Artificial Neural Network (ANN)\ncounterpart, we introduce a self-architectural distillation strategy to\neffectively transfer knowledge. A lightweight Temporal Shift Module is also\nintegrated to enhance the model's ability to fuse information across the\ntemporal dimension with negligible computational overhead. Experiments\ndemonstrate that our model achieves performance comparable to its ANN\ncounterpart, with UTMOS and PESQ scores of 3.74 and 3.45 respectively, while\nconsuming only 14.7% of the energy. The source code is available at\nhttps://github.com/pymaster17/Spiking-Vocos."
                },
                "authors": [
                    {
                        "name": "Yukun Chen"
                    },
                    {
                        "name": "Zhaoxi Mu"
                    },
                    {
                        "name": "Andong Li"
                    },
                    {
                        "name": "Peilin Li"
                    },
                    {
                        "name": "Xinyu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xinyu Yang"
                },
                "author": "Xinyu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13047v1",
                "updated": "2025-09-16T13:04:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    4,
                    48,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T13:04:48Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    13,
                    4,
                    48,
                    1,
                    259,
                    0
                ],
                "title": "Multi-Model Synthetic Training for Mission-Critical Small Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Model Synthetic Training for Mission-Critical Small Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nmany domains, yet their application to specialized fields remains constrained\nby the scarcity and complexity of domain-specific training data. We present a\nnovel approach that achieves a 261x cost reduction for maritime intelligence by\nusing LLMs as one-time teachers rather than using them directly for inference.\nOur method transforms 3.2 billion Automatic Identification System (AIS) vessel\ntracking records into 21,543 synthetic question and answer pairs through\nmulti-model generation (GPT-4o and o3-mini), preventing overfitting and\nensuring accurate reasoning. The resulting fine-tuned Qwen2.5-7B model achieves\n75% accuracy on maritime tasks, while being substantially cheaper than using a\nlarger model for inference. We show that smaller, cheaper models -- when fine\ntuned properly -- can provide similar accuracy compared to larger models that\nare prohibitively expensive. Our work contributes to the growing field of\nsynthetic dataset generation for specialized AI applications and presents a\nhighly reproducible framework for domains where manual annotation is\ninfeasible. Beyond expanding research in the growing field of specialized small\nlanguage models, our approach has immediate applications in maritime safety,\nsecurity operations, and vessel traffic management systems in various\nindustries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nmany domains, yet their application to specialized fields remains constrained\nby the scarcity and complexity of domain-specific training data. We present a\nnovel approach that achieves a 261x cost reduction for maritime intelligence by\nusing LLMs as one-time teachers rather than using them directly for inference.\nOur method transforms 3.2 billion Automatic Identification System (AIS) vessel\ntracking records into 21,543 synthetic question and answer pairs through\nmulti-model generation (GPT-4o and o3-mini), preventing overfitting and\nensuring accurate reasoning. The resulting fine-tuned Qwen2.5-7B model achieves\n75% accuracy on maritime tasks, while being substantially cheaper than using a\nlarger model for inference. We show that smaller, cheaper models -- when fine\ntuned properly -- can provide similar accuracy compared to larger models that\nare prohibitively expensive. Our work contributes to the growing field of\nsynthetic dataset generation for specialized AI applications and presents a\nhighly reproducible framework for domains where manual annotation is\ninfeasible. Beyond expanding research in the growing field of specialized small\nlanguage models, our approach has immediate applications in maritime safety,\nsecurity operations, and vessel traffic management systems in various\nindustries."
                },
                "authors": [
                    {
                        "name": "Nolan Platt"
                    },
                    {
                        "name": "Pragyansmita Nayak"
                    }
                ],
                "author_detail": {
                    "name": "Pragyansmita Nayak"
                },
                "author": "Pragyansmita Nayak",
                "arxiv_comment": "8 pages. Accepted as a full paper to the 3rd International Conference\n  on Foundation and Large Language Models (IEEE FLLM) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16467v2",
                "updated": "2025-09-16T12:56:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    56,
                    24,
                    1,
                    259,
                    0
                ],
                "published": "2025-05-22T09:48:51Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    9,
                    48,
                    51,
                    3,
                    142,
                    0
                ],
                "title": "Reading Between the Prompts: How Stereotypes Shape LLM's Implicit\n  Personalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reading Between the Prompts: How Stereotypes Shape LLM's Implicit\n  Personalization"
                },
                "summary": "Generative Large Language Models (LLMs) infer user's demographic information\nfrom subtle cues in the conversation -- a phenomenon called implicit\npersonalization. Prior work has shown that such inferences can lead to lower\nquality responses for users assumed to be from minority groups, even when no\ndemographic information is explicitly provided. In this work, we systematically\nexplore how LLMs respond to stereotypical cues using controlled synthetic\nconversations, by analyzing the models' latent user representations through\nboth model internals and generated answers to targeted user questions. Our\nfindings reveal that LLMs do infer demographic attributes based on these\nstereotypical signals, which for a number of groups even persists when the user\nexplicitly identifies with a different demographic group. Finally, we show that\nthis form of stereotype-driven implicit personalization can be effectively\nmitigated by intervening on the model's internal representations using a\ntrained linear probe to steer them toward the explicitly stated identity. Our\nresults highlight the need for greater transparency and control in how LLMs\nrepresent user identity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Large Language Models (LLMs) infer user's demographic information\nfrom subtle cues in the conversation -- a phenomenon called implicit\npersonalization. Prior work has shown that such inferences can lead to lower\nquality responses for users assumed to be from minority groups, even when no\ndemographic information is explicitly provided. In this work, we systematically\nexplore how LLMs respond to stereotypical cues using controlled synthetic\nconversations, by analyzing the models' latent user representations through\nboth model internals and generated answers to targeted user questions. Our\nfindings reveal that LLMs do infer demographic attributes based on these\nstereotypical signals, which for a number of groups even persists when the user\nexplicitly identifies with a different demographic group. Finally, we show that\nthis form of stereotype-driven implicit personalization can be effectively\nmitigated by intervening on the model's internal representations using a\ntrained linear probe to steer them toward the explicitly stated identity. Our\nresults highlight the need for greater transparency and control in how LLMs\nrepresent user identity."
                },
                "authors": [
                    {
                        "name": "Vera Neplenbroek"
                    },
                    {
                        "name": "Arianna Bisazza"
                    },
                    {
                        "name": "Raquel FernÃ¡ndez"
                    }
                ],
                "author_detail": {
                    "name": "Raquel FernÃ¡ndez"
                },
                "author": "Raquel FernÃ¡ndez",
                "arxiv_comment": "Accepted at EMNLP Main 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13031v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13031v1",
                "updated": "2025-09-16T12:51:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    51,
                    11,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T12:51:11Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    51,
                    11,
                    1,
                    259,
                    0
                ],
                "title": "Perception Before Reasoning: Two-Stage Reinforcement Learning for Visual\n  Reasoning in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception Before Reasoning: Two-Stage Reinforcement Learning for Visual\n  Reasoning in Vision-Language Models"
                },
                "summary": "Reinforcement learning (RL) has proven highly effective in eliciting the\nreasoning capabilities of large language models (LLMs). Inspired by this\nsuccess, recent studies have explored applying similar techniques to\nvision-language models (VLMs), aiming to enhance their reasoning performance.\nHowever, directly transplanting RL methods from LLMs to VLMs is suboptimal, as\nthe tasks faced by VLMs are inherently more complex. Specifically, VLMs must\nfirst accurately perceive and understand visual inputs before reasoning can be\neffectively performed. To address this challenge, we propose a two-stage\nreinforcement learning framework designed to jointly enhance both the\nperceptual and reasoning capabilities of VLMs. To mitigate the vanishing\nadvantage issue commonly observed in RL training, we first perform\ndataset-level sampling to selectively strengthen specific capabilities using\ndistinct data sources. During training, the first stage focuses on improving\nthe model's visual perception through coarse- and fine-grained visual\nunderstanding, while the second stage targets the enhancement of reasoning\nabilities. After the proposed two-stage reinforcement learning process, we\nobtain PeBR-R1, a vision-language model with significantly enhanced perceptual\nand reasoning capabilities. Experimental results on seven benchmark datasets\ndemonstrate the effectiveness of our approach and validate the superior\nperformance of PeBR-R1 across diverse visual reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has proven highly effective in eliciting the\nreasoning capabilities of large language models (LLMs). Inspired by this\nsuccess, recent studies have explored applying similar techniques to\nvision-language models (VLMs), aiming to enhance their reasoning performance.\nHowever, directly transplanting RL methods from LLMs to VLMs is suboptimal, as\nthe tasks faced by VLMs are inherently more complex. Specifically, VLMs must\nfirst accurately perceive and understand visual inputs before reasoning can be\neffectively performed. To address this challenge, we propose a two-stage\nreinforcement learning framework designed to jointly enhance both the\nperceptual and reasoning capabilities of VLMs. To mitigate the vanishing\nadvantage issue commonly observed in RL training, we first perform\ndataset-level sampling to selectively strengthen specific capabilities using\ndistinct data sources. During training, the first stage focuses on improving\nthe model's visual perception through coarse- and fine-grained visual\nunderstanding, while the second stage targets the enhancement of reasoning\nabilities. After the proposed two-stage reinforcement learning process, we\nobtain PeBR-R1, a vision-language model with significantly enhanced perceptual\nand reasoning capabilities. Experimental results on seven benchmark datasets\ndemonstrate the effectiveness of our approach and validate the superior\nperformance of PeBR-R1 across diverse visual reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Yan Chen"
                    },
                    {
                        "name": "Long Li"
                    },
                    {
                        "name": "Teng Xi"
                    },
                    {
                        "name": "Long Zeng"
                    },
                    {
                        "name": "Jingdong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingdong Wang"
                },
                "author": "Jingdong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13031v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13025v1",
                "updated": "2025-09-16T12:46:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    46,
                    39,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T12:46:39Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    46,
                    39,
                    1,
                    259,
                    0
                ],
                "title": "GView: A Survey of Binary Forensics via Visual, Semantic, and\n  AI-Enhanced Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GView: A Survey of Binary Forensics via Visual, Semantic, and\n  AI-Enhanced Analysis"
                },
                "summary": "Cybersecurity threats continue to become more sophisticated and diverse in\ntheir artifacts, boosting both their volume and complexity. To overcome those\nchallenges, we present GView, an open-source forensic analysis framework with\nvisual and AI-enhanced reasoning. It started with focus on the practical\ncybersecurity industry. It has evolved significantly, incorporating large\nlanguage models (LLMs) to dynamically enhance reasoning and ease the forensic\nworkflows. This paper surveys both the current state of GView with its\npublished papers alongside those that are in the publishing process. It also\nincludes its innovative use of logical inference through predicates and\ninference rules for both the analyzed documents and the user's actions for\nbetter suggestions. We highlight the extensible architecture, showcasing its\npotential as a bridge between the practical forensics worlds with the academic\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cybersecurity threats continue to become more sophisticated and diverse in\ntheir artifacts, boosting both their volume and complexity. To overcome those\nchallenges, we present GView, an open-source forensic analysis framework with\nvisual and AI-enhanced reasoning. It started with focus on the practical\ncybersecurity industry. It has evolved significantly, incorporating large\nlanguage models (LLMs) to dynamically enhance reasoning and ease the forensic\nworkflows. This paper surveys both the current state of GView with its\npublished papers alongside those that are in the publishing process. It also\nincludes its innovative use of logical inference through predicates and\ninference rules for both the analyzed documents and the user's actions for\nbetter suggestions. We highlight the extensible architecture, showcasing its\npotential as a bridge between the practical forensics worlds with the academic\nresearch."
                },
                "authors": [
                    {
                        "name": "Raul Zaharia"
                    },
                    {
                        "name": "DragoÅ GavriluÅ£"
                    },
                    {
                        "name": "GheorghiÅ£Ä Mutu"
                    }
                ],
                "author_detail": {
                    "name": "GheorghiÅ£Ä Mutu"
                },
                "arxiv_affiliation": "Al. I. Cuza University & Bitdefender",
                "author": "GheorghiÅ£Ä Mutu",
                "arxiv_doi": "10.4204/EPTCS.427.9",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4204/EPTCS.427.9",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.13025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings FROM 2025, arXiv:2509.11877",
                "arxiv_journal_ref": "EPTCS 427, 2025, pp. 134-140",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13024v1",
                "updated": "2025-09-16T12:46:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    46,
                    22,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T12:46:22Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    46,
                    22,
                    1,
                    259,
                    0
                ],
                "title": "DVDP: An End-to-End Policy for Mobile Robot Visual Docking with RGB-D\n  Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DVDP: An End-to-End Policy for Mobile Robot Visual Docking with RGB-D\n  Perception"
                },
                "summary": "Automatic docking has long been a significant challenge in the field of\nmobile robotics. Compared to other automatic docking methods, visual docking\nmethods offer higher precision and lower deployment costs, making them an\nefficient and promising choice for this task. However, visual docking methods\nimpose strict requirements on the robot's initial position at the start of the\ndocking process. To overcome the limitations of current vision-based methods,\nwe propose an innovative end-to-end visual docking method named DVDP(direct\nvisual docking policy). This approach requires only a binocular RGB-D camera\ninstalled on the mobile robot to directly output the robot's docking path,\nachieving end-to-end automatic docking. Furthermore, we have collected a\nlarge-scale dataset of mobile robot visual automatic docking dataset through a\ncombination of virtual and real environments using the Unity 3D platform and\nactual mobile robot setups. We developed a series of evaluation metrics to\nquantify the performance of the end-to-end visual docking method. Extensive\nexperiments, including benchmarks against leading perception backbones adapted\ninto our framework, demonstrate that our method achieves superior performance.\nFinally, real-world deployment on the SCOUT Mini confirmed DVDP's efficacy,\nwith our model generating smooth, feasible docking trajectories that meet\nphysical constraints and reach the target pose.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic docking has long been a significant challenge in the field of\nmobile robotics. Compared to other automatic docking methods, visual docking\nmethods offer higher precision and lower deployment costs, making them an\nefficient and promising choice for this task. However, visual docking methods\nimpose strict requirements on the robot's initial position at the start of the\ndocking process. To overcome the limitations of current vision-based methods,\nwe propose an innovative end-to-end visual docking method named DVDP(direct\nvisual docking policy). This approach requires only a binocular RGB-D camera\ninstalled on the mobile robot to directly output the robot's docking path,\nachieving end-to-end automatic docking. Furthermore, we have collected a\nlarge-scale dataset of mobile robot visual automatic docking dataset through a\ncombination of virtual and real environments using the Unity 3D platform and\nactual mobile robot setups. We developed a series of evaluation metrics to\nquantify the performance of the end-to-end visual docking method. Extensive\nexperiments, including benchmarks against leading perception backbones adapted\ninto our framework, demonstrate that our method achieves superior performance.\nFinally, real-world deployment on the SCOUT Mini confirmed DVDP's efficacy,\nwith our model generating smooth, feasible docking trajectories that meet\nphysical constraints and reach the target pose."
                },
                "authors": [
                    {
                        "name": "Haohan Min"
                    },
                    {
                        "name": "Zhoujian Li"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Jinyu Chen"
                    },
                    {
                        "name": "Shenghai Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Shenghai Yuan"
                },
                "author": "Shenghai Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13023v1",
                "updated": "2025-09-16T12:46:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    46,
                    11,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T12:46:11Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    46,
                    11,
                    1,
                    259,
                    0
                ],
                "title": "Validating Solidity Code Defects using Symbolic and Concrete Execution\n  powered by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Validating Solidity Code Defects using Symbolic and Concrete Execution\n  powered by Large Language Models"
                },
                "summary": "The high rate of false alarms from static analysis tools and Large Language\nModels (LLMs) complicates vulnerability detection in Solidity Smart Contracts,\ndemanding methods that can formally or empirically prove the presence of\ndefects. This paper introduces a novel detection pipeline that integrates\ncustom Slither-based detectors, LLMs, Kontrol, and Forge. Our approach is\ndesigned to reliably detect defects and generate proofs. We currently perform\nexperiments with promising results for seven types of critical defects. We\ndemonstrate the pipeline's efficacy by presenting our findings for three\nvulnerabilities -- Reentrancy, Complex Fallback, and Faulty Access Control\nPolicies -- that are challenging for current verification solutions, which\noften generate false alarms or fail to detect them entirely. We highlight the\npotential of either symbolic or concrete execution in correctly classifying\nsuch code faults. By chaining these instruments, our method effectively\nvalidates true positives, significantly reducing the manual verification\nburden. Although we identify potential limitations, such as the inconsistency\nand the cost of LLMs, our findings establish a robust framework for combining\nheuristic analysis with formal verification to achieve more reliable and\nautomated smart contract auditing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The high rate of false alarms from static analysis tools and Large Language\nModels (LLMs) complicates vulnerability detection in Solidity Smart Contracts,\ndemanding methods that can formally or empirically prove the presence of\ndefects. This paper introduces a novel detection pipeline that integrates\ncustom Slither-based detectors, LLMs, Kontrol, and Forge. Our approach is\ndesigned to reliably detect defects and generate proofs. We currently perform\nexperiments with promising results for seven types of critical defects. We\ndemonstrate the pipeline's efficacy by presenting our findings for three\nvulnerabilities -- Reentrancy, Complex Fallback, and Faulty Access Control\nPolicies -- that are challenging for current verification solutions, which\noften generate false alarms or fail to detect them entirely. We highlight the\npotential of either symbolic or concrete execution in correctly classifying\nsuch code faults. By chaining these instruments, our method effectively\nvalidates true positives, significantly reducing the manual verification\nburden. Although we identify potential limitations, such as the inconsistency\nand the cost of LLMs, our findings establish a robust framework for combining\nheuristic analysis with formal verification to achieve more reliable and\nautomated smart contract auditing."
                },
                "authors": [
                    {
                        "name": "Åtefan-Claudiu Susan"
                    },
                    {
                        "name": "Andrei Arusoaie"
                    },
                    {
                        "name": "Dorel Lucanu"
                    }
                ],
                "author_detail": {
                    "name": "Dorel Lucanu"
                },
                "arxiv_affiliation": "\"Alexandru Ioan Cuza\", University of IaÅi, Department of Computer Science",
                "author": "Dorel Lucanu",
                "arxiv_doi": "10.4204/EPTCS.427.7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4204/EPTCS.427.7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.13023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings FROM 2025, arXiv:2509.11877",
                "arxiv_journal_ref": "EPTCS 427, 2025, pp. 98-116",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.2;D.2.5;D.2.4;D.4.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13021v1",
                "updated": "2025-09-16T12:45:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    45,
                    45,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T12:45:45Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    45,
                    45,
                    1,
                    259,
                    0
                ],
                "title": "xOffense: An AI-driven autonomous penetration testing framework with\n  offensive knowledge-enhanced LLMs and multi agent systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xOffense: An AI-driven autonomous penetration testing framework with\n  offensive knowledge-enhanced LLMs and multi agent systems"
                },
                "summary": "This work introduces xOffense, an AI-driven, multi-agent penetration testing\nframework that shifts the process from labor-intensive, expert-driven manual\nefforts to fully automated, machine-executable workflows capable of scaling\nseamlessly with computational infrastructure. At its core, xOffense leverages a\nfine-tuned, mid-scale open-source LLM (Qwen3-32B) to drive reasoning and\ndecision-making in penetration testing. The framework assigns specialized\nagents to reconnaissance, vulnerability scanning, and exploitation, with an\norchestration layer ensuring seamless coordination across phases. Fine-tuning\non Chain-of-Thought penetration testing data further enables the model to\ngenerate precise tool commands and perform consistent multi-step reasoning. We\nevaluate xOffense on two rigorous benchmarks: AutoPenBench and\nAI-Pentest-Benchmark. The results demonstrate that xOffense consistently\noutperforms contemporary methods, achieving a sub-task completion rate of\n79.17%, decisively surpassing leading systems such as VulnBot and PentestGPT.\nThese findings highlight the potential of domain-adapted mid-scale LLMs, when\nembedded within structured multi-agent orchestration, to deliver superior,\ncost-efficient, and reproducible solutions for autonomous penetration testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces xOffense, an AI-driven, multi-agent penetration testing\nframework that shifts the process from labor-intensive, expert-driven manual\nefforts to fully automated, machine-executable workflows capable of scaling\nseamlessly with computational infrastructure. At its core, xOffense leverages a\nfine-tuned, mid-scale open-source LLM (Qwen3-32B) to drive reasoning and\ndecision-making in penetration testing. The framework assigns specialized\nagents to reconnaissance, vulnerability scanning, and exploitation, with an\norchestration layer ensuring seamless coordination across phases. Fine-tuning\non Chain-of-Thought penetration testing data further enables the model to\ngenerate precise tool commands and perform consistent multi-step reasoning. We\nevaluate xOffense on two rigorous benchmarks: AutoPenBench and\nAI-Pentest-Benchmark. The results demonstrate that xOffense consistently\noutperforms contemporary methods, achieving a sub-task completion rate of\n79.17%, decisively surpassing leading systems such as VulnBot and PentestGPT.\nThese findings highlight the potential of domain-adapted mid-scale LLMs, when\nembedded within structured multi-agent orchestration, to deliver superior,\ncost-efficient, and reproducible solutions for autonomous penetration testing."
                },
                "authors": [
                    {
                        "name": "Phung Duc Luong"
                    },
                    {
                        "name": "Le Tran Gia Bao"
                    },
                    {
                        "name": "Nguyen Vu Khai Tam"
                    },
                    {
                        "name": "Dong Huu Nguyen Khoa"
                    },
                    {
                        "name": "Nguyen Huu Quyen"
                    },
                    {
                        "name": "Van-Hau Pham"
                    },
                    {
                        "name": "Phan The Duy"
                    }
                ],
                "author_detail": {
                    "name": "Phan The Duy"
                },
                "author": "Phan The Duy",
                "arxiv_comment": "17 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11552v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11552v2",
                "updated": "2025-09-16T12:36:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    36,
                    35,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-15T03:32:50Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    3,
                    32,
                    50,
                    0,
                    258,
                    0
                ],
                "title": "HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with\n  Hierarchical Chunking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with\n  Hierarchical Chunking"
                },
                "summary": "Retrieval-Augmented Generation (RAG) enhances the response capabilities of\nlanguage models by integrating external knowledge sources. However, document\nchunking as an important part of RAG system often lacks effective evaluation\ntools. This paper first analyzes why existing RAG evaluation benchmarks are\ninadequate for assessing document chunking quality, specifically due to\nevidence sparsity. Based on this conclusion, we propose HiCBench, which\nincludes manually annotated multi-level document chunking points, synthesized\nevidence-dense quetion answer(QA) pairs, and their corresponding evidence\nsources. Additionally, we introduce the HiChunk framework, a multi-level\ndocument structuring framework based on fine-tuned LLMs, combined with the\nAuto-Merge retrieval algorithm to improve retrieval quality. Experiments\ndemonstrate that HiCBench effectively evaluates the impact of different\nchunking methods across the entire RAG pipeline. Moreover, HiChunk achieves\nbetter chunking quality within reasonable time consumption, thereby enhancing\nthe overall performance of RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) enhances the response capabilities of\nlanguage models by integrating external knowledge sources. However, document\nchunking as an important part of RAG system often lacks effective evaluation\ntools. This paper first analyzes why existing RAG evaluation benchmarks are\ninadequate for assessing document chunking quality, specifically due to\nevidence sparsity. Based on this conclusion, we propose HiCBench, which\nincludes manually annotated multi-level document chunking points, synthesized\nevidence-dense quetion answer(QA) pairs, and their corresponding evidence\nsources. Additionally, we introduce the HiChunk framework, a multi-level\ndocument structuring framework based on fine-tuned LLMs, combined with the\nAuto-Merge retrieval algorithm to improve retrieval quality. Experiments\ndemonstrate that HiCBench effectively evaluates the impact of different\nchunking methods across the entire RAG pipeline. Moreover, HiChunk achieves\nbetter chunking quality within reasonable time consumption, thereby enhancing\nthe overall performance of RAG systems."
                },
                "authors": [
                    {
                        "name": "Wensheng Lu"
                    },
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Ruizhi Qiao"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "17 pages, 5 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11552v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11552v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13011v1",
                "updated": "2025-09-16T12:33:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    33,
                    54,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T12:33:54Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    33,
                    54,
                    1,
                    259,
                    0
                ],
                "title": "A Visualized Framework for Event Cooperation with Generative Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Visualized Framework for Event Cooperation with Generative Agents"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the simulation of agent\nsocieties, enabling autonomous planning, memory formation, and social\ninteractions. However, existing frameworks often overlook systematic\nevaluations for event organization and lack visualized integration with\nphysically grounded environments, limiting agents' ability to navigate spaces\nand interact with items realistically. We develop MiniAgentPro, a visualization\nplatform featuring an intuitive map editor for customizing environments and a\nsimulation player with smooth animations. Based on this tool, we introduce a\ncomprehensive test set comprising eight diverse event scenarios with basic and\nhard variants to assess agents' ability. Evaluations using GPT-4o demonstrate\nstrong performance in basic settings but highlight coordination challenges in\nhard variants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the simulation of agent\nsocieties, enabling autonomous planning, memory formation, and social\ninteractions. However, existing frameworks often overlook systematic\nevaluations for event organization and lack visualized integration with\nphysically grounded environments, limiting agents' ability to navigate spaces\nand interact with items realistically. We develop MiniAgentPro, a visualization\nplatform featuring an intuitive map editor for customizing environments and a\nsimulation player with smooth animations. Based on this tool, we introduce a\ncomprehensive test set comprising eight diverse event scenarios with basic and\nhard variants to assess agents' ability. Evaluations using GPT-4o demonstrate\nstrong performance in basic settings but highlight coordination challenges in\nhard variants."
                },
                "authors": [
                    {
                        "name": "Yuyang Tian"
                    },
                    {
                        "name": "Shunqiang Mao"
                    },
                    {
                        "name": "Wenchang Gao"
                    },
                    {
                        "name": "Lanlan Qiu"
                    },
                    {
                        "name": "Tianxing He"
                    }
                ],
                "author_detail": {
                    "name": "Tianxing He"
                },
                "author": "Tianxing He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19894v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19894v4",
                "updated": "2025-09-17T05:54:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    5,
                    54,
                    41,
                    2,
                    260,
                    0
                ],
                "published": "2024-09-30T02:53:03Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    2,
                    53,
                    3,
                    0,
                    274,
                    0
                ],
                "title": "Semantic Alignment-Enhanced Code Translation via an LLM-Based\n  Multi-Agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Alignment-Enhanced Code Translation via an LLM-Based\n  Multi-Agent System"
                },
                "summary": "Code translation converts code from one programming language to another while\nmaintaining its original functionality, which is crucial for software\nmigration, system refactoring, and cross-platform development. Traditional\nrule-based methods rely on manually-written rules, which can be time-consuming\nand often result in less readable code. To overcome this, learning-based\nmethods have been developed, leveraging parallel data to train models for\nautomated code translation. More recently, the advance of Large Language Models\n(LLMs) further boosts learning-based code translation. Although promising,\nLLM-translated program still suffers from diverse quality issues (e.g., syntax\nerrors and semantic errors). In particular, it can be challenging for LLMs to\nself-debug these errors when simply provided with the corresponding error\nmessages.\n  In this work, we propose a novel LLM-based multi-agent system TRANSAGENT,\nwhich enhances LLM-based code translation by fixing the syntax errors and\nsemantic errors with the synergy between four LLM-based agents, including\nInitial Code Translator, Syntax Error Fixer, Code Aligner, and Semantic Error\nFixer. The main insight of TRANSAGENT is to first localize the error code block\nin the target program based on the execution alignment between the target and\nsource program, which can narrow down the fixing space and thus lower down the\nfixing difficulties. To evaluate TRANSAGENT, we first construct a new benchmark\nfrom recent programming tasks to mitigate the potential data leakage issue. On\nour benchmark, TRANSAGENT outperforms the latest LLM-based code translation\ntechnique UniTrans in both translation effectiveness and efficiency;\nadditionally, our evaluation on different LLMs show the generalization of\nTRANSAGENT and our ablation study shows the contribution of each agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code translation converts code from one programming language to another while\nmaintaining its original functionality, which is crucial for software\nmigration, system refactoring, and cross-platform development. Traditional\nrule-based methods rely on manually-written rules, which can be time-consuming\nand often result in less readable code. To overcome this, learning-based\nmethods have been developed, leveraging parallel data to train models for\nautomated code translation. More recently, the advance of Large Language Models\n(LLMs) further boosts learning-based code translation. Although promising,\nLLM-translated program still suffers from diverse quality issues (e.g., syntax\nerrors and semantic errors). In particular, it can be challenging for LLMs to\nself-debug these errors when simply provided with the corresponding error\nmessages.\n  In this work, we propose a novel LLM-based multi-agent system TRANSAGENT,\nwhich enhances LLM-based code translation by fixing the syntax errors and\nsemantic errors with the synergy between four LLM-based agents, including\nInitial Code Translator, Syntax Error Fixer, Code Aligner, and Semantic Error\nFixer. The main insight of TRANSAGENT is to first localize the error code block\nin the target program based on the execution alignment between the target and\nsource program, which can narrow down the fixing space and thus lower down the\nfixing difficulties. To evaluate TRANSAGENT, we first construct a new benchmark\nfrom recent programming tasks to mitigate the potential data leakage issue. On\nour benchmark, TRANSAGENT outperforms the latest LLM-based code translation\ntechnique UniTrans in both translation effectiveness and efficiency;\nadditionally, our evaluation on different LLMs show the generalization of\nTRANSAGENT and our ablation study shows the contribution of each agent."
                },
                "authors": [
                    {
                        "name": "Zhiqiang Yuan"
                    },
                    {
                        "name": "Weitong Chen"
                    },
                    {
                        "name": "Hanlin Wang"
                    },
                    {
                        "name": "Kai Yu"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Yiling Lou"
                    }
                ],
                "author_detail": {
                    "name": "Yiling Lou"
                },
                "author": "Yiling Lou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19894v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19894v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08679v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08679v2",
                "updated": "2025-09-16T12:31:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    31,
                    6,
                    1,
                    259,
                    0
                ],
                "published": "2025-07-11T15:21:49Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    21,
                    49,
                    4,
                    192,
                    0
                ],
                "title": "ByDeWay: Boost Your multimodal LLM with DEpth prompting in a\n  Training-Free Way",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ByDeWay: Boost Your multimodal LLM with DEpth prompting in a\n  Training-Free Way"
                },
                "summary": "We introduce ByDeWay, a training-free framework designed to enhance the\nperformance of Multimodal Large Language Models (MLLMs). ByDeWay uses a novel\nprompting strategy called Layered-Depth-Based Prompting (LDP), which improves\nspatial reasoning and grounding without modifying any model parameters. It\nsegments the scene into closest, mid-range, and farthest layers using monocular\ndepth estimation, then generates region-specific captions with a grounded\nvision-language model. These structured, depth-aware captions are appended to\nthe image-question prompt, enriching it with spatial context. This guides MLLMs\nto produce more grounded and less hallucinated responses. Our method is\nlightweight, modular, and compatible with black-box MLLMs. Experiments on\nhallucination-sensitive (POPE) and reasoning-intensive (GQA) benchmarks show\nconsistent improvements across multiple MLLMs, validating the effectiveness of\ndepth-aware prompting in a zero-training setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ByDeWay, a training-free framework designed to enhance the\nperformance of Multimodal Large Language Models (MLLMs). ByDeWay uses a novel\nprompting strategy called Layered-Depth-Based Prompting (LDP), which improves\nspatial reasoning and grounding without modifying any model parameters. It\nsegments the scene into closest, mid-range, and farthest layers using monocular\ndepth estimation, then generates region-specific captions with a grounded\nvision-language model. These structured, depth-aware captions are appended to\nthe image-question prompt, enriching it with spatial context. This guides MLLMs\nto produce more grounded and less hallucinated responses. Our method is\nlightweight, modular, and compatible with black-box MLLMs. Experiments on\nhallucination-sensitive (POPE) and reasoning-intensive (GQA) benchmarks show\nconsistent improvements across multiple MLLMs, validating the effectiveness of\ndepth-aware prompting in a zero-training setting."
                },
                "authors": [
                    {
                        "name": "Rajarshi Roy"
                    },
                    {
                        "name": "Devleena Das"
                    },
                    {
                        "name": "Ankesh Banerjee"
                    },
                    {
                        "name": "Arjya Bhattacharjee"
                    },
                    {
                        "name": "Kousik Dasgupta"
                    },
                    {
                        "name": "Subarna Tripathi"
                    }
                ],
                "author_detail": {
                    "name": "Subarna Tripathi"
                },
                "author": "Subarna Tripathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08679v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08679v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07002v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07002v5",
                "updated": "2025-09-16T12:23:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    23,
                    45,
                    1,
                    259,
                    0
                ],
                "published": "2025-08-09T14:41:20Z",
                "published_parsed": [
                    2025,
                    8,
                    9,
                    14,
                    41,
                    20,
                    5,
                    221,
                    0
                ],
                "title": "Joint Transmit and Pinching Beamforming Design for Pinching\n  Antenna-assisted Symbiotic Radio",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Transmit and Pinching Beamforming Design for Pinching\n  Antenna-assisted Symbiotic Radio"
                },
                "summary": "This paper investigates a novel downlink symbiotic radio framework enabled by\nthe pinching antenna system (PASS), designed to enhance both primary and\nsecondary transmissions through reconfigurable antenna positioning. This\nreconfigurability introduces additional degrees of freedom for adaptive\npinching beamforming, thereby enabling constructive signal enhancement and\ninterference suppression tailored to the locations of the backscatter device,\nthe Internet of Things (IoT) receiver, and the primary receivers. To fully\nexploit these benefits, we formulate a joint transmit and pinching beamforming\noptimization problem that maximizes the achievable sum rate while satisfying\nthe IoT receiver's detection error probability constraint and feasible\ndeployment constraints for the pinching antennas. The resulting problem is\ninherently nonconvex and highly coupled. To address this challenge, we develop\ntwo complementary solution approaches. The first is a learning-aided gradient\ndescent method, where the constrained optimization is reformulated into a\ndifferentiable form and solved through end-to-end learning. In this approach,\nthe pinching antenna position matrix is reparameterized to automatically\nsatisfy minimum spacing constraints, while transmit power and waveguide length\nlimits are enforced via projection and normalization. The second approach is an\noptimization-based successive convex approximation-particle swarm optimization\nmethod, which first determines the transmit beamforming solution using\nsuccessive convex approximation and subsequently optimizes pinching beamforming\nvia a particle swarm optimization search over candidate pinching antenna\nplacements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates a novel downlink symbiotic radio framework enabled by\nthe pinching antenna system (PASS), designed to enhance both primary and\nsecondary transmissions through reconfigurable antenna positioning. This\nreconfigurability introduces additional degrees of freedom for adaptive\npinching beamforming, thereby enabling constructive signal enhancement and\ninterference suppression tailored to the locations of the backscatter device,\nthe Internet of Things (IoT) receiver, and the primary receivers. To fully\nexploit these benefits, we formulate a joint transmit and pinching beamforming\noptimization problem that maximizes the achievable sum rate while satisfying\nthe IoT receiver's detection error probability constraint and feasible\ndeployment constraints for the pinching antennas. The resulting problem is\ninherently nonconvex and highly coupled. To address this challenge, we develop\ntwo complementary solution approaches. The first is a learning-aided gradient\ndescent method, where the constrained optimization is reformulated into a\ndifferentiable form and solved through end-to-end learning. In this approach,\nthe pinching antenna position matrix is reparameterized to automatically\nsatisfy minimum spacing constraints, while transmit power and waveguide length\nlimits are enforced via projection and normalization. The second approach is an\noptimization-based successive convex approximation-particle swarm optimization\nmethod, which first determines the transmit beamforming solution using\nsuccessive convex approximation and subsequently optimizes pinching beamforming\nvia a particle swarm optimization search over candidate pinching antenna\nplacements."
                },
                "authors": [
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Guoping Zhang"
                    },
                    {
                        "name": "Hongbo Xu"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Ming Zeng"
                    },
                    {
                        "name": "Fang Fang"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07002v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07002v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12067v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12067v3",
                "updated": "2025-09-16T12:21:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    21,
                    22,
                    1,
                    259,
                    0
                ],
                "published": "2025-02-17T17:37:26Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    17,
                    37,
                    26,
                    0,
                    48,
                    0
                ],
                "title": "TokenSkip: Controllable Chain-of-Thought Compression in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSkip: Controllable Chain-of-Thought Compression in LLMs"
                },
                "summary": "Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Recent advancements, such as\nOpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT\nsequences during inference could further boost LLM reasoning performance.\nHowever, due to the autoregressive nature of LLM decoding, longer CoT outputs\nlead to a linear increase in inference latency, adversely affecting user\nexperience, particularly when the CoT exceeds 10,000 tokens. To address this\nlimitation, we analyze the semantic importance of tokens within CoT outputs and\nreveal that their contributions to reasoning vary. Building on this insight, we\npropose TokenSkip, a simple yet effective approach that enables LLMs to\nselectively skip less important tokens, allowing for controllable CoT\ncompression. Extensive experiments across various models and tasks demonstrate\nthe effectiveness of TokenSkip in reducing CoT token usage while preserving\nstrong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct,\nTokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less\nthan a 0.4% performance drop. We release our code and checkpoints in\nhttps://github.com/hemingkx/TokenSkip.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Recent advancements, such as\nOpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT\nsequences during inference could further boost LLM reasoning performance.\nHowever, due to the autoregressive nature of LLM decoding, longer CoT outputs\nlead to a linear increase in inference latency, adversely affecting user\nexperience, particularly when the CoT exceeds 10,000 tokens. To address this\nlimitation, we analyze the semantic importance of tokens within CoT outputs and\nreveal that their contributions to reasoning vary. Building on this insight, we\npropose TokenSkip, a simple yet effective approach that enables LLMs to\nselectively skip less important tokens, allowing for controllable CoT\ncompression. Extensive experiments across various models and tasks demonstrate\nthe effectiveness of TokenSkip in reducing CoT token usage while preserving\nstrong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct,\nTokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less\nthan a 0.4% performance drop. We release our code and checkpoints in\nhttps://github.com/hemingkx/TokenSkip."
                },
                "authors": [
                    {
                        "name": "Heming Xia"
                    },
                    {
                        "name": "Chak Tou Leong"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Wenjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Li"
                },
                "author": "Wenjie Li",
                "arxiv_comment": "EMNLP 2025 (Long Paper), camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12067v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12067v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12997v1",
                "updated": "2025-09-16T12:08:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    8,
                    3,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T12:08:03Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    8,
                    3,
                    1,
                    259,
                    0
                ],
                "title": "Drone Detection Using a Low-Power Neuromorphic Virtual Tripwire",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drone Detection Using a Low-Power Neuromorphic Virtual Tripwire"
                },
                "summary": "Small drones are an increasing threat to both military personnel and civilian\ninfrastructure, making early and automated detection crucial. In this work we\ndevelop a system that uses spiking neural networks and neuromorphic cameras\n(event cameras) to detect drones. The detection model is deployed on a\nneuromorphic chip making this a fully neuromorphic system. Multiple detection\nunits can be deployed to create a virtual tripwire which detects when and where\ndrones enter a restricted zone. We show that our neuromorphic solution is\nseveral orders of magnitude more energy efficient than a reference solution\ndeployed on an edge GPU, allowing the system to run for over a year on battery\npower. We investigate how synthetically generated data can be used for\ntraining, and show that our model most likely relies on the shape of the drone\nrather than the temporal characteristics of its propellers. The small size and\nlow power consumption allows easy deployment in contested areas or locations\nthat lack power infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small drones are an increasing threat to both military personnel and civilian\ninfrastructure, making early and automated detection crucial. In this work we\ndevelop a system that uses spiking neural networks and neuromorphic cameras\n(event cameras) to detect drones. The detection model is deployed on a\nneuromorphic chip making this a fully neuromorphic system. Multiple detection\nunits can be deployed to create a virtual tripwire which detects when and where\ndrones enter a restricted zone. We show that our neuromorphic solution is\nseveral orders of magnitude more energy efficient than a reference solution\ndeployed on an edge GPU, allowing the system to run for over a year on battery\npower. We investigate how synthetically generated data can be used for\ntraining, and show that our model most likely relies on the shape of the drone\nrather than the temporal characteristics of its propellers. The small size and\nlow power consumption allows easy deployment in contested areas or locations\nthat lack power infrastructure."
                },
                "authors": [
                    {
                        "name": "Anton Eldeborg Lundin"
                    },
                    {
                        "name": "Rasmus Winzell"
                    },
                    {
                        "name": "Hanna Hamrell"
                    },
                    {
                        "name": "David Gustafsson"
                    },
                    {
                        "name": "Hannes OvrÃ©n"
                    }
                ],
                "author_detail": {
                    "name": "Hannes OvrÃ©n"
                },
                "author": "Hannes OvrÃ©n",
                "arxiv_doi": "10.1007/978-3-031-92460-6_7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-92460-6_7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.12997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "ECCV 2024 Workshops. ECCV 2024. Lecture Notes in Computer Science,\n  vol 15646. Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12994v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12994v1",
                "updated": "2025-09-16T12:06:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    6,
                    5,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T12:06:05Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    6,
                    5,
                    1,
                    259,
                    0
                ],
                "title": "SitLLM: Large Language Models for Sitting Posture Health Understanding\n  via Pressure Sensor Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SitLLM: Large Language Models for Sitting Posture Health Understanding\n  via Pressure Sensor Data"
                },
                "summary": "Poor sitting posture is a critical yet often overlooked factor contributing\nto long-term musculoskeletal disorders and physiological dysfunctions. Existing\nsitting posture monitoring systems, although leveraging visual, IMU, or\npressure-based modalities, often suffer from coarse-grained recognition and\nlack the semantic expressiveness necessary for personalized feedback. In this\npaper, we propose \\textbf{SitLLM}, a lightweight multimodal framework that\nintegrates flexible pressure sensing with large language models (LLMs) to\nenable fine-grained posture understanding and personalized health-oriented\nresponse generation. SitLLM comprises three key components: (1) a\n\\textit{Gaussian-Robust Sensor Embedding Module} that partitions pressure maps\ninto spatial patches and injects local noise perturbations for robust feature\nextraction; (2) a \\textit{Prompt-Driven Cross-Modal Alignment Module} that\nreprograms sensor embeddings into the LLM's semantic space via multi-head\ncross-attention using the pre-trained vocabulary embeddings; and (3) a\n\\textit{Multi-Context Prompt Module} that fuses feature-level, structure-level,\nstatistical-level, and semantic-level contextual information to guide\ninstruction comprehension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Poor sitting posture is a critical yet often overlooked factor contributing\nto long-term musculoskeletal disorders and physiological dysfunctions. Existing\nsitting posture monitoring systems, although leveraging visual, IMU, or\npressure-based modalities, often suffer from coarse-grained recognition and\nlack the semantic expressiveness necessary for personalized feedback. In this\npaper, we propose \\textbf{SitLLM}, a lightweight multimodal framework that\nintegrates flexible pressure sensing with large language models (LLMs) to\nenable fine-grained posture understanding and personalized health-oriented\nresponse generation. SitLLM comprises three key components: (1) a\n\\textit{Gaussian-Robust Sensor Embedding Module} that partitions pressure maps\ninto spatial patches and injects local noise perturbations for robust feature\nextraction; (2) a \\textit{Prompt-Driven Cross-Modal Alignment Module} that\nreprograms sensor embeddings into the LLM's semantic space via multi-head\ncross-attention using the pre-trained vocabulary embeddings; and (3) a\n\\textit{Multi-Context Prompt Module} that fuses feature-level, structure-level,\nstatistical-level, and semantic-level contextual information to guide\ninstruction comprehension."
                },
                "authors": [
                    {
                        "name": "Jian Gao"
                    },
                    {
                        "name": "Fufangchen Zhao"
                    },
                    {
                        "name": "Yiyang Zhang"
                    },
                    {
                        "name": "Danfeng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Danfeng Yan"
                },
                "author": "Danfeng Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12994v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12994v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12993v1",
                "updated": "2025-09-16T12:04:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    4,
                    0,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T12:04:00Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    4,
                    0,
                    1,
                    259,
                    0
                ],
                "title": "HPIM: Heterogeneous Processing-In-Memory-based Accelerator for Large\n  Language Models Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPIM: Heterogeneous Processing-In-Memory-based Accelerator for Large\n  Language Models Inference"
                },
                "summary": "The deployment of large language models (LLMs) presents significant\nchallenges due to their enormous memory footprints, low arithmetic intensity,\nand stringent latency requirements, particularly during the autoregressive\ndecoding stage. Traditional compute-centric accelerators, such as GPUs, suffer\nfrom severe resource underutilization and memory bandwidth bottlenecks in these\nmemory-bound workloads. To overcome these fundamental limitations, we propose\nHPIM, the first memory-centric heterogeneous Processing-In-Memory (PIM)\naccelerator that integrates SRAM-PIM and HBM-PIM subsystems designed\nspecifically for LLM inference. HPIM employs a software-hardware co-design\napproach that combines a specialized compiler framework with a heterogeneous\nhardware architecture. It intelligently partitions workloads based on their\ncharacteristics: latency-critical attention operations are mapped to the\nSRAM-PIM subsystem to exploit its ultra-low latency and high computational\nflexibility, while weight-intensive GEMV computations are assigned to the\nHBM-PIM subsystem to leverage its high internal bandwidth and large storage\ncapacity. Furthermore, HPIM introduces a tightly coupled pipeline strategy\nacross SRAM-PIM and HBM-PIM subsystems to maximize intra-token parallelism,\nthereby significantly mitigating serial dependency of the autoregressive\ndecoding stage. Comprehensive evaluations using a cycle-accurate simulator\ndemonstrate that HPIM significantly outperforms state-of-the-art accelerators,\nachieving a peak speedup of up to 22.8x compared to the NVIDIA A100 GPU.\nMoreover, HPIM exhibits superior performance over contemporary PIM-based\naccelerators, highlighting its potential as a highly practical and scalable\nsolution for accelerating large-scale LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) presents significant\nchallenges due to their enormous memory footprints, low arithmetic intensity,\nand stringent latency requirements, particularly during the autoregressive\ndecoding stage. Traditional compute-centric accelerators, such as GPUs, suffer\nfrom severe resource underutilization and memory bandwidth bottlenecks in these\nmemory-bound workloads. To overcome these fundamental limitations, we propose\nHPIM, the first memory-centric heterogeneous Processing-In-Memory (PIM)\naccelerator that integrates SRAM-PIM and HBM-PIM subsystems designed\nspecifically for LLM inference. HPIM employs a software-hardware co-design\napproach that combines a specialized compiler framework with a heterogeneous\nhardware architecture. It intelligently partitions workloads based on their\ncharacteristics: latency-critical attention operations are mapped to the\nSRAM-PIM subsystem to exploit its ultra-low latency and high computational\nflexibility, while weight-intensive GEMV computations are assigned to the\nHBM-PIM subsystem to leverage its high internal bandwidth and large storage\ncapacity. Furthermore, HPIM introduces a tightly coupled pipeline strategy\nacross SRAM-PIM and HBM-PIM subsystems to maximize intra-token parallelism,\nthereby significantly mitigating serial dependency of the autoregressive\ndecoding stage. Comprehensive evaluations using a cycle-accurate simulator\ndemonstrate that HPIM significantly outperforms state-of-the-art accelerators,\nachieving a peak speedup of up to 22.8x compared to the NVIDIA A100 GPU.\nMoreover, HPIM exhibits superior performance over contemporary PIM-based\naccelerators, highlighting its potential as a highly practical and scalable\nsolution for accelerating large-scale LLM inference."
                },
                "authors": [
                    {
                        "name": "Cenlin Duan"
                    },
                    {
                        "name": "Jianlei Yang"
                    },
                    {
                        "name": "Rubing Yang"
                    },
                    {
                        "name": "Yikun Wang"
                    },
                    {
                        "name": "Yiou Wang"
                    },
                    {
                        "name": "Lingkun Long"
                    },
                    {
                        "name": "Yingjie Qi"
                    },
                    {
                        "name": "Xiaolin He"
                    },
                    {
                        "name": "Ao Zhou"
                    },
                    {
                        "name": "Xueyan Wang"
                    },
                    {
                        "name": "Weisheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Weisheng Zhao"
                },
                "author": "Weisheng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16408v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16408v2",
                "updated": "2025-09-16T11:55:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    55,
                    23,
                    1,
                    259,
                    0
                ],
                "published": "2025-05-22T09:00:01Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    9,
                    0,
                    1,
                    3,
                    142,
                    0
                ],
                "title": "From Surveys to Narratives: Rethinking Cultural Value Adaptation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Surveys to Narratives: Rethinking Cultural Value Adaptation in LLMs"
                },
                "summary": "Adapting cultural values in Large Language Models (LLMs) presents significant\nchallenges, particularly due to biases and limited training data. Prior work\nprimarily aligns LLMs with different cultural values using World Values Survey\n(WVS) data. However, it remains unclear whether this approach effectively\ncaptures cultural nuances or produces distinct cultural representations for\nvarious downstream tasks. In this paper, we systematically investigate\nWVS-based training for cultural value adaptation and find that relying solely\non survey data can homogenize cultural norms and interfere with factual\nknowledge. To investigate these issues, we augment WVS with encyclopedic and\nscenario-based cultural narratives from Wikipedia and NormAd. While these\nnarratives may have variable effects on downstream tasks, they consistently\nimprove cultural distinctiveness than survey data alone. Our work highlights\nthe inherent complexity of aligning cultural values with the goal of guiding\ntask-specific behavior. We release our code at\nhttps://github.com/faridlazuarda/from-surveys-to-narratives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting cultural values in Large Language Models (LLMs) presents significant\nchallenges, particularly due to biases and limited training data. Prior work\nprimarily aligns LLMs with different cultural values using World Values Survey\n(WVS) data. However, it remains unclear whether this approach effectively\ncaptures cultural nuances or produces distinct cultural representations for\nvarious downstream tasks. In this paper, we systematically investigate\nWVS-based training for cultural value adaptation and find that relying solely\non survey data can homogenize cultural norms and interfere with factual\nknowledge. To investigate these issues, we augment WVS with encyclopedic and\nscenario-based cultural narratives from Wikipedia and NormAd. While these\nnarratives may have variable effects on downstream tasks, they consistently\nimprove cultural distinctiveness than survey data alone. Our work highlights\nthe inherent complexity of aligning cultural values with the goal of guiding\ntask-specific behavior. We release our code at\nhttps://github.com/faridlazuarda/from-surveys-to-narratives."
                },
                "authors": [
                    {
                        "name": "Muhammad Farid Adilazuarda"
                    },
                    {
                        "name": "Chen Cecilia Liu"
                    },
                    {
                        "name": "Iryna Gurevych"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    }
                ],
                "author_detail": {
                    "name": "Alham Fikri Aji"
                },
                "author": "Alham Fikri Aji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16408v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16408v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12987v1",
                "updated": "2025-09-16T11:51:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    51,
                    7,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T11:51:07Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    51,
                    7,
                    1,
                    259,
                    0
                ],
                "title": "Toward PDDL Planning Copilot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward PDDL Planning Copilot"
                },
                "summary": "Large Language Models (LLMs) are increasingly being used as autonomous agents\ncapable of performing complicated tasks. However, they lack the ability to\nperform reliable long-horizon planning on their own. This paper bridges this\ngap by introducing the Planning Copilot, a chatbot that integrates multiple\nplanning tools and allows users to invoke them through instructions in natural\nlanguage. The Planning Copilot leverages the Model Context Protocol (MCP), a\nrecently developed standard for connecting LLMs with external tools and\nsystems. This approach allows using any LLM that supports MCP without\ndomain-specific fine-tuning. Our Planning Copilot supports common planning\ntasks such as checking the syntax of planning problems, selecting an\nappropriate planner, calling it, validating the plan it generates, and\nsimulating their execution. We empirically evaluate the ability of our Planning\nCopilot to perform these tasks using three open-source LLMs. The results show\nthat the Planning Copilot highly outperforms using the same LLMs without the\nplanning tools. We also conducted a limited qualitative comparison of our tool\nagainst Chat GPT-5, a very recent commercial LLM. Our results shows that our\nPlanning Copilot significantly outperforms GPT-5 despite relying on a much\nsmaller LLM. This suggests dedicated planning tools may be an effective way to\nenable LLMs to perform planning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being used as autonomous agents\ncapable of performing complicated tasks. However, they lack the ability to\nperform reliable long-horizon planning on their own. This paper bridges this\ngap by introducing the Planning Copilot, a chatbot that integrates multiple\nplanning tools and allows users to invoke them through instructions in natural\nlanguage. The Planning Copilot leverages the Model Context Protocol (MCP), a\nrecently developed standard for connecting LLMs with external tools and\nsystems. This approach allows using any LLM that supports MCP without\ndomain-specific fine-tuning. Our Planning Copilot supports common planning\ntasks such as checking the syntax of planning problems, selecting an\nappropriate planner, calling it, validating the plan it generates, and\nsimulating their execution. We empirically evaluate the ability of our Planning\nCopilot to perform these tasks using three open-source LLMs. The results show\nthat the Planning Copilot highly outperforms using the same LLMs without the\nplanning tools. We also conducted a limited qualitative comparison of our tool\nagainst Chat GPT-5, a very recent commercial LLM. Our results shows that our\nPlanning Copilot significantly outperforms GPT-5 despite relying on a much\nsmaller LLM. This suggests dedicated planning tools may be an effective way to\nenable LLMs to perform planning tasks."
                },
                "authors": [
                    {
                        "name": "Yarin Benyamin"
                    },
                    {
                        "name": "Argaman Mordoch"
                    },
                    {
                        "name": "Shahaf S. Shperberg"
                    },
                    {
                        "name": "Roni Stern"
                    }
                ],
                "author_detail": {
                    "name": "Roni Stern"
                },
                "author": "Roni Stern",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10171v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10171v2",
                "updated": "2025-09-16T11:49:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    49,
                    30,
                    1,
                    259,
                    0
                ],
                "published": "2025-07-14T11:33:47Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    33,
                    47,
                    0,
                    195,
                    0
                ],
                "title": "SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump\n  Prediction via Video Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump\n  Prediction via Video Analysis"
                },
                "summary": "Concrete workability is essential for construction quality, with the slump\ntest being the most common on-site method for its assessment. However,\ntraditional slump testing is manual, time-consuming, and prone to\ninconsistency, limiting its applicability for real-time monitoring. To address\nthese challenges, we propose SlumpGuard, an AI-powered, video-based system that\nautomatically analyzes concrete flow from the truck chute to assess workability\nin real time. Our system enables full-batch inspection without manual\nintervention, improving both the accuracy and efficiency of quality control. We\npresent the system design, the construction of a dedicated dataset, and\nempirical results from real-world deployment, demonstrating the effectiveness\nof SlumpGuard as a practical solution for modern concrete quality assurance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concrete workability is essential for construction quality, with the slump\ntest being the most common on-site method for its assessment. However,\ntraditional slump testing is manual, time-consuming, and prone to\ninconsistency, limiting its applicability for real-time monitoring. To address\nthese challenges, we propose SlumpGuard, an AI-powered, video-based system that\nautomatically analyzes concrete flow from the truck chute to assess workability\nin real time. Our system enables full-batch inspection without manual\nintervention, improving both the accuracy and efficiency of quality control. We\npresent the system design, the construction of a dedicated dataset, and\nempirical results from real-world deployment, demonstrating the effectiveness\nof SlumpGuard as a practical solution for modern concrete quality assurance."
                },
                "authors": [
                    {
                        "name": "Youngmin Kim"
                    },
                    {
                        "name": "Giyeong Oh"
                    },
                    {
                        "name": "Kwangsoo Youm"
                    },
                    {
                        "name": "Youngjae Yu"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Yu"
                },
                "author": "Youngjae Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10171v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10171v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12973v1",
                "updated": "2025-09-16T11:30:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    30,
                    10,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T11:30:10Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    30,
                    10,
                    1,
                    259,
                    0
                ],
                "title": "Evaluating Large Language Models for Code Translation: Effects of Prompt\n  Language and Prompt Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models for Code Translation: Effects of Prompt\n  Language and Prompt Design"
                },
                "summary": "Large language models (LLMs) have shown promise for automated source-code\ntranslation, a capability critical to software migration, maintenance, and\ninteroperability. Yet comparative evidence on how model choice, prompt design,\nand prompt language shape translation quality across multiple programming\nlanguages remains limited. This study conducts a systematic empirical\nassessment of state-of-the-art LLMs for code translation among C++, Java,\nPython, and C#, alongside a traditional baseline (TransCoder). Using BLEU and\nCodeBLEU, we quantify syntactic fidelity and structural correctness under two\nprompt styles (concise instruction and detailed specification) and two prompt\nlanguages (English and Arabic), with direction-aware evaluation across language\npairs. Experiments show that detailed prompts deliver consistent gains across\nmodels and translation directions, and English prompts outperform Arabic by\n13-15%. The top-performing model attains the highest CodeBLEU on challenging\npairs such as Java to C# and Python to C++. Our evaluation shows that each LLM\noutperforms TransCoder across the benchmark. These results demonstrate the\nvalue of careful prompt engineering and prompt language choice, and provide\npractical guidance for software modernization and cross-language\ninteroperability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promise for automated source-code\ntranslation, a capability critical to software migration, maintenance, and\ninteroperability. Yet comparative evidence on how model choice, prompt design,\nand prompt language shape translation quality across multiple programming\nlanguages remains limited. This study conducts a systematic empirical\nassessment of state-of-the-art LLMs for code translation among C++, Java,\nPython, and C#, alongside a traditional baseline (TransCoder). Using BLEU and\nCodeBLEU, we quantify syntactic fidelity and structural correctness under two\nprompt styles (concise instruction and detailed specification) and two prompt\nlanguages (English and Arabic), with direction-aware evaluation across language\npairs. Experiments show that detailed prompts deliver consistent gains across\nmodels and translation directions, and English prompts outperform Arabic by\n13-15%. The top-performing model attains the highest CodeBLEU on challenging\npairs such as Java to C# and Python to C++. Our evaluation shows that each LLM\noutperforms TransCoder across the benchmark. These results demonstrate the\nvalue of careful prompt engineering and prompt language choice, and provide\npractical guidance for software modernization and cross-language\ninteroperability."
                },
                "authors": [
                    {
                        "name": "Aamer Aljagthami"
                    },
                    {
                        "name": "Mohammed Banabila"
                    },
                    {
                        "name": "Musab Alshehri"
                    },
                    {
                        "name": "Mohammed Kabini"
                    },
                    {
                        "name": "Mohammad D. Alahmadi"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad D. Alahmadi"
                },
                "author": "Mohammad D. Alahmadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12769v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12769v3",
                "updated": "2025-09-16T11:12:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    12,
                    12,
                    1,
                    259,
                    0
                ],
                "published": "2025-02-18T11:32:43Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    11,
                    32,
                    43,
                    1,
                    49,
                    0
                ],
                "title": "How Much Do LLMs Hallucinate across Languages? On Multilingual\n  Estimation of LLM Hallucination in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Much Do LLMs Hallucinate across Languages? On Multilingual\n  Estimation of LLM Hallucination in the Wild"
                },
                "summary": "In the age of misinformation, hallucination -- the tendency of Large Language\nModels (LLMs) to generate non-factual or unfaithful responses -- represents the\nmain risk for their global utility. Despite LLMs becoming increasingly\nmultilingual, the vast majority of research on detecting and quantifying LLM\nhallucination are (a) English-centric and (b) focus on machine translation (MT)\nand summarization, tasks that are less common ``in the wild'' than open\ninformation seeking. In contrast, we aim to quantify the extent of LLM\nhallucination across languages in knowledge-intensive long-form question\nanswering. To this end, we train a multilingual hallucination detection model\nand conduct a large-scale study across 30 languages and 6 open-source LLM\nfamilies. We start from an English hallucination detection dataset and rely on\nMT to generate (noisy) training data in other languages. We also manually\nannotate gold data for five high-resource languages; we then demonstrate, for\nthese languages, that the estimates of hallucination rates are similar between\nsilver (LLM-generated) and gold test sets, validating the use of silver data\nfor estimating hallucination rates for other languages. For the final rates\nestimation, we build a knowledge-intensive QA dataset for 30 languages with\nLLM-generated prompts and Wikipedia articles as references. We find that, while\nLLMs generate longer responses with more hallucinated tokens for\nhigher-resource languages, there is no correlation between length-normalized\nhallucination rates of languages and their digital representation. Further, we\nfind that smaller LLMs exhibit larger hallucination rates than larger models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the age of misinformation, hallucination -- the tendency of Large Language\nModels (LLMs) to generate non-factual or unfaithful responses -- represents the\nmain risk for their global utility. Despite LLMs becoming increasingly\nmultilingual, the vast majority of research on detecting and quantifying LLM\nhallucination are (a) English-centric and (b) focus on machine translation (MT)\nand summarization, tasks that are less common ``in the wild'' than open\ninformation seeking. In contrast, we aim to quantify the extent of LLM\nhallucination across languages in knowledge-intensive long-form question\nanswering. To this end, we train a multilingual hallucination detection model\nand conduct a large-scale study across 30 languages and 6 open-source LLM\nfamilies. We start from an English hallucination detection dataset and rely on\nMT to generate (noisy) training data in other languages. We also manually\nannotate gold data for five high-resource languages; we then demonstrate, for\nthese languages, that the estimates of hallucination rates are similar between\nsilver (LLM-generated) and gold test sets, validating the use of silver data\nfor estimating hallucination rates for other languages. For the final rates\nestimation, we build a knowledge-intensive QA dataset for 30 languages with\nLLM-generated prompts and Wikipedia articles as references. We find that, while\nLLMs generate longer responses with more hallucinated tokens for\nhigher-resource languages, there is no correlation between length-normalized\nhallucination rates of languages and their digital representation. Further, we\nfind that smaller LLMs exhibit larger hallucination rates than larger models."
                },
                "authors": [
                    {
                        "name": "Saad Obaid ul Islam"
                    },
                    {
                        "name": "Anne Lauscher"
                    },
                    {
                        "name": "Goran GlavaÅ¡"
                    }
                ],
                "author_detail": {
                    "name": "Goran GlavaÅ¡"
                },
                "author": "Goran GlavaÅ¡",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12769v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12769v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12961v1",
                "updated": "2025-09-16T11:10:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    10,
                    30,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T11:10:30Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    10,
                    30,
                    1,
                    259,
                    0
                ],
                "title": "Do LLMs Understand Wine Descriptors Across Cultures? A Benchmark for\n  Cultural Adaptations of Wine Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Understand Wine Descriptors Across Cultures? A Benchmark for\n  Cultural Adaptations of Wine Reviews"
                },
                "summary": "Recent advances in large language models (LLMs) have opened the door to\nculture-aware language tasks. We introduce the novel problem of adapting wine\nreviews across Chinese and English, which goes beyond literal translation by\nincorporating regional taste preferences and culture-specific flavor\ndescriptors. In a case study on cross-cultural wine review adaptation, we\ncompile the first parallel corpus of professional reviews, containing 8k\nChinese and 16k Anglophone reviews. We benchmark both\nneural-machine-translation baselines and state-of-the-art LLMs with automatic\nmetrics and human evaluation. For the latter, we propose three culture-oriented\ncriteria -- Cultural Proximity, Cultural Neutrality, and Cultural Genuineness\n-- to assess how naturally a translated review resonates with target-culture\nreaders. Our analysis shows that current models struggle to capture cultural\nnuances, especially in translating wine descriptions across different cultures.\nThis highlights the challenges and limitations of translation models in\nhandling cultural content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have opened the door to\nculture-aware language tasks. We introduce the novel problem of adapting wine\nreviews across Chinese and English, which goes beyond literal translation by\nincorporating regional taste preferences and culture-specific flavor\ndescriptors. In a case study on cross-cultural wine review adaptation, we\ncompile the first parallel corpus of professional reviews, containing 8k\nChinese and 16k Anglophone reviews. We benchmark both\nneural-machine-translation baselines and state-of-the-art LLMs with automatic\nmetrics and human evaluation. For the latter, we propose three culture-oriented\ncriteria -- Cultural Proximity, Cultural Neutrality, and Cultural Genuineness\n-- to assess how naturally a translated review resonates with target-culture\nreaders. Our analysis shows that current models struggle to capture cultural\nnuances, especially in translating wine descriptions across different cultures.\nThis highlights the challenges and limitations of translation models in\nhandling cultural content."
                },
                "authors": [
                    {
                        "name": "Chenye Zou"
                    },
                    {
                        "name": "Xingyue Wen"
                    },
                    {
                        "name": "Tianyi Hu"
                    },
                    {
                        "name": "Qian Janice Wang"
                    },
                    {
                        "name": "Daniel Hershcovich"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Hershcovich"
                },
                "author": "Daniel Hershcovich",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12960v1",
                "updated": "2025-09-16T11:06:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    6,
                    58,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T11:06:58Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    6,
                    58,
                    1,
                    259,
                    0
                ],
                "title": "Investigating ReLoRA: Effects on the Learning Dynamics of Small Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating ReLoRA: Effects on the Learning Dynamics of Small Language\n  Models"
                },
                "summary": "Parameter-efficient methods such as LoRA have revolutionised the fine-tuning\nof LLMs. Still, their extension to pretraining via ReLoRA is less well\nunderstood, especially for small language models (SLMs), which offer lower\ncomputational and environmental costs. This work is the first systematic study\nof ReLoRA in SLMs (11M-66M parameters), evaluating both performance and\nlearning dynamics. Through ablation experiments, we find that ReLoRA generally\nperforms worse than standard training on loss, Paloma perplexity and BLiMP,\nwith the gap widening for the larger models. Further analysis of the learning\ndynamics of the models indicates that ReLoRA reinforces the rank deficiencies\nfound in smaller models. These results indicate that low-rank update strategies\nmay not transfer easily to SLM pretraining, highlighting the need for more\nresearch in the low-compute regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-efficient methods such as LoRA have revolutionised the fine-tuning\nof LLMs. Still, their extension to pretraining via ReLoRA is less well\nunderstood, especially for small language models (SLMs), which offer lower\ncomputational and environmental costs. This work is the first systematic study\nof ReLoRA in SLMs (11M-66M parameters), evaluating both performance and\nlearning dynamics. Through ablation experiments, we find that ReLoRA generally\nperforms worse than standard training on loss, Paloma perplexity and BLiMP,\nwith the gap widening for the larger models. Further analysis of the learning\ndynamics of the models indicates that ReLoRA reinforces the rank deficiencies\nfound in smaller models. These results indicate that low-rank update strategies\nmay not transfer easily to SLM pretraining, highlighting the need for more\nresearch in the low-compute regime."
                },
                "authors": [
                    {
                        "name": "Yuval Weiss"
                    },
                    {
                        "name": "David Demitri Africa"
                    },
                    {
                        "name": "Paula Buttery"
                    },
                    {
                        "name": "Richard Diehl Martinez"
                    }
                ],
                "author_detail": {
                    "name": "Richard Diehl Martinez"
                },
                "author": "Richard Diehl Martinez",
                "arxiv_comment": "12 Pages, 6 Tables, 8 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12958v1",
                "updated": "2025-09-16T11:01:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    1,
                    59,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T11:01:59Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    1,
                    59,
                    1,
                    259,
                    0
                ],
                "title": "Forget What's Sensitive, Remember What Matters: Token-Level Differential\n  Privacy in Memory Sculpting for Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forget What's Sensitive, Remember What Matters: Token-Level Differential\n  Privacy in Memory Sculpting for Continual Learning"
                },
                "summary": "Continual Learning (CL) models, while adept at sequential knowledge\nacquisition, face significant and often overlooked privacy challenges due to\naccumulating diverse information. Traditional privacy methods, like a uniform\nDifferential Privacy (DP) budget, indiscriminately protect all data, leading to\nsubstantial model utility degradation and hindering CL deployment in\nprivacy-sensitive areas. To overcome this, we propose a privacy-enhanced\ncontinual learning (PeCL) framework that forgets what's sensitive and remembers\nwhat matters. Our approach first introduces a token-level dynamic Differential\nPrivacy strategy that adaptively allocates privacy budgets based on the\nsemantic sensitivity of individual tokens. This ensures robust protection for\nprivate entities while minimizing noise injection for non-sensitive, general\nknowledge. Second, we integrate a privacy-guided memory sculpting module. This\nmodule leverages the sensitivity analysis from our dynamic DP mechanism to\nintelligently forget sensitive information from the model's memory and\nparameters, while explicitly preserving the task-invariant historical knowledge\ncrucial for mitigating catastrophic forgetting. Extensive experiments show that\nPeCL achieves a superior balance between privacy preserving and model utility,\noutperforming baseline models by maintaining high accuracy on previous tasks\nwhile ensuring robust privacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Learning (CL) models, while adept at sequential knowledge\nacquisition, face significant and often overlooked privacy challenges due to\naccumulating diverse information. Traditional privacy methods, like a uniform\nDifferential Privacy (DP) budget, indiscriminately protect all data, leading to\nsubstantial model utility degradation and hindering CL deployment in\nprivacy-sensitive areas. To overcome this, we propose a privacy-enhanced\ncontinual learning (PeCL) framework that forgets what's sensitive and remembers\nwhat matters. Our approach first introduces a token-level dynamic Differential\nPrivacy strategy that adaptively allocates privacy budgets based on the\nsemantic sensitivity of individual tokens. This ensures robust protection for\nprivate entities while minimizing noise injection for non-sensitive, general\nknowledge. Second, we integrate a privacy-guided memory sculpting module. This\nmodule leverages the sensitivity analysis from our dynamic DP mechanism to\nintelligently forget sensitive information from the model's memory and\nparameters, while explicitly preserving the task-invariant historical knowledge\ncrucial for mitigating catastrophic forgetting. Extensive experiments show that\nPeCL achieves a superior balance between privacy preserving and model utility,\noutperforming baseline models by maintaining high accuracy on previous tasks\nwhile ensuring robust privacy."
                },
                "authors": [
                    {
                        "name": "Bihao Zhan"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Junsong Li"
                    },
                    {
                        "name": "Yutao Yang"
                    },
                    {
                        "name": "Shilian Chen"
                    },
                    {
                        "name": "Qianjun Pan"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Wen Wu"
                    },
                    {
                        "name": "Xingjiao Wu"
                    },
                    {
                        "name": "Qin Chen"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12951v1",
                "updated": "2025-09-16T10:55:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    55,
                    50,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T10:55:50Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    55,
                    50,
                    1,
                    259,
                    0
                ],
                "title": "Black-box Model Merging for Language-Model-as-a-Service with Massive\n  Model Repositories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box Model Merging for Language-Model-as-a-Service with Massive\n  Model Repositories"
                },
                "summary": "Model merging refers to the process of integrating multiple distinct models\ninto a unified model that preserves and combines the strengths and capabilities\nof the individual models. Most existing approaches rely on task vectors to\ncombine models, typically under the assumption that model parameters are\naccessible. However, for extremely large language models (LLMs) such as GPT-4,\nwhich are often provided solely as black-box services through API interfaces\n(Language-Model-as-a-Service), model weights are not available to end users.\nThis presents a significant challenge, which we refer to as black-box model\nmerging (BMM) with massive LLMs. To address this challenge, we propose a\nderivative-free optimization framework based on the evolutionary algorithm\n(Evo-Merging) that enables effective model merging using only inference-time\nAPI queries. Our method consists of two key components: (1) sparsity-based\ndenoising, designed to identify and filter out irrelevant or redundant\ninformation across models, and (2) sign-aware scaling, which dynamically\ncomputes optimal combination weights for the relevant models based on their\nperformance. We also provide a formal justification, along with a theoretical\nanalysis, for our asymmetric sparsification. Extensive experimental evaluations\ndemonstrate that our approach achieves state-of-the-art results on a range of\ntasks, significantly outperforming existing strong baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging refers to the process of integrating multiple distinct models\ninto a unified model that preserves and combines the strengths and capabilities\nof the individual models. Most existing approaches rely on task vectors to\ncombine models, typically under the assumption that model parameters are\naccessible. However, for extremely large language models (LLMs) such as GPT-4,\nwhich are often provided solely as black-box services through API interfaces\n(Language-Model-as-a-Service), model weights are not available to end users.\nThis presents a significant challenge, which we refer to as black-box model\nmerging (BMM) with massive LLMs. To address this challenge, we propose a\nderivative-free optimization framework based on the evolutionary algorithm\n(Evo-Merging) that enables effective model merging using only inference-time\nAPI queries. Our method consists of two key components: (1) sparsity-based\ndenoising, designed to identify and filter out irrelevant or redundant\ninformation across models, and (2) sign-aware scaling, which dynamically\ncomputes optimal combination weights for the relevant models based on their\nperformance. We also provide a formal justification, along with a theoretical\nanalysis, for our asymmetric sparsification. Extensive experimental evaluations\ndemonstrate that our approach achieves state-of-the-art results on a range of\ntasks, significantly outperforming existing strong baselines."
                },
                "authors": [
                    {
                        "name": "Shilian Chen"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Tianyu Huai"
                    },
                    {
                        "name": "Yujiang Lu"
                    },
                    {
                        "name": "Junsong Li"
                    },
                    {
                        "name": "Bihao Zhan"
                    },
                    {
                        "name": "Qianjun Pan"
                    },
                    {
                        "name": "Yutao Yang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Qin Chen"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12949v1",
                "updated": "2025-09-16T10:53:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    53,
                    57,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T10:53:57Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    53,
                    57,
                    1,
                    259,
                    0
                ],
                "title": "First Practical Experiences Integrating Quantum Computers with HPC\n  Resources: A Case Study With a 20-qubit Superconducting Quantum Computer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First Practical Experiences Integrating Quantum Computers with HPC\n  Resources: A Case Study With a 20-qubit Superconducting Quantum Computer"
                },
                "summary": "Incorporating Quantum Computers into High Performance Computing (HPC)\nenvironments (commonly referred to as HPC+QC integration) marks a pivotal step\nin advancing computational capabilities for scientific research. Here we report\nthe integration of a superconducting 20-qubit quantum computer into the HPC\ninfrastructure at Leibniz Supercomputing Centre (LRZ), one of the first\npractical implementations of its kind. This yielded four key lessons: (1)\nquantum computers have stricter facility requirements than classical systems,\nyet their deployment in HPC environments is feasible when preceded by a\nrigorous site survey to ensure compliance; (2) quantum computers are inherently\ndynamic systems that require regular recalibration that is automatic and\ncontrollable by the HPC scheduler; (3) redundant power and cooling\ninfrastructure is essential; and (4) effective hands-on onboarding should be\nprovided for both quantum experts and new users. The identified conclusions\nprovide a roadmap to guide future HPC center integrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating Quantum Computers into High Performance Computing (HPC)\nenvironments (commonly referred to as HPC+QC integration) marks a pivotal step\nin advancing computational capabilities for scientific research. Here we report\nthe integration of a superconducting 20-qubit quantum computer into the HPC\ninfrastructure at Leibniz Supercomputing Centre (LRZ), one of the first\npractical implementations of its kind. This yielded four key lessons: (1)\nquantum computers have stricter facility requirements than classical systems,\nyet their deployment in HPC environments is feasible when preceded by a\nrigorous site survey to ensure compliance; (2) quantum computers are inherently\ndynamic systems that require regular recalibration that is automatic and\ncontrollable by the HPC scheduler; (3) redundant power and cooling\ninfrastructure is essential; and (4) effective hands-on onboarding should be\nprovided for both quantum experts and new users. The identified conclusions\nprovide a roadmap to guide future HPC center integrations."
                },
                "authors": [
                    {
                        "name": "Eric Mansfield"
                    },
                    {
                        "name": "Stefan Seegerer"
                    },
                    {
                        "name": "Panu Vesanen"
                    },
                    {
                        "name": "Jorge Echavarria"
                    },
                    {
                        "name": "Burak Mete"
                    },
                    {
                        "name": "Muhammad Nufail Farooqi"
                    },
                    {
                        "name": "Laura Schulz"
                    }
                ],
                "author_detail": {
                    "name": "Laura Schulz"
                },
                "author": "Laura Schulz",
                "arxiv_comment": "submitted and accepted to SFWM workshop at SC25, St Louis, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17356v2",
                "updated": "2025-09-16T10:52:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    52,
                    32,
                    1,
                    259,
                    0
                ],
                "published": "2025-04-24T08:16:36Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    16,
                    36,
                    3,
                    114,
                    0
                ],
                "title": "Comprehend, Divide, and Conquer: Feature Subspace Exploration via\n  Multi-Agent Hierarchical Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehend, Divide, and Conquer: Feature Subspace Exploration via\n  Multi-Agent Hierarchical Reinforcement Learning"
                },
                "summary": "Feature selection aims to preprocess the target dataset, find an optimal and\nmost streamlined feature subset, and enhance the downstream machine learning\ntask. Among filter, wrapper, and embedded-based approaches, the reinforcement\nlearning (RL)-based subspace exploration strategy provides a novel objective\noptimization-directed perspective and promising performance. Nevertheless, even\nwith improved performance, current reinforcement learning approaches face\nchallenges similar to conventional methods when dealing with complex datasets.\nThese challenges stem from the inefficient paradigm of using one agent per\nfeature and the inherent complexities present in the datasets. This observation\nmotivates us to investigate and address the above issue and propose a novel\napproach, namely HRLFS. Our methodology initially employs a Large Language\nModel (LLM)-based hybrid state extractor to capture each feature's mathematical\nand semantic characteristics. Based on this information, features are\nclustered, facilitating the construction of hierarchical agents for each\ncluster and sub-cluster. Extensive experiments demonstrate the efficiency,\nscalability, and robustness of our approach. Compared to contemporary or the\none-feature-one-agent RL-based approaches, HRLFS improves the downstream ML\nperformance with iterative feature subspace exploration while accelerating\ntotal run time by reducing the number of agents involved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature selection aims to preprocess the target dataset, find an optimal and\nmost streamlined feature subset, and enhance the downstream machine learning\ntask. Among filter, wrapper, and embedded-based approaches, the reinforcement\nlearning (RL)-based subspace exploration strategy provides a novel objective\noptimization-directed perspective and promising performance. Nevertheless, even\nwith improved performance, current reinforcement learning approaches face\nchallenges similar to conventional methods when dealing with complex datasets.\nThese challenges stem from the inefficient paradigm of using one agent per\nfeature and the inherent complexities present in the datasets. This observation\nmotivates us to investigate and address the above issue and propose a novel\napproach, namely HRLFS. Our methodology initially employs a Large Language\nModel (LLM)-based hybrid state extractor to capture each feature's mathematical\nand semantic characteristics. Based on this information, features are\nclustered, facilitating the construction of hierarchical agents for each\ncluster and sub-cluster. Extensive experiments demonstrate the efficiency,\nscalability, and robustness of our approach. Compared to contemporary or the\none-feature-one-agent RL-based approaches, HRLFS improves the downstream ML\nperformance with iterative feature subspace exploration while accelerating\ntotal run time by reducing the number of agents involved."
                },
                "authors": [
                    {
                        "name": "Weiliang Zhang"
                    },
                    {
                        "name": "Xiaohan Huang"
                    },
                    {
                        "name": "Yi Du"
                    },
                    {
                        "name": "Ziyue Qiao"
                    },
                    {
                        "name": "Qingqing Long"
                    },
                    {
                        "name": "Zhen Meng"
                    },
                    {
                        "name": "Yuanchun Zhou"
                    },
                    {
                        "name": "Meng Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Meng Xiao"
                },
                "author": "Meng Xiao",
                "arxiv_comment": "20 pages, keywords: Automated Feature Engineering, Tabular Dataset,\n  Multi-Agent Reinforcement Learning, Feature Selection",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13223v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13223v2",
                "updated": "2025-09-16T10:43:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    43,
                    40,
                    1,
                    259,
                    0
                ],
                "published": "2025-03-17T14:36:08Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    36,
                    8,
                    0,
                    76,
                    0
                ],
                "title": "Robust Decision-Making Via Free Energy Minimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Decision-Making Via Free Energy Minimization"
                },
                "summary": "Despite their groundbreaking performance, state-of-the-art autonomous agents\ncan misbehave when training and environmental conditions become inconsistent,\nwith minor mismatches leading to undesirable behaviors or even catastrophic\nfailures. Robustness towards these training/environment ambiguities is a core\nrequirement for intelligent agents and its fulfillment is a long-standing\nchallenge when deploying agents in the real world. Here, we introduce a\nDistributionally Robust Free Energy model (DR-FREE) that instills this core\nproperty by design. It directly wires robustness into the agent decision-making\nmechanisms via free energy minimization. By combining a robust extension of the\nfree energy principle with a novel resolution engine, DR-FREE returns a policy\nthat is optimal-yet-robust against ambiguity. The policy has an explicit,\nsoft-max, structure that reveals the mechanistic role of ambiguity on optimal\ndecisions and requisite Bayesian belief updating. We evaluate DR-FREE on an\nexperimental testbed involving real rovers navigating an ambiguous environment\nfilled with obstacles. Across all the experiments, DR-FREE enables robots to\nsuccessfully navigate towards their goal even when, in contrast,\nstate-of-the-art free energy models fail. In short, DR-FREE can tackle\nscenarios that elude previous methods: this milestone may inspire both\ndeployment in multi-agent settings and, at a perhaps deeper level, the quest\nfor a biologically plausible explanation of how natural agents -- with little\nor no training -- survive in capricious environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their groundbreaking performance, state-of-the-art autonomous agents\ncan misbehave when training and environmental conditions become inconsistent,\nwith minor mismatches leading to undesirable behaviors or even catastrophic\nfailures. Robustness towards these training/environment ambiguities is a core\nrequirement for intelligent agents and its fulfillment is a long-standing\nchallenge when deploying agents in the real world. Here, we introduce a\nDistributionally Robust Free Energy model (DR-FREE) that instills this core\nproperty by design. It directly wires robustness into the agent decision-making\nmechanisms via free energy minimization. By combining a robust extension of the\nfree energy principle with a novel resolution engine, DR-FREE returns a policy\nthat is optimal-yet-robust against ambiguity. The policy has an explicit,\nsoft-max, structure that reveals the mechanistic role of ambiguity on optimal\ndecisions and requisite Bayesian belief updating. We evaluate DR-FREE on an\nexperimental testbed involving real rovers navigating an ambiguous environment\nfilled with obstacles. Across all the experiments, DR-FREE enables robots to\nsuccessfully navigate towards their goal even when, in contrast,\nstate-of-the-art free energy models fail. In short, DR-FREE can tackle\nscenarios that elude previous methods: this milestone may inspire both\ndeployment in multi-agent settings and, at a perhaps deeper level, the quest\nfor a biologically plausible explanation of how natural agents -- with little\nor no training -- survive in capricious environments."
                },
                "authors": [
                    {
                        "name": "Allahkaram Shafiei"
                    },
                    {
                        "name": "Hozefa Jesawada"
                    },
                    {
                        "name": "Karl Friston"
                    },
                    {
                        "name": "Giovanni Russo"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Russo"
                },
                "author": "Giovanni Russo",
                "arxiv_comment": "Contains main text and supplementary information",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13223v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13223v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08171v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08171v2",
                "updated": "2025-09-16T10:35:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    35,
                    33,
                    1,
                    259,
                    0
                ],
                "published": "2025-06-09T19:33:30Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    33,
                    30,
                    0,
                    160,
                    0
                ],
                "title": "Worst-Case Symbolic Constraints Analysis and Generalisation with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Worst-Case Symbolic Constraints Analysis and Generalisation with Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated strong performance on coding\ntasks such as generation, completion and repair, but their ability to handle\ncomplex symbolic reasoning over code still remains underexplored. We introduce\nthe task of worst-case symbolic constraints analysis, which requires inferring\nthe symbolic constraints that characterise worst-case program executions; these\nconstraints can be solved to obtain inputs that expose performance bottlenecks\nor denial-of-service vulnerabilities in software systems. We show that even\nstate-of-the-art LLMs (e.g., GPT-5) struggle when applied directly on this\ntask. To address this challenge, we propose WARP, an innovative neurosymbolic\napproach that computes worst-case constraints on smaller concrete input sizes\nusing existing program analysis tools, and then leverages LLMs to generalise\nthese constraints to larger input sizes. Concretely, WARP comprises: (1) an\nincremental strategy for LLM-based worst-case reasoning, (2) a solver-aligned\nneurosymbolic framework that integrates reinforcement learning with SMT\n(Satisfiability Modulo Theories) solving, and (3) a curated dataset of symbolic\nconstraints. Experimental results show that WARP consistently improves\nperformance on worst-case constraint reasoning. Leveraging the curated\nconstraint dataset, we use reinforcement learning to fine-tune a model,\nWARP-1.0-3B, which significantly outperforms size-matched and even larger\nbaselines. These results demonstrate that incremental constraint reasoning\nenhances LLMs' ability to handle symbolic reasoning and highlight the potential\nfor deeper integration between neural learning and formal methods in rigorous\nprogram analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong performance on coding\ntasks such as generation, completion and repair, but their ability to handle\ncomplex symbolic reasoning over code still remains underexplored. We introduce\nthe task of worst-case symbolic constraints analysis, which requires inferring\nthe symbolic constraints that characterise worst-case program executions; these\nconstraints can be solved to obtain inputs that expose performance bottlenecks\nor denial-of-service vulnerabilities in software systems. We show that even\nstate-of-the-art LLMs (e.g., GPT-5) struggle when applied directly on this\ntask. To address this challenge, we propose WARP, an innovative neurosymbolic\napproach that computes worst-case constraints on smaller concrete input sizes\nusing existing program analysis tools, and then leverages LLMs to generalise\nthese constraints to larger input sizes. Concretely, WARP comprises: (1) an\nincremental strategy for LLM-based worst-case reasoning, (2) a solver-aligned\nneurosymbolic framework that integrates reinforcement learning with SMT\n(Satisfiability Modulo Theories) solving, and (3) a curated dataset of symbolic\nconstraints. Experimental results show that WARP consistently improves\nperformance on worst-case constraint reasoning. Leveraging the curated\nconstraint dataset, we use reinforcement learning to fine-tune a model,\nWARP-1.0-3B, which significantly outperforms size-matched and even larger\nbaselines. These results demonstrate that incremental constraint reasoning\nenhances LLMs' ability to handle symbolic reasoning and highlight the potential\nfor deeper integration between neural learning and formal methods in rigorous\nprogram analysis."
                },
                "authors": [
                    {
                        "name": "Daniel Koh"
                    },
                    {
                        "name": "Yannic Noller"
                    },
                    {
                        "name": "Corina S. Pasareanu"
                    },
                    {
                        "name": "Adrians Skapars"
                    },
                    {
                        "name": "Youcheng Sun"
                    }
                ],
                "author_detail": {
                    "name": "Youcheng Sun"
                },
                "author": "Youcheng Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08171v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08171v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12937v1",
                "updated": "2025-09-16T10:34:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    34,
                    26,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T10:34:26Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    34,
                    26,
                    1,
                    259,
                    0
                ],
                "title": "Jailbreaking Large Language Models Through Content Concretization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking Large Language Models Through Content Concretization"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed for task automation\nand content generation, yet their safety mechanisms remain vulnerable to\ncircumvention through different jailbreaking techniques. In this paper, we\nintroduce \\textit{Content Concretization} (CC), a novel jailbreaking technique\nthat iteratively transforms abstract malicious requests into concrete,\nexecutable implementations. CC is a two-stage process: first, generating\ninitial LLM responses using lower-tier, less constrained safety filters models,\nthen refining them through higher-tier models that process both the preliminary\noutput and original prompt. We evaluate our technique using 350\ncybersecurity-specific prompts, demonstrating substantial improvements in\njailbreak Success Rates (SRs), increasing from 7\\% (no refinements) to 62\\%\nafter three refinement iterations, while maintaining a cost of 7.5\\textcent~per\nprompt. Comparative A/B testing across nine different LLM evaluators confirms\nthat outputs from additional refinement steps are consistently rated as more\nmalicious and technically superior. Moreover, manual code analysis reveals that\ngenerated outputs execute with minimal modification, although optimal\ndeployment typically requires target-specific fine-tuning. With eventual\nimproved harmful code generation, these results highlight critical\nvulnerabilities in current LLM safety frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed for task automation\nand content generation, yet their safety mechanisms remain vulnerable to\ncircumvention through different jailbreaking techniques. In this paper, we\nintroduce \\textit{Content Concretization} (CC), a novel jailbreaking technique\nthat iteratively transforms abstract malicious requests into concrete,\nexecutable implementations. CC is a two-stage process: first, generating\ninitial LLM responses using lower-tier, less constrained safety filters models,\nthen refining them through higher-tier models that process both the preliminary\noutput and original prompt. We evaluate our technique using 350\ncybersecurity-specific prompts, demonstrating substantial improvements in\njailbreak Success Rates (SRs), increasing from 7\\% (no refinements) to 62\\%\nafter three refinement iterations, while maintaining a cost of 7.5\\textcent~per\nprompt. Comparative A/B testing across nine different LLM evaluators confirms\nthat outputs from additional refinement steps are consistently rated as more\nmalicious and technically superior. Moreover, manual code analysis reveals that\ngenerated outputs execute with minimal modification, although optimal\ndeployment typically requires target-specific fine-tuning. With eventual\nimproved harmful code generation, these results highlight critical\nvulnerabilities in current LLM safety frameworks."
                },
                "authors": [
                    {
                        "name": "Johan WahrÃ©us"
                    },
                    {
                        "name": "Ahmed Hussain"
                    },
                    {
                        "name": "Panos Papadimitratos"
                    }
                ],
                "author_detail": {
                    "name": "Panos Papadimitratos"
                },
                "author": "Panos Papadimitratos",
                "arxiv_comment": "Accepted for presentation in the Conference on Game Theory and AI for\n  Security (GameSec) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v3",
                "updated": "2025-09-16T10:33:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    33,
                    29,
                    1,
                    259,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_doi": "10.1145/3744916.3764523",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3744916.3764523",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08523v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ICSE '26 (The 48th IEEE/ACM International Conference on\n  Software Engineering)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12936v1",
                "updated": "2025-09-16T10:32:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    32,
                    59,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T10:32:59Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    32,
                    59,
                    1,
                    259,
                    0
                ],
                "title": "Rethinking the Evaluation of Alignment Methods: Insights into Diversity,\n  Generalisation, and Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking the Evaluation of Alignment Methods: Insights into Diversity,\n  Generalisation, and Safety"
                },
                "summary": "Large language models (LLMs) require careful alignment to balance competing\nobjectives - factuality, safety, conciseness, proactivity, and diversity.\nExisting studies focus on individual techniques or specific dimensions, lacking\na holistic assessment of the inherent trade-offs. We propose a unified\nevaluation framework that compares LLM alignment methods (PPO, DPO, ORPO, KTO)\nacross these five axes, using both in-distribution and out-of-distribution\ndatasets. Leveraging a specialized LLM-as-Judge prompt, validated through human\nstudies, we reveal that DPO and KTO excel in factual accuracy, PPO and DPO lead\nin safety, and PPO best balances conciseness with proactivity. Our findings\nprovide insights into trade-offs of common alignment methods, guiding the\ndevelopment of more balanced and reliable LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) require careful alignment to balance competing\nobjectives - factuality, safety, conciseness, proactivity, and diversity.\nExisting studies focus on individual techniques or specific dimensions, lacking\na holistic assessment of the inherent trade-offs. We propose a unified\nevaluation framework that compares LLM alignment methods (PPO, DPO, ORPO, KTO)\nacross these five axes, using both in-distribution and out-of-distribution\ndatasets. Leveraging a specialized LLM-as-Judge prompt, validated through human\nstudies, we reveal that DPO and KTO excel in factual accuracy, PPO and DPO lead\nin safety, and PPO best balances conciseness with proactivity. Our findings\nprovide insights into trade-offs of common alignment methods, guiding the\ndevelopment of more balanced and reliable LLMs."
                },
                "authors": [
                    {
                        "name": "Denis Janiak"
                    },
                    {
                        "name": "Julia Moska"
                    },
                    {
                        "name": "Dawid Motyka"
                    },
                    {
                        "name": "Karolina Seweryn"
                    },
                    {
                        "name": "PaweÅ Walkowiak"
                    },
                    {
                        "name": "Bartosz Å»uk"
                    },
                    {
                        "name": "Arkadiusz Janz"
                    }
                ],
                "author_detail": {
                    "name": "Arkadiusz Janz"
                },
                "author": "Arkadiusz Janz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09767v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09767v2",
                "updated": "2025-09-16T10:28:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    28,
                    11,
                    1,
                    259,
                    0
                ],
                "published": "2025-08-13T12:52:38Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    52,
                    38,
                    2,
                    225,
                    0
                ],
                "title": "UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in\n  Multilingual Text-to-Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in\n  Multilingual Text-to-Speech"
                },
                "summary": "We propose UtterTune, a lightweight adaptation method that fine-tunes a\nmultilingual text-to-speech (TTS) system based on a large language model (LLM)\narchitecture, designed to enhance the controllability of pronunciation in a\ntarget language while preserving performance in others. While LLM architectures\nhave enabled TTS models to achieve remarkable naturalness, accurately modeling\ngrapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially\nwhen the model omits an explicit G2P module and directly processes minimally\nencoded text (e.g., byte-pair encoding). UtterTune leverages low-rank\nadaptation to enable the control of segmental pronunciation and pitch accent at\nthe phoneme level for Japanese speech, the target language in this paper, while\nmaintaining naturalness and speaker similarity in a zero-shot setting.\nObjective and subjective evaluations confirm its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose UtterTune, a lightweight adaptation method that fine-tunes a\nmultilingual text-to-speech (TTS) system based on a large language model (LLM)\narchitecture, designed to enhance the controllability of pronunciation in a\ntarget language while preserving performance in others. While LLM architectures\nhave enabled TTS models to achieve remarkable naturalness, accurately modeling\ngrapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially\nwhen the model omits an explicit G2P module and directly processes minimally\nencoded text (e.g., byte-pair encoding). UtterTune leverages low-rank\nadaptation to enable the control of segmental pronunciation and pitch accent at\nthe phoneme level for Japanese speech, the target language in this paper, while\nmaintaining naturalness and speaker similarity in a zero-shot setting.\nObjective and subjective evaluations confirm its effectiveness."
                },
                "authors": [
                    {
                        "name": "Shuhei Kato"
                    }
                ],
                "author_detail": {
                    "name": "Shuhei Kato"
                },
                "author": "Shuhei Kato",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09767v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09767v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12927v1",
                "updated": "2025-09-16T10:26:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    26,
                    12,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T10:26:12Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    26,
                    12,
                    1,
                    259,
                    0
                ],
                "title": "HLSMAC: A New StarCraft Multi-Agent Challenge for High-Level Strategic\n  Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HLSMAC: A New StarCraft Multi-Agent Challenge for High-Level Strategic\n  Decision-Making"
                },
                "summary": "Benchmarks are crucial for assessing multi-agent reinforcement learning\n(MARL) algorithms. While StarCraft II-related environments have driven\nsignificant advances in MARL, existing benchmarks like SMAC focus primarily on\nmicromanagement, limiting comprehensive evaluation of high-level strategic\nintelligence. To address this, we introduce HLSMAC, a new cooperative MARL\nbenchmark with 12 carefully designed StarCraft II scenarios based on classical\nstratagems from the Thirty-Six Stratagems. Each scenario corresponds to a\nspecific stratagem and is designed to challenge agents with diverse strategic\nelements, including tactical maneuvering, timing coordination, and deception,\nthereby opening up avenues for evaluating high-level strategic decision-making\ncapabilities. We also propose novel metrics across multiple dimensions beyond\nconventional win rate, such as ability utilization and advancement efficiency,\nto assess agents' overall performance within the HLSMAC environment. We\nintegrate state-of-the-art MARL algorithms and LLM-based agents with our\nbenchmark and conduct comprehensive experiments. The results demonstrate that\nHLSMAC serves as a robust testbed for advancing multi-agent strategic\ndecision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarks are crucial for assessing multi-agent reinforcement learning\n(MARL) algorithms. While StarCraft II-related environments have driven\nsignificant advances in MARL, existing benchmarks like SMAC focus primarily on\nmicromanagement, limiting comprehensive evaluation of high-level strategic\nintelligence. To address this, we introduce HLSMAC, a new cooperative MARL\nbenchmark with 12 carefully designed StarCraft II scenarios based on classical\nstratagems from the Thirty-Six Stratagems. Each scenario corresponds to a\nspecific stratagem and is designed to challenge agents with diverse strategic\nelements, including tactical maneuvering, timing coordination, and deception,\nthereby opening up avenues for evaluating high-level strategic decision-making\ncapabilities. We also propose novel metrics across multiple dimensions beyond\nconventional win rate, such as ability utilization and advancement efficiency,\nto assess agents' overall performance within the HLSMAC environment. We\nintegrate state-of-the-art MARL algorithms and LLM-based agents with our\nbenchmark and conduct comprehensive experiments. The results demonstrate that\nHLSMAC serves as a robust testbed for advancing multi-agent strategic\ndecision-making."
                },
                "authors": [
                    {
                        "name": "Xingxing Hong"
                    },
                    {
                        "name": "Yungong Wang"
                    },
                    {
                        "name": "Dexin Jin"
                    },
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Ximing Huang"
                    },
                    {
                        "name": "Zijian Wu"
                    },
                    {
                        "name": "Wenxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenxin Li"
                },
                "author": "Wenxin Li",
                "arxiv_comment": "30 pages, 13 figures with appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19075v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19075v2",
                "updated": "2025-09-16T10:22:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    22,
                    22,
                    1,
                    259,
                    0
                ],
                "published": "2025-04-27T02:10:12Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    2,
                    10,
                    12,
                    6,
                    117,
                    0
                ],
                "title": "HoloDx: Knowledge- and Data-Driven Multimodal Diagnosis of Alzheimer's\n  Disease",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HoloDx: Knowledge- and Data-Driven Multimodal Diagnosis of Alzheimer's\n  Disease"
                },
                "summary": "Accurate diagnosis of Alzheimer's disease (AD) requires effectively\nintegrating multimodal data and clinical expertise. However, existing methods\noften struggle to fully utilize multimodal information and lack structured\nmechanisms to incorporate dynamic domain knowledge. To address these\nlimitations, we propose HoloDx, a knowledge- and data-driven framework that\nenhances AD diagnosis by aligning domain knowledge with multimodal clinical\ndata. HoloDx incorporates a knowledge injection module with a knowledge-aware\ngated cross-attention, allowing the model to dynamically integrate\ndomain-specific insights from both large language models (LLMs) and clinical\nexpertise. Also, a memory injection module with a designed prototypical memory\nattention enables the model to retain and retrieve subject-specific\ninformation, ensuring consistency in decision-making. By jointly leveraging\nthese mechanisms, HoloDx enhances interpretability, improves robustness, and\neffectively aligns prior knowledge with current subject data. Evaluations on\nfive AD datasets demonstrate that HoloDx outperforms state-of-the-art methods,\nachieving superior diagnostic accuracy and strong generalization across diverse\ncohorts. The source code will be released upon publication acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate diagnosis of Alzheimer's disease (AD) requires effectively\nintegrating multimodal data and clinical expertise. However, existing methods\noften struggle to fully utilize multimodal information and lack structured\nmechanisms to incorporate dynamic domain knowledge. To address these\nlimitations, we propose HoloDx, a knowledge- and data-driven framework that\nenhances AD diagnosis by aligning domain knowledge with multimodal clinical\ndata. HoloDx incorporates a knowledge injection module with a knowledge-aware\ngated cross-attention, allowing the model to dynamically integrate\ndomain-specific insights from both large language models (LLMs) and clinical\nexpertise. Also, a memory injection module with a designed prototypical memory\nattention enables the model to retain and retrieve subject-specific\ninformation, ensuring consistency in decision-making. By jointly leveraging\nthese mechanisms, HoloDx enhances interpretability, improves robustness, and\neffectively aligns prior knowledge with current subject data. Evaluations on\nfive AD datasets demonstrate that HoloDx outperforms state-of-the-art methods,\nachieving superior diagnostic accuracy and strong generalization across diverse\ncohorts. The source code will be released upon publication acceptance."
                },
                "authors": [
                    {
                        "name": "Qiuhui Chen"
                    },
                    {
                        "name": "Jintao Wang"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Yi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Yi Hong"
                },
                "author": "Yi Hong",
                "arxiv_comment": "Accepted by IEEE Transactions on Medical Imaging (TMI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19075v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19075v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12918v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12918v2",
                "updated": "2025-09-17T12:59:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    12,
                    59,
                    25,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-16T10:11:59Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    11,
                    59,
                    1,
                    259,
                    0
                ],
                "title": "A Novel Compression Framework for YOLOv8: Achieving Real-Time Aerial\n  Object Detection on Edge Devices via Structured Pruning and Channel-Wise\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Compression Framework for YOLOv8: Achieving Real-Time Aerial\n  Object Detection on Edge Devices via Structured Pruning and Channel-Wise\n  Distillation"
                },
                "summary": "Efficient deployment of deep learning models for aerial object detection on\nresource-constrained devices requires significant compression without\ncom-promising performance. In this study, we propose a novel three-stage\ncompression pipeline for the YOLOv8 object detection model, integrating\nsparsity-aware training, structured channel pruning, and Channel-Wise Knowledge\nDistillation (CWD). First, sparsity-aware training introduces dynamic sparsity\nduring model optimization, effectively balancing parameter reduction and\ndetection accuracy. Second, we apply structured channel pruning by leveraging\nbatch normalization scaling factors to eliminate redundant channels,\nsignificantly reducing model size and computational complexity. Finally, to\nmitigate the accuracy drop caused by pruning, we employ CWD to transfer\nknowledge from the original model, using an adjustable temperature and loss\nweighting scheme tailored for small and medium object detection. Extensive\nexperiments on the VisDrone dataset demonstrate the effectiveness of our\napproach across multiple YOLOv8 variants. For YOLOv8m, our method reduces model\nparameters from 25.85M to 6.85M (a 73.51% reduction), FLOPs from 49.6G to\n13.3G, and MACs from 101G to 34.5G, while reducing AP50 by only 2.7%. The\nresulting compressed model achieves 47.9 AP50 and boosts inference speed from\n26 FPS (YOLOv8m baseline) to 45 FPS, enabling real-time deployment on edge\ndevices. We further apply TensorRT as a lightweight optimization step. While\nthis introduces a minor drop in AP50 (from 47.9 to 47.6), it significantly\nimproves inference speed from 45 to 68 FPS, demonstrating the practicality of\nour approach for high-throughput, re-source-constrained scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient deployment of deep learning models for aerial object detection on\nresource-constrained devices requires significant compression without\ncom-promising performance. In this study, we propose a novel three-stage\ncompression pipeline for the YOLOv8 object detection model, integrating\nsparsity-aware training, structured channel pruning, and Channel-Wise Knowledge\nDistillation (CWD). First, sparsity-aware training introduces dynamic sparsity\nduring model optimization, effectively balancing parameter reduction and\ndetection accuracy. Second, we apply structured channel pruning by leveraging\nbatch normalization scaling factors to eliminate redundant channels,\nsignificantly reducing model size and computational complexity. Finally, to\nmitigate the accuracy drop caused by pruning, we employ CWD to transfer\nknowledge from the original model, using an adjustable temperature and loss\nweighting scheme tailored for small and medium object detection. Extensive\nexperiments on the VisDrone dataset demonstrate the effectiveness of our\napproach across multiple YOLOv8 variants. For YOLOv8m, our method reduces model\nparameters from 25.85M to 6.85M (a 73.51% reduction), FLOPs from 49.6G to\n13.3G, and MACs from 101G to 34.5G, while reducing AP50 by only 2.7%. The\nresulting compressed model achieves 47.9 AP50 and boosts inference speed from\n26 FPS (YOLOv8m baseline) to 45 FPS, enabling real-time deployment on edge\ndevices. We further apply TensorRT as a lightweight optimization step. While\nthis introduces a minor drop in AP50 (from 47.9 to 47.6), it significantly\nimproves inference speed from 45 to 68 FPS, demonstrating the practicality of\nour approach for high-throughput, re-source-constrained scenarios."
                },
                "authors": [
                    {
                        "name": "Melika Sabaghian"
                    },
                    {
                        "name": "Mohammad Ali Keyvanrad"
                    },
                    {
                        "name": "Seyyedeh Mahila Moghadami"
                    }
                ],
                "author_detail": {
                    "name": "Seyyedeh Mahila Moghadami"
                },
                "author": "Seyyedeh Mahila Moghadami",
                "arxiv_comment": "28 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12918v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12918v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12914v1",
                "updated": "2025-09-16T10:09:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    9,
                    0,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T10:09:00Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    9,
                    0,
                    1,
                    259,
                    0
                ],
                "title": "Stochastic Streets: A Walk Through Random LLM Address Generation in four\n  European Cities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Streets: A Walk Through Random LLM Address Generation in four\n  European Cities"
                },
                "summary": "Large Language Models (LLMs) are capable of solving complex math problems or\nanswer difficult questions on almost any topic, but can they generate random\nstreet addresses for European cities?",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are capable of solving complex math problems or\nanswer difficult questions on almost any topic, but can they generate random\nstreet addresses for European cities?"
                },
                "authors": [
                    {
                        "name": "Tairan Fu"
                    },
                    {
                        "name": "David Campo-Nazareno"
                    },
                    {
                        "name": "Javier Coronado-BlÃ¡zquez"
                    },
                    {
                        "name": "Javier Conde"
                    },
                    {
                        "name": "Pedro Reviriego"
                    },
                    {
                        "name": "Fabrizio Lombardi"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Lombardi"
                },
                "author": "Fabrizio Lombardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12908v1",
                "updated": "2025-09-16T10:02:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    2,
                    52,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T10:02:52Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    2,
                    52,
                    1,
                    259,
                    0
                ],
                "title": "All Roads Lead to Rome: Graph-Based Confidence Estimation for Large\n  Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All Roads Lead to Rome: Graph-Based Confidence Estimation for Large\n  Language Model Reasoning"
                },
                "summary": "Confidence estimation is essential for the reliable deployment of large\nlanguage models (LLMs). Existing methods are primarily designed for factual QA\ntasks and often fail to generalize to reasoning tasks. To address this gap, we\npropose a set of training-free, graph-based confidence estimation methods\ntailored to reasoning tasks. Our approach models reasoning paths as directed\ngraphs and estimates confidence by exploiting graph properties such as\ncentrality, path convergence, and path weighting. Experiments with two LLMs on\nthree reasoning datasets demonstrate improved confidence estimation and\nenhanced performance on two downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence estimation is essential for the reliable deployment of large\nlanguage models (LLMs). Existing methods are primarily designed for factual QA\ntasks and often fail to generalize to reasoning tasks. To address this gap, we\npropose a set of training-free, graph-based confidence estimation methods\ntailored to reasoning tasks. Our approach models reasoning paths as directed\ngraphs and estimates confidence by exploiting graph properties such as\ncentrality, path convergence, and path weighting. Experiments with two LLMs on\nthree reasoning datasets demonstrate improved confidence estimation and\nenhanced performance on two downstream tasks."
                },
                "authors": [
                    {
                        "name": "Caiqi Zhang"
                    },
                    {
                        "name": "Chang Shu"
                    },
                    {
                        "name": "Ehsan Shareghi"
                    },
                    {
                        "name": "Nigel Collier"
                    }
                ],
                "author_detail": {
                    "name": "Nigel Collier"
                },
                "author": "Nigel Collier",
                "arxiv_comment": "EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12892v1",
                "updated": "2025-09-16T09:48:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    48,
                    11,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:48:11Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    48,
                    11,
                    1,
                    259,
                    0
                ],
                "title": "Conan-Embedding-v2: Training an LLM from Scratch for Text Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conan-Embedding-v2: Training an LLM from Scratch for Text Embeddings"
                },
                "summary": "Large language models (LLMs) have recently demonstrated excellent performance\nin text embedding tasks. Previous work usually use LoRA to fine-tune existing\nLLMs, which are limited by the data and training gap between LLMs and embedding\nmodels. In this work, we introduce Conan-embedding-v2, a new 1.4B-parameter LLM\ntrained from scratch and fine-tuned as a text embedder. First, we add news data\nand multilingual pairs for LLM pretraining to bridge the data gap. Based on\nthis, we propose a cross-lingual retrieval dataset that enables the LLM to\nbetter integrate embeddings across different languages. Second, whereas LLMs\nuse a causal mask with token-level loss, embedding models use a bidirectional\nmask with sentence-level loss. This training gap makes full fine-tuning less\neffective than LoRA. We introduce a soft-masking mechanism to gradually\ntransition between these two types of masks, enabling the model to learn more\ncomprehensive representations. Based on this, we propose a dynamic hard\nnegative mining method that exposes the model to more difficult negative\nexamples throughout the training process. Being intuitive and effective, with\nonly approximately 1.4B parameters, Conan-embedding-v2 achieves SOTA\nperformance on both the Massive Text Embedding Benchmark (MTEB) and Chinese\nMTEB (May 19, 2025).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently demonstrated excellent performance\nin text embedding tasks. Previous work usually use LoRA to fine-tune existing\nLLMs, which are limited by the data and training gap between LLMs and embedding\nmodels. In this work, we introduce Conan-embedding-v2, a new 1.4B-parameter LLM\ntrained from scratch and fine-tuned as a text embedder. First, we add news data\nand multilingual pairs for LLM pretraining to bridge the data gap. Based on\nthis, we propose a cross-lingual retrieval dataset that enables the LLM to\nbetter integrate embeddings across different languages. Second, whereas LLMs\nuse a causal mask with token-level loss, embedding models use a bidirectional\nmask with sentence-level loss. This training gap makes full fine-tuning less\neffective than LoRA. We introduce a soft-masking mechanism to gradually\ntransition between these two types of masks, enabling the model to learn more\ncomprehensive representations. Based on this, we propose a dynamic hard\nnegative mining method that exposes the model to more difficult negative\nexamples throughout the training process. Being intuitive and effective, with\nonly approximately 1.4B parameters, Conan-embedding-v2 achieves SOTA\nperformance on both the Massive Text Embedding Benchmark (MTEB) and Chinese\nMTEB (May 19, 2025)."
                },
                "authors": [
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Yang Tang"
                    },
                    {
                        "name": "Ruijie Liu"
                    },
                    {
                        "name": "Shi-Zhe Chen"
                    },
                    {
                        "name": "Xi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xi Chen"
                },
                "author": "Xi Chen",
                "arxiv_comment": "EMNLP 2025 Oral",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12886v1",
                "updated": "2025-09-16T09:38:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    38,
                    41,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:38:41Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    38,
                    41,
                    1,
                    259,
                    0
                ],
                "title": "The LLM Already Knows: Estimating LLM-Perceived Question Difficulty via\n  Hidden Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LLM Already Knows: Estimating LLM-Perceived Question Difficulty via\n  Hidden Representations"
                },
                "summary": "Estimating the difficulty of input questions as perceived by large language\nmodels (LLMs) is essential for accurate performance evaluation and adaptive\ninference. Existing methods typically rely on repeated response sampling,\nauxiliary models, or fine-tuning the target model itself, which may incur\nsubstantial computational costs or compromise generality. In this paper, we\npropose a novel approach for difficulty estimation that leverages only the\nhidden representations produced by the target LLM. We model the token-level\ngeneration process as a Markov chain and define a value function to estimate\nthe expected output quality given any hidden state. This allows for efficient\nand accurate difficulty estimation based solely on the initial hidden state,\nwithout generating any output tokens. Extensive experiments across both textual\nand multimodal tasks demonstrate that our method consistently outperforms\nexisting baselines in difficulty estimation. Moreover, we apply our difficulty\nestimates to guide adaptive reasoning strategies, including Self-Consistency,\nBest-of-N, and Self-Refine, achieving higher inference efficiency with fewer\ngenerated tokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating the difficulty of input questions as perceived by large language\nmodels (LLMs) is essential for accurate performance evaluation and adaptive\ninference. Existing methods typically rely on repeated response sampling,\nauxiliary models, or fine-tuning the target model itself, which may incur\nsubstantial computational costs or compromise generality. In this paper, we\npropose a novel approach for difficulty estimation that leverages only the\nhidden representations produced by the target LLM. We model the token-level\ngeneration process as a Markov chain and define a value function to estimate\nthe expected output quality given any hidden state. This allows for efficient\nand accurate difficulty estimation based solely on the initial hidden state,\nwithout generating any output tokens. Extensive experiments across both textual\nand multimodal tasks demonstrate that our method consistently outperforms\nexisting baselines in difficulty estimation. Moreover, we apply our difficulty\nestimates to guide adaptive reasoning strategies, including Self-Consistency,\nBest-of-N, and Self-Refine, achieving higher inference efficiency with fewer\ngenerated tokens."
                },
                "authors": [
                    {
                        "name": "Yubo Zhu"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Zecheng Lin"
                    },
                    {
                        "name": "Wei Tong"
                    },
                    {
                        "name": "Sheng Zhong"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13695v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13695v4",
                "updated": "2025-09-16T09:35:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    35,
                    9,
                    1,
                    259,
                    0
                ],
                "published": "2025-06-16T16:58:55Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    58,
                    55,
                    0,
                    167,
                    0
                ],
                "title": "OneRec Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneRec Technical Report"
                },
                "summary": "Recommender systems have been widely used in various large-scale\nuser-oriented platforms for many years. However, compared to the rapid\ndevelopments in the AI community, recommendation systems have not achieved a\nbreakthrough in recent years. For instance, they still rely on a multi-stage\ncascaded architecture rather than an end-to-end approach, leading to\ncomputational fragmentation and optimization inconsistencies, and hindering the\neffective application of key breakthrough technologies from the AI community in\nrecommendation scenarios.\n  To address these issues, we propose OneRec, which reshapes the recommendation\nsystem through an end-to-end generative approach and achieves promising\nresults. Firstly, we have enhanced the computational FLOPs of the current\nrecommendation model by 10 $\\times$ and have identified the scaling laws for\nrecommendations within certain boundaries. Secondly, reinforcement learning\ntechniques, previously difficult to apply for optimizing recommendations, show\nsignificant potential in this framework. Lastly, through infrastructure\noptimizations, we have achieved 23.7% and 28.8% Model FLOPs Utilization (MFU)\non flagship GPUs during training and inference, respectively, aligning closely\nwith the LLM community. This architecture significantly reduces communication\nand storage overhead, resulting in operating expense that is only 10.6% of\ntraditional recommendation pipelines. Deployed in Kuaishou/Kuaishou Lite APP,\nit handles 25% of total queries per second, enhancing overall App Stay Time by\n0.54% and 1.24%, respectively. Additionally, we have observed significant\nincreases in metrics such as 7-day Lifetime, which is a crucial indicator of\nrecommendation experience. We also provide practical lessons and insights\nderived from developing, optimizing, and maintaining a production-scale\nrecommendation system with significant real-world impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems have been widely used in various large-scale\nuser-oriented platforms for many years. However, compared to the rapid\ndevelopments in the AI community, recommendation systems have not achieved a\nbreakthrough in recent years. For instance, they still rely on a multi-stage\ncascaded architecture rather than an end-to-end approach, leading to\ncomputational fragmentation and optimization inconsistencies, and hindering the\neffective application of key breakthrough technologies from the AI community in\nrecommendation scenarios.\n  To address these issues, we propose OneRec, which reshapes the recommendation\nsystem through an end-to-end generative approach and achieves promising\nresults. Firstly, we have enhanced the computational FLOPs of the current\nrecommendation model by 10 $\\times$ and have identified the scaling laws for\nrecommendations within certain boundaries. Secondly, reinforcement learning\ntechniques, previously difficult to apply for optimizing recommendations, show\nsignificant potential in this framework. Lastly, through infrastructure\noptimizations, we have achieved 23.7% and 28.8% Model FLOPs Utilization (MFU)\non flagship GPUs during training and inference, respectively, aligning closely\nwith the LLM community. This architecture significantly reduces communication\nand storage overhead, resulting in operating expense that is only 10.6% of\ntraditional recommendation pipelines. Deployed in Kuaishou/Kuaishou Lite APP,\nit handles 25% of total queries per second, enhancing overall App Stay Time by\n0.54% and 1.24%, respectively. Additionally, we have observed significant\nincreases in metrics such as 7-day Lifetime, which is a crucial indicator of\nrecommendation experience. We also provide practical lessons and insights\nderived from developing, optimizing, and maintaining a production-scale\nrecommendation system with significant real-world impact."
                },
                "authors": [
                    {
                        "name": "Guorui Zhou"
                    },
                    {
                        "name": "Jiaxin Deng"
                    },
                    {
                        "name": "Jinghao Zhang"
                    },
                    {
                        "name": "Kuo Cai"
                    },
                    {
                        "name": "Lejian Ren"
                    },
                    {
                        "name": "Qiang Luo"
                    },
                    {
                        "name": "Qianqian Wang"
                    },
                    {
                        "name": "Qigen Hu"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Shiyao Wang"
                    },
                    {
                        "name": "Weifeng Ding"
                    },
                    {
                        "name": "Wuchao Li"
                    },
                    {
                        "name": "Xinchen Luo"
                    },
                    {
                        "name": "Xingmei Wang"
                    },
                    {
                        "name": "Zexuan Cheng"
                    },
                    {
                        "name": "Zixing Zhang"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Boxuan Wang"
                    },
                    {
                        "name": "Chaoyi Ma"
                    },
                    {
                        "name": "Chengru Song"
                    },
                    {
                        "name": "Chenhui Wang"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Dongxue Meng"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Fangyu Zhang"
                    },
                    {
                        "name": "Feng Jiang"
                    },
                    {
                        "name": "Fuxing Zhang"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Guowang Zhang"
                    },
                    {
                        "name": "Han Li"
                    },
                    {
                        "name": "Hengrui Hu"
                    },
                    {
                        "name": "Hezheng Lin"
                    },
                    {
                        "name": "Hongtao Cheng"
                    },
                    {
                        "name": "Hongyang Cao"
                    },
                    {
                        "name": "Huanjie Wang"
                    },
                    {
                        "name": "Jiaming Huang"
                    },
                    {
                        "name": "Jiapeng Chen"
                    },
                    {
                        "name": "Jiaqiang Liu"
                    },
                    {
                        "name": "Jinghui Jia"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Lantao Hu"
                    },
                    {
                        "name": "Liang Zeng"
                    },
                    {
                        "name": "Liao Yu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Qidong Zhou"
                    },
                    {
                        "name": "Shengzhe Wang"
                    },
                    {
                        "name": "Shihui He"
                    },
                    {
                        "name": "Shuang Yang"
                    },
                    {
                        "name": "Shujie Yang"
                    },
                    {
                        "name": "Sui Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Tiantian He"
                    },
                    {
                        "name": "Tingting Gao"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Xiaoxiao Xu"
                    },
                    {
                        "name": "Xugang Liu"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Yiwu Liu"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Yunfan Wu"
                    },
                    {
                        "name": "Yunfeng Zhao"
                    },
                    {
                        "name": "Zhanyu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhanyu Liu"
                },
                "author": "Zhanyu Liu",
                "arxiv_comment": "Authors are listed alphabetically by their first name",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13695v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13695v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01129v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01129v5",
                "updated": "2025-09-16T09:27:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    27,
                    11,
                    1,
                    259,
                    0
                ],
                "published": "2024-04-01T14:11:45Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    14,
                    11,
                    45,
                    0,
                    92,
                    0
                ],
                "title": "Emphasising Structured Information: Integrating Abstract Meaning\n  Representation into LLMs for Enhanced Open-Domain Dialogue Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emphasising Structured Information: Integrating Abstract Meaning\n  Representation into LLMs for Enhanced Open-Domain Dialogue Evaluation"
                },
                "summary": "Automatic open-domain dialogue evaluation has attracted increasing attention,\nyet remains challenging due to the complexity of assessing response\nappropriateness. Traditional evaluation metrics, typically trained with true\npositive and randomly selected negative responses, tend to assign higher scores\nto responses that share greater content similarity with contexts. However,\nadversarial negative responses, despite possessing high lexical overlap with\ncontexts, can be semantically incongruous. Consequently, existing metrics\nstruggle to effectively evaluate such responses, resulting in low correlations\nwith human judgments. While recent studies have demonstrated the effectiveness\nof Large Language Models (LLMs) for open-domain dialogue evaluation, they still\nface challenges in handling adversarial negative examples. We propose a novel\nevaluation framework that integrates Abstract Meaning Representation (AMR)\nenhanced domain-specific language models (SLMs) with LLMs. Our SLMs explicitly\nincorporate AMR graph information through a gating mechanism for enhanced\nsemantic representation learning, while both SLM predictions and AMR knowledge\nare integrated into LLM prompts for robust evaluation. Extensive experiments on\nopen-domain dialogue evaluation tasks demonstrate the superiority of our method\ncompared to state-of-the-art baselines. Our comprehensive ablation studies\nreveal that AMR graph information contributes substantially more to performance\nimprovements. Our framework achieves strong correlations with human judgments\nacross multiple datasets, establishing a new benchmark for dialogue evaluation.\nOur code and data are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic open-domain dialogue evaluation has attracted increasing attention,\nyet remains challenging due to the complexity of assessing response\nappropriateness. Traditional evaluation metrics, typically trained with true\npositive and randomly selected negative responses, tend to assign higher scores\nto responses that share greater content similarity with contexts. However,\nadversarial negative responses, despite possessing high lexical overlap with\ncontexts, can be semantically incongruous. Consequently, existing metrics\nstruggle to effectively evaluate such responses, resulting in low correlations\nwith human judgments. While recent studies have demonstrated the effectiveness\nof Large Language Models (LLMs) for open-domain dialogue evaluation, they still\nface challenges in handling adversarial negative examples. We propose a novel\nevaluation framework that integrates Abstract Meaning Representation (AMR)\nenhanced domain-specific language models (SLMs) with LLMs. Our SLMs explicitly\nincorporate AMR graph information through a gating mechanism for enhanced\nsemantic representation learning, while both SLM predictions and AMR knowledge\nare integrated into LLM prompts for robust evaluation. Extensive experiments on\nopen-domain dialogue evaluation tasks demonstrate the superiority of our method\ncompared to state-of-the-art baselines. Our comprehensive ablation studies\nreveal that AMR graph information contributes substantially more to performance\nimprovements. Our framework achieves strong correlations with human judgments\nacross multiple datasets, establishing a new benchmark for dialogue evaluation.\nOur code and data are publicly available."
                },
                "authors": [
                    {
                        "name": "Bohao Yang"
                    },
                    {
                        "name": "Kun Zhao"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Liang Zhan"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01129v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01129v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12871v1",
                "updated": "2025-09-16T09:24:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    24,
                    37,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:24:37Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    24,
                    37,
                    1,
                    259,
                    0
                ],
                "title": "Cumulative Consensus Score: Label-Free and Model-Agnostic Evaluation of\n  Object Detectors in Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cumulative Consensus Score: Label-Free and Model-Agnostic Evaluation of\n  Object Detectors in Deployment"
                },
                "summary": "Evaluating object detection models in deployment is challenging because\nground-truth annotations are rarely available. We introduce the Cumulative\nConsensus Score (CCS), a label-free metric that enables continuous monitoring\nand comparison of detectors in real-world settings. CCS applies test-time data\naugmentation to each image, collects predicted bounding boxes across augmented\nviews, and computes overlaps using Intersection over Union. Maximum overlaps\nare normalized and averaged across augmentation pairs, yielding a measure of\nspatial consistency that serves as a proxy for reliability without annotations.\nIn controlled experiments on Open Images and KITTI, CCS achieved over 90%\ncongruence with F1-score, Probabilistic Detection Quality, and Optimal\nCorrection Cost. The method is model-agnostic, working across single-stage and\ntwo-stage detectors, and operates at the case level to highlight\nunder-performing scenarios. Altogether, CCS provides a robust foundation for\nDevOps-style monitoring of object detectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating object detection models in deployment is challenging because\nground-truth annotations are rarely available. We introduce the Cumulative\nConsensus Score (CCS), a label-free metric that enables continuous monitoring\nand comparison of detectors in real-world settings. CCS applies test-time data\naugmentation to each image, collects predicted bounding boxes across augmented\nviews, and computes overlaps using Intersection over Union. Maximum overlaps\nare normalized and averaged across augmentation pairs, yielding a measure of\nspatial consistency that serves as a proxy for reliability without annotations.\nIn controlled experiments on Open Images and KITTI, CCS achieved over 90%\ncongruence with F1-score, Probabilistic Detection Quality, and Optimal\nCorrection Cost. The method is model-agnostic, working across single-stage and\ntwo-stage detectors, and operates at the case level to highlight\nunder-performing scenarios. Altogether, CCS provides a robust foundation for\nDevOps-style monitoring of object detectors."
                },
                "authors": [
                    {
                        "name": "Avinaash Manoharan"
                    },
                    {
                        "name": "Xiangyu Yin"
                    },
                    {
                        "name": "Domenik Helm"
                    },
                    {
                        "name": "Chih-Hong Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Chih-Hong Cheng"
                },
                "author": "Chih-Hong Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12870v1",
                "updated": "2025-09-16T09:24:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    24,
                    8,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:24:08Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    24,
                    8,
                    1,
                    259,
                    0
                ],
                "title": "Towards personalized, precise and survey-free environment recognition:\n  AI-enhanced sensor fusion without pre-deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards personalized, precise and survey-free environment recognition:\n  AI-enhanced sensor fusion without pre-deployment"
                },
                "summary": "Accurate and personalized environment recognition is essential for seamless\nindoor positioning and optimized connectivity, yet traditional fingerprinting\nrequires costly site surveys and lacks user-level adaptation. We present a\nsurvey-free, on-device sensor-fusion framework that builds a personalized,\nlightweight multi-source fingerprint (FP) database from pedestrian dead\nreckoning (PDR), WiFi/cellular, GNSS, and interaction time tags. Matching is\nperformed by an AI-enhanced dynamic time warping module (AIDTW) that aligns\nnoisy, asynchronous sequences. To turn perception into continually improving\nactions, a cloud-edge online Reinforcement Learning from Human Feedback (RLHF)\nloop aggregates desensitized summaries and human feedback in the cloud to\noptimize a policy via proximal policy optimization (PPO), and periodically\ndistills updates to devices. Across indoor/outdoor scenarios, our system\nreduces network-transition latency (measured by time-to-switch, TTS) by 32-65%\nin daily environments compared with conventional baselines, without\nsite-specific pre-deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and personalized environment recognition is essential for seamless\nindoor positioning and optimized connectivity, yet traditional fingerprinting\nrequires costly site surveys and lacks user-level adaptation. We present a\nsurvey-free, on-device sensor-fusion framework that builds a personalized,\nlightweight multi-source fingerprint (FP) database from pedestrian dead\nreckoning (PDR), WiFi/cellular, GNSS, and interaction time tags. Matching is\nperformed by an AI-enhanced dynamic time warping module (AIDTW) that aligns\nnoisy, asynchronous sequences. To turn perception into continually improving\nactions, a cloud-edge online Reinforcement Learning from Human Feedback (RLHF)\nloop aggregates desensitized summaries and human feedback in the cloud to\noptimize a policy via proximal policy optimization (PPO), and periodically\ndistills updates to devices. Across indoor/outdoor scenarios, our system\nreduces network-transition latency (measured by time-to-switch, TTS) by 32-65%\nin daily environments compared with conventional baselines, without\nsite-specific pre-deployment."
                },
                "authors": [
                    {
                        "name": "Ruichen Wang"
                    },
                    {
                        "name": "Zhikang Ni"
                    },
                    {
                        "name": "Pengzhou Wang"
                    },
                    {
                        "name": "Xiya Cao"
                    },
                    {
                        "name": "Zhi Li"
                    },
                    {
                        "name": "Bao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Bao Zhang"
                },
                "author": "Bao Zhang",
                "arxiv_comment": "5 pages, 7 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12867v1",
                "updated": "2025-09-16T09:22:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    22,
                    21,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:22:21Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    22,
                    21,
                    1,
                    259,
                    0
                ],
                "title": "Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use"
                },
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding and reasoning, yet they remain limited when tackling\nreal-world tasks that require up-to-date knowledge, precise operations, or\nspecialized tool use. To address this, we propose Tool-R1, a reinforcement\nlearning framework that enables LLMs to perform general, compositional, and\nmulti-step tool use by generating executable Python code. Tool-R1 supports\nintegration of user-defined tools and standard libraries, with variable sharing\nacross steps to construct coherent workflows. An outcome-based reward function,\ncombining LLM-based answer judgment and code execution success, guides policy\noptimization. To improve training efficiency, we maintain a dynamic sample\nqueue to cache and reuse high-quality trajectories, reducing the overhead of\ncostly online sampling. Experiments on the GAIA benchmark show that Tool-R1\nsubstantially improves both accuracy and robustness, achieving about 10\\% gain\nover strong baselines, with larger improvements on complex multi-step tasks.\nThese results highlight the potential of Tool-R1 for enabling reliable and\nefficient tool-augmented reasoning in real-world applications. Our code will be\navailable at https://github.com/YBYBZhang/Tool-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding and reasoning, yet they remain limited when tackling\nreal-world tasks that require up-to-date knowledge, precise operations, or\nspecialized tool use. To address this, we propose Tool-R1, a reinforcement\nlearning framework that enables LLMs to perform general, compositional, and\nmulti-step tool use by generating executable Python code. Tool-R1 supports\nintegration of user-defined tools and standard libraries, with variable sharing\nacross steps to construct coherent workflows. An outcome-based reward function,\ncombining LLM-based answer judgment and code execution success, guides policy\noptimization. To improve training efficiency, we maintain a dynamic sample\nqueue to cache and reuse high-quality trajectories, reducing the overhead of\ncostly online sampling. Experiments on the GAIA benchmark show that Tool-R1\nsubstantially improves both accuracy and robustness, achieving about 10\\% gain\nover strong baselines, with larger improvements on complex multi-step tasks.\nThese results highlight the potential of Tool-R1 for enabling reliable and\nefficient tool-augmented reasoning in real-world applications. Our code will be\navailable at https://github.com/YBYBZhang/Tool-R1."
                },
                "authors": [
                    {
                        "name": "Yabo Zhang"
                    },
                    {
                        "name": "Yihan Zeng"
                    },
                    {
                        "name": "Qingyun Li"
                    },
                    {
                        "name": "Zhen Hu"
                    },
                    {
                        "name": "Kavin Han"
                    },
                    {
                        "name": "Wangmeng Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Wangmeng Zuo"
                },
                "author": "Wangmeng Zuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12866v1",
                "updated": "2025-09-16T09:22:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    22,
                    17,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:22:17Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    22,
                    17,
                    1,
                    259,
                    0
                ],
                "title": "Leveraging Large Language Models to Effectively Generate Visual Data for\n  Canine Musculoskeletal Diagnoses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models to Effectively Generate Visual Data for\n  Canine Musculoskeletal Diagnoses"
                },
                "summary": "It is well-established that more data generally improves AI model\nperformance. However, data collection can be challenging for certain tasks due\nto the rarity of occurrences or high costs. These challenges are evident in our\nuse case, where we apply AI models to a novel approach for visually documenting\nthe musculoskeletal condition of dogs. Here, abnormalities are marked as\ncolored strokes on a body map of a dog. Since these strokes correspond to\ndistinct muscles or joints, they can be mapped to the textual domain in which\nlarge language models (LLMs) operate. LLMs have demonstrated impressive\ncapabilities across a wide range of tasks, including medical applications,\noffering promising potential for generating synthetic training data. In this\nwork, we investigate whether LLMs can effectively generate synthetic visual\ntraining data for canine musculoskeletal diagnoses. For this, we developed a\nmapping that segments visual documentations into over 200 labeled regions\nrepresenting muscles or joints. Using techniques like guided decoding,\nchain-of-thought reasoning, and few-shot prompting, we generated 1,000\nsynthetic visual documentations for patellar luxation (kneecap dislocation)\ndiagnosis, the diagnosis for which we have the most real-world data. Our\nanalysis shows that the generated documentations are sensitive to location and\nseverity of the diagnosis while remaining independent of the dog's sex. We\nfurther generated 1,000 visual documentations for various other diagnoses to\ncreate a binary classification dataset. A model trained solely on this\nsynthetic data achieved an F1 score of 88% on 70 real-world documentations.\nThese results demonstrate the potential of LLM-generated synthetic data, which\nis particularly valuable for addressing data scarcity in rare diseases. While\nour methodology is tailored to the medical domain, the insights and techniques\ncan be adapted to other fields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is well-established that more data generally improves AI model\nperformance. However, data collection can be challenging for certain tasks due\nto the rarity of occurrences or high costs. These challenges are evident in our\nuse case, where we apply AI models to a novel approach for visually documenting\nthe musculoskeletal condition of dogs. Here, abnormalities are marked as\ncolored strokes on a body map of a dog. Since these strokes correspond to\ndistinct muscles or joints, they can be mapped to the textual domain in which\nlarge language models (LLMs) operate. LLMs have demonstrated impressive\ncapabilities across a wide range of tasks, including medical applications,\noffering promising potential for generating synthetic training data. In this\nwork, we investigate whether LLMs can effectively generate synthetic visual\ntraining data for canine musculoskeletal diagnoses. For this, we developed a\nmapping that segments visual documentations into over 200 labeled regions\nrepresenting muscles or joints. Using techniques like guided decoding,\nchain-of-thought reasoning, and few-shot prompting, we generated 1,000\nsynthetic visual documentations for patellar luxation (kneecap dislocation)\ndiagnosis, the diagnosis for which we have the most real-world data. Our\nanalysis shows that the generated documentations are sensitive to location and\nseverity of the diagnosis while remaining independent of the dog's sex. We\nfurther generated 1,000 visual documentations for various other diagnoses to\ncreate a binary classification dataset. A model trained solely on this\nsynthetic data achieved an F1 score of 88% on 70 real-world documentations.\nThese results demonstrate the potential of LLM-generated synthetic data, which\nis particularly valuable for addressing data scarcity in rare diseases. While\nour methodology is tailored to the medical domain, the insights and techniques\ncan be adapted to other fields."
                },
                "authors": [
                    {
                        "name": "Martin ThiÃen"
                    },
                    {
                        "name": "Thi Ngoc Diep Tran"
                    },
                    {
                        "name": "Barbara Esteve Ratsch"
                    },
                    {
                        "name": "Ben Joel SchÃ¶nbein"
                    },
                    {
                        "name": "Ute Trapp"
                    },
                    {
                        "name": "Beate Egner"
                    },
                    {
                        "name": "Romana Piat"
                    },
                    {
                        "name": "Elke HergenrÃ¶ther"
                    }
                ],
                "author_detail": {
                    "name": "Elke HergenrÃ¶ther"
                },
                "author": "Elke HergenrÃ¶ther",
                "arxiv_doi": "10.24132/CSRN.2025-3",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.24132/CSRN.2025-3",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.12866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Computer Science Research Notes 3501(1) (2025) 27-38",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20900v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20900v3",
                "updated": "2025-09-16T09:20:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    20,
                    46,
                    1,
                    259,
                    0
                ],
                "published": "2025-08-28T15:29:51Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    29,
                    51,
                    3,
                    240,
                    0
                ],
                "title": "OneRec-V2 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneRec-V2 Technical Report"
                },
                "summary": "Recent breakthroughs in generative AI have transformed recommender systems\nthrough end-to-end generation. OneRec reformulates recommendation as an\nautoregressive generation task, achieving high Model FLOPs Utilization. While\nOneRec-V1 has shown significant empirical success in real-world deployment, two\ncritical challenges hinder its scalability and performance: (1) inefficient\ncomputational allocation where 97.66% of resources are consumed by sequence\nencoding rather than generation, and (2) limitations in reinforcement learning\nrelying solely on reward models.\n  To address these challenges, we propose OneRec-V2, featuring: (1) Lazy\nDecoder-Only Architecture: Eliminates encoder bottlenecks, reducing total\ncomputation by 94% and training resources by 90%, enabling successful scaling\nto 8B parameters. (2) Preference Alignment with Real-World User Interactions:\nIncorporates Duration-Aware Reward Shaping and Adaptive Ratio Clipping to\nbetter align with user preferences using real-world feedback.\n  Extensive A/B tests on Kuaishou demonstrate OneRec-V2's effectiveness,\nimproving App Stay Time by 0.467%/0.741% while balancing multi-objective\nrecommendations. This work advances generative recommendation scalability and\nalignment with real-world feedback, representing a step forward in the\ndevelopment of end-to-end recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in generative AI have transformed recommender systems\nthrough end-to-end generation. OneRec reformulates recommendation as an\nautoregressive generation task, achieving high Model FLOPs Utilization. While\nOneRec-V1 has shown significant empirical success in real-world deployment, two\ncritical challenges hinder its scalability and performance: (1) inefficient\ncomputational allocation where 97.66% of resources are consumed by sequence\nencoding rather than generation, and (2) limitations in reinforcement learning\nrelying solely on reward models.\n  To address these challenges, we propose OneRec-V2, featuring: (1) Lazy\nDecoder-Only Architecture: Eliminates encoder bottlenecks, reducing total\ncomputation by 94% and training resources by 90%, enabling successful scaling\nto 8B parameters. (2) Preference Alignment with Real-World User Interactions:\nIncorporates Duration-Aware Reward Shaping and Adaptive Ratio Clipping to\nbetter align with user preferences using real-world feedback.\n  Extensive A/B tests on Kuaishou demonstrate OneRec-V2's effectiveness,\nimproving App Stay Time by 0.467%/0.741% while balancing multi-objective\nrecommendations. This work advances generative recommendation scalability and\nalignment with real-world feedback, representing a step forward in the\ndevelopment of end-to-end recommender systems."
                },
                "authors": [
                    {
                        "name": "Guorui Zhou"
                    },
                    {
                        "name": "Hengrui Hu"
                    },
                    {
                        "name": "Hongtao Cheng"
                    },
                    {
                        "name": "Huanjie Wang"
                    },
                    {
                        "name": "Jiaxin Deng"
                    },
                    {
                        "name": "Jinghao Zhang"
                    },
                    {
                        "name": "Kuo Cai"
                    },
                    {
                        "name": "Lejian Ren"
                    },
                    {
                        "name": "Lu Ren"
                    },
                    {
                        "name": "Liao Yu"
                    },
                    {
                        "name": "Pengfei Zheng"
                    },
                    {
                        "name": "Qiang Luo"
                    },
                    {
                        "name": "Qianqian Wang"
                    },
                    {
                        "name": "Qigen Hu"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Shiyao Wang"
                    },
                    {
                        "name": "Shujie Yang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Wuchao Li"
                    },
                    {
                        "name": "Xinchen Luo"
                    },
                    {
                        "name": "Xingmei Wang"
                    },
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Yunfan Wu"
                    },
                    {
                        "name": "Zexuan Cheng"
                    },
                    {
                        "name": "Zhanyu Liu"
                    },
                    {
                        "name": "Zixing Zhang"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Boxuan Wang"
                    },
                    {
                        "name": "Chaoyi Ma"
                    },
                    {
                        "name": "Chengru Song"
                    },
                    {
                        "name": "Chenhui Wang"
                    },
                    {
                        "name": "Chenglong Chu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Dongxue Meng"
                    },
                    {
                        "name": "Dunju Zang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Fangyu Zhang"
                    },
                    {
                        "name": "Feng Jiang"
                    },
                    {
                        "name": "Fuxing Zhang"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Guowang Zhang"
                    },
                    {
                        "name": "Han Li"
                    },
                    {
                        "name": "Honghui Bao"
                    },
                    {
                        "name": "Hongyang Cao"
                    },
                    {
                        "name": "Jiaming Huang"
                    },
                    {
                        "name": "Jiapeng Chen"
                    },
                    {
                        "name": "Jiaqiang Liu"
                    },
                    {
                        "name": "Jinghui Jia"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Lantao Hu"
                    },
                    {
                        "name": "Liang Zeng"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Qidong Zhou"
                    },
                    {
                        "name": "Rongzhou Zhang"
                    },
                    {
                        "name": "Shengzhe Wang"
                    },
                    {
                        "name": "Shihui He"
                    },
                    {
                        "name": "Shuang Yang"
                    },
                    {
                        "name": "Siyang Mao"
                    },
                    {
                        "name": "Sui Huang"
                    },
                    {
                        "name": "Tiantian He"
                    },
                    {
                        "name": "Tingting Gao"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Xiaoxiao Xu"
                    },
                    {
                        "name": "Xugang Liu"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Yiwu Liu"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Yunfeng Zhao"
                    },
                    {
                        "name": "Zhixin Ling"
                    },
                    {
                        "name": "Ziming Li"
                    }
                ],
                "author_detail": {
                    "name": "Ziming Li"
                },
                "author": "Ziming Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20900v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20900v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09315v2",
                "updated": "2025-09-16T09:17:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    17,
                    36,
                    1,
                    259,
                    0
                ],
                "published": "2025-05-14T12:10:41Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    10,
                    41,
                    2,
                    134,
                    0
                ],
                "title": "TransDiffuser: Diverse Trajectory Generation with Decorrelated\n  Multi-modal Representation for End-to-end Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransDiffuser: Diverse Trajectory Generation with Decorrelated\n  Multi-modal Representation for End-to-end Autonomous Driving"
                },
                "summary": "In recent years, diffusion models have demonstrated remarkable potential\nacross diverse domains, from vision generation to language modeling.\nTransferring its generative capabilities to modern end-to-end autonomous\ndriving systems has also emerged as a promising direction. However, existing\ndiffusion-based trajectory generative models often exhibit mode collapse where\ndifferent random noises converge to similar trajectories after the denoising\nprocess.Therefore, state-of-the-art models often rely on anchored trajectories\nfrom pre-defined trajectory vocabulary or scene priors in the training set to\nmitigate collapse and enrich the diversity of generated trajectories, but such\ninductive bias are not available in real-world deployment, which can be\nchallenged when generalizing to unseen scenarios. In this work, we investigate\nthe possibility of effectively tackling the mode collapse challenge without the\nassumption of pre-defined trajectory vocabulary or pre-computed scene priors.\nSpecifically, we propose TransDiffuser, an encoder-decoder based generative\ntrajectory planning model, where the encoded scene information and motion\nstates serve as the multi-modal conditional input of the denoising decoder.\nDifferent from existing approaches, we exploit a simple yet effective\nmulti-modal representation decorrelation optimization mechanism during the\ndenoising process to enrich the latent representation space which better guides\nthe downstream generation. Without any predefined trajectory anchors or\npre-computed scene priors, TransDiffuser achieves the PDMS of 94.85 on the\nclosed-loop planning-oriented benchmark NAVSIM, surpassing previous\nstate-of-the-art methods. Qualitative evaluation further showcases\nTransDiffuser generates more diverse and plausible trajectories which explore\nmore drivable area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, diffusion models have demonstrated remarkable potential\nacross diverse domains, from vision generation to language modeling.\nTransferring its generative capabilities to modern end-to-end autonomous\ndriving systems has also emerged as a promising direction. However, existing\ndiffusion-based trajectory generative models often exhibit mode collapse where\ndifferent random noises converge to similar trajectories after the denoising\nprocess.Therefore, state-of-the-art models often rely on anchored trajectories\nfrom pre-defined trajectory vocabulary or scene priors in the training set to\nmitigate collapse and enrich the diversity of generated trajectories, but such\ninductive bias are not available in real-world deployment, which can be\nchallenged when generalizing to unseen scenarios. In this work, we investigate\nthe possibility of effectively tackling the mode collapse challenge without the\nassumption of pre-defined trajectory vocabulary or pre-computed scene priors.\nSpecifically, we propose TransDiffuser, an encoder-decoder based generative\ntrajectory planning model, where the encoded scene information and motion\nstates serve as the multi-modal conditional input of the denoising decoder.\nDifferent from existing approaches, we exploit a simple yet effective\nmulti-modal representation decorrelation optimization mechanism during the\ndenoising process to enrich the latent representation space which better guides\nthe downstream generation. Without any predefined trajectory anchors or\npre-computed scene priors, TransDiffuser achieves the PDMS of 94.85 on the\nclosed-loop planning-oriented benchmark NAVSIM, surpassing previous\nstate-of-the-art methods. Qualitative evaluation further showcases\nTransDiffuser generates more diverse and plausible trajectories which explore\nmore drivable area."
                },
                "authors": [
                    {
                        "name": "Xuefeng Jiang"
                    },
                    {
                        "name": "Yuan Ma"
                    },
                    {
                        "name": "Pengxiang Li"
                    },
                    {
                        "name": "Leimeng Xu"
                    },
                    {
                        "name": "Xin Wen"
                    },
                    {
                        "name": "Kun Zhan"
                    },
                    {
                        "name": "Zhongpu Xia"
                    },
                    {
                        "name": "Peng Jia"
                    },
                    {
                        "name": "Xianpeng Lang"
                    },
                    {
                        "name": "Sheng Sun"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Sun"
                },
                "author": "Sheng Sun",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12858v1",
                "updated": "2025-09-16T09:15:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    15,
                    52,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:15:52Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    15,
                    52,
                    1,
                    259,
                    0
                ],
                "title": "Contrastive Representation Learning for Robust Sim-to-Real Transfer of\n  Adaptive Humanoid Locomotion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Representation Learning for Robust Sim-to-Real Transfer of\n  Adaptive Humanoid Locomotion"
                },
                "summary": "Reinforcement learning has produced remarkable advances in humanoid\nlocomotion, yet a fundamental dilemma persists for real-world deployment:\npolicies must choose between the robustness of reactive proprioceptive control\nor the proactivity of complex, fragile perception-driven systems. This paper\nresolves this dilemma by introducing a paradigm that imbues a purely\nproprioceptive policy with proactive capabilities, achieving the foresight of\nperception without its deployment-time costs. Our core contribution is a\ncontrastive learning framework that compels the actor's latent state to encode\nprivileged environmental information from simulation. Crucially, this\n``distilled awareness\" empowers an adaptive gait clock, allowing the policy to\nproactively adjust its rhythm based on an inferred understanding of the\nterrain. This synergy resolves the classic trade-off between rigid, clocked\ngaits and unstable clock-free policies. We validate our approach with zero-shot\nsim-to-real transfer to a full-sized humanoid, demonstrating highly robust\nlocomotion over challenging terrains, including 30 cm high steps and 26.5{\\deg}\nslopes, proving the effectiveness of our method. Website:\nhttps://lu-yidan.github.io/cra-loco.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning has produced remarkable advances in humanoid\nlocomotion, yet a fundamental dilemma persists for real-world deployment:\npolicies must choose between the robustness of reactive proprioceptive control\nor the proactivity of complex, fragile perception-driven systems. This paper\nresolves this dilemma by introducing a paradigm that imbues a purely\nproprioceptive policy with proactive capabilities, achieving the foresight of\nperception without its deployment-time costs. Our core contribution is a\ncontrastive learning framework that compels the actor's latent state to encode\nprivileged environmental information from simulation. Crucially, this\n``distilled awareness\" empowers an adaptive gait clock, allowing the policy to\nproactively adjust its rhythm based on an inferred understanding of the\nterrain. This synergy resolves the classic trade-off between rigid, clocked\ngaits and unstable clock-free policies. We validate our approach with zero-shot\nsim-to-real transfer to a full-sized humanoid, demonstrating highly robust\nlocomotion over challenging terrains, including 30 cm high steps and 26.5{\\deg}\nslopes, proving the effectiveness of our method. Website:\nhttps://lu-yidan.github.io/cra-loco."
                },
                "authors": [
                    {
                        "name": "Yidan Lu"
                    },
                    {
                        "name": "Rurui Yang"
                    },
                    {
                        "name": "Qiran Kou"
                    },
                    {
                        "name": "Mengting Chen"
                    },
                    {
                        "name": "Tao Fan"
                    },
                    {
                        "name": "Peter Cui"
                    },
                    {
                        "name": "Yinzhao Dong"
                    },
                    {
                        "name": "Peng Lu"
                    }
                ],
                "author_detail": {
                    "name": "Peng Lu"
                },
                "author": "Peng Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17962v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17962v7",
                "updated": "2025-09-16T09:13:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    13,
                    7,
                    1,
                    259,
                    0
                ],
                "published": "2024-06-25T22:44:17Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    22,
                    44,
                    17,
                    1,
                    177,
                    0
                ],
                "title": "Crafting Customisable Characters with LLMs: A Persona-Driven\n  Role-Playing Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crafting Customisable Characters with LLMs: A Persona-Driven\n  Role-Playing Agent Framework"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable ability to comprehend\ninstructions and generate human-like text, enabling sophisticated agent\nsimulation beyond basic behavior replication. However, the potential for\ncreating freely customisable characters remains underexplored. We introduce the\nCustomisable Conversation Agent Framework, which employs LLMs to simulate\nreal-world characters through personalised characteristic feature injection,\nenabling diverse character creation according to user preferences. We propose\nthe SimsConv dataset, comprising 68 customised characters and 13,971 multi-turn\nrole-playing dialogues across 1,360 real-world scenes. Characters are initially\ncustomised using pre-defined elements (career, aspiration, traits, skills),\nthen expanded through personal and social profiles. Building on this, we\npresent SimsChat, a freely customisable role-playing agent incorporating\nvarious realistic settings and topic-specified character interactions.\nExperimental results on both SimsConv and WikiRoleEval datasets demonstrate\nSimsChat's superior performance in maintaining character consistency, knowledge\naccuracy, and appropriate question rejection compared to existing models. Our\nframework provides valuable insights for developing more accurate and\ncustomisable human simulacra. Our data and code are publicly available at\nhttps://github.com/Bernard-Yang/SimsChat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable ability to comprehend\ninstructions and generate human-like text, enabling sophisticated agent\nsimulation beyond basic behavior replication. However, the potential for\ncreating freely customisable characters remains underexplored. We introduce the\nCustomisable Conversation Agent Framework, which employs LLMs to simulate\nreal-world characters through personalised characteristic feature injection,\nenabling diverse character creation according to user preferences. We propose\nthe SimsConv dataset, comprising 68 customised characters and 13,971 multi-turn\nrole-playing dialogues across 1,360 real-world scenes. Characters are initially\ncustomised using pre-defined elements (career, aspiration, traits, skills),\nthen expanded through personal and social profiles. Building on this, we\npresent SimsChat, a freely customisable role-playing agent incorporating\nvarious realistic settings and topic-specified character interactions.\nExperimental results on both SimsConv and WikiRoleEval datasets demonstrate\nSimsChat's superior performance in maintaining character consistency, knowledge\naccuracy, and appropriate question rejection compared to existing models. Our\nframework provides valuable insights for developing more accurate and\ncustomisable human simulacra. Our data and code are publicly available at\nhttps://github.com/Bernard-Yang/SimsChat."
                },
                "authors": [
                    {
                        "name": "Bohao Yang"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Chenghao Xiao"
                    },
                    {
                        "name": "Kun Zhao"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Lin Yuan"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17962v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17962v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]