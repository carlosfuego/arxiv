[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.20330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20330v1",
                "updated": "2025-02-27T17:59:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "title": "Long-Context Inference with Retrieval-Augmented Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Inference with Retrieval-Augmented Speculative Decoding"
                },
                "summary": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications."
                },
                "authors": [
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Qilong Feng"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Qizhe Shieh"
                },
                "author": "Michael Qizhe Shieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v2",
                "updated": "2025-02-27T15:29:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    29,
                    3,
                    3,
                    58,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v3",
                "updated": "2025-02-27T12:30:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    30,
                    43,
                    3,
                    58,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "ICLR 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20722v2",
                "updated": "2025-02-27T12:15:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    15,
                    38,
                    3,
                    58,
                    0
                ],
                "published": "2024-07-30T10:34:40Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    10,
                    34,
                    40,
                    1,
                    212,
                    0
                ],
                "title": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo"
                },
                "summary": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks."
                },
                "authors": [
                    {
                        "name": "Minas Karamanis"
                    },
                    {
                        "name": "Uroš Seljak"
                    }
                ],
                "author_detail": {
                    "name": "Uroš Seljak"
                },
                "author": "Uroš Seljak",
                "arxiv_comment": "36 pages, 9 figures. Submitted to Statistics & Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16235v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16235v2",
                "updated": "2025-02-27T06:39:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    6,
                    39,
                    6,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-22T14:13:37Z",
                "published_parsed": [
                    2025,
                    2,
                    22,
                    14,
                    13,
                    37,
                    5,
                    53,
                    0
                ],
                "title": "Dynamic Parallel Tree Search for Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Parallel Tree Search for Efficient LLM Reasoning"
                },
                "summary": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by\nstructuring problem-solving as a spanning tree. However, recent methods focus\non search accuracy while overlooking computational efficiency. The challenges\nof accelerating the ToT lie in the frequent switching of reasoning focus, and\nthe redundant exploration of suboptimal solutions. To alleviate this dilemma,\nwe propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework\nthat aims to dynamically optimize the reasoning path in inference. It includes\nthe Parallelism Streamline in the generation phase to build up a flexible and\nadaptive parallelism with arbitrary paths by fine-grained cache management and\nalignment. Meanwhile, the Search and Transition Mechanism filters potential\ncandidates to dynamically maintain the reasoning focus on more possible\nsolutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with\nMath500 and GSM8K datasets show that DPTS significantly improves efficiency by\n2-4x on average while maintaining or even surpassing existing reasoning\nalgorithms in accuracy, making ToT-based reasoning more scalable and\ncomputationally efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by\nstructuring problem-solving as a spanning tree. However, recent methods focus\non search accuracy while overlooking computational efficiency. The challenges\nof accelerating the ToT lie in the frequent switching of reasoning focus, and\nthe redundant exploration of suboptimal solutions. To alleviate this dilemma,\nwe propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework\nthat aims to dynamically optimize the reasoning path in inference. It includes\nthe Parallelism Streamline in the generation phase to build up a flexible and\nadaptive parallelism with arbitrary paths by fine-grained cache management and\nalignment. Meanwhile, the Search and Transition Mechanism filters potential\ncandidates to dynamically maintain the reasoning focus on more possible\nsolutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with\nMath500 and GSM8K datasets show that DPTS significantly improves efficiency by\n2-4x on average while maintaining or even surpassing existing reasoning\nalgorithms in accuracy, making ToT-based reasoning more scalable and\ncomputationally efficient."
                },
                "authors": [
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Wentao Jiang"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Yongcheng Jing"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Yingjie Wang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Zengmao Wang"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "17 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16235v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v4",
                "updated": "2025-02-27T03:22:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    3,
                    22,
                    41,
                    3,
                    58,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "The paper is accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15766v3",
                "updated": "2025-02-26T11:47:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    11,
                    47,
                    58,
                    2,
                    57,
                    0
                ],
                "published": "2024-08-28T12:59:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    59,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "Learning Harmonized Representations for Speculative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Harmonized Representations for Speculative Sampling"
                },
                "summary": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%. The code is available at\nhttps://github.com/HArmonizedSS/HASS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%. The code is available at\nhttps://github.com/HArmonizedSS/HASS."
                },
                "authors": [
                    {
                        "name": "Lefan Zhang"
                    },
                    {
                        "name": "Xiaodan Wang"
                    },
                    {
                        "name": "Yanhua Huang"
                    },
                    {
                        "name": "Ruiwen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruiwen Xu"
                },
                "author": "Ruiwen Xu",
                "arxiv_comment": "Published as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02747v3",
                "updated": "2025-02-26T10:49:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    10,
                    49,
                    33,
                    2,
                    57,
                    0
                ],
                "published": "2024-04-03T13:44:41Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    13,
                    44,
                    41,
                    2,
                    94,
                    0
                ],
                "title": "Faster Diffusion via Temporal Attention Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Diffusion via Temporal Attention Decomposition"
                },
                "summary": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE."
                },
                "authors": [
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Wentian Zhang"
                    },
                    {
                        "name": "Jinheng Xie"
                    },
                    {
                        "name": "Francesco Faccio"
                    },
                    {
                        "name": "Mengmeng Xu"
                    },
                    {
                        "name": "Tao Xiang"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Juan-Manuel Perez-Rua"
                    },
                    {
                        "name": "Jürgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Schmidhuber"
                },
                "author": "Jürgen Schmidhuber",
                "arxiv_comment": "Accepted by TMLR: https://openreview.net/forum?id=xXs2GKXPnH",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18890v1",
                "updated": "2025-02-26T07:10:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T07:10:08Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "title": "From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence\n  Generation up to 100K Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence\n  Generation up to 100K Tokens"
                },
                "summary": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift."
                },
                "authors": [
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Junzhe Shen"
                    },
                    {
                        "name": "Zixia Jia"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Zilong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zilong Zheng"
                },
                "author": "Zilong Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v2",
                "updated": "2025-02-26T02:48:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    48,
                    22,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18755v1",
                "updated": "2025-02-26T02:16:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    16,
                    46,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T02:16:46Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    16,
                    46,
                    2,
                    57,
                    0
                ],
                "title": "M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically\n  Adaptive Numerical Type",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically\n  Adaptive Numerical Type"
                },
                "summary": "Large language models (LLMs) are one of the most important killer computer\napplications. The recent algorithmic advancement proposes a fine-grained\ngroup-wise quantization for LLMs, which treats a small set (e.g., 64) of values\nin a tensor as a compression unit. It effectively preserves the model accuracy\nwithout retraining, and has become the standard approach to efficiently deploy\nLLMs. On the other hand, there are works that propose various adaptive data\ntypes to better adapt to different distributions and further reduce the\nrequired bit length for LLMs. In this work, our detailed analysis unveils a key\nfinding that while different tensors exhibit similar distributions, small\ngroups can have markedly different distributions. As such, the group-level\ndiversity requires a new level of adaptivity for which existing adaptive data\ntypes fail to provide.\n  In this paper, we propose MANT, a mathematically adaptive numeric type,\nfeaturing a more flexible encoding paradigm with a wider range of data\ndistribution and more efficient decodingcomputation fusion mechanism to address\nthese challenges. Based on MANT, we develop a supporting framework to assign\nthe appropriate data type for each group adaptively. Meanwhile, the dynamically\ngenerated Key-Value (KV) caches in LLMs introduce further complexity for\nreal-time quantization. To tackle this, we propose an efficient real-time\nquantization mechanism. Besides, we implement a specific processing element\n(PE) to efficiently support MANT and incorporate a real-time quantization unit.\nBy integrating these components into a systolic array, MANT unifies the\ngroup-wise weight and KV cache quantization and addresses the associated\nchallenges. Our evaluation shows achieving, on average, 2.99x (up to 4.46x)\nspeedup and 2.81x (up to 4.10x) energy reduction to the state-of-the-art LLM\naccelerator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are one of the most important killer computer\napplications. The recent algorithmic advancement proposes a fine-grained\ngroup-wise quantization for LLMs, which treats a small set (e.g., 64) of values\nin a tensor as a compression unit. It effectively preserves the model accuracy\nwithout retraining, and has become the standard approach to efficiently deploy\nLLMs. On the other hand, there are works that propose various adaptive data\ntypes to better adapt to different distributions and further reduce the\nrequired bit length for LLMs. In this work, our detailed analysis unveils a key\nfinding that while different tensors exhibit similar distributions, small\ngroups can have markedly different distributions. As such, the group-level\ndiversity requires a new level of adaptivity for which existing adaptive data\ntypes fail to provide.\n  In this paper, we propose MANT, a mathematically adaptive numeric type,\nfeaturing a more flexible encoding paradigm with a wider range of data\ndistribution and more efficient decodingcomputation fusion mechanism to address\nthese challenges. Based on MANT, we develop a supporting framework to assign\nthe appropriate data type for each group adaptively. Meanwhile, the dynamically\ngenerated Key-Value (KV) caches in LLMs introduce further complexity for\nreal-time quantization. To tackle this, we propose an efficient real-time\nquantization mechanism. Besides, we implement a specific processing element\n(PE) to efficiently support MANT and incorporate a real-time quantization unit.\nBy integrating these components into a systolic array, MANT unifies the\ngroup-wise weight and KV cache quantization and addresses the associated\nchallenges. Our evaluation shows achieving, on average, 2.99x (up to 4.46x)\nspeedup and 2.81x (up to 4.10x) energy reduction to the state-of-the-art LLM\naccelerator."
                },
                "authors": [
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Haoyan Zhang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Renyang Guan"
                    },
                    {
                        "name": "Zhendong Hua"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2203.02550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2203.02550v3",
                "updated": "2025-02-25T13:03:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    3,
                    44,
                    1,
                    56,
                    0
                ],
                "published": "2022-03-04T19:56:56Z",
                "published_parsed": [
                    2022,
                    3,
                    4,
                    19,
                    56,
                    56,
                    4,
                    63,
                    0
                ],
                "title": "AgileWatts: An Energy-Efficient CPU Core Idle-State Architecture for\n  Latency-Sensitive Server Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgileWatts: An Energy-Efficient CPU Core Idle-State Architecture for\n  Latency-Sensitive Server Applications"
                },
                "summary": "User-facing applications running in modern datacenters exhibit irregular\nrequest patterns and are implemented using a multitude of services with tight\nlatency requirements. These characteristics render ineffective existing energy\nconserving techniques when processors are idle due to the long transition time\nfrom a deep idle power state (C-state). While prior works propose management\ntechniques to mitigate this inefficiency, we tackle it at its root with\nAgileWatts (AW): a new deep C-state architecture optimized for datacenter\nserver processors targeting latency-sensitive applications. AW is based on\nthree key ideas. First, AW eliminates the latency overhead of saving/restoring\nthe core context (i.e., micro-architectural state) when powering-off/-on the\ncore in a deep idle power state by i) implementing medium-grained power-gates,\ncarefully distributed across the CPU core, and ii) retaining context in the\npower-ungated domain. Second, AW eliminates the flush latency overhead (several\ntens of microseconds) of the L1/L2 caches when entering a deep idle power state\nby keeping L1/L2 cache content power-ungated. A minimal control logic also\nremains power-ungated to serve cache coherence traffic (i.e., snoops)\nseamlessly. AW implements sleep-mode in caches to reduce caches leakage power\nconsumption and lowers a core voltage to the minimum operational voltage level\nto minimize the leakage power of the power-ungated domain. Third, using a\nstate-of-the-art power efficient all-digital phase-locked loop (ADPLL) clock\ngenerator, AW keeps the PLL active and locked during the idle state, further\ncutting precious microseconds of wake-up latency at a negligible power cost.\nOur evaluation with an accurate simulator calibrated against an Intel Skylake\nserver shows that AW reduces the energy consumption of Memcached by up to 71%\n(35% on average) with up to 1% performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User-facing applications running in modern datacenters exhibit irregular\nrequest patterns and are implemented using a multitude of services with tight\nlatency requirements. These characteristics render ineffective existing energy\nconserving techniques when processors are idle due to the long transition time\nfrom a deep idle power state (C-state). While prior works propose management\ntechniques to mitigate this inefficiency, we tackle it at its root with\nAgileWatts (AW): a new deep C-state architecture optimized for datacenter\nserver processors targeting latency-sensitive applications. AW is based on\nthree key ideas. First, AW eliminates the latency overhead of saving/restoring\nthe core context (i.e., micro-architectural state) when powering-off/-on the\ncore in a deep idle power state by i) implementing medium-grained power-gates,\ncarefully distributed across the CPU core, and ii) retaining context in the\npower-ungated domain. Second, AW eliminates the flush latency overhead (several\ntens of microseconds) of the L1/L2 caches when entering a deep idle power state\nby keeping L1/L2 cache content power-ungated. A minimal control logic also\nremains power-ungated to serve cache coherence traffic (i.e., snoops)\nseamlessly. AW implements sleep-mode in caches to reduce caches leakage power\nconsumption and lowers a core voltage to the minimum operational voltage level\nto minimize the leakage power of the power-ungated domain. Third, using a\nstate-of-the-art power efficient all-digital phase-locked loop (ADPLL) clock\ngenerator, AW keeps the PLL active and locked during the idle state, further\ncutting precious microseconds of wake-up latency at a negligible power cost.\nOur evaluation with an accurate simulator calibrated against an Intel Skylake\nserver shows that AW reduces the energy consumption of Memcached by up to 71%\n(35% on average) with up to 1% performance degradation."
                },
                "authors": [
                    {
                        "name": "Jawad Haj Yahya"
                    },
                    {
                        "name": "Haris Volos"
                    },
                    {
                        "name": "Davide B. Bartolini"
                    },
                    {
                        "name": "Georgia Antoniou"
                    },
                    {
                        "name": "Jeremie S. Kim"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Kleovoulos Kalaitzidis"
                    },
                    {
                        "name": "Tom Rollet"
                    },
                    {
                        "name": "Zhirui Chen"
                    },
                    {
                        "name": "Ye Geng"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Yiannakis Sazeides"
                    }
                ],
                "author_detail": {
                    "name": "Yiannakis Sazeides"
                },
                "author": "Yiannakis Sazeides",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2203.02550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2203.02550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18113v1",
                "updated": "2025-02-25T11:36:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    36,
                    43,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T11:36:43Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    36,
                    43,
                    1,
                    56,
                    0
                ],
                "title": "Accelerating Graph Indexing for ANNS on Modern CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Graph Indexing for ANNS on Modern CPUs"
                },
                "summary": "In high-dimensional vector spaces, Approximate Nearest Neighbor Search (ANNS)\nis a key component in database and artificial intelligence infrastructures.\nGraph-based methods, particularly HNSW, have emerged as leading solutions among\nvarious ANNS approaches, offering an impressive trade-off between search\nefficiency and accuracy. Many modern vector databases utilize graph indexes as\ntheir core algorithms, benefiting from various optimizations to enhance search\nperformance. However, the high indexing time associated with graph algorithms\nposes a significant challenge, especially given the increasing volume of data,\nquery processing complexity, and dynamic index maintenance demand. This has\nrendered indexing time a critical performance metric for users. In this paper,\nwe comprehensively analyze the underlying causes of the low graph indexing\nefficiency on modern CPUs, identifying that distance computation dominates\nindexing time, primarily due to high memory access latency and suboptimal\narithmetic operation efficiency. We demonstrate that distance comparisons\nduring index construction can be effectively performed using compact vector\ncodes at an appropriate compression error. Drawing from insights gained through\nintegrating existing compact coding methods in the graph indexing process, we\npropose a novel compact coding strategy, named Flash, designed explicitly for\ngraph indexing and optimized for modern CPU architectures. By minimizing random\nmemory accesses and maximizing the utilization of SIMD (Single Instruction,\nMultiple Data) instructions, Flash significantly enhances cache hit rates and\narithmetic operations. Extensive experiments conducted on eight real-world\ndatasets, ranging from ten million to one billion vectors, exhibit that Flash\nachieves a speedup of 10.4$\\times$ to 22.9$\\times$ in index construction\nefficiency, while maintaining or improving search performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In high-dimensional vector spaces, Approximate Nearest Neighbor Search (ANNS)\nis a key component in database and artificial intelligence infrastructures.\nGraph-based methods, particularly HNSW, have emerged as leading solutions among\nvarious ANNS approaches, offering an impressive trade-off between search\nefficiency and accuracy. Many modern vector databases utilize graph indexes as\ntheir core algorithms, benefiting from various optimizations to enhance search\nperformance. However, the high indexing time associated with graph algorithms\nposes a significant challenge, especially given the increasing volume of data,\nquery processing complexity, and dynamic index maintenance demand. This has\nrendered indexing time a critical performance metric for users. In this paper,\nwe comprehensively analyze the underlying causes of the low graph indexing\nefficiency on modern CPUs, identifying that distance computation dominates\nindexing time, primarily due to high memory access latency and suboptimal\narithmetic operation efficiency. We demonstrate that distance comparisons\nduring index construction can be effectively performed using compact vector\ncodes at an appropriate compression error. Drawing from insights gained through\nintegrating existing compact coding methods in the graph indexing process, we\npropose a novel compact coding strategy, named Flash, designed explicitly for\ngraph indexing and optimized for modern CPU architectures. By minimizing random\nmemory accesses and maximizing the utilization of SIMD (Single Instruction,\nMultiple Data) instructions, Flash significantly enhances cache hit rates and\narithmetic operations. Extensive experiments conducted on eight real-world\ndatasets, ranging from ten million to one billion vectors, exhibit that Flash\nachieves a speedup of 10.4$\\times$ to 22.9$\\times$ in index construction\nefficiency, while maintaining or improving search performance."
                },
                "authors": [
                    {
                        "name": "Mengzhao Wang"
                    },
                    {
                        "name": "Haotian Wu"
                    },
                    {
                        "name": "Xiangyu Ke"
                    },
                    {
                        "name": "Yunjun Gao"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Wenchao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wenchao Zhou"
                },
                "author": "Wenchao Zhou",
                "arxiv_comment": "SIGMOD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17363v2",
                "updated": "2025-02-25T09:42:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    42,
                    11,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-24T17:40:09Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    40,
                    9,
                    0,
                    55,
                    0
                ],
                "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Edit: Training-Free Image Editing for Precise Background Preservation"
                },
                "summary": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit"
                },
                "authors": [
                    {
                        "name": "Tianrui Zhu"
                    },
                    {
                        "name": "Shiyi Zhang"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang",
                "arxiv_comment": "Project webpage is available at\n  https://xilluill.github.io/projectpages/KV-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v3",
                "updated": "2025-02-25T03:42:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    3,
                    42,
                    15,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "arxiv_comment": "36 pages. Code: https://github.com/cmd2001/KVTuner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17606v1",
                "updated": "2025-02-24T19:48:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    48,
                    48,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T19:48:48Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    48,
                    48,
                    0,
                    55,
                    0
                ],
                "title": "ELMo-Tune-V2: LLM-Assisted Full-Cycle Auto-Tuning to Optimize LSM-Based\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELMo-Tune-V2: LLM-Assisted Full-Cycle Auto-Tuning to Optimize LSM-Based\n  Key-Value Stores"
                },
                "summary": "Log-Structured Merge-tree-based Key-Value Store (LSM-KVS) is a foundational\nstorage engine serving diverse modern workloads, systems, and applications. To\nsuit varying use cases, LSM-KVS allows a vast configuration space that controls\ncore parameters like compaction, flush, and cache sizes, each consuming a\nshared pool of CPU, Memory, and Storage resources. Navigating the LSM-KVS\nconfiguration space necessitates knowledge of the impact of each configuration\non the expected workload and underlying hardware. Beyond expensive and\ntime-intensive human-expert-based tuning, existing LSM-KVS tuning solutions\nfocus on tuning with specific workload expectations while limited to a narrow\nsubset of parameters.\n  This paper introduces ELMo-Tune-V2, a framework that integrates Large\nLanguage Models (LLMs) at its foundation to demonstrate the potential of\napplying modern LLMs in data system optimization problems. ELMo-Tune-V2\nleverages the contextual reasoning, cross-domain, and generative capabilities\nof LLMs to perform 1) self-navigated characterization and modeling of LSM-KVS\nworkloads, 2) automatic tuning across a broad parameter space using\ncross-domain knowledge, and 3) real-time dynamic configuration adjustments for\nLSM-KVS. ELMo-Tune-V2 integrates three innovations: LLM-based workload\nsynthesis for adaptive benchmark generation, feedback-driven iterative\nfine-tuning for configuration refinement, and real-time tuning to handle\nevolving workloads. Through detailed evaluation using RocksDB under several\nreal-world applications across diverse scenarios, ELMo-Tune-V2 achieves\nperformance improvements up to ~14X our YCSB benchmarks compared against\ndefault RocksDB configurations, and our end-to-end tests with upper-level\napplications, NebulaGraph and Kvrocks, demonstrate performance gains of 34% and\n26%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log-Structured Merge-tree-based Key-Value Store (LSM-KVS) is a foundational\nstorage engine serving diverse modern workloads, systems, and applications. To\nsuit varying use cases, LSM-KVS allows a vast configuration space that controls\ncore parameters like compaction, flush, and cache sizes, each consuming a\nshared pool of CPU, Memory, and Storage resources. Navigating the LSM-KVS\nconfiguration space necessitates knowledge of the impact of each configuration\non the expected workload and underlying hardware. Beyond expensive and\ntime-intensive human-expert-based tuning, existing LSM-KVS tuning solutions\nfocus on tuning with specific workload expectations while limited to a narrow\nsubset of parameters.\n  This paper introduces ELMo-Tune-V2, a framework that integrates Large\nLanguage Models (LLMs) at its foundation to demonstrate the potential of\napplying modern LLMs in data system optimization problems. ELMo-Tune-V2\nleverages the contextual reasoning, cross-domain, and generative capabilities\nof LLMs to perform 1) self-navigated characterization and modeling of LSM-KVS\nworkloads, 2) automatic tuning across a broad parameter space using\ncross-domain knowledge, and 3) real-time dynamic configuration adjustments for\nLSM-KVS. ELMo-Tune-V2 integrates three innovations: LLM-based workload\nsynthesis for adaptive benchmark generation, feedback-driven iterative\nfine-tuning for configuration refinement, and real-time tuning to handle\nevolving workloads. Through detailed evaluation using RocksDB under several\nreal-world applications across diverse scenarios, ELMo-Tune-V2 achieves\nperformance improvements up to ~14X our YCSB benchmarks compared against\ndefault RocksDB configurations, and our end-to-end tests with upper-level\napplications, NebulaGraph and Kvrocks, demonstrate performance gains of 34% and\n26%, respectively."
                },
                "authors": [
                    {
                        "name": "Viraj Thakkar"
                    },
                    {
                        "name": "Qi Lin"
                    },
                    {
                        "name": "Kenanya Keandra Adriel Prasetyo"
                    },
                    {
                        "name": "Raden Haryosatyo Wisjnunandono"
                    },
                    {
                        "name": "Achmad Imam Kistijantoro"
                    },
                    {
                        "name": "Reza Fuad Rachmadi"
                    },
                    {
                        "name": "Zhichao Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Cao"
                },
                "author": "Zhichao Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17599v1",
                "updated": "2025-02-24T19:34:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T19:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference"
                },
                "summary": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Zheda Mai"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17421v1",
                "updated": "2025-02-24T18:53:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:53:31Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "title": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification"
                },
                "summary": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec."
                },
                "authors": [
                    {
                        "name": "Penghui Yang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01418v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01418v2",
                "updated": "2025-02-24T18:51:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    51,
                    48,
                    0,
                    55,
                    0
                ],
                "published": "2024-05-02T16:08:03Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    16,
                    8,
                    3,
                    3,
                    123,
                    0
                ],
                "title": "GTX: A Write-Optimized Latch-free Graph Data System with Transactional\n  Support -- Extended Version",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTX: A Write-Optimized Latch-free Graph Data System with Transactional\n  Support -- Extended Version"
                },
                "summary": "This paper introduces GTX, a standalone main-memory write-optimized graph\ndata system that specializes in structural and graph property updates while\nenabling concurrent reads and graph analytics through ACID transactions. Recent\ngraph systems target concurrent read and write support while guaranteeing\ntransaction semantics. However, their performance suffers from updates with\nreal-world temporal locality over the same vertices and edges due to\nvertex-centric lock contentions. GTX has an adaptive delta-chain locking\nprotocol on top of a carefully designed latch-free graph storage. It eliminates\nvertex-level locking contention, and adapts to real-life workloads while\nmaintaining sequential access to the graph's adjacency lists storage. GTX's\ntransactions further support cache-friendly block level concurrency control,\nand cooperative group commit and garbage collection. This combination of\nfeatures ensures high update throughput and provides low-latency graph\nanalytics. Based on experimental evaluation, in addition to not sacrificing the\nperformance of read-heavy analytical workloads, and having competitive\nperformance similar to state-of-the-art systems, GTX has high read-write\ntransaction throughput. For write-heavy transactional workloads, GTX achieves\nup to 11x better transaction throughput than the best-performing\nstate-of-the-art system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces GTX, a standalone main-memory write-optimized graph\ndata system that specializes in structural and graph property updates while\nenabling concurrent reads and graph analytics through ACID transactions. Recent\ngraph systems target concurrent read and write support while guaranteeing\ntransaction semantics. However, their performance suffers from updates with\nreal-world temporal locality over the same vertices and edges due to\nvertex-centric lock contentions. GTX has an adaptive delta-chain locking\nprotocol on top of a carefully designed latch-free graph storage. It eliminates\nvertex-level locking contention, and adapts to real-life workloads while\nmaintaining sequential access to the graph's adjacency lists storage. GTX's\ntransactions further support cache-friendly block level concurrency control,\nand cooperative group commit and garbage collection. This combination of\nfeatures ensures high update throughput and provides low-latency graph\nanalytics. Based on experimental evaluation, in addition to not sacrificing the\nperformance of read-heavy analytical workloads, and having competitive\nperformance similar to state-of-the-art systems, GTX has high read-write\ntransaction throughput. For write-heavy transactional workloads, GTX achieves\nup to 11x better transaction throughput than the best-performing\nstate-of-the-art system."
                },
                "authors": [
                    {
                        "name": "Libin Zhou"
                    },
                    {
                        "name": "Lu Xing"
                    },
                    {
                        "name": "Yeasir Rayhan"
                    },
                    {
                        "name": "Walid. G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid. G. Aref"
                },
                "author": "Walid. G. Aref",
                "arxiv_comment": "technical report for our main paper GTX: A Write-Optimized Latch-free\n  Graph Data System with Transactional Support",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01418v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01418v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17398v1",
                "updated": "2025-02-24T18:26:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:26:22Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "title": "Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded\n  Heterogeneous SoCs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded\n  Heterogeneous SoCs"
                },
                "summary": "Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific\nhardware accelerators to improve performance and energy efficiency. In\nparticular, programmable multi-core accelerators feature a cluster of\nprocessing elements and tightly coupled scratchpad memories to balance\nperformance, energy efficiency, and flexibility. In embedded systems running a\ngeneral-purpose OS, accelerators access data via dedicated, physically\naddressed memory regions. This negatively impacts memory utilization and\nperformance by requiring a copy from the virtual host address to the physical\naccelerator address space. Input-Output Memory Management Units (IOMMUs)\novercome this limitation by allowing devices and hosts to use a shared virtual\npaged address space. However, resolving IO virtual addresses can be\nparticularly costly on high-latency memory systems as it requires up to three\nsequential memory accesses on IOTLB miss. In this work, we present a\nquantitative evaluation of shared virtual addressing in RISC-V heterogeneous\nembedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V\nSoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated\nthe system performance by emulating the design on FPGA and implementing compute\nkernels from the RajaPERF benchmark suite using heterogeneous OpenMP\nprogramming. We measure the transfers and computation time on the host and\naccelerators for systems with different DRAM access latencies. We first show\nthat IO virtual address translation can account for 4.2% up to 17.6% of the\naccelerator's runtime for gemm (General Matrix Multiplication) at low and high\nmemory bandwidth. Then, we show that in systems containing a last-level cache,\nthis IO address translation cost falls to 0.4% and 0.7% under the same\nconditions, making shared virtual addressing and zero-copy offloading suitable\nfor such RISC-V heterogeneous SoCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific\nhardware accelerators to improve performance and energy efficiency. In\nparticular, programmable multi-core accelerators feature a cluster of\nprocessing elements and tightly coupled scratchpad memories to balance\nperformance, energy efficiency, and flexibility. In embedded systems running a\ngeneral-purpose OS, accelerators access data via dedicated, physically\naddressed memory regions. This negatively impacts memory utilization and\nperformance by requiring a copy from the virtual host address to the physical\naccelerator address space. Input-Output Memory Management Units (IOMMUs)\novercome this limitation by allowing devices and hosts to use a shared virtual\npaged address space. However, resolving IO virtual addresses can be\nparticularly costly on high-latency memory systems as it requires up to three\nsequential memory accesses on IOTLB miss. In this work, we present a\nquantitative evaluation of shared virtual addressing in RISC-V heterogeneous\nembedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V\nSoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated\nthe system performance by emulating the design on FPGA and implementing compute\nkernels from the RajaPERF benchmark suite using heterogeneous OpenMP\nprogramming. We measure the transfers and computation time on the host and\naccelerators for systems with different DRAM access latencies. We first show\nthat IO virtual address translation can account for 4.2% up to 17.6% of the\naccelerator's runtime for gemm (General Matrix Multiplication) at low and high\nmemory bandwidth. Then, we show that in systems containing a last-level cache,\nthis IO address translation cost falls to 0.4% and 0.7% under the same\nconditions, making shared virtual addressing and zero-copy offloading suitable\nfor such RISC-V heterogeneous SoCs."
                },
                "authors": [
                    {
                        "name": "Cyril Koenig"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v5",
                "updated": "2025-02-24T15:42:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    42,
                    59,
                    0,
                    55,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on GitHub\n  ^_^ Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17535v1",
                "updated": "2025-02-24T15:39:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    39,
                    35,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T15:39:35Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    39,
                    35,
                    0,
                    55,
                    0
                ],
                "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM\n  Compression Preserve?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM\n  Compression Preserve?"
                },
                "summary": "Motivated by reducing the computational and storage costs of LLMs, model\ncompression and KV cache compression have attracted much attention from\nresearchers. However, current methods predominantly emphasize maintaining the\nperformance of compressed LLMs, as measured by perplexity or simple accuracy on\ntasks of common sense knowledge QA and basic arithmetic reasoning. In this\nblog, we present a brief review of recent advancements in LLMs related to\nretrieval-augmented generation, multi-step reasoning, external tools, and\ncomputational expressivity, all of which substantially enhance LLM performance.\nThen, we propose a lottery LLM hypothesis suggesting that for a given LLM and\ntask, there exists a smaller lottery LLM capable of producing the same\nperformance as the original LLM with the assistance of multi-step reasoning and\nexternal tools. Based on the review of current progress in LLMs, we discuss and\nsummarize the essential capabilities that the lottery LLM and KV cache\ncompression must possess, which are currently overlooked in existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by reducing the computational and storage costs of LLMs, model\ncompression and KV cache compression have attracted much attention from\nresearchers. However, current methods predominantly emphasize maintaining the\nperformance of compressed LLMs, as measured by perplexity or simple accuracy on\ntasks of common sense knowledge QA and basic arithmetic reasoning. In this\nblog, we present a brief review of recent advancements in LLMs related to\nretrieval-augmented generation, multi-step reasoning, external tools, and\ncomputational expressivity, all of which substantially enhance LLM performance.\nThen, we propose a lottery LLM hypothesis suggesting that for a given LLM and\ntask, there exists a smaller lottery LLM capable of producing the same\nperformance as the original LLM with the assistance of multi-step reasoning and\nexternal tools. Based on the review of current progress in LLMs, we discuss and\nsummarize the essential capabilities that the lottery LLM and KV cache\ncompression must possess, which are currently overlooked in existing methods."
                },
                "authors": [
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Bo Li"
                    }
                ],
                "author_detail": {
                    "name": "Bo Li"
                },
                "author": "Bo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15294v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15294v2",
                "updated": "2025-02-24T13:35:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    35,
                    18,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-21T08:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    40,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference"
                },
                "summary": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance."
                },
                "authors": [
                    {
                        "name": "Yaohua Tang"
                    },
                    {
                        "name": "Zhicheng Hu"
                    },
                    {
                        "name": "Kun Cheng"
                    },
                    {
                        "name": "Fan Mo"
                    },
                    {
                        "name": "Qiheng Lv"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15294v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15294v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17139v1",
                "updated": "2025-02-24T13:30:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T13:30:30Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "title": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation"
                },
                "summary": "Code generation is a latency-sensitive task that demands high timeliness, but\nthe autoregressive decoding mechanism of Large Language Models (LLMs) leads to\npoor inference efficiency. Existing LLM inference acceleration methods mainly\nfocus on standalone functions using only built-in components. Moreover, they\ntreat code like natural language sequences, ignoring its unique syntax and\nsemantic characteristics. As a result, the effectiveness of these approaches in\ncode generation tasks remains limited and fails to align with real-world\nprogramming scenarios. To alleviate this issue, we propose CodeSwift, a simple\nyet highly efficient inference acceleration approach specifically designed for\ncode generation, without comprising the quality of the output. CodeSwift\nconstructs a multi-source datastore, providing access to both general and\nproject-specific knowledge, facilitating the retrieval of high-quality draft\nsequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval\ntiming, and enhances efficiency through parallel retrieval and a context- and\nLLM preference-aware cache. Experimental results show that CodeSwift can reach\nup to 2.53x and 2.54x speedup compared to autoregressive decoding in\nrepository-level and standalone code generation tasks, respectively,\noutperforming state-of-the-art inference acceleration approaches by up to 88%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation is a latency-sensitive task that demands high timeliness, but\nthe autoregressive decoding mechanism of Large Language Models (LLMs) leads to\npoor inference efficiency. Existing LLM inference acceleration methods mainly\nfocus on standalone functions using only built-in components. Moreover, they\ntreat code like natural language sequences, ignoring its unique syntax and\nsemantic characteristics. As a result, the effectiveness of these approaches in\ncode generation tasks remains limited and fails to align with real-world\nprogramming scenarios. To alleviate this issue, we propose CodeSwift, a simple\nyet highly efficient inference acceleration approach specifically designed for\ncode generation, without comprising the quality of the output. CodeSwift\nconstructs a multi-source datastore, providing access to both general and\nproject-specific knowledge, facilitating the retrieval of high-quality draft\nsequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval\ntiming, and enhances efficiency through parallel retrieval and a context- and\nLLM preference-aware cache. Experimental results show that CodeSwift can reach\nup to 2.53x and 2.54x speedup compared to autoregressive decoding in\nrepository-level and standalone code generation tasks, respectively,\noutperforming state-of-the-art inference acceleration approaches by up to 88%."
                },
                "authors": [
                    {
                        "name": "Qianhui Zhao"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Qiaoyuanhe Meng"
                    },
                    {
                        "name": "Ziqian Jiao"
                    },
                    {
                        "name": "Zetong Zhou"
                    },
                    {
                        "name": "Borui Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16886v1",
                "updated": "2025-02-24T06:33:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T06:33:39Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "title": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance"
                },
                "summary": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. However, these techniques often require a\npre-defined cache budget; as the optimal budget varies with different input\nlengths and task types, it limits their practical deployment accepting\nopen-domain instructions. To address this limitation, we propose a new KV cache\ncompression objective: to always ensure the full-cache performance regardless\nof specific inputs, while maximizing KV cache pruning as much as possible. To\nachieve this goal, we introduce a novel KV cache compression method dubbed\nDBudgetKV, which features an attention-based metric to signal when the\nremaining KV cache is unlikely to match the full-cache performance, then\nhalting the pruning process. Empirical evaluation spanning diverse context\nlengths, task types, and model sizes suggests that our method achieves lossless\nKV pruning effectively and robustly, exceeding 25% compression ratio on\naverage. Furthermore, our method is easy to integrate within LLM inference, not\nonly optimizing memory space, but also showing reduced inference time compared\nto existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. However, these techniques often require a\npre-defined cache budget; as the optimal budget varies with different input\nlengths and task types, it limits their practical deployment accepting\nopen-domain instructions. To address this limitation, we propose a new KV cache\ncompression objective: to always ensure the full-cache performance regardless\nof specific inputs, while maximizing KV cache pruning as much as possible. To\nachieve this goal, we introduce a novel KV cache compression method dubbed\nDBudgetKV, which features an attention-based metric to signal when the\nremaining KV cache is unlikely to match the full-cache performance, then\nhalting the pruning process. Empirical evaluation spanning diverse context\nlengths, task types, and model sizes suggests that our method achieves lossless\nKV pruning effectively and robustly, exceeding 25% compression ratio on\naverage. Furthermore, our method is easy to integrate within LLM inference, not\nonly optimizing memory space, but also showing reduced inference time compared\nto existing methods."
                },
                "authors": [
                    {
                        "name": "Xuanfan Ni"
                    },
                    {
                        "name": "Liyan Xu"
                    },
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Piji Li"
                    }
                ],
                "author_detail": {
                    "name": "Piji Li"
                },
                "author": "Piji Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13176v2",
                "updated": "2025-02-24T01:28:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    1,
                    28,
                    27,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-18T04:08:29Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    4,
                    8,
                    29,
                    1,
                    49,
                    0
                ],
                "title": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference"
                },
                "summary": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels."
                },
                "authors": [
                    {
                        "name": "Ahmed Burak Gulhan"
                    },
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Mahmut Kandemir"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    }
                ],
                "author_detail": {
                    "name": "Venkatram Vishwanath"
                },
                "author": "Venkatram Vishwanath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v2",
                "updated": "2025-02-23T19:48:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    19,
                    48,
                    12,
                    6,
                    54,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_doi": "10.1145/3701716.3715490",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715490",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.15605v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, accepted by the Web Conference 2025 (WWW '25) as a short\n  paper",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16632v1",
                "updated": "2025-02-23T16:17:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    16,
                    17,
                    34,
                    6,
                    54,
                    0
                ],
                "published": "2025-02-23T16:17:34Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    16,
                    17,
                    34,
                    6,
                    54,
                    0
                ],
                "title": "Simultaneously Transmitting And Reflecting Surfaces (STARS) for\n  Multi-Functional 6G",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneously Transmitting And Reflecting Surfaces (STARS) for\n  Multi-Functional 6G"
                },
                "summary": "Simultaneously transmitting and reflecting surface (STARS) empowered\nmulti-functional 6G wireless networks are investigated. Starting with the\ncommunication functionality, various types of STARS are introduced in terms of\npower amplification capabilities, reciprocity features, and spatial density of\nelements. Then, three STARS-empowered wireless sensing architectures are\nproposed, namely STARS-aided monostatic sensing, STARS-enabled bistatic\nsensing, and sensing with target-mounted STARS, where the representative\nbenefits and application challenges are identified. Furthermore, promising\napplications of STARS for computing and caching functionalities are explored to\nimprove the computation efficiency and reduce the content delivery latency.\nFinally, recent standardization progress for reconfigurable intelligent\nsurfaces is presented for motivating the employment of STARS in\nmulti-functional 6G.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneously transmitting and reflecting surface (STARS) empowered\nmulti-functional 6G wireless networks are investigated. Starting with the\ncommunication functionality, various types of STARS are introduced in terms of\npower amplification capabilities, reciprocity features, and spatial density of\nelements. Then, three STARS-empowered wireless sensing architectures are\nproposed, namely STARS-aided monostatic sensing, STARS-enabled bistatic\nsensing, and sensing with target-mounted STARS, where the representative\nbenefits and application challenges are identified. Furthermore, promising\napplications of STARS for computing and caching functionalities are explored to\nimprove the computation efficiency and reduce the content delivery latency.\nFinally, recent standardization progress for reconfigurable intelligent\nsurfaces is presented for motivating the employment of STARS in\nmulti-functional 6G."
                },
                "authors": [
                    {
                        "name": "Xidong Mu"
                    },
                    {
                        "name": "Zhaolin Wang"
                    },
                    {
                        "name": "Yuanwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuanwei Liu"
                },
                "author": "Yuanwei Liu",
                "arxiv_doi": "10.1109/MNET.2024.3481293",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MNET.2024.3481293",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.16632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 figures, 8 pages, published in IEEE Network",
                "arxiv_journal_ref": "in IEEE Network, vol. 39, no. 1, pp. 47-55, Jan. 2025",
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11855v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11855v3",
                "updated": "2025-02-23T11:52:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    11,
                    52,
                    45,
                    6,
                    54,
                    0
                ],
                "published": "2025-01-21T03:13:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing"
                },
                "summary": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Huimei Wei"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11855v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11855v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v4",
                "updated": "2025-02-23T03:27:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    3,
                    27,
                    1,
                    6,
                    54,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "Cache Coherence Over Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Coherence Over Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol , thereby ensuring\nboth atomicity of data access and cache coherence with sequential consistency.\nSELCC embeds cache-ownership metadata directly into the RDMA latch word,\nenabling efficient cache ownership management via RDMA atomic operations. SELCC\ncan serve as an abstraction layer over disaggregated memory with APIs that\nresemble main-memory accesses. A concurrent B-tree and three transaction\nconcurrency control algorithms are realized using SELCC's abstraction layer.\nExperimental results show that SELCC significantly outperforms\nRemote-Procedure-Call-based protocols for cache coherence under limited remote\ncomputing power. Applications on SELCC achieve comparable or superior\nperformance over disaggregated memory compared to competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol , thereby ensuring\nboth atomicity of data access and cache coherence with sequential consistency.\nSELCC embeds cache-ownership metadata directly into the RDMA latch word,\nenabling efficient cache ownership management via RDMA atomic operations. SELCC\ncan serve as an abstraction layer over disaggregated memory with APIs that\nresemble main-memory accesses. A concurrent B-tree and three transaction\nconcurrency control algorithms are realized using SELCC's abstraction layer.\nExperimental results show that SELCC significantly outperforms\nRemote-Procedure-Call-based protocols for cache coherence under limited remote\ncomputing power. Applications on SELCC achieve comparable or superior\nperformance over disaggregated memory compared to competitors."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13502v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13502v2",
                "updated": "2025-02-22T22:32:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    22,
                    22,
                    32,
                    8,
                    5,
                    53,
                    0
                ],
                "published": "2025-02-19T07:43:36Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    7,
                    43,
                    36,
                    2,
                    50,
                    0
                ],
                "title": "PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own\n  Deep Neural Net At Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own\n  Deep Neural Net At Inference"
                },
                "summary": "We show that Large Language Model from Power Law Decoder Representations\n(PLDR-LLM) is a foundational model whose deductive outputs are invariant\ntensors up to a small perturbation. PLDR-LLM learns a singularity condition for\nthe deductive outputs that enable the once-inferred energy-curvature tensor\n$\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph\nattention (PLGA) generating the deductive outputs at inference. We demonstrate\nthat a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in\na straightforward manner to improve the inference time. The invariance and\ngeneralizable nature of deductive outputs is at a very high fidelity where\ndeductive outputs have same RMSE and determinant values up to 15 decimal places\nafter caching, and zero-shot benchmark scores remain unchanged. Ablation\nstudies show that learned deductive outputs have distinct loss and accuracy\ncharacteristics from models pretrained with transferred, randomly initialized\nor identity tensors as a constant tensor operator and an LLM with scaled-dot\nproduct attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$\nis predefined as identity. The observed invariance characteristic introduces a\nnovel asymmetry between training and inference phases with caching. We outline\nobserved common characteristics of the deductive outputs for the learned\nsingularity condition. We provide an implementation of a training and inference\nframework for PLDR-LLM with KV-cache and G-cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that Large Language Model from Power Law Decoder Representations\n(PLDR-LLM) is a foundational model whose deductive outputs are invariant\ntensors up to a small perturbation. PLDR-LLM learns a singularity condition for\nthe deductive outputs that enable the once-inferred energy-curvature tensor\n$\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph\nattention (PLGA) generating the deductive outputs at inference. We demonstrate\nthat a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in\na straightforward manner to improve the inference time. The invariance and\ngeneralizable nature of deductive outputs is at a very high fidelity where\ndeductive outputs have same RMSE and determinant values up to 15 decimal places\nafter caching, and zero-shot benchmark scores remain unchanged. Ablation\nstudies show that learned deductive outputs have distinct loss and accuracy\ncharacteristics from models pretrained with transferred, randomly initialized\nor identity tensors as a constant tensor operator and an LLM with scaled-dot\nproduct attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$\nis predefined as identity. The observed invariance characteristic introduces a\nnovel asymmetry between training and inference phases with caching. We outline\nobserved common characteristics of the deductive outputs for the learned\nsingularity condition. We provide an implementation of a training and inference\nframework for PLDR-LLM with KV-cache and G-cache."
                },
                "authors": [
                    {
                        "name": "Burc Gokden"
                    }
                ],
                "author_detail": {
                    "name": "Burc Gokden"
                },
                "author": "Burc Gokden",
                "arxiv_comment": "15 pages, 1 figure, 12 tables, more ablation data included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13502v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13502v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15197v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15197v3",
                "updated": "2025-02-22T10:31:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    22,
                    10,
                    31,
                    51,
                    5,
                    53,
                    0
                ],
                "published": "2024-05-24T04:00:04Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    4,
                    0,
                    4,
                    4,
                    145,
                    0
                ],
                "title": "Warp-centric GPU meta-meshing and fast triangulation of billion-scale\n  lattice structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Warp-centric GPU meta-meshing and fast triangulation of billion-scale\n  lattice structures"
                },
                "summary": "Lattice structures have been widely used in applications due to their\nsuperior mechanical properties. To fabricate such structures, a geometric\nprocessing step called triangulation is often employed to transform them into\nthe STL format before sending them to 3D printers. Because lattice structures\ntend to have high geometric complexity, this step usually generates a large\namount of triangles, a memory and compute-intensive task. This problem\nmanifests itself clearly through large-scale lattice structures that have\nmillions or billions of struts. To address this problem, this paper proposes to\ntransform a lattice structure into an intermediate model called meta-mesh\nbefore undergoing real triangulation. Compared to triangular meshes,\nmeta-meshes are very lightweight and much less compute-demanding. The meta-mesh\ncan also work as a base mesh reusable for conveniently and efficiently\ntriangulating lattice structures with arbitrary resolutions. A CPU+GPU\nasynchronous meta-meshing pipeline has been developed to efficiently generate\nmeta-meshes from lattice structures. It shifts from the thread-centric GPU\nalgorithm design paradigm commonly used in CAD to the recent warp-centric\ndesign paradigm to achieve high performance. This is achieved by a new data\ncompression method, a GPU cache-aware data structure, and a workload-balanced\nscheduling method that can significantly reduce memory divergence and branch\ndivergence. Experimenting with various billion-scale lattice structures, the\nproposed method is seen to be two orders of magnitude faster than previously\nachievable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lattice structures have been widely used in applications due to their\nsuperior mechanical properties. To fabricate such structures, a geometric\nprocessing step called triangulation is often employed to transform them into\nthe STL format before sending them to 3D printers. Because lattice structures\ntend to have high geometric complexity, this step usually generates a large\namount of triangles, a memory and compute-intensive task. This problem\nmanifests itself clearly through large-scale lattice structures that have\nmillions or billions of struts. To address this problem, this paper proposes to\ntransform a lattice structure into an intermediate model called meta-mesh\nbefore undergoing real triangulation. Compared to triangular meshes,\nmeta-meshes are very lightweight and much less compute-demanding. The meta-mesh\ncan also work as a base mesh reusable for conveniently and efficiently\ntriangulating lattice structures with arbitrary resolutions. A CPU+GPU\nasynchronous meta-meshing pipeline has been developed to efficiently generate\nmeta-meshes from lattice structures. It shifts from the thread-centric GPU\nalgorithm design paradigm commonly used in CAD to the recent warp-centric\ndesign paradigm to achieve high performance. This is achieved by a new data\ncompression method, a GPU cache-aware data structure, and a workload-balanced\nscheduling method that can significantly reduce memory divergence and branch\ndivergence. Experimenting with various billion-scale lattice structures, the\nproposed method is seen to be two orders of magnitude faster than previously\nachievable."
                },
                "authors": [
                    {
                        "name": "Qiang Zou"
                    },
                    {
                        "name": "Yunzhu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunzhu Gao"
                },
                "author": "Yunzhu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15197v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15197v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16002v1",
                "updated": "2025-02-21T23:34:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T23:34:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"
                },
                "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we propose a new\nstrategy to eliminate such inefficiency, where the KV cache of each document is\nprecomputed independently. During inference, the KV caches of retrieved\ndocuments are concatenated, allowing the model to reuse cached representations\ninstead of recomputing them. To mitigate the performance degradation of LLMs\nwhen using KV caches computed independently for each document, KVLink\nintroduces three key components: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments, and applying mixed-data fine-tuning to enhance performance while\npreserving the model's original capabilities. Experiments across 7 datasets\ndemonstrate that KVLink improves question answering accuracy by an average of\n4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV\ncaches, our approach reduces time-to-first-token by up to 90% compared to\nstandard LLM inference, making it a scalable and efficient solution for context\nreuse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we propose a new\nstrategy to eliminate such inefficiency, where the KV cache of each document is\nprecomputed independently. During inference, the KV caches of retrieved\ndocuments are concatenated, allowing the model to reuse cached representations\ninstead of recomputing them. To mitigate the performance degradation of LLMs\nwhen using KV caches computed independently for each document, KVLink\nintroduces three key components: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments, and applying mixed-data fine-tuning to enhance performance while\npreserving the model's original capabilities. Experiments across 7 datasets\ndemonstrate that KVLink improves question answering accuracy by an average of\n4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV\ncaches, our approach reduces time-to-first-token by up to 90% compared to\nstandard LLM inference, making it a scalable and efficient solution for context\nreuse."
                },
                "authors": [
                    {
                        "name": "Jingbo Yang"
                    },
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15955v1",
                "updated": "2025-02-21T21:37:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    21,
                    37,
                    52,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T21:37:52Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    21,
                    37,
                    52,
                    4,
                    52,
                    0
                ],
                "title": "Compression Barriers for Autoregressive Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compression Barriers for Autoregressive Transformers"
                },
                "summary": "A key limitation of autoregressive Transformers is the large memory needed at\ninference-time to cache all previous key-value (KV) embeddings. Prior works\naddress this by compressing the KV cache, but often assume specific structural\nproperties of the embeddings. This raises the following natural question: Can\ntruly sublinear space utilization be achieved without such assumptions? In this\nwork, we answer this question in the negative. Any algorithm for\nattention-based token generation must use $\\Theta(nd)$ space, where $n$ is the\nnumber of tokens generated so far and $d = \\Omega(\\log n)$ is the dimension of\nthe KV embeddings. Our proof involves a reduction from a classic communication\ncomplexity problem and uses a randomized construction that leverages properties\nof projections in the spirit of the Johnson-Linderstrauss lemma. For the\nlow-dimensional regime $d = o(\\log n)$, we show that any algorithm requires\n$\\Omega(d\\cdot e^d)$ space and prove, using tight bounds on covering numbers,\nthat SubGen, proposed by Zandieh, Han, Mirrokni and Karbasi, matches this\nbound. Further, we investigate how sparsity assumptions enable token generation\nin truly sublinear space, presenting impossibility results and proposing a new\nKV cache compression algorithm for sliding window attention when the value\ncache outside the window is unmasked. Finally, we analyze token generation's\ntime complexity, using an indistinguishability argument to prove that no\nnon-adaptive algorithm can compute attention online in sublinear time for all\ntokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key limitation of autoregressive Transformers is the large memory needed at\ninference-time to cache all previous key-value (KV) embeddings. Prior works\naddress this by compressing the KV cache, but often assume specific structural\nproperties of the embeddings. This raises the following natural question: Can\ntruly sublinear space utilization be achieved without such assumptions? In this\nwork, we answer this question in the negative. Any algorithm for\nattention-based token generation must use $\\Theta(nd)$ space, where $n$ is the\nnumber of tokens generated so far and $d = \\Omega(\\log n)$ is the dimension of\nthe KV embeddings. Our proof involves a reduction from a classic communication\ncomplexity problem and uses a randomized construction that leverages properties\nof projections in the spirit of the Johnson-Linderstrauss lemma. For the\nlow-dimensional regime $d = o(\\log n)$, we show that any algorithm requires\n$\\Omega(d\\cdot e^d)$ space and prove, using tight bounds on covering numbers,\nthat SubGen, proposed by Zandieh, Han, Mirrokni and Karbasi, matches this\nbound. Further, we investigate how sparsity assumptions enable token generation\nin truly sublinear space, presenting impossibility results and proposing a new\nKV cache compression algorithm for sliding window attention when the value\ncache outside the window is unmasked. Finally, we analyze token generation's\ntime complexity, using an indistinguishability argument to prove that no\nnon-adaptive algorithm can compute attention online in sublinear time for all\ntokens."
                },
                "authors": [
                    {
                        "name": "Themistoklis Haris"
                    },
                    {
                        "name": "Krzysztof Onak"
                    }
                ],
                "author_detail": {
                    "name": "Krzysztof Onak"
                },
                "author": "Krzysztof Onak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14488v2",
                "updated": "2025-02-21T13:35:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    35,
                    43,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-20T12:09:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    9,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "U-index: A Universal Indexing Framework for Matching Long Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-index: A Universal Indexing Framework for Matching Long Patterns"
                },
                "summary": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but areslow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but areslow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping."
                },
                "authors": [
                    {
                        "name": "Lorraine A. K. Ayad"
                    },
                    {
                        "name": "Gabriele Fici"
                    },
                    {
                        "name": "Ragnar Groot Koerkamp"
                    },
                    {
                        "name": "Grigorios Loukides"
                    },
                    {
                        "name": "Rob Patro"
                    },
                    {
                        "name": "Giulio Ermanno Pibiri"
                    },
                    {
                        "name": "Solon P. Pissis"
                    }
                ],
                "author_detail": {
                    "name": "Solon P. Pissis"
                },
                "author": "Solon P. Pissis",
                "arxiv_comment": "18 pages, 6 figures, code available at\n  https://github.com/u-index/u-index-rs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17501v1",
                "updated": "2025-02-21T12:03:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    3,
                    7,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T12:03:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    3,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "CoKV: Optimizing KV Cache Allocation via Cooperative Game",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoKV: Optimizing KV Cache Allocation via Cooperative Game"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success on various\naspects of human life. However, one of the major challenges in deploying these\nmodels is the substantial memory consumption required to store key-value pairs\n(KV), which imposes significant resource demands. Recent research has focused\non KV cache budget allocation, with several approaches proposing head-level\nbudget distribution by evaluating the importance of individual attention heads.\nThese methods, however, assess the importance of heads independently,\noverlooking their cooperative contributions within the model, which may result\nin a deviation from their true impact on model performance. In light of this\nlimitation, we propose CoKV, a novel method that models the cooperation between\nheads in model inference as a cooperative game. By evaluating the contribution\nof each head within the cooperative game, CoKV can allocate the cache budget\nmore effectively. Extensive experiments show that CoKV achieves\nstate-of-the-art performance on the LongBench benchmark using\nLLama-3-8B-Instruct and Mistral-7B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success on various\naspects of human life. However, one of the major challenges in deploying these\nmodels is the substantial memory consumption required to store key-value pairs\n(KV), which imposes significant resource demands. Recent research has focused\non KV cache budget allocation, with several approaches proposing head-level\nbudget distribution by evaluating the importance of individual attention heads.\nThese methods, however, assess the importance of heads independently,\noverlooking their cooperative contributions within the model, which may result\nin a deviation from their true impact on model performance. In light of this\nlimitation, we propose CoKV, a novel method that models the cooperation between\nheads in model inference as a cooperative game. By evaluating the contribution\nof each head within the cooperative game, CoKV can allocate the cache budget\nmore effectively. Extensive experiments show that CoKV achieves\nstate-of-the-art performance on the LongBench benchmark using\nLLama-3-8B-Instruct and Mistral-7B models."
                },
                "authors": [
                    {
                        "name": "Qiheng Sun"
                    },
                    {
                        "name": "Hongwei Zhang"
                    },
                    {
                        "name": "Haocheng Xia"
                    },
                    {
                        "name": "Jiayao Zhang"
                    },
                    {
                        "name": "Jinfei Liu"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15304v1",
                "updated": "2025-02-21T08:55:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    55,
                    21,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T08:55:21Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    55,
                    21,
                    4,
                    52,
                    0
                ],
                "title": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention"
                },
                "summary": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs."
                },
                "authors": [
                    {
                        "name": "Hong Yankun"
                    },
                    {
                        "name": "Li Xing"
                    },
                    {
                        "name": "Zhen Hui-Ling"
                    },
                    {
                        "name": "Yu Xianzhi"
                    },
                    {
                        "name": "Liu Wulong"
                    },
                    {
                        "name": "Yuan Mingxuan"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Mingxuan"
                },
                "author": "Yuan Mingxuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15192v1",
                "updated": "2025-02-21T04:07:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T04:07:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "SAAP: Spatial awareness and Association based Prefetching of Virtual\n  Objects in Augmented Reality at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAAP: Spatial awareness and Association based Prefetching of Virtual\n  Objects in Augmented Reality at the Edge"
                },
                "summary": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SAAP, a Spatial Awareness and\nAssociation-based Prefetching policy specifically designed for MAR Caches. SAAP\nintelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SAAP significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3\\% to 40\\% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SAAP parameters\nto achieve optimal performance. Our findings demonstrate the potential of SAAP\nto substantially enhance the user experience in MAR applications by ensuring\nthe timely availability of virtual objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SAAP, a Spatial Awareness and\nAssociation-based Prefetching policy specifically designed for MAR Caches. SAAP\nintelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SAAP significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3\\% to 40\\% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SAAP parameters\nto achieve optimal performance. Our findings demonstrate the potential of SAAP\nto substantially enhance the user experience in MAR applications by ensuring\nthe timely availability of virtual objects."
                },
                "authors": [
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Abhishek Chandra"
                    },
                    {
                        "name": "Jon Weissman"
                    }
                ],
                "author_detail": {
                    "name": "Jon Weissman"
                },
                "author": "Jon Weissman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03065v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03065v2",
                "updated": "2025-02-20T23:28:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    23,
                    28,
                    1,
                    3,
                    51,
                    0
                ],
                "published": "2024-10-04T01:11:09Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    1,
                    11,
                    9,
                    4,
                    278,
                    0
                ],
                "title": "Compute Or Load KV Cache? Why Not Both?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Or Load KV Cache? Why Not Both?"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in large-scale online\nservices, enabling sophisticated applications. However, the computational\noverhead of generating key-value (KV) caches in the prefill stage presents a\nmajor bottleneck, particularly for long-context inputs. Prefix caching\nmitigates this issue by storing KV caches for reuse, reducing redundant\ncomputation. Despite its advantages, prefix caching suffers from high latency\ndue to the limited I/O bandwidth of storage devices, constraining inference\nefficiency. To address this challenge, we introduce Cake, a novel KV cache\nloading system that optimally utilizes both computational and I/O resources in\nparallel. Cake employs a bidirectional scheduling strategy that dynamically\nbalances KV cache computation and loading, ensuring efficient resource\nutilization. Additionally, Cake incorporates an adaptive scheduling mechanism\nthat seamlessly integrates with non-prefix caching requests, improving system\nthroughput and adapting to fluctuating resource availabilty. Through extensive\nevaluations across various hardware configurations, datasets, and storage\nconditions, Cake achieves on average 2.6x reduction in Time to First Token\n(TTFT) compared to compute-only and I/O-only methods. Our findings highlight\nCake as an effective and practical solution for optimizing long-context LLM\ninference, bridging the gap between computation and I/O efficiency in\nlarge-scale AI deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in large-scale online\nservices, enabling sophisticated applications. However, the computational\noverhead of generating key-value (KV) caches in the prefill stage presents a\nmajor bottleneck, particularly for long-context inputs. Prefix caching\nmitigates this issue by storing KV caches for reuse, reducing redundant\ncomputation. Despite its advantages, prefix caching suffers from high latency\ndue to the limited I/O bandwidth of storage devices, constraining inference\nefficiency. To address this challenge, we introduce Cake, a novel KV cache\nloading system that optimally utilizes both computational and I/O resources in\nparallel. Cake employs a bidirectional scheduling strategy that dynamically\nbalances KV cache computation and loading, ensuring efficient resource\nutilization. Additionally, Cake incorporates an adaptive scheduling mechanism\nthat seamlessly integrates with non-prefix caching requests, improving system\nthroughput and adapting to fluctuating resource availabilty. Through extensive\nevaluations across various hardware configurations, datasets, and storage\nconditions, Cake achieves on average 2.6x reduction in Time to First Token\n(TTFT) compared to compute-only and I/O-only methods. Our findings highlight\nCake as an effective and practical solution for optimizing long-context LLM\ninference, bridging the gap between computation and I/O efficiency in\nlarge-scale AI deployments."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03065v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03065v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15075v1",
                "updated": "2025-02-20T22:24:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T22:24:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "More for Keys, Less for Values: Adaptive KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More for Keys, Less for Values: Adaptive KV Cache Quantization"
                },
                "summary": "This paper introduces an information-aware quantization framework that\nadaptively compresses the key-value (KV) cache in large language models (LLMs).\nAlthough prior work has underscored the distinct roles of key and value cache\nduring inference, our systematic analysis -- examining singular value\ndistributions, spectral norms, and Frobenius norms -- reveals, for the first\ntime, that key matrices consistently exhibit higher norm values and are more\nsensitive to quantization than value matrices. Furthermore, our theoretical\nanalysis shows that matrices with higher spectral norms amplify quantization\nerrors more significantly. Motivated by these insights, we propose a\nmixed-precision quantization strategy, KV-AdaQuant, which allocates more\nbit-width for keys and fewer for values since key matrices have higher norm\nvalues. With the same total KV bit budget, this approach effectively mitigates\nerror propagation across transformer layers while achieving significant memory\nsavings. Our extensive experiments on multiple LLMs (1B--70B) demonstrate that\nour mixed-precision quantization scheme maintains high model accuracy even\nunder aggressive compression. For instance, using 4-bit for Key and 2-bit for\nValue achieves an accuracy of 75.2%, whereas reversing the assignment (2-bit\nfor Key and 4-bit for Value) yields only 54.7% accuracy. The code is available\nat https://tinyurl.com/kv-adaquant",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an information-aware quantization framework that\nadaptively compresses the key-value (KV) cache in large language models (LLMs).\nAlthough prior work has underscored the distinct roles of key and value cache\nduring inference, our systematic analysis -- examining singular value\ndistributions, spectral norms, and Frobenius norms -- reveals, for the first\ntime, that key matrices consistently exhibit higher norm values and are more\nsensitive to quantization than value matrices. Furthermore, our theoretical\nanalysis shows that matrices with higher spectral norms amplify quantization\nerrors more significantly. Motivated by these insights, we propose a\nmixed-precision quantization strategy, KV-AdaQuant, which allocates more\nbit-width for keys and fewer for values since key matrices have higher norm\nvalues. With the same total KV bit budget, this approach effectively mitigates\nerror propagation across transformer layers while achieving significant memory\nsavings. Our extensive experiments on multiple LLMs (1B--70B) demonstrate that\nour mixed-precision quantization scheme maintains high model accuracy even\nunder aggressive compression. For instance, using 4-bit for Key and 2-bit for\nValue achieves an accuracy of 75.2%, whereas reversing the assignment (2-bit\nfor Key and 4-bit for Value) yields only 54.7% accuracy. The code is available\nat https://tinyurl.com/kv-adaquant"
                },
                "authors": [
                    {
                        "name": "Mohsen Hariri"
                    },
                    {
                        "name": "Lam Nguyen"
                    },
                    {
                        "name": "Sixu Chen"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Vipin Chaudhary"
                },
                "author": "Vipin Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v1",
                "updated": "2025-02-20T18:59:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14837v1",
                "updated": "2025-02-20T18:50:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:50:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs"
                },
                "summary": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance."
                },
                "authors": [
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Bin Guo"
                    },
                    {
                        "name": "Yuanbin Wu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Lixing Shen"
                    },
                    {
                        "name": "Zhan Chen"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    }
                ],
                "author_detail": {
                    "name": "Tao Gui"
                },
                "author": "Tao Gui",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v3",
                "updated": "2025-02-20T16:01:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    1,
                    34,
                    3,
                    51,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14938v1",
                "updated": "2025-02-20T14:01:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    1,
                    17,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T14:01:17Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    1,
                    17,
                    3,
                    51,
                    0
                ],
                "title": "GS-Cache: A GS-Cache Inference Framework for Large-scale Gaussian\n  Splatting Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GS-Cache: A GS-Cache Inference Framework for Large-scale Gaussian\n  Splatting Models"
                },
                "summary": "Rendering large-scale 3D Gaussian Splatting (3DGS) model faces significant\nchallenges in achieving real-time, high-fidelity performance on consumer-grade\ndevices. Fully realizing the potential of 3DGS in applications such as virtual\nreality (VR) requires addressing critical system-level challenges to support\nreal-time, immersive experiences. We propose GS-Cache, an end-to-end framework\nthat seamlessly integrates 3DGS's advanced representation with a highly\noptimized rendering system. GS-Cache introduces a cache-centric pipeline to\neliminate redundant computations, an efficiency-aware scheduler for elastic\nmulti-GPU rendering, and optimized CUDA kernels to overcome computational\nbottlenecks. This synergy between 3DGS and system design enables GS-Cache to\nachieve up to 5.35x performance improvement, 35% latency reduction, and 42%\nlower GPU memory usage, supporting 2K binocular rendering at over 120 FPS with\nhigh visual quality. By bridging the gap between 3DGS's representation power\nand the demands of VR systems, GS-Cache establishes a scalable and efficient\nframework for real-time neural rendering in immersive environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rendering large-scale 3D Gaussian Splatting (3DGS) model faces significant\nchallenges in achieving real-time, high-fidelity performance on consumer-grade\ndevices. Fully realizing the potential of 3DGS in applications such as virtual\nreality (VR) requires addressing critical system-level challenges to support\nreal-time, immersive experiences. We propose GS-Cache, an end-to-end framework\nthat seamlessly integrates 3DGS's advanced representation with a highly\noptimized rendering system. GS-Cache introduces a cache-centric pipeline to\neliminate redundant computations, an efficiency-aware scheduler for elastic\nmulti-GPU rendering, and optimized CUDA kernels to overcome computational\nbottlenecks. This synergy between 3DGS and system design enables GS-Cache to\nachieve up to 5.35x performance improvement, 35% latency reduction, and 42%\nlower GPU memory usage, supporting 2K binocular rendering at over 120 FPS with\nhigh visual quality. By bridging the gap between 3DGS's representation power\nand the demands of VR systems, GS-Cache establishes a scalable and efficient\nframework for real-time neural rendering in immersive environments."
                },
                "authors": [
                    {
                        "name": "Miao Tao"
                    },
                    {
                        "name": "Yuanzhen Zhou"
                    },
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Zeyu He"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yuchang Zhang"
                    },
                    {
                        "name": "Zhongling Su"
                    },
                    {
                        "name": "Linning Xu"
                    },
                    {
                        "name": "Zhenxiang Ma"
                    },
                    {
                        "name": "Rong Fu"
                    },
                    {
                        "name": "Hengjie Li"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14504v1",
                "updated": "2025-02-20T12:31:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    31,
                    31,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T12:31:31Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    31,
                    31,
                    3,
                    51,
                    0
                ],
                "title": "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large\n  Vision-Language Models"
                },
                "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities across a range of multimodal tasks. However, their inference\nefficiency is constrained by the large number of visual tokens processed during\ndecoding. To address this challenge, we propose Per-Layer Per-Head Vision Token\nPruning (PLPHP), a two-level fine-grained pruning method including Layer-Level\nRetention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the\nVision Token Re-attention phenomenon across decoder layers, we dynamically\nadjust token retention rates layer by layer. Layers that exhibit stronger\nattention to visual information preserve more vision tokens, while layers with\nlower vision attention are aggressively pruned. Furthermore, PLPHP applies\npruning at the attention head level, enabling different heads within the same\nlayer to independently retain critical context. Experiments on multiple\nbenchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and\nreduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of\n0.46% average performance drop, while also achieving notable performance\nimprovements in multi-image tasks. These results highlight the effectiveness of\nfine-grained token pruning and contribute to advancing the efficiency and\nscalability of LVLMs. Our source code will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities across a range of multimodal tasks. However, their inference\nefficiency is constrained by the large number of visual tokens processed during\ndecoding. To address this challenge, we propose Per-Layer Per-Head Vision Token\nPruning (PLPHP), a two-level fine-grained pruning method including Layer-Level\nRetention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the\nVision Token Re-attention phenomenon across decoder layers, we dynamically\nadjust token retention rates layer by layer. Layers that exhibit stronger\nattention to visual information preserve more vision tokens, while layers with\nlower vision attention are aggressively pruned. Furthermore, PLPHP applies\npruning at the attention head level, enabling different heads within the same\nlayer to independently retain critical context. Experiments on multiple\nbenchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and\nreduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of\n0.46% average performance drop, while also achieving notable performance\nimprovements in multi-image tasks. These results highlight the effectiveness of\nfine-grained token pruning and contribute to advancing the efficiency and\nscalability of LVLMs. Our source code will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Yu Meng"
                    },
                    {
                        "name": "Kaiyuan Li"
                    },
                    {
                        "name": "Chenran Huang"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Xiaoping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoping Zhang"
                },
                "author": "Xiaoping Zhang",
                "arxiv_comment": "12 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v2",
                "updated": "2025-02-20T12:14:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    14,
                    49,
                    3,
                    51,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v2",
                "updated": "2025-02-20T09:03:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    9,
                    3,
                    5,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14347v1",
                "updated": "2025-02-20T08:00:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    8,
                    0,
                    25,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T08:00:25Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    8,
                    0,
                    25,
                    3,
                    51,
                    0
                ],
                "title": "Discovery of a new phase in thin flakes of KV$_{3}$Sb$_{5}$ under\n  pressure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovery of a new phase in thin flakes of KV$_{3}$Sb$_{5}$ under\n  pressure"
                },
                "summary": "We report results of magnetotransport measurements on KV$_3$Sb$_5$ thin\nflakes under pressure. Our zero-field electrical resistance reveals an\nadditional anomaly emerging under pressure ($p$), marking a previously\nunidentified phase boundary $T^{\\rm \\ast}$($p$). Together with the established\n$T_{\\rm CDW}(p)$ and $T_c(p)$, denoting the charge-density-wave transition and\na superconducting transition, respectively, the temperature-pressure phase\ndiagram of KV$_3$Sb$_5$ features a rich interplay among multiple phases. The\nHall coefficient evolves reasonably smoothly when crossing the $T^{\\rm \\ast}$\nphase boundary compared with the variation when crossing $T_{\\rm CDW}$,\nindicating the preservation of the pristine electronic structure. The mobility\nspectrum analysis provides further insights into distinguishing different\nphases. Finally, our high-pressure quantum oscillation studies up to 31 T\ncombined with density functional theory calculations further demonstrate that\nthe new phase does not reconstruct the Fermi surface, confirming that the\ntranslational symmetry of the pristine metallic state is preserved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report results of magnetotransport measurements on KV$_3$Sb$_5$ thin\nflakes under pressure. Our zero-field electrical resistance reveals an\nadditional anomaly emerging under pressure ($p$), marking a previously\nunidentified phase boundary $T^{\\rm \\ast}$($p$). Together with the established\n$T_{\\rm CDW}(p)$ and $T_c(p)$, denoting the charge-density-wave transition and\na superconducting transition, respectively, the temperature-pressure phase\ndiagram of KV$_3$Sb$_5$ features a rich interplay among multiple phases. The\nHall coefficient evolves reasonably smoothly when crossing the $T^{\\rm \\ast}$\nphase boundary compared with the variation when crossing $T_{\\rm CDW}$,\nindicating the preservation of the pristine electronic structure. The mobility\nspectrum analysis provides further insights into distinguishing different\nphases. Finally, our high-pressure quantum oscillation studies up to 31 T\ncombined with density functional theory calculations further demonstrate that\nthe new phase does not reconstruct the Fermi surface, confirming that the\ntranslational symmetry of the pristine metallic state is preserved."
                },
                "authors": [
                    {
                        "name": "Zheyu Wang"
                    },
                    {
                        "name": "Lingfei Wang"
                    },
                    {
                        "name": "King Yau Yip"
                    },
                    {
                        "name": "Ying Kit Tsui"
                    },
                    {
                        "name": "Tsz Fung Poon"
                    },
                    {
                        "name": "Wenyan Wang"
                    },
                    {
                        "name": "Chun Wai Tsang"
                    },
                    {
                        "name": "Shanmin Wang"
                    },
                    {
                        "name": "David Graf"
                    },
                    {
                        "name": "Alexandre Pourret"
                    },
                    {
                        "name": "Gabriel Seyfarth"
                    },
                    {
                        "name": "Georg Knebel"
                    },
                    {
                        "name": "Kwing To Lai"
                    },
                    {
                        "name": "Wing Chi Yu"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Swee K. Goh"
                    }
                ],
                "author_detail": {
                    "name": "Swee K. Goh"
                },
                "author": "Swee K. Goh",
                "arxiv_comment": "10 pages, 5 figures. Advanced Science (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14317v1",
                "updated": "2025-02-20T07:10:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T07:10:43Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "title": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation"
                },
                "summary": "Efficiently handling long contexts is crucial for large language models\n(LLMs). While rotary position embeddings (RoPEs) enhance length generalization,\neffective length extrapolation remains challenging and often requires costly\nfine-tuning. In contrast, recent training-free approaches suffer from the\nattention sink phenomenon, leading to severe performance degradation. In this\npaper, we introduce ParallelComp, a novel training-free method for long-context\nextrapolation that extends LLMs' context length from 4K to 128K while\nmaintaining high throughput and preserving perplexity, and integrates\nseamlessly with Flash Attention. Our analysis offers new insights into\nattention biases in parallel attention mechanisms and provides practical\nsolutions to tackle these challenges. To mitigate the attention sink issue, we\npropose an attention calibration strategy that reduces biases, ensuring more\nstable long-range attention. Additionally, we introduce a chunk eviction\nstrategy to efficiently manage ultra-long contexts on a single A100 80GB GPU.\nTo further enhance efficiency, we propose a parallel KV cache eviction\ntechnique, which improves chunk throughput by 1.76x, thereby achieving a 23.50x\nacceleration in the prefilling stage with negligible performance loss due to\nattention calibration. Furthermore, ParallelComp achieves 91.17% of GPT-4's\nperformance on long-context tasks using an 8B model trained on 8K-length\ncontext, outperforming powerful closed-source models such as Claude-2 and\nKimi-Chat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently handling long contexts is crucial for large language models\n(LLMs). While rotary position embeddings (RoPEs) enhance length generalization,\neffective length extrapolation remains challenging and often requires costly\nfine-tuning. In contrast, recent training-free approaches suffer from the\nattention sink phenomenon, leading to severe performance degradation. In this\npaper, we introduce ParallelComp, a novel training-free method for long-context\nextrapolation that extends LLMs' context length from 4K to 128K while\nmaintaining high throughput and preserving perplexity, and integrates\nseamlessly with Flash Attention. Our analysis offers new insights into\nattention biases in parallel attention mechanisms and provides practical\nsolutions to tackle these challenges. To mitigate the attention sink issue, we\npropose an attention calibration strategy that reduces biases, ensuring more\nstable long-range attention. Additionally, we introduce a chunk eviction\nstrategy to efficiently manage ultra-long contexts on a single A100 80GB GPU.\nTo further enhance efficiency, we propose a parallel KV cache eviction\ntechnique, which improves chunk throughput by 1.76x, thereby achieving a 23.50x\nacceleration in the prefilling stage with negligible performance loss due to\nattention calibration. Furthermore, ParallelComp achieves 91.17% of GPT-4's\nperformance on long-context tasks using an 8B model trained on 8K-length\ncontext, outperforming powerful closed-source models such as Claude-2 and\nKimi-Chat."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Chiwun Yang"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "We will release the code soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14307v1",
                "updated": "2025-02-20T06:42:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    42,
                    3,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T06:42:03Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    42,
                    3,
                    3,
                    51,
                    0
                ],
                "title": "μRL: Discovering Transient Execution Vulnerabilities Using\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "μRL: Discovering Transient Execution Vulnerabilities Using\n  Reinforcement Learning"
                },
                "summary": "We propose using reinforcement learning to address the challenges of\ndiscovering microarchitectural vulnerabilities, such as Spectre and Meltdown,\nwhich exploit subtle interactions in modern processors. Traditional methods\nlike random fuzzing fail to efficiently explore the vast instruction space and\noften miss vulnerabilities that manifest under specific conditions. To overcome\nthis, we introduce an intelligent, feedback-driven approach using RL. Our RL\nagents interact with the processor, learning from real-time feedback to\nprioritize instruction sequences more likely to reveal vulnerabilities,\nsignificantly improving the efficiency of the discovery process.\n  We also demonstrate that RL systems adapt effectively to various\nmicroarchitectures, providing a scalable solution across processor generations.\nBy automating the exploration process, we reduce the need for human\nintervention, enabling continuous learning that uncovers hidden\nvulnerabilities. Additionally, our approach detects subtle signals, such as\ntiming anomalies or unusual cache behavior, that may indicate\nmicroarchitectural weaknesses. This proposal advances hardware security testing\nby introducing a more efficient, adaptive, and systematic framework for\nprotecting modern processors.\n  When unleashed on Intel Skylake-X and Raptor Lake microarchitectures, our RL\nagent was indeed able to generate instruction sequences that cause significant\nobservable byte leakages through transient execution without generating any\n$\\mu$code assists, faults or interrupts. The newly identified leaky sequences\nstem from a variety of Intel instructions, e.g. including SERIALIZE, VERR/VERW,\nCLMUL, MMX-x87 transitions, LSL+RDSCP and LAR. These initial results give\ncredence to the proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose using reinforcement learning to address the challenges of\ndiscovering microarchitectural vulnerabilities, such as Spectre and Meltdown,\nwhich exploit subtle interactions in modern processors. Traditional methods\nlike random fuzzing fail to efficiently explore the vast instruction space and\noften miss vulnerabilities that manifest under specific conditions. To overcome\nthis, we introduce an intelligent, feedback-driven approach using RL. Our RL\nagents interact with the processor, learning from real-time feedback to\nprioritize instruction sequences more likely to reveal vulnerabilities,\nsignificantly improving the efficiency of the discovery process.\n  We also demonstrate that RL systems adapt effectively to various\nmicroarchitectures, providing a scalable solution across processor generations.\nBy automating the exploration process, we reduce the need for human\nintervention, enabling continuous learning that uncovers hidden\nvulnerabilities. Additionally, our approach detects subtle signals, such as\ntiming anomalies or unusual cache behavior, that may indicate\nmicroarchitectural weaknesses. This proposal advances hardware security testing\nby introducing a more efficient, adaptive, and systematic framework for\nprotecting modern processors.\n  When unleashed on Intel Skylake-X and Raptor Lake microarchitectures, our RL\nagent was indeed able to generate instruction sequences that cause significant\nobservable byte leakages through transient execution without generating any\n$\\mu$code assists, faults or interrupts. The newly identified leaky sequences\nstem from a variety of Intel instructions, e.g. including SERIALIZE, VERR/VERW,\nCLMUL, MMX-x87 transitions, LSL+RDSCP and LAR. These initial results give\ncredence to the proposed approach."
                },
                "authors": [
                    {
                        "name": "M. Caner Tol"
                    },
                    {
                        "name": "Kemal Derya"
                    },
                    {
                        "name": "Berk Sunar"
                    }
                ],
                "author_detail": {
                    "name": "Berk Sunar"
                },
                "author": "Berk Sunar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16406v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16406v4",
                "updated": "2025-02-20T06:07:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    7,
                    0,
                    3,
                    51,
                    0
                ],
                "published": "2024-05-26T02:15:49Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    2,
                    15,
                    49,
                    6,
                    147,
                    0
                ],
                "title": "SpinQuant: LLM quantization with learned rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpinQuant: LLM quantization with learned rotations"
                },
                "summary": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\nCode is available at https://github.com/facebookresearch/SpinQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\nCode is available at https://github.com/facebookresearch/SpinQuant."
                },
                "authors": [
                    {
                        "name": "Zechun Liu"
                    },
                    {
                        "name": "Changsheng Zhao"
                    },
                    {
                        "name": "Igor Fedorov"
                    },
                    {
                        "name": "Bilge Soran"
                    },
                    {
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "name": "Raghuraman Krishnamoorthi"
                    },
                    {
                        "name": "Vikas Chandra"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Tijmen Blankevoort"
                    }
                ],
                "author_detail": {
                    "name": "Tijmen Blankevoort"
                },
                "author": "Tijmen Blankevoort",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16406v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16406v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14280v1",
                "updated": "2025-02-20T05:41:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    5,
                    41,
                    15,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T05:41:15Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    5,
                    41,
                    15,
                    3,
                    51,
                    0
                ],
                "title": "EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have yielded impressive\nsuccesses on many language tasks. However, efficient processing of long\ncontexts using LLMs remains a significant challenge. We introduce\n\\textbf{EpMAN} -- a method for processing long contexts in an \\textit{episodic\nmemory} module while \\textit{holistically attending to} semantically relevant\ncontext chunks. The output of \\textit{episodic attention} is then used to\nreweigh the decoder's self-attention to the stored KV cache of the context\nduring training and generation. When an LLM decoder is trained using\n\\textbf{EpMAN}, its performance on multiple challenging single-hop long-context\nrecall and question-answering benchmarks is found to be stronger and more\nrobust across the range from 16k to 256k tokens than baseline decoders trained\nwith self-attention, and popular retrieval-augmented generation frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have yielded impressive\nsuccesses on many language tasks. However, efficient processing of long\ncontexts using LLMs remains a significant challenge. We introduce\n\\textbf{EpMAN} -- a method for processing long contexts in an \\textit{episodic\nmemory} module while \\textit{holistically attending to} semantically relevant\ncontext chunks. The output of \\textit{episodic attention} is then used to\nreweigh the decoder's self-attention to the stored KV cache of the context\nduring training and generation. When an LLM decoder is trained using\n\\textbf{EpMAN}, its performance on multiple challenging single-hop long-context\nrecall and question-answering benchmarks is found to be stronger and more\nrobust across the range from 16k to 256k tokens than baseline decoders trained\nwith self-attention, and popular retrieval-augmented generation frameworks."
                },
                "authors": [
                    {
                        "name": "Subhajit Chaudhury"
                    },
                    {
                        "name": "Payel Das"
                    },
                    {
                        "name": "Sarathkrishna Swaminathan"
                    },
                    {
                        "name": "Georgios Kollias"
                    },
                    {
                        "name": "Elliot Nelson"
                    },
                    {
                        "name": "Khushbu Pahwa"
                    },
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "Igor Melnyk"
                    },
                    {
                        "name": "Matthew Riemer"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Riemer"
                },
                "author": "Matthew Riemer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14220v1",
                "updated": "2025-02-20T03:27:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    3,
                    27,
                    0,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T03:27:00Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    3,
                    27,
                    0,
                    3,
                    51,
                    0
                ],
                "title": "NDPage: Efficient Address Translation for Near-Data Processing\n  Architectures via Tailored Page Table",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NDPage: Efficient Address Translation for Near-Data Processing\n  Architectures via Tailored Page Table"
                },
                "summary": "Near-Data Processing (NDP) has been a promising architectural paradigm to\naddress the memory wall problem for data-intensive applications. Practical\nimplementation of NDP architectures calls for system support for better\nprogrammability, where having virtual memory (VM) is critical. Modern computing\nsystems incorporate a 4-level page table design to support address translation\nin VM. However, simply adopting an existing 4-level page table in NDP systems\ncauses significant address translation overhead because (1) NDP applications\ngenerate a lot of address translations, and (2) the limited L1 cache in NDP\nsystems cannot cover the accesses to page table entries (PTEs). We extensively\nanalyze the 4-level page table design in the NDP scenario and observe that (1)\nthe memory access to page table entries is highly irregular, thus cannot\nbenefit from the L1 cache, and (2) the last two levels of page tables are\nnearly fully occupied. Based on our observations, we propose NDPage, an\nefficient page table design tailored for NDP systems. The key mechanisms of\nNDPage are (1) an L1 cache bypass mechanism for PTEs that not only accelerates\nthe memory accesses of PTEs but also prevents the pollution of PTEs in the\ncache system, and (2) a flattened page table design that merges the last two\nlevels of page tables, allowing the page table to enjoy the flexibility of a\n4KB page while reducing the number of PTE accesses. We evaluate NDPage using a\nvariety of data-intensive workloads. Our evaluation shows that in a single-core\nNDP system, NDPage improves the end-to-end performance over the\nstate-of-the-art address translation mechanism of 14.3\\%; in 4-core and 8-core\nNDP systems, NDPage enhances the performance of 9.8\\% and 30.5\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near-Data Processing (NDP) has been a promising architectural paradigm to\naddress the memory wall problem for data-intensive applications. Practical\nimplementation of NDP architectures calls for system support for better\nprogrammability, where having virtual memory (VM) is critical. Modern computing\nsystems incorporate a 4-level page table design to support address translation\nin VM. However, simply adopting an existing 4-level page table in NDP systems\ncauses significant address translation overhead because (1) NDP applications\ngenerate a lot of address translations, and (2) the limited L1 cache in NDP\nsystems cannot cover the accesses to page table entries (PTEs). We extensively\nanalyze the 4-level page table design in the NDP scenario and observe that (1)\nthe memory access to page table entries is highly irregular, thus cannot\nbenefit from the L1 cache, and (2) the last two levels of page tables are\nnearly fully occupied. Based on our observations, we propose NDPage, an\nefficient page table design tailored for NDP systems. The key mechanisms of\nNDPage are (1) an L1 cache bypass mechanism for PTEs that not only accelerates\nthe memory accesses of PTEs but also prevents the pollution of PTEs in the\ncache system, and (2) a flattened page table design that merges the last two\nlevels of page tables, allowing the page table to enjoy the flexibility of a\n4KB page while reducing the number of PTE accesses. We evaluate NDPage using a\nvariety of data-intensive workloads. Our evaluation shows that in a single-core\nNDP system, NDPage improves the end-to-end performance over the\nstate-of-the-art address translation mechanism of 14.3\\%; in 4-core and 8-core\nNDP systems, NDPage enhances the performance of 9.8\\% and 30.5\\%, respectively."
                },
                "authors": [
                    {
                        "name": "Qingcai Jiang"
                    },
                    {
                        "name": "Buxin Tu"
                    },
                    {
                        "name": "Hong An"
                    }
                ],
                "author_detail": {
                    "name": "Hong An"
                },
                "author": "Hong An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v1",
                "updated": "2025-02-19T19:12:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy designed\nspecifically to reduce both memory bandwidth and capacity demand of KV cache\nduring the decode phase. RocketKV contains two consecutive stages. In the first\nstage, it performs coarse-grain KV cache eviction on the input sequence tokens\nwith SnapKV++, a method improved upon SnapKV by introducing adaptive pooling\nsize and full compatibility with grouped-query attention. In the second stage,\nit adopts a hybrid attention method to conduct fine-grain top-k sparse\nattention, approximating the attention scores by leveraging both head and\nsequence dimensional reductions. Combining these two stages, RocketKV achieves\nsignificant KV cache fetching bandwidth and storage savings while maintaining\ncomparable accuracy to full KV cache attention. We show that RocketKV provides\nend-to-end speedup by up to 3$\\times$ as well as peak memory reduction by up to\n31% in the decode phase on an NVIDIA H100 GPU compared to the full KV cache\nbaseline, while achieving negligible accuracy loss on a variety of long-context\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy designed\nspecifically to reduce both memory bandwidth and capacity demand of KV cache\nduring the decode phase. RocketKV contains two consecutive stages. In the first\nstage, it performs coarse-grain KV cache eviction on the input sequence tokens\nwith SnapKV++, a method improved upon SnapKV by introducing adaptive pooling\nsize and full compatibility with grouped-query attention. In the second stage,\nit adopts a hybrid attention method to conduct fine-grain top-k sparse\nattention, approximating the attention scores by leveraging both head and\nsequence dimensional reductions. Combining these two stages, RocketKV achieves\nsignificant KV cache fetching bandwidth and storage savings while maintaining\ncomparable accuracy to full KV cache attention. We show that RocketKV provides\nend-to-end speedup by up to 3$\\times$ as well as peak memory reduction by up to\n31% in the decode phase on an NVIDIA H100 GPU compared to the full KV cache\nbaseline, while achieving negligible accuracy loss on a variety of long-context\ntasks."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v4",
                "updated": "2025-02-19T17:53:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    53,
                    11,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning"
                },
                "summary": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is the SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 13.3\\% fewer model parameters and 15.4\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is the SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 13.3\\% fewer model parameters and 15.4\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Fares Obeid"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13873v1",
                "updated": "2025-02-19T16:54:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T16:54:58Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "title": "NVR: Vector Runahead on NPUs for Sparse Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVR: Vector Runahead on NPUs for Sparse Memory Access"
                },
                "summary": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zhengpeng Zhao"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Bing Guo"
                    },
                    {
                        "name": "He Xiao"
                    },
                    {
                        "name": "Chenhao Ma"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v2",
                "updated": "2025-02-19T11:10:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    10,
                    9,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v4",
                "updated": "2025-02-19T10:39:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    39,
                    58,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "ToCa is honored to be accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13575v1",
                "updated": "2025-02-19T09:30:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T09:30:38Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "title": "ETS: Efficient Tree Search for Inference-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETS: Efficient Tree Search for Inference-Time Scaling"
                },
                "summary": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Suhong Moon"
                    },
                    {
                        "name": "Kerem Dilmen"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "Nicholas Lee"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13542v1",
                "updated": "2025-02-19T08:50:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    50,
                    44,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T08:50:44Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    50,
                    44,
                    2,
                    50,
                    0
                ],
                "title": "Activation-aware Probe-Query: Effective Key-Value Retrieval for\n  Long-Context LLMs Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation-aware Probe-Query: Effective Key-Value Retrieval for\n  Long-Context LLMs Inference"
                },
                "summary": "Recent advances in large language models (LLMs) have showcased exceptional\nperformance in long-context tasks, while facing significant inference\nefficiency challenges with limited GPU memory. Existing solutions first\nproposed the sliding-window approach to accumulate a set of historical\n\\textbf{key-value} (KV) pairs for reuse, then further improvements selectively\nretain its subsets at each step. However, due to the sparse attention\ndistribution across a long context, it is hard to identify and recall relevant\nKV pairs, as the attention is distracted by massive candidate pairs.\nAdditionally, we found it promising to select representative tokens as\nprobe-Query in each sliding window to effectively represent the entire context,\nwhich is an approach overlooked by existing methods. Thus, we propose\n\\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that\ndynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the\nrelevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a\ntoken-level indicator, Activation Bias, within each context window, enabling\nthe proper construction of probe-Query for retrieval at pre-filling stage. To\naccurately recall the relevant KV pairs and minimize the irrelevant ones, we\ndesign a dynamic KV cut-off mechanism guided by information density across\nlayers at the decoding stage. Experiments on the Long-Bench and $\\infty$\nBenchmarks demonstrate its state-of-the-art performance with competitive\ninference quality and resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have showcased exceptional\nperformance in long-context tasks, while facing significant inference\nefficiency challenges with limited GPU memory. Existing solutions first\nproposed the sliding-window approach to accumulate a set of historical\n\\textbf{key-value} (KV) pairs for reuse, then further improvements selectively\nretain its subsets at each step. However, due to the sparse attention\ndistribution across a long context, it is hard to identify and recall relevant\nKV pairs, as the attention is distracted by massive candidate pairs.\nAdditionally, we found it promising to select representative tokens as\nprobe-Query in each sliding window to effectively represent the entire context,\nwhich is an approach overlooked by existing methods. Thus, we propose\n\\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that\ndynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the\nrelevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a\ntoken-level indicator, Activation Bias, within each context window, enabling\nthe proper construction of probe-Query for retrieval at pre-filling stage. To\naccurately recall the relevant KV pairs and minimize the irrelevant ones, we\ndesign a dynamic KV cut-off mechanism guided by information density across\nlayers at the decoding stage. Experiments on the Long-Bench and $\\infty$\nBenchmarks demonstrate its state-of-the-art performance with competitive\ninference quality and resource efficiency."
                },
                "authors": [
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Jiachuan Wang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jiaqi Tang"
                    },
                    {
                        "name": "Shuangyin Li"
                    },
                    {
                        "name": "Yongqi Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15804v1",
                "updated": "2025-02-19T06:14:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    6,
                    14,
                    27,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T06:14:27Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    6,
                    14,
                    27,
                    2,
                    50,
                    0
                ],
                "title": "FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference"
                },
                "summary": "KV cache techniques in Transformer models aim to reduce redundant\ncomputations at the expense of substantially increased memory usage, making KV\ncache compression an important and popular research topic. Recently,\nstate-of-the-art KV cache compression methods implement imbalanced, per-head\nallocation algorithms that dynamically adjust the KV cache budget for each\nattention head, achieving excellent performance in single-GPU scenarios.\nHowever, we observe that such imbalanced compression leads to significant load\nimbalance when deploying multi-GPU inference, as some GPUs become overburdened\nwhile others remain underutilized. In this paper, we propose FairKV, a method\ndesigned to ensure fair memory usage among attention heads in systems employing\nimbalanced KV cache compression. The core technique of FairKV is Fair-Copying,\nwhich replicates a small subset of memory-intensive attention heads across GPUs\nusing data parallelism to mitigate load imbalance. Our experiments on popular\nmodels, including LLaMA 70b and Mistral 24b model, demonstrate that FairKV\nincreases throughput by 1.66x compared to standard tensor parallelism\ninference. Our code will be released as open source upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache techniques in Transformer models aim to reduce redundant\ncomputations at the expense of substantially increased memory usage, making KV\ncache compression an important and popular research topic. Recently,\nstate-of-the-art KV cache compression methods implement imbalanced, per-head\nallocation algorithms that dynamically adjust the KV cache budget for each\nattention head, achieving excellent performance in single-GPU scenarios.\nHowever, we observe that such imbalanced compression leads to significant load\nimbalance when deploying multi-GPU inference, as some GPUs become overburdened\nwhile others remain underutilized. In this paper, we propose FairKV, a method\ndesigned to ensure fair memory usage among attention heads in systems employing\nimbalanced KV cache compression. The core technique of FairKV is Fair-Copying,\nwhich replicates a small subset of memory-intensive attention heads across GPUs\nusing data parallelism to mitigate load imbalance. Our experiments on popular\nmodels, including LLaMA 70b and Mistral 24b model, demonstrate that FairKV\nincreases throughput by 1.66x compared to standard tensor parallelism\ninference. Our code will be released as open source upon acceptance."
                },
                "authors": [
                    {
                        "name": "Bingzhe Zhao"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Aomufei Yuan"
                    },
                    {
                        "name": "Yuxuan Tian"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Lian Yu"
                    }
                ],
                "author_detail": {
                    "name": "Lian Yu"
                },
                "author": "Lian Yu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13145v1",
                "updated": "2025-02-18T18:59:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:59:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba"
                },
                "authors": [
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Yingyue Li"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Code and model are available at https://github.com/hustvl/mmMamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13063v1",
                "updated": "2025-02-18T17:08:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T17:08:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity"
                },
                "summary": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches allow to\nreduce the amount of compute in existing language models. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches allow to\nreduce the amount of compute in existing language models. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign."
                },
                "authors": [
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Mikhail Arkhipov"
                    },
                    {
                        "name": "Aydar Bulatov"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Burtsev"
                },
                "author": "Mikhail Burtsev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12875v1",
                "updated": "2025-02-18T14:05:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    5,
                    12,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:05:12Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    5,
                    12,
                    1,
                    49,
                    0
                ],
                "title": "A Survey on DRL based UAV Communications and Networking: DRL\n  Fundamentals, Applications and Implementations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on DRL based UAV Communications and Networking: DRL\n  Fundamentals, Applications and Implementations"
                },
                "summary": "Unmanned aerial vehicles (UAVs) are playing an increasingly pivotal role in\nmodern communication networks,offering flexibility and enhanced coverage for a\nvariety of applica-tions. However, UAV networks pose significant challenges due\nto their dynamic and distributed nature, particularly when dealing with tasks\nsuch as power allocation, channel assignment, caching,and task offloading.\nTraditional optimization techniques often struggle to handle the complexity and\nunpredictability of these environments, leading to suboptimal performance. This\nsurvey provides a comprehensive examination of how deep reinforcement learning\n(DRL) can be applied to solve these mathematical optimization problems in UAV\ncommunications and networking.Rather than simply introducing DRL methods, the\nfocus is on demonstrating how these methods can be utilized to solve complex\nmathematical models of the underlying problems. We begin by reviewing the\nfundamental concepts of DRL, including value-based, policy-based, and\nactor-critic approaches. Then,we illustrate how DRL algorithms are applied to\nspecific UAV network tasks by discussing from problem formulations to DRL\nimplementation. By framing UAV communication challenges as optimization\nproblems, this survey emphasizes the practical value of DRL in dynamic and\nuncertain environments. We also explore the strengths of DRL in handling\nlarge-scale network scenarios and the ability to continuously adapt to changes\nin the environment. In addition, future research directions are outlined,\nhighlighting the potential for DRL to further enhance UAV communications and\nexpand its applicability to more complex,multi-agent settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned aerial vehicles (UAVs) are playing an increasingly pivotal role in\nmodern communication networks,offering flexibility and enhanced coverage for a\nvariety of applica-tions. However, UAV networks pose significant challenges due\nto their dynamic and distributed nature, particularly when dealing with tasks\nsuch as power allocation, channel assignment, caching,and task offloading.\nTraditional optimization techniques often struggle to handle the complexity and\nunpredictability of these environments, leading to suboptimal performance. This\nsurvey provides a comprehensive examination of how deep reinforcement learning\n(DRL) can be applied to solve these mathematical optimization problems in UAV\ncommunications and networking.Rather than simply introducing DRL methods, the\nfocus is on demonstrating how these methods can be utilized to solve complex\nmathematical models of the underlying problems. We begin by reviewing the\nfundamental concepts of DRL, including value-based, policy-based, and\nactor-critic approaches. Then,we illustrate how DRL algorithms are applied to\nspecific UAV network tasks by discussing from problem formulations to DRL\nimplementation. By framing UAV communication challenges as optimization\nproblems, this survey emphasizes the practical value of DRL in dynamic and\nuncertain environments. We also explore the strengths of DRL in handling\nlarge-scale network scenarios and the ability to continuously adapt to changes\nin the environment. In addition, future research directions are outlined,\nhighlighting the potential for DRL to further enhance UAV communications and\nexpand its applicability to more complex,multi-agent settings."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Shaoxin Cui"
                    },
                    {
                        "name": "Wen Qiu"
                    },
                    {
                        "name": "Zhiqiang He"
                    },
                    {
                        "name": "Zhi Liu"
                    },
                    {
                        "name": "Xiao Zheng"
                    },
                    {
                        "name": "Bomin Mao"
                    },
                    {
                        "name": "Nei Kato"
                    }
                ],
                "author_detail": {
                    "name": "Nei Kato"
                },
                "author": "Nei Kato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12665v1",
                "updated": "2025-02-18T09:11:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    51,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T09:11:51Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    51,
                    1,
                    49,
                    0
                ],
                "title": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization"
                },
                "summary": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$."
                },
                "authors": [
                    {
                        "name": "Junhui He"
                    },
                    {
                        "name": "Junna Xing"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Shangyu Wu"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Chun Jason Xue"
                    },
                    {
                        "name": "Qingan Li"
                    }
                ],
                "author_detail": {
                    "name": "Qingan Li"
                },
                "author": "Qingan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v2",
                "updated": "2025-02-18T07:58:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    7,
                    58,
                    29,
                    1,
                    49,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Vehicular Networks: An\n  Operator's Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Vehicular Networks: An\n  Operator's Perspective"
                },
                "summary": "Access to sensing data (SD) is crucial for vehicular networks to ensure safe\nand efficient transportation services. Given the vast volume of data involved,\nproactive caching required SD is a pivotal strategy for alleviating network\ncongestion and improving data accessibility. Despite merits, existing studies\npredominantly address SD caching within a single slot. Therefore, these\napproaches lack scalability for scenarios involving multi-slots and are not\nwell-suited for network operators who manage resources within a long-term cost\nbudget. Moreover, the oversight of service capacity at caching nodes may result\nin substantial queuing delays for SD reception. To tackle these limitations, we\njointly consider the problem of anchoring SD caching and allocating from an\noperator's perspective. A value model incorporating both temporal and spacial\ncharacteristics is given to estimate the significance of various caching\ndecisions. Subsequently, a stochastic programming model is proposed to optimize\nthe long-term system performance, which is converted into a series of online\noptimization problem by leveraging the Lyapunov method and linearized via\nintroducing auxiliary variables. To expedite the solution, we provide a binary\nquantum particle swarm optimization based algorithm with quadratic time\ncomplexity. Numerical investigations demonstrate the superiority of proposed\nalgorithms compared with other schemes in terms of energy consumption, response\nlatency, and cache-hit ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Access to sensing data (SD) is crucial for vehicular networks to ensure safe\nand efficient transportation services. Given the vast volume of data involved,\nproactive caching required SD is a pivotal strategy for alleviating network\ncongestion and improving data accessibility. Despite merits, existing studies\npredominantly address SD caching within a single slot. Therefore, these\napproaches lack scalability for scenarios involving multi-slots and are not\nwell-suited for network operators who manage resources within a long-term cost\nbudget. Moreover, the oversight of service capacity at caching nodes may result\nin substantial queuing delays for SD reception. To tackle these limitations, we\njointly consider the problem of anchoring SD caching and allocating from an\noperator's perspective. A value model incorporating both temporal and spacial\ncharacteristics is given to estimate the significance of various caching\ndecisions. Subsequently, a stochastic programming model is proposed to optimize\nthe long-term system performance, which is converted into a series of online\noptimization problem by leveraging the Lyapunov method and linearized via\nintroducing auxiliary variables. To expedite the solution, we provide a binary\nquantum particle swarm optimization based algorithm with quadratic time\ncomplexity. Numerical investigations demonstrate the superiority of proposed\nalgorithms compared with other schemes in terms of energy consumption, response\nlatency, and cache-hit ratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12574v1",
                "updated": "2025-02-18T06:26:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    6,
                    26,
                    5,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T06:26:05Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    6,
                    26,
                    5,
                    1,
                    49,
                    0
                ],
                "title": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading"
                },
                "summary": "Transformer-based large language models (LLMs) demonstrate impressive\nperformance in long context generation. Extending the context length has\ndisproportionately shifted the memory footprint of LLMs during inference to the\nkey-value cache (KV cache). In this paper, we propose HEADINFER, which offloads\nthe KV cache to CPU RAM while avoiding the need to fully store the KV cache for\nany transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise\noffloading strategy, maintaining only selective attention heads KV cache on the\nGPU while computing attention output dynamically. Through roofline analysis, we\ndemonstrate that HEADINFER maintains computational efficiency while\nsignificantly reducing memory footprint. We evaluate HEADINFER on the\nLlama-3-8B model with a 1-million-token sequence, reducing the GPU memory\nfootprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage\nfrom 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline\ninference. Notably, HEADINFER enables 4-million-token inference with an 8B\nmodel on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without\napproximation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) demonstrate impressive\nperformance in long context generation. Extending the context length has\ndisproportionately shifted the memory footprint of LLMs during inference to the\nkey-value cache (KV cache). In this paper, we propose HEADINFER, which offloads\nthe KV cache to CPU RAM while avoiding the need to fully store the KV cache for\nany transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise\noffloading strategy, maintaining only selective attention heads KV cache on the\nGPU while computing attention output dynamically. Through roofline analysis, we\ndemonstrate that HEADINFER maintains computational efficiency while\nsignificantly reducing memory footprint. We evaluate HEADINFER on the\nLlama-3-8B model with a 1-million-token sequence, reducing the GPU memory\nfootprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage\nfrom 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline\ninference. Notably, HEADINFER enables 4-million-token inference with an 8B\nmodel on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without\napproximation methods."
                },
                "authors": [
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Jinqi Xiao"
                    },
                    {
                        "name": "Bo Yuan"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12224v1",
                "updated": "2025-02-17T14:54:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T14:54:14Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "title": "Accurate Expert Predictions in MoE Inference via Cross-Layer Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate Expert Predictions in MoE Inference via Cross-Layer Gate"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Fang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Yuegui Huang"
                    },
                    {
                        "name": "Yufeng Lyu"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v2",
                "updated": "2025-02-17T14:34:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    34,
                    58,
                    0,
                    48,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12216v1",
                "updated": "2025-02-17T08:39:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    39,
                    43,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T08:39:43Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    39,
                    43,
                    0,
                    48,
                    0
                ],
                "title": "Tactic: Adaptive Sparse Attention with Clustering and Distribution\n  Fitting for Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tactic: Adaptive Sparse Attention with Clustering and Distribution\n  Fitting for Long-Context LLMs"
                },
                "summary": "Long-context models are essential for many applications but face\ninefficiencies in loading large KV caches during decoding. Prior methods\nenforce fixed token budgets for sparse attention, assuming a set number of\ntokens can approximate full attention. However, these methods overlook\nvariations in the importance of attention across heads, layers, and contexts.\nTo address these limitations, we propose Tactic, a sparsity-adaptive and\ncalibration-free sparse attention mechanism that dynamically selects tokens\nbased on their cumulative attention scores rather than a fixed token budget. By\nsetting a target fraction of total attention scores, Tactic ensures that token\nselection naturally adapts to variations in attention sparsity. To efficiently\napproximate this selection, Tactic leverages clustering-based sorting and\ndistribution fitting, allowing it to accurately estimate token importance with\nminimal computational overhead. We show that Tactic outperforms existing sparse\nattention algorithms, achieving superior accuracy and up to 7.29x decode\nattention speedup. This improvement translates to an overall 1.58x end-to-end\ninference speedup, making Tactic a practical and effective solution for\nlong-context LLM inference in accuracy-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context models are essential for many applications but face\ninefficiencies in loading large KV caches during decoding. Prior methods\nenforce fixed token budgets for sparse attention, assuming a set number of\ntokens can approximate full attention. However, these methods overlook\nvariations in the importance of attention across heads, layers, and contexts.\nTo address these limitations, we propose Tactic, a sparsity-adaptive and\ncalibration-free sparse attention mechanism that dynamically selects tokens\nbased on their cumulative attention scores rather than a fixed token budget. By\nsetting a target fraction of total attention scores, Tactic ensures that token\nselection naturally adapts to variations in attention sparsity. To efficiently\napproximate this selection, Tactic leverages clustering-based sorting and\ndistribution fitting, allowing it to accurately estimate token importance with\nminimal computational overhead. We show that Tactic outperforms existing sparse\nattention algorithms, achieving superior accuracy and up to 7.29x decode\nattention speedup. This improvement translates to an overall 1.58x end-to-end\ninference speedup, making Tactic a practical and effective solution for\nlong-context LLM inference in accuracy-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Qinyu Xu"
                    },
                    {
                        "name": "Yile Gu"
                    },
                    {
                        "name": "Zhichen Zeng"
                    },
                    {
                        "name": "Rohan Kadekodi"
                    },
                    {
                        "name": "Liangyu Zhao"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Baris Kasikci"
                    }
                ],
                "author_detail": {
                    "name": "Baris Kasikci"
                },
                "author": "Baris Kasikci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15779v1",
                "updated": "2025-02-17T08:12:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    12,
                    34,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T08:12:34Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    12,
                    34,
                    0,
                    48,
                    0
                ],
                "title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer"
                },
                "summary": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code will be made available at blind_review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code will be made available at blind_review."
                },
                "authors": [
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sumin Song"
                    },
                    {
                        "name": "Woosang Lim"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11501v1",
                "updated": "2025-02-17T07:05:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    7,
                    5,
                    36,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T07:05:36Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    7,
                    5,
                    36,
                    0,
                    48,
                    0
                ],
                "title": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?"
                },
                "summary": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods."
                },
                "authors": [
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Weijia Li"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11444v1",
                "updated": "2025-02-17T05:02:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    5,
                    2,
                    25,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T05:02:25Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    5,
                    2,
                    25,
                    0,
                    48,
                    0
                ],
                "title": "Does RAG Really Perform Bad For Long-Context Processing?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does RAG Really Perform Bad For Long-Context Processing?"
                },
                "summary": "The efficient processing of long context poses a serious challenge for large\nlanguage models (LLMs). Recently, retrieval-augmented generation (RAG) has\nemerged as a promising strategy for this problem, as it enables LLMs to make\nselective use of the long context for efficient computation. However, existing\nRAG approaches lag behind other long-context processing methods due to inherent\nlimitations on inaccurate retrieval and fragmented contexts. To address these\nchallenges, we introduce RetroLM, a novel RAG framework for long-context\nprocessing. Unlike traditional methods, RetroLM employs KV-level retrieval\naugmentation, where it partitions the LLM's KV cache into contiguous pages and\nretrieves the most crucial ones for efficient computation. This approach\nenhances robustness to retrieval inaccuracy, facilitates effective utilization\nof fragmented contexts, and saves the cost from repeated computation. Building\non this framework, we further develop a specialized retriever for precise\nretrieval of critical pages and conduct unsupervised post-training to optimize\nthe model's ability to leverage retrieved information. We conduct comprehensive\nevaluations with a variety of benchmarks, including LongBench, InfiniteBench,\nand RULER, where RetroLM significantly outperforms existing long-context LLMs\nand efficient long-context processing methods, particularly in tasks requiring\nintensive reasoning or extremely long-context comprehension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficient processing of long context poses a serious challenge for large\nlanguage models (LLMs). Recently, retrieval-augmented generation (RAG) has\nemerged as a promising strategy for this problem, as it enables LLMs to make\nselective use of the long context for efficient computation. However, existing\nRAG approaches lag behind other long-context processing methods due to inherent\nlimitations on inaccurate retrieval and fragmented contexts. To address these\nchallenges, we introduce RetroLM, a novel RAG framework for long-context\nprocessing. Unlike traditional methods, RetroLM employs KV-level retrieval\naugmentation, where it partitions the LLM's KV cache into contiguous pages and\nretrieves the most crucial ones for efficient computation. This approach\nenhances robustness to retrieval inaccuracy, facilitates effective utilization\nof fragmented contexts, and saves the cost from repeated computation. Building\non this framework, we further develop a specialized retriever for precise\nretrieval of critical pages and conduct unsupervised post-training to optimize\nthe model's ability to leverage retrieved information. We conduct comprehensive\nevaluations with a variety of benchmarks, including LongBench, InfiniteBench,\nand RULER, where RetroLM significantly outperforms existing long-context LLMs\nand efficient long-context processing methods, particularly in tasks requiring\nintensive reasoning or extremely long-context comprehension."
                },
                "authors": [
                    {
                        "name": "Kun Luo"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09383v2",
                "updated": "2025-02-16T18:31:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    18,
                    31,
                    10,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-13T14:59:03Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    59,
                    3,
                    3,
                    44,
                    0
                ],
                "title": "Capitalizing on a Crisis: A Computational Analysis of all Five Million\n  British Firms During the Covid-19 Pandemic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capitalizing on a Crisis: A Computational Analysis of all Five Million\n  British Firms During the Covid-19 Pandemic"
                },
                "summary": "The Covid-19 pandemic brought unprecedented changes to business ownership in\nthe UK which affects a generation of entrepreneurs and their employees.\nNonetheless, the impact remains poorly understood. This is because research on\ncapital accumulation has typically lacked high-quality, individualized,\npopulation-level data. We overcome these barriers to examine who benefits from\neconomic crises through a computationally orientated lens of firm creation.\nLeveraging a comprehensive cache of administrative data on every UK firm and\nall nine million people running them, combined with probabilistic algorithms,\nwe conduct individual-level analyses to understand who became Covid\nentrepreneurs. Using these techniques, we explore characteristics of\nentrepreneurs--such as age, gender, region, business experience, and\nindustry--which potentially predict Covid entrepreneurship. By employing an\nautomated time series model selection procedure to generate counterfactuals, we\nshow that Covid entrepreneurs were typically aged 35-49 (40.4%), men (73.1%),\nand had previously held roles in existing firms (59.4%). For most industries,\ngrowth was disproportionately concentrated around London. It was therefore\nexisting corporate elites who were most able to capitalize on the Covid crisis\nand not, as some hypothesized, young entrepreneurs who were setting up their\nfirst businesses. In this respect, the pandemic will likely impact future\nwealth inequalities. Our work offers methodological guidance for future\npolicymakers during economic crises and highlights the long-term consequences\nfor capital and wealth inequality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Covid-19 pandemic brought unprecedented changes to business ownership in\nthe UK which affects a generation of entrepreneurs and their employees.\nNonetheless, the impact remains poorly understood. This is because research on\ncapital accumulation has typically lacked high-quality, individualized,\npopulation-level data. We overcome these barriers to examine who benefits from\neconomic crises through a computationally orientated lens of firm creation.\nLeveraging a comprehensive cache of administrative data on every UK firm and\nall nine million people running them, combined with probabilistic algorithms,\nwe conduct individual-level analyses to understand who became Covid\nentrepreneurs. Using these techniques, we explore characteristics of\nentrepreneurs--such as age, gender, region, business experience, and\nindustry--which potentially predict Covid entrepreneurship. By employing an\nautomated time series model selection procedure to generate counterfactuals, we\nshow that Covid entrepreneurs were typically aged 35-49 (40.4%), men (73.1%),\nand had previously held roles in existing firms (59.4%). For most industries,\ngrowth was disproportionately concentrated around London. It was therefore\nexisting corporate elites who were most able to capitalize on the Covid crisis\nand not, as some hypothesized, young entrepreneurs who were setting up their\nfirst businesses. In this respect, the pandemic will likely impact future\nwealth inequalities. Our work offers methodological guidance for future\npolicymakers during economic crises and highlights the long-term consequences\nfor capital and wealth inequality."
                },
                "authors": [
                    {
                        "name": "Naomi Muggleton"
                    },
                    {
                        "name": "Charles Rahal"
                    },
                    {
                        "name": "Aaron Reeves"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Reeves"
                },
                "author": "Aaron Reeves",
                "arxiv_doi": "10.1007/s42001-025-00360-4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s42001-025-00360-4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Computational Social Science, 8(2), 1-29 (2025)",
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07627v2",
                "updated": "2025-02-16T16:41:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    16,
                    41,
                    43,
                    6,
                    47,
                    0
                ],
                "published": "2024-11-12T08:17:15Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "title": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion"
                },
                "summary": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively."
                },
                "authors": [
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v2",
                "updated": "2025-02-16T14:50:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    50,
                    0,
                    6,
                    47,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11147v1",
                "updated": "2025-02-16T14:28:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    28,
                    52,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T14:28:52Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    28,
                    52,
                    6,
                    47,
                    0
                ],
                "title": "Efficient Long-Decoding Inference with Reasoning-Aware Attention\n  Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Long-Decoding Inference with Reasoning-Aware Attention\n  Sparsity"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nlong decoding chains (of thoughts), which incur $O(N)$ time and memory\nconsumption, where $N$ is the chain length. To mitigate $O(N)$ time and memory\nconsumption, existing sparsity-based algorithms propose retaining only the most\ncritical token's intermediate data (i.e., key-value cache) and discarding the\nrest. However, these existing algorithms struggle with the ``impossible\ntrinity'' of accuracy, time, and memory. For example, the state-of-the-art\nalgorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory\n($L$ is the cache budget, $L \\ll N$). To address this issue, in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm named RaaS that identifies and retains milestone tokens\nonly until they are no longer needed, achieving high accuracy with $O(L)$ time\nand $O(L)$ memory complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nlong decoding chains (of thoughts), which incur $O(N)$ time and memory\nconsumption, where $N$ is the chain length. To mitigate $O(N)$ time and memory\nconsumption, existing sparsity-based algorithms propose retaining only the most\ncritical token's intermediate data (i.e., key-value cache) and discarding the\nrest. However, these existing algorithms struggle with the ``impossible\ntrinity'' of accuracy, time, and memory. For example, the state-of-the-art\nalgorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory\n($L$ is the cache budget, $L \\ll N$). To address this issue, in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm named RaaS that identifies and retains milestone tokens\nonly until they are no longer needed, achieving high accuracy with $O(L)$ time\nand $O(L)$ memory complexity."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Zhenwen Li"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Zhixia Liu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11101v1",
                "updated": "2025-02-16T12:33:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    12,
                    33,
                    16,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T12:33:16Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    12,
                    33,
                    16,
                    6,
                    47,
                    0
                ],
                "title": "CacheFocus: Dynamic Cache Re-Positioning for Efficient\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFocus: Dynamic Cache Re-Positioning for Efficient\n  Retrieval-Augmented Generation"
                },
                "summary": "Large Language Models (LLMs) excel across a variety of language tasks yet are\nconstrained by limited input lengths and high computational costs. Existing\napproaches\\textemdash such as relative positional encodings (e.g., RoPE, ALiBi)\nand sliding window mechanisms\\textemdash partially alleviate these issues but\noften require additional training or suffer from performance degradation with\nlonger inputs. In this paper, we introduce \\textbf{\\textit{CacheFocus}}, a\nmethod that enhances length normalization and reduces inference latency without\nany further training. Our approach leverages query-independent, offline caching\nto efficiently reuse a Context KV Cache Store. We address the amplification of\nabnormal token distributions problem by re-positioning cached keys and\nintroducing Layer-Adaptive Cache Pruning to discard low-relevance caches during\npre-filling. Additionally, our Adaptive Positional Allocation Strategy\ndynamically reassigns cache positions to maximize the use of the available\npositional encoding range. Experiments on the Natural Questions and TriviaQA\ndatasets demonstrate that CacheFocus outperforms alternative methods even when\ninputs exceed the $4$K limit of the \\texttt{LLaMA-2} model, emphasizing its\npractical effectiveness for long-context LLMs. Moreover, even with large\nmaximum input length of \\texttt{Qwen2}, the performance of CacheFocus shows\nthat it maintains consistent performance even as the number of documents\nincreases, effectively managing long-text generation without degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel across a variety of language tasks yet are\nconstrained by limited input lengths and high computational costs. Existing\napproaches\\textemdash such as relative positional encodings (e.g., RoPE, ALiBi)\nand sliding window mechanisms\\textemdash partially alleviate these issues but\noften require additional training or suffer from performance degradation with\nlonger inputs. In this paper, we introduce \\textbf{\\textit{CacheFocus}}, a\nmethod that enhances length normalization and reduces inference latency without\nany further training. Our approach leverages query-independent, offline caching\nto efficiently reuse a Context KV Cache Store. We address the amplification of\nabnormal token distributions problem by re-positioning cached keys and\nintroducing Layer-Adaptive Cache Pruning to discard low-relevance caches during\npre-filling. Additionally, our Adaptive Positional Allocation Strategy\ndynamically reassigns cache positions to maximize the use of the available\npositional encoding range. Experiments on the Natural Questions and TriviaQA\ndatasets demonstrate that CacheFocus outperforms alternative methods even when\ninputs exceed the $4$K limit of the \\texttt{LLaMA-2} model, emphasizing its\npractical effectiveness for long-context LLMs. Moreover, even with large\nmaximum input length of \\texttt{Qwen2}, the performance of CacheFocus shows\nthat it maintains consistent performance even as the number of documents\nincreases, effectively managing long-text generation without degradation."
                },
                "authors": [
                    {
                        "name": "Kun-Hui Lee"
                    },
                    {
                        "name": "Eunhwan Park"
                    },
                    {
                        "name": "Donghoon Han"
                    },
                    {
                        "name": "Seung-Hoon Na"
                    }
                ],
                "author_detail": {
                    "name": "Seung-Hoon Na"
                },
                "author": "Seung-Hoon Na",
                "arxiv_comment": "11 pages (Work in progress)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11083v1",
                "updated": "2025-02-16T11:37:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    11,
                    37,
                    14,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T11:37:14Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    11,
                    37,
                    14,
                    6,
                    47,
                    0
                ],
                "title": "Streamlining the Collaborative Chain of Models into A Single Forward\n  Pass in Generation-Based Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streamlining the Collaborative Chain of Models into A Single Forward\n  Pass in Generation-Based Tasks"
                },
                "summary": "In Retrieval-Augmented Generation (RAG) and agent-based frameworks, the\n\"Chain of Models\" approach is widely used, where multiple specialized models\nwork sequentially on distinct sub-tasks. This approach is effective but\nincreases resource demands as each model must be deployed separately. Recent\nadvancements attempt to address this by applying prompt tuning, which allows a\nshared base model to adapt to multiple tasks with minimal parameter changes.\nHowever, a key challenge remains: intermediate outputs, passed between models\nas plain text, require recomputation of hidden states (i.e., Key and Value (KV)\nstates in Transformers) during inference. In this paper, we introduce FTHSS, a\nnovel prompt-tuning method that enables models to share KV hidden states,\neliminating redundant forward passes and reducing KV cache storage. By\nmodifying input and attention masks during training, FTHSS allows models to\neffectively utilize KV hidden states from prior models in both single- and\nmulti-round scenarios. Empirical results on four tasks show that FTHSS matches\nthe performance of traditional model chains while improving inference\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Retrieval-Augmented Generation (RAG) and agent-based frameworks, the\n\"Chain of Models\" approach is widely used, where multiple specialized models\nwork sequentially on distinct sub-tasks. This approach is effective but\nincreases resource demands as each model must be deployed separately. Recent\nadvancements attempt to address this by applying prompt tuning, which allows a\nshared base model to adapt to multiple tasks with minimal parameter changes.\nHowever, a key challenge remains: intermediate outputs, passed between models\nas plain text, require recomputation of hidden states (i.e., Key and Value (KV)\nstates in Transformers) during inference. In this paper, we introduce FTHSS, a\nnovel prompt-tuning method that enables models to share KV hidden states,\neliminating redundant forward passes and reducing KV cache storage. By\nmodifying input and attention masks during training, FTHSS allows models to\neffectively utilize KV hidden states from prior models in both single- and\nmulti-round scenarios. Empirical results on four tasks show that FTHSS matches\nthe performance of traditional model chains while improving inference\nefficiency."
                },
                "authors": [
                    {
                        "name": "Yuanjie Lyu"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Yuhao Chen"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Tong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Tong Xu"
                },
                "author": "Tong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11046v1",
                "updated": "2025-02-16T09:08:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T09:08:36Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "title": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing"
                },
                "summary": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies."
                },
                "authors": [
                    {
                        "name": "Zhao Wang"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Dimin Niu"
                    },
                    {
                        "name": "Tianchan Guan"
                    },
                    {
                        "name": "Zhaoyang Du"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05231v2",
                "updated": "2025-02-15T23:54:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    23,
                    54,
                    38,
                    5,
                    46,
                    0
                ],
                "published": "2024-05-08T17:27:11Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    17,
                    27,
                    11,
                    2,
                    129,
                    0
                ],
                "title": "DiskGNN: Bridging I/O Efficiency and Model Accuracy for Out-of-Core GNN\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiskGNN: Bridging I/O Efficiency and Model Accuracy for Out-of-Core GNN\n  Training"
                },
                "summary": "Graph neural networks (GNNs) are machine learning models specialized for\ngraph data and widely used in many applications. To train GNNs on large graphs\nthat exceed CPU memory, several systems store data on disk and conduct\nout-of-core processing. However, these systems suffer from either read\namplification when reading node features that are usually smaller than a disk\npage or degraded model accuracy by treating the graph as disconnected\npartitions. To close this gap, we build a system called DiskGNN, which achieves\nhigh I/O efficiency and thus fast training without hurting model accuracy. The\nkey technique used by DiskGNN is offline sampling, which helps decouple graph\nsampling from model computation. In particular, by conducting graph sampling\nbeforehand, DiskGNN acquires the node features that will be accessed by model\ncomputation, and such information is utilized to pack the target node features\ncontiguously on disk to avoid read amplification. Besides, \\name{} also adopts\ndesigns including four-level feature store to fully utilize the memory\nhierarchy to cache node features and reduce disk access, batched packing to\naccelerate the feature packing process, and pipelined training to overlap disk\naccess with other operations. We compare DiskGNN with Ginex and MariusGNN,\nwhich are state-of-the-art systems for out-of-core GNN training. The results\nshow that DiskGNN can speed up the baselines by over 8x while matching their\nbest model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) are machine learning models specialized for\ngraph data and widely used in many applications. To train GNNs on large graphs\nthat exceed CPU memory, several systems store data on disk and conduct\nout-of-core processing. However, these systems suffer from either read\namplification when reading node features that are usually smaller than a disk\npage or degraded model accuracy by treating the graph as disconnected\npartitions. To close this gap, we build a system called DiskGNN, which achieves\nhigh I/O efficiency and thus fast training without hurting model accuracy. The\nkey technique used by DiskGNN is offline sampling, which helps decouple graph\nsampling from model computation. In particular, by conducting graph sampling\nbeforehand, DiskGNN acquires the node features that will be accessed by model\ncomputation, and such information is utilized to pack the target node features\ncontiguously on disk to avoid read amplification. Besides, \\name{} also adopts\ndesigns including four-level feature store to fully utilize the memory\nhierarchy to cache node features and reduce disk access, batched packing to\naccelerate the feature packing process, and pipelined training to overlap disk\naccess with other operations. We compare DiskGNN with Ginex and MariusGNN,\nwhich are state-of-the-art systems for out-of-core GNN training. The results\nshow that DiskGNN can speed up the baselines by over 8x while matching their\nbest model accuracy."
                },
                "authors": [
                    {
                        "name": "Renjie Liu"
                    },
                    {
                        "name": "Yichuan Wang"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Haitian Jiang"
                    },
                    {
                        "name": "Zhenkun Cai"
                    },
                    {
                        "name": "Minjie Wang"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Jinyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Li"
                },
                "author": "Jinyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01939v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01939v2",
                "updated": "2025-02-15T18:09:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    18,
                    9,
                    50,
                    5,
                    46,
                    0
                ],
                "published": "2024-06-04T03:48:08Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    3,
                    48,
                    8,
                    1,
                    156,
                    0
                ],
                "title": "Speeding up Policy Simulation in Supply Chain RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speeding up Policy Simulation in Supply Chain RL"
                },
                "summary": "Simulating a single trajectory of a dynamical system under some\nstate-dependent policy is a core bottleneck in policy optimization (PO)\nalgorithms. The many inherently serial policy evaluations that must be\nperformed in a single simulation constitute the bulk of this bottleneck. In\napplying PO to supply chain optimization (SCO) problems, simulating a single\nsample path corresponding to one month of a supply chain can take several\nhours. We present an iterative algorithm to accelerate policy simulation,\ndubbed Picard Iteration. This scheme carefully assigns policy evaluation tasks\nto independent processes. Within an iteration, any given process evaluates the\npolicy only on its assigned tasks while assuming a certain \"cached\" evaluation\nfor other tasks; the cache is updated at the end of the iteration. Implemented\non GPUs, this scheme admits batched evaluation of the policy across a single\ntrajectory. We prove that the structure afforded by many SCO problems allows\nconvergence in a small number of iterations independent of the horizon. We\ndemonstrate practical speedups of 400x on large-scale SCO problems even with a\nsingle GPU, and also demonstrate practical efficacy in other RL environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating a single trajectory of a dynamical system under some\nstate-dependent policy is a core bottleneck in policy optimization (PO)\nalgorithms. The many inherently serial policy evaluations that must be\nperformed in a single simulation constitute the bulk of this bottleneck. In\napplying PO to supply chain optimization (SCO) problems, simulating a single\nsample path corresponding to one month of a supply chain can take several\nhours. We present an iterative algorithm to accelerate policy simulation,\ndubbed Picard Iteration. This scheme carefully assigns policy evaluation tasks\nto independent processes. Within an iteration, any given process evaluates the\npolicy only on its assigned tasks while assuming a certain \"cached\" evaluation\nfor other tasks; the cache is updated at the end of the iteration. Implemented\non GPUs, this scheme admits batched evaluation of the policy across a single\ntrajectory. We prove that the structure afforded by many SCO problems allows\nconvergence in a small number of iterations independent of the horizon. We\ndemonstrate practical speedups of 400x on large-scale SCO problems even with a\nsingle GPU, and also demonstrate practical efficacy in other RL environments."
                },
                "authors": [
                    {
                        "name": "Vivek Farias"
                    },
                    {
                        "name": "Joren Gijsbrechts"
                    },
                    {
                        "name": "Aryan Khojandi"
                    },
                    {
                        "name": "Tianyi Peng"
                    },
                    {
                        "name": "Andrew Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Zheng"
                },
                "author": "Andrew Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01939v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01939v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14882v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14882v1",
                "updated": "2025-02-15T05:08:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    5,
                    8,
                    1,
                    5,
                    46,
                    0
                ],
                "published": "2025-02-15T05:08:01Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    5,
                    8,
                    1,
                    5,
                    46,
                    0
                ],
                "title": "From 16-Bit to 1-Bit: Visual KV Cache Quantization for Memory-Efficient\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From 16-Bit to 1-Bit: Visual KV Cache Quantization for Memory-Efficient\n  Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success\nacross various applications, yet their computational overhead during deployment\nremains a critical challenge. While Key-Value (KV) caching improves inference\nefficiency by trading memory for computation, the growing memory footprint from\nstoring extensive KV caches reduces throughput and limits long-term execution\non devices with constrained GPU memory. Existing approaches primarily focus on\ndropping unimportant tokens to reduce the KV cache size, mitigating memory\nconstraints at the cost of potential information loss. In contrast, we propose\na simple yet effective visual quantization strategy that preserves all visual\ntokens while significantly reducing memory consumption. To achieve an extreme\nquantization ratio, i.e., 1-bit quantization, we propose group-specific\nquantization and quantile-based quantization approaches, motivated by the\ninherent patterns of the KV cache. Our method is plug-and-play, enabling\nseamless integration into various MLLMs to improve memory efficiency without\narchitectural modifications. Extensive experiments demonstrate that our\napproach effectively reduces memory overhead while maintaining computational\nefficiency and preserving multimodal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success\nacross various applications, yet their computational overhead during deployment\nremains a critical challenge. While Key-Value (KV) caching improves inference\nefficiency by trading memory for computation, the growing memory footprint from\nstoring extensive KV caches reduces throughput and limits long-term execution\non devices with constrained GPU memory. Existing approaches primarily focus on\ndropping unimportant tokens to reduce the KV cache size, mitigating memory\nconstraints at the cost of potential information loss. In contrast, we propose\na simple yet effective visual quantization strategy that preserves all visual\ntokens while significantly reducing memory consumption. To achieve an extreme\nquantization ratio, i.e., 1-bit quantization, we propose group-specific\nquantization and quantile-based quantization approaches, motivated by the\ninherent patterns of the KV cache. Our method is plug-and-play, enabling\nseamless integration into various MLLMs to improve memory efficiency without\narchitectural modifications. Extensive experiments demonstrate that our\napproach effectively reduces memory overhead while maintaining computational\nefficiency and preserving multimodal performance."
                },
                "authors": [
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Susan Liang"
                    },
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Jiani Liu"
                    },
                    {
                        "name": "Haiting Lin"
                    },
                    {
                        "name": "Mingjie Zhao"
                    },
                    {
                        "name": "Chenliang Xu"
                    },
                    {
                        "name": "Kun Wan"
                    },
                    {
                        "name": "Wentian Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wentian Zhao"
                },
                "author": "Wentian Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14882v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14882v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10659v1",
                "updated": "2025-02-15T03:56:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    3,
                    56,
                    22,
                    5,
                    46,
                    0
                ],
                "published": "2025-02-15T03:56:22Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    3,
                    56,
                    22,
                    5,
                    46,
                    0
                ],
                "title": "Pushing up to the Limit of Memory Bandwidth and Capacity Utilization for\n  Efficient LLM Decoding on Embedded FPGA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing up to the Limit of Memory Bandwidth and Capacity Utilization for\n  Efficient LLM Decoding on Embedded FPGA"
                },
                "summary": "The extremely high computational and storage demands of large language models\nhave excluded most edge devices, which were widely used for efficient machine\nlearning, from being viable options. A typical edge device usually only has 4GB\nof memory capacity and a bandwidth of less than 20GB/s, while a large language\nmodel quantized to 4-bit precision with 7B parameters already requires 3.5GB of\ncapacity, and its decoding process is purely bandwidth-bound. In this paper, we\naim to explore these limits by proposing a hardware accelerator for large\nlanguage model (LLM) inference on the Zynq-based KV260 platform, equipped with\n4GB of 64-bit 2400Mbps DDR4 memory. We successfully deploy a LLaMA2-7B model,\nachieving a decoding speed of around 5 token/s, utilizing 93.3% of the memory\ncapacity and reaching 85% decoding speed of the theoretical memory bandwidth\nlimit. To fully reserve the memory capacity for model weights and key-value\ncache, we develop the system in a bare-metal environment without an operating\nsystem. To fully reserve the bandwidth for model weight transfers, we implement\na customized dataflow with an operator fusion pipeline and propose a data\narrangement format that can maximize the data transaction efficiency. This\nresearch marks the first attempt to deploy a 7B level LLM on a standalone\nembedded field programmable gate array (FPGA) device. It provides key insights\ninto efficient LLM inference on embedded FPGA devices and provides guidelines\nfor future architecture design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extremely high computational and storage demands of large language models\nhave excluded most edge devices, which were widely used for efficient machine\nlearning, from being viable options. A typical edge device usually only has 4GB\nof memory capacity and a bandwidth of less than 20GB/s, while a large language\nmodel quantized to 4-bit precision with 7B parameters already requires 3.5GB of\ncapacity, and its decoding process is purely bandwidth-bound. In this paper, we\naim to explore these limits by proposing a hardware accelerator for large\nlanguage model (LLM) inference on the Zynq-based KV260 platform, equipped with\n4GB of 64-bit 2400Mbps DDR4 memory. We successfully deploy a LLaMA2-7B model,\nachieving a decoding speed of around 5 token/s, utilizing 93.3% of the memory\ncapacity and reaching 85% decoding speed of the theoretical memory bandwidth\nlimit. To fully reserve the memory capacity for model weights and key-value\ncache, we develop the system in a bare-metal environment without an operating\nsystem. To fully reserve the bandwidth for model weight transfers, we implement\na customized dataflow with an operator fusion pipeline and propose a data\narrangement format that can maximize the data transaction efficiency. This\nresearch marks the first attempt to deploy a 7B level LLM on a standalone\nembedded field programmable gate array (FPGA) device. It provides key insights\ninto efficient LLM inference on embedded FPGA devices and provides guidelines\nfor future architecture design."
                },
                "authors": [
                    {
                        "name": "Jindong Li"
                    },
                    {
                        "name": "Tenglong Li"
                    },
                    {
                        "name": "Guobin Shen"
                    },
                    {
                        "name": "Dongcheng Zhao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Yi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zeng"
                },
                "author": "Yi Zeng",
                "arxiv_comment": "Accepted by DATE2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10389v1",
                "updated": "2025-02-14T18:59:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    36,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T18:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    36,
                    4,
                    45,
                    0
                ],
                "title": "Region-Adaptive Sampling for Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Region-Adaptive Sampling for Diffusion Transformers"
                },
                "summary": "Diffusion models (DMs) have become the leading choice for generative tasks\nacross diverse domains. However, their reliance on multiple sequential forward\npasses significantly limits real-time performance. Previous acceleration\nmethods have primarily focused on reducing the number of sampling steps or\nreusing intermediate results, failing to leverage variations across spatial\nregions within the image due to the constraints of convolutional U-Net\nstructures. By harnessing the flexibility of Diffusion Transformers (DiTs) in\nhandling variable number of tokens, we introduce RAS, a novel, training-free\nsampling strategy that dynamically assigns different sampling ratios to regions\nwithin an image based on the focus of the DiT model. Our key observation is\nthat during each sampling step, the model concentrates on semantically\nmeaningful regions, and these areas of focus exhibit strong continuity across\nconsecutive steps. Leveraging this insight, RAS updates only the regions\ncurrently in focus, while other regions are updated using cached noise from the\nprevious step. The model's focus is determined based on the output from the\npreceding step, capitalizing on the temporal consistency we observed. We\nevaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up\nto 2.36x and 2.51x, respectively, with minimal degradation in generation\nquality. Additionally, a user study reveals that RAS delivers comparable\nqualities under human evaluation while achieving a 1.6x speedup. Our approach\nmakes a significant step towards more efficient diffusion transformers,\nenhancing their potential for real-time applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) have become the leading choice for generative tasks\nacross diverse domains. However, their reliance on multiple sequential forward\npasses significantly limits real-time performance. Previous acceleration\nmethods have primarily focused on reducing the number of sampling steps or\nreusing intermediate results, failing to leverage variations across spatial\nregions within the image due to the constraints of convolutional U-Net\nstructures. By harnessing the flexibility of Diffusion Transformers (DiTs) in\nhandling variable number of tokens, we introduce RAS, a novel, training-free\nsampling strategy that dynamically assigns different sampling ratios to regions\nwithin an image based on the focus of the DiT model. Our key observation is\nthat during each sampling step, the model concentrates on semantically\nmeaningful regions, and these areas of focus exhibit strong continuity across\nconsecutive steps. Leveraging this insight, RAS updates only the regions\ncurrently in focus, while other regions are updated using cached noise from the\nprevious step. The model's focus is determined based on the output from the\npreceding step, capitalizing on the temporal consistency we observed. We\nevaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up\nto 2.36x and 2.51x, respectively, with minimal degradation in generation\nquality. Additionally, a user study reveals that RAS delivers comparable\nqualities under human evaluation while achieving a 1.6x speedup. Our approach\nmakes a significant step towards more efficient diffusion transformers,\nenhancing their potential for real-time applications."
                },
                "authors": [
                    {
                        "name": "Ziming Liu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yiqi Zhang"
                    },
                    {
                        "name": "Lili Qiu"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Yuqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yuqing Yang"
                },
                "author": "Yuqing Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09057v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09057v2",
                "updated": "2025-02-14T17:17:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    17,
                    20,
                    4,
                    45,
                    0
                ],
                "published": "2024-12-12T08:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "title": "PhishIntel: Toward Practical Deployment of Reference-Based Phishing\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhishIntel: Toward Practical Deployment of Reference-Based Phishing\n  Detection"
                },
                "summary": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) have achieved notable advancements\nin detection accuracy, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) have achieved notable advancements\nin detection accuracy, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility."
                },
                "authors": [
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Hiok Kuek Tan"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Mei Lin Lock"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_doi": "10.1145/3701716.3715192",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715192",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.09057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09057v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by WWW 2025 (Demo Track)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10220v1",
                "updated": "2025-02-14T15:14:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    14,
                    53,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T15:14:53Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    14,
                    53,
                    4,
                    45,
                    0
                ],
                "title": "Optimal and Coordinated Voltage Control: Case Study on a 132 kV\n  Norwegian Grid Subsystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal and Coordinated Voltage Control: Case Study on a 132 kV\n  Norwegian Grid Subsystem"
                },
                "summary": "This work presents a framework for dynamic performance assessment of the\nhigher layers in the hierarchical voltage regulation scheme, with case studies\napplied to specific areas of the Norwegian grid. Unlike the primary (PVR)\nlevel, the secondary (SVR) and tertiary (TVR) levels are not tuned to a single\ndevice at a time, handling instead several reactive power resources available\nwithin a control zone including generator units, static VAr compensators and\nothers. Proper SVR-TVR coordination for realistic transmission systems is a\nchallenging topic at the core of many ongoing discussions in voltage control\nliterature. Special focus is placed on practical considerations from the system\noperator perspective, since this research is also aimed at simplifying daily\ncontrol centre routines. Dynamic simulation results concern a 21-bus equivalent\nof a 132 kV network model that accurately represents a Norwegian grid\nsubsystem. Case studies address daily grid operation with real-life load demand\nand wind power generation profiles, showing that the proposed strategy is\neffective not only to minimize total active power losses as much as possible\nwithin system-wide limitations, but also to maintain adequate voltage profiles\nand reactive power flows. Findings pertaining to this work showcase the\nbenefits of applying hierarchical voltage regulation layers as an asset to\nday-to-day control center management of a realistic transmission network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a framework for dynamic performance assessment of the\nhigher layers in the hierarchical voltage regulation scheme, with case studies\napplied to specific areas of the Norwegian grid. Unlike the primary (PVR)\nlevel, the secondary (SVR) and tertiary (TVR) levels are not tuned to a single\ndevice at a time, handling instead several reactive power resources available\nwithin a control zone including generator units, static VAr compensators and\nothers. Proper SVR-TVR coordination for realistic transmission systems is a\nchallenging topic at the core of many ongoing discussions in voltage control\nliterature. Special focus is placed on practical considerations from the system\noperator perspective, since this research is also aimed at simplifying daily\ncontrol centre routines. Dynamic simulation results concern a 21-bus equivalent\nof a 132 kV network model that accurately represents a Norwegian grid\nsubsystem. Case studies address daily grid operation with real-life load demand\nand wind power generation profiles, showing that the proposed strategy is\neffective not only to minimize total active power losses as much as possible\nwithin system-wide limitations, but also to maintain adequate voltage profiles\nand reactive power flows. Findings pertaining to this work showcase the\nbenefits of applying hierarchical voltage regulation layers as an asset to\nday-to-day control center management of a realistic transmission network."
                },
                "authors": [
                    {
                        "name": "Hugo Rodrigues de Brito"
                    },
                    {
                        "name": "Daniel Simon Baltensperger"
                    },
                    {
                        "name": "Kjetil Obstfelder Uhlen"
                    }
                ],
                "author_detail": {
                    "name": "Kjetil Obstfelder Uhlen"
                },
                "author": "Kjetil Obstfelder Uhlen",
                "arxiv_comment": "11 pages, 8 figures, CIGRE Symposium 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10167v1",
                "updated": "2025-02-14T13:55:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T13:55:01Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "title": "Modeling and Simulating Emerging Memory Technologies: A Tutorial",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Simulating Emerging Memory Technologies: A Tutorial"
                },
                "summary": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Tristan Seidl"
                    },
                    {
                        "name": "Nils Hölscher"
                    },
                    {
                        "name": "Christian Hakert"
                    },
                    {
                        "name": "Minh Duy Truong"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "João Paulo C. de Lima"
                    },
                    {
                        "name": "Asif Ali Khan"
                    },
                    {
                        "name": "Jeronimo Castrillon"
                    },
                    {
                        "name": "Ali Nezhadi"
                    },
                    {
                        "name": "Lokesh Siddhu"
                    },
                    {
                        "name": "Hassan Nassar"
                    },
                    {
                        "name": "Mahta Mayahinia"
                    },
                    {
                        "name": "Mehdi Baradaran Tahoori"
                    },
                    {
                        "name": "Jörg Henkel"
                    },
                    {
                        "name": "Nils Wilbert"
                    },
                    {
                        "name": "Stefan Wildermann"
                    },
                    {
                        "name": "Jürgen Teich"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Teich"
                },
                "author": "Jürgen Teich",
                "arxiv_comment": "DFG Priority Program 2377 - Disruptive Memory Technologies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09921v1",
                "updated": "2025-02-14T05:19:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    5,
                    19,
                    46,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T05:19:46Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    5,
                    19,
                    46,
                    4,
                    45,
                    0
                ],
                "title": "INF^2: High-Throughput Generative Inference of Large Language Models\n  using Near-Storage Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INF^2: High-Throughput Generative Inference of Large Language Models\n  using Near-Storage Processing"
                },
                "summary": "The growing memory and computational demands of large language models (LLMs)\nfor generative inference present significant challenges for practical\ndeployment. One promising solution to address these challenges is\noffloading-based batched inference, which leverages host memory and disk as an\nextended memory hierarchy for GPUs. While the approach cost-effectively enables\nLLM inference, its performance is limited by substantial I/O overhead,\nprimarily due to the large key-value (KV) cache sizes, which increase with\nbatch size and LLM context window length.\n  In this paper, we introduce INFerence-INFinity (INF^2), a framework that\nboosts generative inference throughput using computational storage devices\n(CSDs). The core of INF^2 is attention-near storage, which offloads\nmemory-intensive self-attention operations to near-storage accelerators,\nsignificantly reducing traffic through the system interconnect. We also propose\ndelayed KV cache writeback to hide storage write latency by delaying newly\ngenerated KV cache writes until the cache reaches sufficient size in system\nmemory. Additionally, we introduce cooperative X-cache, a technique designed to\nfurther trade off the remaining memory capacity for storage bandwidth. Our\nmethods effectively minimize idle time for computation, improving the overall\nthroughput.\n  To demonstrate the effectiveness of our approach, \\thiswork has been\nimplemented on PyTorch and evaluated on a real system. Our experiments show\nthat INF^2 achieves up to 3.46$\\times$ throughput improvement compared to\nstate-of-the-art baselines. We will open-source INF^2 to facilitate broader\nadoption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing memory and computational demands of large language models (LLMs)\nfor generative inference present significant challenges for practical\ndeployment. One promising solution to address these challenges is\noffloading-based batched inference, which leverages host memory and disk as an\nextended memory hierarchy for GPUs. While the approach cost-effectively enables\nLLM inference, its performance is limited by substantial I/O overhead,\nprimarily due to the large key-value (KV) cache sizes, which increase with\nbatch size and LLM context window length.\n  In this paper, we introduce INFerence-INFinity (INF^2), a framework that\nboosts generative inference throughput using computational storage devices\n(CSDs). The core of INF^2 is attention-near storage, which offloads\nmemory-intensive self-attention operations to near-storage accelerators,\nsignificantly reducing traffic through the system interconnect. We also propose\ndelayed KV cache writeback to hide storage write latency by delaying newly\ngenerated KV cache writes until the cache reaches sufficient size in system\nmemory. Additionally, we introduce cooperative X-cache, a technique designed to\nfurther trade off the remaining memory capacity for storage bandwidth. Our\nmethods effectively minimize idle time for computation, improving the overall\nthroughput.\n  To demonstrate the effectiveness of our approach, \\thiswork has been\nimplemented on PyTorch and evaluated on a real system. Our experiments show\nthat INF^2 achieves up to 3.46$\\times$ throughput improvement compared to\nstate-of-the-art baselines. We will open-source INF^2 to facilitate broader\nadoption."
                },
                "authors": [
                    {
                        "name": "Hongsun Jang"
                    },
                    {
                        "name": "Siung Noh"
                    },
                    {
                        "name": "Changmin Shin"
                    },
                    {
                        "name": "Jaewon Jung"
                    },
                    {
                        "name": "Jaeyong Song"
                    },
                    {
                        "name": "Jinho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinho Lee"
                },
                "author": "Jinho Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09888v1",
                "updated": "2025-02-14T03:25:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T03:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "title": "An Efficient Large Recommendation Model: Towards a Resource-Optimal\n  Scaling Law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Large Recommendation Model: Towards a Resource-Optimal\n  Scaling Law"
                },
                "summary": "The pursuit of scaling up recommendation models confronts intrinsic tensions\nbetween expanding model capacity and preserving computational tractability.\nWhile prior studies have explored scaling laws for recommendation systems,\ntheir resource-intensive paradigms -- often requiring tens of thousands of A100\nGPU hours -- remain impractical for most industrial applications. This work\naddresses a critical gap: achieving sustainable model scaling under strict\ncomputational budgets. We propose Climber, a resource-efficient recommendation\nframework comprising two synergistic components: the ASTRO model architecture\nfor algorithmic innovation and the TURBO acceleration framework for engineering\noptimization. ASTRO (Adaptive Scalable Transformer for RecOmmendation) adopts\ntwo core innovations: (1) multi-scale sequence partitioning that reduces\nattention complexity from O(n^2d) to O(n^2d/Nb) via hierarchical blocks,\nenabling more efficient scaling with sequence length; (2) dynamic temperature\nmodulation that adaptively adjusts attention scores for multimodal\ndistributions arising from inherent multi-scenario and multi-behavior\ninteractions. Complemented by TURBO (Two-stage Unified Ranking with Batched\nOutput), a co-designed acceleration framework integrating gradient-aware\nfeature compression and memory-efficient Key-Value caching, Climber achieves\n5.15x throughput gains without performance degradation. Comprehensive offline\nexperiments on multiple datasets validate that Climber exhibits a more ideal\nscaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pursuit of scaling up recommendation models confronts intrinsic tensions\nbetween expanding model capacity and preserving computational tractability.\nWhile prior studies have explored scaling laws for recommendation systems,\ntheir resource-intensive paradigms -- often requiring tens of thousands of A100\nGPU hours -- remain impractical for most industrial applications. This work\naddresses a critical gap: achieving sustainable model scaling under strict\ncomputational budgets. We propose Climber, a resource-efficient recommendation\nframework comprising two synergistic components: the ASTRO model architecture\nfor algorithmic innovation and the TURBO acceleration framework for engineering\noptimization. ASTRO (Adaptive Scalable Transformer for RecOmmendation) adopts\ntwo core innovations: (1) multi-scale sequence partitioning that reduces\nattention complexity from O(n^2d) to O(n^2d/Nb) via hierarchical blocks,\nenabling more efficient scaling with sequence length; (2) dynamic temperature\nmodulation that adaptively adjusts attention scores for multimodal\ndistributions arising from inherent multi-scenario and multi-behavior\ninteractions. Complemented by TURBO (Two-stage Unified Ranking with Batched\nOutput), a co-designed acceleration framework integrating gradient-aware\nfeature compression and memory-efficient Key-Value caching, Climber achieves\n5.15x throughput gains without performance degradation. Comprehensive offline\nexperiments on multiple datasets validate that Climber exhibits a more ideal\nscaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily."
                },
                "authors": [
                    {
                        "name": "Songpei Xu"
                    },
                    {
                        "name": "Shijia Wang"
                    },
                    {
                        "name": "Da Guo"
                    },
                    {
                        "name": "Xianwen Guo"
                    },
                    {
                        "name": "Qiang Xiao"
                    },
                    {
                        "name": "Fangjian Li"
                    },
                    {
                        "name": "Chuanjiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Chuanjiang Luo"
                },
                "author": "Chuanjiang Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09726v1",
                "updated": "2025-02-13T19:16:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    16,
                    39,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T19:16:39Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    16,
                    39,
                    3,
                    44,
                    0
                ],
                "title": "Analysis of Robust and Secure DNS Protocols for IoT Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of Robust and Secure DNS Protocols for IoT Devices"
                },
                "summary": "The DNS (Domain Name System) protocol has been in use since the early days of\nthe Internet. Although DNS as a de facto networking protocol had no security\nconsiderations in its early years, there have been many security enhancements,\nsuch as DNSSec (Domain Name System Security Extensions), DoT (DNS over\nTransport Layer Security), DoH (DNS over HTTPS) and DoQ (DNS over QUIC). With\nall these security improvements, it is not yet clear what resource-constrained\nInternet-of-Things (IoT) devices should be used for robustness. In this paper,\nwe investigate different DNS security approaches using an edge DNS resolver\nimplemented as a Virtual Network Function (VNF) to replicate the impact of the\nprotocol from an IoT perspective and compare their performances under different\nconditions. We present our results for cache-based and non-cached responses and\nevaluate the corresponding security benefits. Our results and framework can\ngreatly help consumers, manufacturers, and the research community decide and\nimplement their DNS protocols depending on the given dynamic network conditions\nand enable robust Internet access via DNS for different devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The DNS (Domain Name System) protocol has been in use since the early days of\nthe Internet. Although DNS as a de facto networking protocol had no security\nconsiderations in its early years, there have been many security enhancements,\nsuch as DNSSec (Domain Name System Security Extensions), DoT (DNS over\nTransport Layer Security), DoH (DNS over HTTPS) and DoQ (DNS over QUIC). With\nall these security improvements, it is not yet clear what resource-constrained\nInternet-of-Things (IoT) devices should be used for robustness. In this paper,\nwe investigate different DNS security approaches using an edge DNS resolver\nimplemented as a Virtual Network Function (VNF) to replicate the impact of the\nprotocol from an IoT perspective and compare their performances under different\nconditions. We present our results for cache-based and non-cached responses and\nevaluate the corresponding security benefits. Our results and framework can\ngreatly help consumers, manufacturers, and the research community decide and\nimplement their DNS protocols depending on the given dynamic network conditions\nand enable robust Internet access via DNS for different devices."
                },
                "authors": [
                    {
                        "name": "Abdullah Aydeger"
                    },
                    {
                        "name": "Sanzida Hoque"
                    },
                    {
                        "name": "Engin Zeydan"
                    },
                    {
                        "name": "Kapal Dev"
                    }
                ],
                "author_detail": {
                    "name": "Kapal Dev"
                },
                "author": "Kapal Dev",
                "arxiv_comment": "6 pages, 2 tables, 2 figures. This paper has been accepted in the\n  2025 IEEE International Conference on Communications (ICC): SAC Cloud\n  Computing, Networking, and Storage Track. The final version will be published\n  in the IEEE Xplore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09720v1",
                "updated": "2025-02-13T19:11:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T19:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs"
                },
                "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent work have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Meta's SpinQuant (perplexity 7.3). Comparisons on various\nLLM evaluation benchmarks also show a reduction in performance degradation\ninduced by quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent work have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Meta's SpinQuant (perplexity 7.3). Comparisons on various\nLLM evaluation benchmarks also show a reduction in performance degradation\ninduced by quantization."
                },
                "authors": [
                    {
                        "name": "Semyon Savkin"
                    },
                    {
                        "name": "Eitan Porat"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Yury Polyanskiy"
                    }
                ],
                "author_detail": {
                    "name": "Yury Polyanskiy"
                },
                "author": "Yury Polyanskiy",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v2",
                "updated": "2025-02-13T18:07:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    7,
                    4,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Multi-Head Latent Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Multi-Head Latent Attention Is All You Need"
                },
                "summary": "Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09541v1",
                "updated": "2025-02-13T17:57:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    57,
                    5,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T17:57:05Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    57,
                    5,
                    3,
                    44,
                    0
                ],
                "title": "Vortex: Overcoming Memory Capacity Limitations in GPU-Accelerated\n  Large-Scale Data Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vortex: Overcoming Memory Capacity Limitations in GPU-Accelerated\n  Large-Scale Data Analytics"
                },
                "summary": "Despite the high computational throughput of GPUs, limited memory capacity\nand bandwidth-limited CPU-GPU communication via PCIe links remain significant\nbottlenecks for accelerating large-scale data analytics workloads. This paper\nintroduces Vortex, a GPU-accelerated framework designed for data analytics\nworkloads that exceed GPU memory capacity. A key aspect of our framework is an\noptimized IO primitive that leverages all available PCIe links in multi-GPU\nsystems for the IO demand of a single target GPU. It routes data through other\nGPUs to such target GPU that handles IO-intensive analytics tasks. This\napproach is advantageous when other GPUs are occupied with compute-bound\nworkloads, such as popular AI applications that typically underutilize IO\nresources. We also introduce a novel programming model that separates GPU\nkernel development from IO scheduling, reducing programmer burden and enabling\nGPU code reuse. Additionally, we present the design of certain important query\noperators and discuss a late materialization technique based on GPU's zero-copy\nmemory access. Without caching any data in GPU memory, Vortex improves the\nperformance of the state-of-the-art GPU baseline, Proteus, by 5.7$\\times$ on\naverage and enhances price performance by 2.5$\\times$ compared to a CPU-based\nDuckDB baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the high computational throughput of GPUs, limited memory capacity\nand bandwidth-limited CPU-GPU communication via PCIe links remain significant\nbottlenecks for accelerating large-scale data analytics workloads. This paper\nintroduces Vortex, a GPU-accelerated framework designed for data analytics\nworkloads that exceed GPU memory capacity. A key aspect of our framework is an\noptimized IO primitive that leverages all available PCIe links in multi-GPU\nsystems for the IO demand of a single target GPU. It routes data through other\nGPUs to such target GPU that handles IO-intensive analytics tasks. This\napproach is advantageous when other GPUs are occupied with compute-bound\nworkloads, such as popular AI applications that typically underutilize IO\nresources. We also introduce a novel programming model that separates GPU\nkernel development from IO scheduling, reducing programmer burden and enabling\nGPU code reuse. Additionally, we present the design of certain important query\noperators and discuss a late materialization technique based on GPU's zero-copy\nmemory access. Without caching any data in GPU memory, Vortex improves the\nperformance of the state-of-the-art GPU baseline, Proteus, by 5.7$\\times$ on\naverage and enhances price performance by 2.5$\\times$ compared to a CPU-based\nDuckDB baseline."
                },
                "authors": [
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Advait Iyer"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "arxiv_comment": "VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v2",
                "updated": "2025-02-13T12:54:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    54,
                    36,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09003v1",
                "updated": "2025-02-13T06:44:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T06:44:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models"
                },
                "summary": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia and Llama\nmodels of different sizes demonstrate the effectiveness of RoSTE. Compared to\nexisting post-SFT quantization baselines, our method consistently achieves\nsuperior performances across various tasks and different LLM architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia and Llama\nmodels of different sizes demonstrate the effectiveness of RoSTE. Compared to\nexisting post-SFT quantization baselines, our method consistently achieves\nsuperior performances across various tasks and different LLM architectures."
                },
                "authors": [
                    {
                        "name": "Quan Wei"
                    },
                    {
                        "name": "Chung-Yiu Yau"
                    },
                    {
                        "name": "Hoi-To Wai"
                    },
                    {
                        "name": "Yang"
                    },
                    {
                        "name": "Zhao"
                    },
                    {
                        "name": "Dongyeop Kang"
                    },
                    {
                        "name": "Youngsuk Park"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "arxiv_affiliation": "Katie",
                "author": "Mingyi Hong",
                "arxiv_comment": "18 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08982v1",
                "updated": "2025-02-13T05:40:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    5,
                    40,
                    28,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T05:40:28Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    5,
                    40,
                    28,
                    3,
                    44,
                    0
                ],
                "title": "Outback: Fast and Communication-efficient Index for Key-Value Store on\n  Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outback: Fast and Communication-efficient Index for Key-Value Store on\n  Disaggregated Memory"
                },
                "summary": "Disaggregated memory systems achieve resource utilization efficiency and\nsystem scalability by distributing computation and memory resources into\ndistinct pools of nodes. RDMA is an attractive solution to support\nhigh-throughput communication between different disaggregated resource pools.\nHowever, existing RDMA solutions face a dilemma: one-sided RDMA completely\nbypasses computation at memory nodes, but its communication takes multiple\nround trips; two-sided RDMA achieves one-round-trip communication but requires\nnon-trivial computation for index lookups at memory nodes, which violates the\nprinciple of disaggregated memory. This work presents Outback, a novel indexing\nsolution for key-value stores with a one-round-trip RDMA-based network that\ndoes not incur computation-heavy tasks at memory nodes. Outback is the first to\nutilize dynamic minimal perfect hashing and separates its index into two\ncomponents: one memory-efficient and compute-heavy component at compute nodes\nand the other memory-heavy and compute-efficient component at memory nodes. We\nimplement a prototype of Outback and evaluate its performance in a public\ncloud. The experimental results show that Outback achieves higher throughput\nthan both the state-of-the-art one-sided RDMA and two-sided RDMA-based\nin-memory KVS by 1.06-5.03x, due to the unique strength of applying a separated\nperfect hashing index.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory systems achieve resource utilization efficiency and\nsystem scalability by distributing computation and memory resources into\ndistinct pools of nodes. RDMA is an attractive solution to support\nhigh-throughput communication between different disaggregated resource pools.\nHowever, existing RDMA solutions face a dilemma: one-sided RDMA completely\nbypasses computation at memory nodes, but its communication takes multiple\nround trips; two-sided RDMA achieves one-round-trip communication but requires\nnon-trivial computation for index lookups at memory nodes, which violates the\nprinciple of disaggregated memory. This work presents Outback, a novel indexing\nsolution for key-value stores with a one-round-trip RDMA-based network that\ndoes not incur computation-heavy tasks at memory nodes. Outback is the first to\nutilize dynamic minimal perfect hashing and separates its index into two\ncomponents: one memory-efficient and compute-heavy component at compute nodes\nand the other memory-heavy and compute-efficient component at memory nodes. We\nimplement a prototype of Outback and evaluate its performance in a public\ncloud. The experimental results show that Outback achieves higher throughput\nthan both the state-of-the-art one-sided RDMA and two-sided RDMA-based\nin-memory KVS by 1.06-5.03x, due to the unique strength of applying a separated\nperfect hashing index."
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Minghao Xie"
                    },
                    {
                        "name": "Shouqian Shi"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Heiner Litz"
                    },
                    {
                        "name": "Chen Qian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Qian"
                },
                "author": "Chen Qian",
                "arxiv_doi": "10.14778/3705829.3705849",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3705829.3705849",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.08982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "PVLDB, 18(2): 335-348, 2024",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08910v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08910v1",
                "updated": "2025-02-13T02:52:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    2,
                    52,
                    1,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T02:52:01Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    2,
                    52,
                    1,
                    3,
                    44,
                    0
                ],
                "title": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on\n  a Single GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on\n  a Single GPU"
                },
                "summary": "In modern large language models (LLMs), handling very long context lengths\npresents significant challenges as it causes slower inference speeds and\nincreased memory costs. Additionally, most existing pre-trained LLMs fail to\ngeneralize beyond their original training sequence lengths. To enable efficient\nand practical long-context utilization, we introduce InfiniteHiP, a novel, and\npractical LLM inference framework that accelerates processing by dynamically\neliminating irrelevant context tokens through a modular hierarchical token\npruning algorithm. Our method also allows generalization to longer sequences by\nselectively applying various RoPE adjustment methods according to the internal\nattention patterns within LLMs. Furthermore, we offload the key-value cache to\nhost memory during inference, significantly reducing GPU memory pressure. As a\nresult, InfiniteHiP enables the processing of up to 3 million tokens on a\nsingle L40s 48GB GPU -- 3x larger -- without any permanent loss of context\ninformation. Our framework achieves an 18.95x speedup in attention decoding for\na 1 million token context without requiring additional training. We implement\nour method in the SGLang framework and demonstrate its effectiveness and\npracticality through extensive evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), handling very long context lengths\npresents significant challenges as it causes slower inference speeds and\nincreased memory costs. Additionally, most existing pre-trained LLMs fail to\ngeneralize beyond their original training sequence lengths. To enable efficient\nand practical long-context utilization, we introduce InfiniteHiP, a novel, and\npractical LLM inference framework that accelerates processing by dynamically\neliminating irrelevant context tokens through a modular hierarchical token\npruning algorithm. Our method also allows generalization to longer sequences by\nselectively applying various RoPE adjustment methods according to the internal\nattention patterns within LLMs. Furthermore, we offload the key-value cache to\nhost memory during inference, significantly reducing GPU memory pressure. As a\nresult, InfiniteHiP enables the processing of up to 3 million tokens on a\nsingle L40s 48GB GPU -- 3x larger -- without any permanent loss of context\ninformation. Our framework achieves an 18.95x speedup in attention decoding for\na 1 million token context without requiring additional training. We implement\nour method in the SGLang framework and demonstrate its effectiveness and\npracticality through extensive evaluations."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08910v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08910v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02690v2",
                "updated": "2025-02-12T14:32:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    32,
                    46,
                    2,
                    43,
                    0
                ],
                "published": "2024-04-03T12:37:34Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    12,
                    37,
                    34,
                    2,
                    94,
                    0
                ],
                "title": "How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse"
                },
                "summary": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths."
                },
                "authors": [
                    {
                        "name": "Yichuan Deng"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Chiwun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chiwun Yang"
                },
                "author": "Chiwun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05431v2",
                "updated": "2025-02-12T13:54:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    54,
                    1,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-08T03:41:16Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    41,
                    16,
                    5,
                    39,
                    0
                ],
                "title": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding"
                },
                "summary": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context."
                },
                "authors": [
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.20395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20395v1",
                "updated": "2025-02-27T18:59:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    59,
                    32,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T18:59:32Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    59,
                    32,
                    3,
                    58,
                    0
                ],
                "title": "R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts"
                },
                "summary": "In large multimodal models (LMMs), the perception of non-language modalities\n(e.g., visual representations) is usually not on par with the large language\nmodels (LLMs)' powerful reasoning capabilities, deterring LMMs' performance on\nchallenging downstream tasks. This weakness has been recently mitigated by\nreplacing the vision encoder with a mixture-of-experts (MoE), which provides\nrich, multi-granularity, and diverse representations required by diverse\ndownstream tasks. The performance of multimodal MoE largely depends on its\nrouter, which reweights and mixes the representations of different experts for\neach input. However, we find that the end-to-end trained router does not always\nproduce the optimal routing weights for every test sample. To bridge the gap,\nwe propose a novel and efficient method \"Re-Routing in Test-Time(R2-T2) that\nlocally optimizes the vector of routing weights in test-time by moving it\ntoward those vectors of the correctly predicted samples in a neighborhood of\nthe test sample. We propose three R2-T2 strategies with different optimization\nobjectives and neighbor-search spaces. R2-T2 consistently and greatly improves\nstate-of-the-art LMMs' performance on challenging benchmarks of diverse tasks,\nwithout training any base-model parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large multimodal models (LMMs), the perception of non-language modalities\n(e.g., visual representations) is usually not on par with the large language\nmodels (LLMs)' powerful reasoning capabilities, deterring LMMs' performance on\nchallenging downstream tasks. This weakness has been recently mitigated by\nreplacing the vision encoder with a mixture-of-experts (MoE), which provides\nrich, multi-granularity, and diverse representations required by diverse\ndownstream tasks. The performance of multimodal MoE largely depends on its\nrouter, which reweights and mixes the representations of different experts for\neach input. However, we find that the end-to-end trained router does not always\nproduce the optimal routing weights for every test sample. To bridge the gap,\nwe propose a novel and efficient method \"Re-Routing in Test-Time(R2-T2) that\nlocally optimizes the vector of routing weights in test-time by moving it\ntoward those vectors of the correctly predicted samples in a neighborhood of\nthe test sample. We propose three R2-T2 strategies with different optimization\nobjectives and neighbor-search spaces. R2-T2 consistently and greatly improves\nstate-of-the-art LMMs' performance on challenging benchmarks of diverse tasks,\nwithout training any base-model parameters."
                },
                "authors": [
                    {
                        "name": "Zhongyang Li"
                    },
                    {
                        "name": "Ziyue Li"
                    },
                    {
                        "name": "Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhou"
                },
                "author": "Tianyi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20388v1",
                "updated": "2025-02-27T18:59:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    59,
                    8,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T18:59:08Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    59,
                    8,
                    3,
                    58,
                    0
                ],
                "title": "Beyond Next-Token: Next-X Prediction for Autoregressive Visual\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Next-Token: Next-X Prediction for Autoregressive Visual\n  Generation"
                },
                "summary": "Autoregressive (AR) modeling, known for its next-token prediction paradigm,\nunderpins state-of-the-art language and visual generative models.\nTraditionally, a ``token'' is treated as the smallest prediction unit, often a\ndiscrete symbol in language or a quantized patch in vision. However, the\noptimal token definition for 2D image structures remains an open question.\nMoreover, AR models suffer from exposure bias, where teacher forcing during\ntraining leads to error accumulation at inference. In this paper, we propose\nxAR, a generalized AR framework that extends the notion of a token to an entity\nX, which can represent an individual patch token, a cell (a $k\\times k$\ngrouping of neighboring patches), a subsample (a non-local grouping of distant\npatches), a scale (coarse-to-fine resolution), or even a whole image.\nAdditionally, we reformulate discrete token classification as\n\\textbf{continuous entity regression}, leveraging flow-matching methods at each\nAR step. This approach conditions training on noisy entities instead of ground\ntruth tokens, leading to Noisy Context Learning, which effectively alleviates\nexposure bias. As a result, xAR offers two key advantages: (1) it enables\nflexible prediction units that capture different contextual granularity and\nspatial structures, and (2) it mitigates exposure bias by avoiding reliance on\nteacher forcing. On ImageNet-256 generation benchmark, our base model, xAR-B\n(172M), outperforms DiT-XL/SiT-XL (675M) while achieving 20$\\times$ faster\ninference. Meanwhile, xAR-H sets a new state-of-the-art with an FID of 1.24,\nrunning 2.2$\\times$ faster than the previous best-performing model without\nrelying on vision foundation modules (\\eg, DINOv2) or advanced guidance\ninterval sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) modeling, known for its next-token prediction paradigm,\nunderpins state-of-the-art language and visual generative models.\nTraditionally, a ``token'' is treated as the smallest prediction unit, often a\ndiscrete symbol in language or a quantized patch in vision. However, the\noptimal token definition for 2D image structures remains an open question.\nMoreover, AR models suffer from exposure bias, where teacher forcing during\ntraining leads to error accumulation at inference. In this paper, we propose\nxAR, a generalized AR framework that extends the notion of a token to an entity\nX, which can represent an individual patch token, a cell (a $k\\times k$\ngrouping of neighboring patches), a subsample (a non-local grouping of distant\npatches), a scale (coarse-to-fine resolution), or even a whole image.\nAdditionally, we reformulate discrete token classification as\n\\textbf{continuous entity regression}, leveraging flow-matching methods at each\nAR step. This approach conditions training on noisy entities instead of ground\ntruth tokens, leading to Noisy Context Learning, which effectively alleviates\nexposure bias. As a result, xAR offers two key advantages: (1) it enables\nflexible prediction units that capture different contextual granularity and\nspatial structures, and (2) it mitigates exposure bias by avoiding reliance on\nteacher forcing. On ImageNet-256 generation benchmark, our base model, xAR-B\n(172M), outperforms DiT-XL/SiT-XL (675M) while achieving 20$\\times$ faster\ninference. Meanwhile, xAR-H sets a new state-of-the-art with an FID of 1.24,\nrunning 2.2$\\times$ faster than the previous best-performing model without\nrelying on vision foundation modules (\\eg, DINOv2) or advanced guidance\ninterval sampling."
                },
                "authors": [
                    {
                        "name": "Sucheng Ren"
                    },
                    {
                        "name": "Qihang Yu"
                    },
                    {
                        "name": "Ju He"
                    },
                    {
                        "name": "Xiaohui Shen"
                    },
                    {
                        "name": "Alan Yuille"
                    },
                    {
                        "name": "Liang-Chieh Chen"
                    }
                ],
                "author_detail": {
                    "name": "Liang-Chieh Chen"
                },
                "author": "Liang-Chieh Chen",
                "arxiv_comment": "Project page at \\url{https://oliverrensu.github.io/project/xAR}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20385v1",
                "updated": "2025-02-27T18:57:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    57,
                    44,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T18:57:44Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    57,
                    44,
                    3,
                    58,
                    0
                ],
                "title": "rSPDE: tools for statistical modeling using fractional SPDEs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "rSPDE: tools for statistical modeling using fractional SPDEs"
                },
                "summary": "The R software package rSPDE contains methods for approximating Gaussian\nrandom fields based on fractional-order stochastic partial differential\nequations (SPDEs). A common example of such fields are Whittle-Mat\\'ern fields\non bounded domains in $\\mathbb{R}^d$, manifolds, or metric graphs. The package\nalso implements various other models which are briefly introduced in this\narticle. Besides the approximation methods, the package contains methods for\nsimulation, prediction, and statistical inference for such models, as well as\ninterfaces to INLA, inlabru and MetricGraph. With these interfaces,\nfractional-order SPDEs can be used as model components in general latent\nGaussian models, for which full Bayesian inference can be performed, also for\nfractional models on metric graphs. This includes estimation of the smoothness\nparameter of the fields. This article describes the computational methods used\nin the package and summarizes the theoretical basis for these. The main\nfunctions of the package are introduced, and their usage is illustrated through\nvarious examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The R software package rSPDE contains methods for approximating Gaussian\nrandom fields based on fractional-order stochastic partial differential\nequations (SPDEs). A common example of such fields are Whittle-Mat\\'ern fields\non bounded domains in $\\mathbb{R}^d$, manifolds, or metric graphs. The package\nalso implements various other models which are briefly introduced in this\narticle. Besides the approximation methods, the package contains methods for\nsimulation, prediction, and statistical inference for such models, as well as\ninterfaces to INLA, inlabru and MetricGraph. With these interfaces,\nfractional-order SPDEs can be used as model components in general latent\nGaussian models, for which full Bayesian inference can be performed, also for\nfractional models on metric graphs. This includes estimation of the smoothness\nparameter of the fields. This article describes the computational methods used\nin the package and summarizes the theoretical basis for these. The main\nfunctions of the package are introduced, and their usage is illustrated through\nvarious examples."
                },
                "authors": [
                    {
                        "name": "David Bolin"
                    },
                    {
                        "name": "Alexandre B. Simas"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre B. Simas"
                },
                "author": "Alexandre B. Simas",
                "arxiv_comment": "35 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20383v1",
                "updated": "2025-02-27T18:56:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    56,
                    26,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T18:56:26Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    56,
                    26,
                    3,
                    58,
                    0
                ],
                "title": "Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security\n  Analysis"
                },
                "summary": "Recent advancements in Web AI agents have demonstrated remarkable\ncapabilities in addressing complex web navigation tasks. However, emerging\nresearch shows that these agents exhibit greater vulnerability compared to\nstandalone Large Language Models (LLMs), despite both being built upon the same\nsafety-aligned models. This discrepancy is particularly concerning given the\ngreater flexibility of Web AI Agent compared to standalone LLMs, which may\nexpose them to a wider range of adversarial user inputs. To build a scaffold\nthat addresses these concerns, this study investigates the underlying factors\nthat contribute to the increased vulnerability of Web AI agents. Notably, this\ndisparity stems from the multifaceted differences between Web AI agents and\nstandalone LLMs, as well as the complex signals - nuances that simple\nevaluation metrics, such as success rate, often fail to capture. To tackle\nthese challenges, we propose a component-level analysis and a more granular,\nsystematic evaluation framework. Through this fine-grained investigation, we\nidentify three critical factors that amplify the vulnerability of Web AI\nagents; (1) embedding user goals into the system prompt, (2) multi-step action\ngeneration, and (3) observational capabilities. Our findings highlights the\npressing need to enhance security and robustness in AI agent design and provide\nactionable insights for targeted defense strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Web AI agents have demonstrated remarkable\ncapabilities in addressing complex web navigation tasks. However, emerging\nresearch shows that these agents exhibit greater vulnerability compared to\nstandalone Large Language Models (LLMs), despite both being built upon the same\nsafety-aligned models. This discrepancy is particularly concerning given the\ngreater flexibility of Web AI Agent compared to standalone LLMs, which may\nexpose them to a wider range of adversarial user inputs. To build a scaffold\nthat addresses these concerns, this study investigates the underlying factors\nthat contribute to the increased vulnerability of Web AI agents. Notably, this\ndisparity stems from the multifaceted differences between Web AI agents and\nstandalone LLMs, as well as the complex signals - nuances that simple\nevaluation metrics, such as success rate, often fail to capture. To tackle\nthese challenges, we propose a component-level analysis and a more granular,\nsystematic evaluation framework. Through this fine-grained investigation, we\nidentify three critical factors that amplify the vulnerability of Web AI\nagents; (1) embedding user goals into the system prompt, (2) multi-step action\ngeneration, and (3) observational capabilities. Our findings highlights the\npressing need to enhance security and robustness in AI agent design and provide\nactionable insights for targeted defense strategies."
                },
                "authors": [
                    {
                        "name": "Jeffrey Yang Fan Chiang"
                    },
                    {
                        "name": "Seungjae Lee"
                    },
                    {
                        "name": "Jia-Bin Huang"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Yizheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yizheng Chen"
                },
                "author": "Yizheng Chen",
                "arxiv_comment": "Project website: http://vulnerable-ai-agents.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20379v1",
                "updated": "2025-02-27T18:53:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    53,
                    30,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T18:53:30Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    53,
                    30,
                    3,
                    58,
                    0
                ],
                "title": "Multi-Agent Verification: Scaling Test-Time Compute with Multiple\n  Verifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Verification: Scaling Test-Time Compute with Multiple\n  Verifiers"
                },
                "summary": "By utilizing more computational resources at test-time, large language models\n(LLMs) can improve without additional training. One common strategy uses\nverifiers to evaluate candidate outputs. In this work, we propose a novel\nscaling dimension for test-time compute: scaling the number of verifiers. We\nintroduce Multi-Agent Verification (MAV) as a test-time compute paradigm that\ncombines multiple verifiers to improve performance. We propose using Aspect\nVerifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of\noutputs, as one possible choice for the verifiers in a MAV system. AVs are a\nconvenient building block for MAV since they can be easily combined without\nadditional training. Moreover, we introduce BoN-MAV, a simple multi-agent\nverification algorithm that combines best-of-n sampling with multiple\nverifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency\nand reward model verification, and we demonstrate both weak-to-strong\ngeneralization, where combining weak verifiers improves even stronger LLMs, and\nself-improvement, where the same base model is used to both generate and verify\noutputs. Our results establish scaling the number of verifiers as a promising\nnew dimension for improving language model performance at test-time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By utilizing more computational resources at test-time, large language models\n(LLMs) can improve without additional training. One common strategy uses\nverifiers to evaluate candidate outputs. In this work, we propose a novel\nscaling dimension for test-time compute: scaling the number of verifiers. We\nintroduce Multi-Agent Verification (MAV) as a test-time compute paradigm that\ncombines multiple verifiers to improve performance. We propose using Aspect\nVerifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of\noutputs, as one possible choice for the verifiers in a MAV system. AVs are a\nconvenient building block for MAV since they can be easily combined without\nadditional training. Moreover, we introduce BoN-MAV, a simple multi-agent\nverification algorithm that combines best-of-n sampling with multiple\nverifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency\nand reward model verification, and we demonstrate both weak-to-strong\ngeneralization, where combining weak verifiers improves even stronger LLMs, and\nself-improvement, where the same base model is used to both generate and verify\noutputs. Our results establish scaling the number of verifiers as a promising\nnew dimension for improving language model performance at test-time."
                },
                "authors": [
                    {
                        "name": "Shalev Lifshitz"
                    },
                    {
                        "name": "Sheila A. McIlraith"
                    },
                    {
                        "name": "Yilun Du"
                    }
                ],
                "author_detail": {
                    "name": "Yilun Du"
                },
                "author": "Yilun Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20377v1",
                "updated": "2025-02-27T18:51:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    51,
                    22,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T18:51:22Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    51,
                    22,
                    3,
                    58,
                    0
                ],
                "title": "PhantomWiki: On-Demand Datasets for Reasoning and Retrieval Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhantomWiki: On-Demand Datasets for Reasoning and Retrieval Evaluation"
                },
                "summary": "High-quality benchmarks are essential for evaluating reasoning and retrieval\ncapabilities of large language models (LLMs). However, curating datasets for\nthis purpose is not a permanent solution as they are prone to data leakage and\ninflated performance results. To address these challenges, we propose\nPhantomWiki: a pipeline to generate unique, factually consistent document\ncorpora with diverse question-answer pairs. Unlike prior work, PhantomWiki is\nneither a fixed dataset, nor is it based on any existing data. Instead, a new\nPhantomWiki instance is generated on demand for each evaluation. We vary the\nquestion difficulty and corpus size to disentangle reasoning and retrieval\ncapabilities respectively, and find that PhantomWiki datasets are surprisingly\nchallenging for frontier LLMs. Thus, we contribute a scalable and data\nleakage-resistant framework for disentangled evaluation of reasoning,\nretrieval, and tool-use abilities. Our code is available at\nhttps://github.com/kilian-group/phantom-wiki.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality benchmarks are essential for evaluating reasoning and retrieval\ncapabilities of large language models (LLMs). However, curating datasets for\nthis purpose is not a permanent solution as they are prone to data leakage and\ninflated performance results. To address these challenges, we propose\nPhantomWiki: a pipeline to generate unique, factually consistent document\ncorpora with diverse question-answer pairs. Unlike prior work, PhantomWiki is\nneither a fixed dataset, nor is it based on any existing data. Instead, a new\nPhantomWiki instance is generated on demand for each evaluation. We vary the\nquestion difficulty and corpus size to disentangle reasoning and retrieval\ncapabilities respectively, and find that PhantomWiki datasets are surprisingly\nchallenging for frontier LLMs. Thus, we contribute a scalable and data\nleakage-resistant framework for disentangled evaluation of reasoning,\nretrieval, and tool-use abilities. Our code is available at\nhttps://github.com/kilian-group/phantom-wiki."
                },
                "authors": [
                    {
                        "name": "Albert Gong"
                    },
                    {
                        "name": "Kamilė Stankevičiūtė"
                    },
                    {
                        "name": "Chao Wan"
                    },
                    {
                        "name": "Anmol Kabra"
                    },
                    {
                        "name": "Raphael Thesmar"
                    },
                    {
                        "name": "Johann Lee"
                    },
                    {
                        "name": "Julius Klenke"
                    },
                    {
                        "name": "Carla P. Gomes"
                    },
                    {
                        "name": "Kilian Q. Weinberger"
                    }
                ],
                "author_detail": {
                    "name": "Kilian Q. Weinberger"
                },
                "author": "Kilian Q. Weinberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20364v1",
                "updated": "2025-02-27T18:35:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    35,
                    39,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T18:35:39Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    35,
                    39,
                    3,
                    58,
                    0
                ],
                "title": "Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with\n  Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix\n  Factorization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with\n  Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix\n  Factorization"
                },
                "summary": "Agentic Generative AI, powered by Large Language Models (LLMs) with\nRetrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores\n(VSs), represents a transformative technology applicable to specialized domains\nsuch as legal systems, research, recommender systems, cybersecurity, and global\nsecurity, including proliferation research. This technology excels at inferring\nrelationships within vast unstructured or semi-structured datasets. The legal\ndomain here comprises complex data characterized by extensive, interrelated,\nand semi-structured knowledge systems with complex relations. It comprises\nconstitutions, statutes, regulations, and case law. Extracting insights and\nnavigating the intricate networks of legal documents and their relations is\ncrucial for effective legal research. Here, we introduce a generative AI system\nthat integrates RAG, VS, and KG, constructed via Non-Negative Matrix\nFactorization (NMF), to enhance legal information retrieval and AI reasoning\nand minimize hallucinations. In the legal system, these technologies empower AI\nagents to identify and analyze complex connections among cases, statutes, and\nlegal precedents, uncovering hidden relationships and predicting legal\ntrends-challenging tasks that are essential for ensuring justice and improving\noperational efficiency. Our system employs web scraping techniques to\nsystematically collect legal texts, such as statutes, constitutional\nprovisions, and case law, from publicly accessible platforms like Justia. It\nbridges the gap between traditional keyword-based searches and contextual\nunderstanding by leveraging advanced semantic representations, hierarchical\nrelationships, and latent topic discovery. This framework supports legal\ndocument clustering, summarization, and cross-referencing, for scalable,\ninterpretable, and accurate retrieval for semi-structured data while advancing\ncomputational law and AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Generative AI, powered by Large Language Models (LLMs) with\nRetrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores\n(VSs), represents a transformative technology applicable to specialized domains\nsuch as legal systems, research, recommender systems, cybersecurity, and global\nsecurity, including proliferation research. This technology excels at inferring\nrelationships within vast unstructured or semi-structured datasets. The legal\ndomain here comprises complex data characterized by extensive, interrelated,\nand semi-structured knowledge systems with complex relations. It comprises\nconstitutions, statutes, regulations, and case law. Extracting insights and\nnavigating the intricate networks of legal documents and their relations is\ncrucial for effective legal research. Here, we introduce a generative AI system\nthat integrates RAG, VS, and KG, constructed via Non-Negative Matrix\nFactorization (NMF), to enhance legal information retrieval and AI reasoning\nand minimize hallucinations. In the legal system, these technologies empower AI\nagents to identify and analyze complex connections among cases, statutes, and\nlegal precedents, uncovering hidden relationships and predicting legal\ntrends-challenging tasks that are essential for ensuring justice and improving\noperational efficiency. Our system employs web scraping techniques to\nsystematically collect legal texts, such as statutes, constitutional\nprovisions, and case law, from publicly accessible platforms like Justia. It\nbridges the gap between traditional keyword-based searches and contextual\nunderstanding by leveraging advanced semantic representations, hierarchical\nrelationships, and latent topic discovery. This framework supports legal\ndocument clustering, summarization, and cross-referencing, for scalable,\ninterpretable, and accurate retrieval for semi-structured data while advancing\ncomputational law and AI."
                },
                "authors": [
                    {
                        "name": "Ryan C. Barron"
                    },
                    {
                        "name": "Maksim E. Eren"
                    },
                    {
                        "name": "Olga M. Serafimova"
                    },
                    {
                        "name": "Cynthia Matuszek"
                    },
                    {
                        "name": "Boian S. Alexandrov"
                    }
                ],
                "author_detail": {
                    "name": "Boian S. Alexandrov"
                },
                "author": "Boian S. Alexandrov",
                "arxiv_comment": "10 pages, 6 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20356v1",
                "updated": "2025-02-27T18:29:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    29,
                    9,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T18:29:09Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    29,
                    9,
                    3,
                    58,
                    0
                ],
                "title": "Bridging the Creativity Understanding Gap: Small-Scale Human Alignment\n  Enables Expert-Level Humor Ranking in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Creativity Understanding Gap: Small-Scale Human Alignment\n  Enables Expert-Level Humor Ranking in LLMs"
                },
                "summary": "Large Language Models (LLMs) have shown significant limitations in\nunderstanding creative content, as demonstrated by Hessel et al. (2023)'s\ninfluential work on the New Yorker Cartoon Caption Contest (NYCCC). Their study\nexposed a substantial gap between LLMs and humans in humor comprehension,\nestablishing that understanding and evaluating creative content is key\nchallenge in AI development. We revisit this challenge by decomposing humor\nunderstanding into three components and systematically improve each: enhancing\nvisual understanding through improved annotation, utilizing LLM-generated humor\nreasoning and explanations, and implementing targeted alignment with human\npreference data. Our refined approach achieves 82.4% accuracy in caption\nranking, singificantly improving upon the previous 67% benchmark and matching\nthe performance of world-renowned human experts in this domain. Notably, while\nattempts to mimic subgroup preferences through various persona prompts showed\nminimal impact, model finetuning with crowd preferences proved remarkably\neffective. These findings reveal that LLM limitations in creative judgment can\nbe effectively addressed through focused alignment to specific subgroups and\nindividuals. Lastly, we propose the position that achieving artificial general\nintelligence necessitates systematic collection of human preference data across\ncreative domains. We advocate that just as human creativity is deeply\ninfluenced by individual and cultural preferences, training LLMs with diverse\nhuman preference data may be essential for developing true creative\nunderstanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown significant limitations in\nunderstanding creative content, as demonstrated by Hessel et al. (2023)'s\ninfluential work on the New Yorker Cartoon Caption Contest (NYCCC). Their study\nexposed a substantial gap between LLMs and humans in humor comprehension,\nestablishing that understanding and evaluating creative content is key\nchallenge in AI development. We revisit this challenge by decomposing humor\nunderstanding into three components and systematically improve each: enhancing\nvisual understanding through improved annotation, utilizing LLM-generated humor\nreasoning and explanations, and implementing targeted alignment with human\npreference data. Our refined approach achieves 82.4% accuracy in caption\nranking, singificantly improving upon the previous 67% benchmark and matching\nthe performance of world-renowned human experts in this domain. Notably, while\nattempts to mimic subgroup preferences through various persona prompts showed\nminimal impact, model finetuning with crowd preferences proved remarkably\neffective. These findings reveal that LLM limitations in creative judgment can\nbe effectively addressed through focused alignment to specific subgroups and\nindividuals. Lastly, we propose the position that achieving artificial general\nintelligence necessitates systematic collection of human preference data across\ncreative domains. We advocate that just as human creativity is deeply\ninfluenced by individual and cultural preferences, training LLMs with diverse\nhuman preference data may be essential for developing true creative\nunderstanding."
                },
                "authors": [
                    {
                        "name": "Kuan Lok Zhou"
                    },
                    {
                        "name": "Jiayi Chen"
                    },
                    {
                        "name": "Siddharth Suresh"
                    },
                    {
                        "name": "Reuben Narad"
                    },
                    {
                        "name": "Timothy T. Rogers"
                    },
                    {
                        "name": "Lalit K Jain"
                    },
                    {
                        "name": "Robert D Nowak"
                    },
                    {
                        "name": "Bob Mankoff"
                    },
                    {
                        "name": "Jifan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jifan Zhang"
                },
                "author": "Jifan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20350v1",
                "updated": "2025-02-27T18:22:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    22,
                    33,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T18:22:33Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    22,
                    33,
                    3,
                    58,
                    0
                ],
                "title": "KEDRec-LM: A Knowledge-distilled Explainable Drug Recommendation Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KEDRec-LM: A Knowledge-distilled Explainable Drug Recommendation Large\n  Language Model"
                },
                "summary": "Drug discovery is a critical task in biomedical natural language processing\n(NLP), yet explainable drug discovery remains underexplored. Meanwhile, large\nlanguage models (LLMs) have shown remarkable abilities in natural language\nunderstanding and generation. Leveraging LLMs for explainable drug discovery\nhas the potential to improve downstream tasks and real-world applications. In\nthis study, we utilize open-source drug knowledge graphs, clinical trial data,\nand PubMed publications to construct a comprehensive dataset for the\nexplainable drug discovery task, named \\textbf{expRxRec}. Furthermore, we\nintroduce \\textbf{KEDRec-LM}, an instruction-tuned LLM which distills knowledge\nfrom rich medical knowledge corpus for drug recommendation and rationale\ngeneration. To encourage further research in this area, we will publicly\nrelease\\footnote{A copy is attached with this submission} both the dataset and\nKEDRec-LM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drug discovery is a critical task in biomedical natural language processing\n(NLP), yet explainable drug discovery remains underexplored. Meanwhile, large\nlanguage models (LLMs) have shown remarkable abilities in natural language\nunderstanding and generation. Leveraging LLMs for explainable drug discovery\nhas the potential to improve downstream tasks and real-world applications. In\nthis study, we utilize open-source drug knowledge graphs, clinical trial data,\nand PubMed publications to construct a comprehensive dataset for the\nexplainable drug discovery task, named \\textbf{expRxRec}. Furthermore, we\nintroduce \\textbf{KEDRec-LM}, an instruction-tuned LLM which distills knowledge\nfrom rich medical knowledge corpus for drug recommendation and rationale\ngeneration. To encourage further research in this area, we will publicly\nrelease\\footnote{A copy is attached with this submission} both the dataset and\nKEDRec-LM."
                },
                "authors": [
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Rui Zhu"
                    },
                    {
                        "name": "Shutian Ma"
                    },
                    {
                        "name": "Jingwei Xiong"
                    },
                    {
                        "name": "Yejin Kim"
                    },
                    {
                        "name": "Fabricio Murai"
                    },
                    {
                        "name": "Xiaozhong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaozhong Liu"
                },
                "author": "Xiaozhong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20344v1",
                "updated": "2025-02-27T18:16:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    16,
                    47,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T18:16:47Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    16,
                    47,
                    3,
                    58,
                    0
                ],
                "title": "Sparse Auto-Encoder Interprets Linguistic Features in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Auto-Encoder Interprets Linguistic Features in Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) excel in tasks that require complex linguistic\nabilities, such as reference disambiguation and metaphor\nrecognition/generation. Although LLMs possess impressive capabilities, their\ninternal mechanisms for processing and representing linguistic knowledge remain\nlargely opaque. Previous work on linguistic mechanisms has been limited by\ncoarse granularity, insufficient causal analysis, and a narrow focus. In this\nstudy, we present a systematic and comprehensive causal investigation using\nsparse auto-encoders (SAEs). We extract a wide range of linguistic features\nfrom six dimensions: phonetics, phonology, morphology, syntax, semantics, and\npragmatics. We extract, evaluate, and intervene on these features by\nconstructing minimal contrast datasets and counterfactual sentence datasets. We\nintroduce two indices-Feature Representation Confidence (FRC) and Feature\nIntervention Confidence (FIC)-to measure the ability of linguistic features to\ncapture and control linguistic phenomena. Our results reveal inherent\nrepresentations of linguistic knowledge in LLMs and demonstrate the potential\nfor controlling model outputs. This work provides strong evidence that LLMs\npossess genuine linguistic knowledge and lays the foundation for more\ninterpretable and controllable language modeling in future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in tasks that require complex linguistic\nabilities, such as reference disambiguation and metaphor\nrecognition/generation. Although LLMs possess impressive capabilities, their\ninternal mechanisms for processing and representing linguistic knowledge remain\nlargely opaque. Previous work on linguistic mechanisms has been limited by\ncoarse granularity, insufficient causal analysis, and a narrow focus. In this\nstudy, we present a systematic and comprehensive causal investigation using\nsparse auto-encoders (SAEs). We extract a wide range of linguistic features\nfrom six dimensions: phonetics, phonology, morphology, syntax, semantics, and\npragmatics. We extract, evaluate, and intervene on these features by\nconstructing minimal contrast datasets and counterfactual sentence datasets. We\nintroduce two indices-Feature Representation Confidence (FRC) and Feature\nIntervention Confidence (FIC)-to measure the ability of linguistic features to\ncapture and control linguistic phenomena. Our results reveal inherent\nrepresentations of linguistic knowledge in LLMs and demonstrate the potential\nfor controlling model outputs. This work provides strong evidence that LLMs\npossess genuine linguistic knowledge and lays the foundation for more\ninterpretable and controllable language modeling in future research."
                },
                "authors": [
                    {
                        "name": "Yi Jing"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Lingxu Ran"
                    },
                    {
                        "name": "Hongzhu Guo"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20339v1",
                "updated": "2025-02-27T18:08:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    8,
                    16,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T18:08:16Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    8,
                    16,
                    3,
                    58,
                    0
                ],
                "title": "Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners"
                },
                "summary": "Recent advancements have demonstrated that the performance of large language\nmodels (LLMs) can be significantly enhanced by scaling computational resources\nat test time. A common strategy involves generating multiple Chain-of-Thought\n(CoT) trajectories and aggregating their outputs through various selection\nmechanisms. This raises a fundamental question: can models with lower\ncomplexity leverage their superior generation throughput to outperform\nsimilarly sized Transformers for a fixed computational budget? To address this\nquestion and overcome the lack of strong subquadratic reasoners, we distill\npure and hybrid Mamba models from pretrained Transformers. Trained on only 8\nbillion tokens, our distilled models show strong performance and scaling on\nmathematical reasoning datasets while being much faster at inference for large\nbatches and long sequences. Despite the zero-shot performance hit due to\ndistillation, both pure and hybrid Mamba models can scale their coverage and\naccuracy performance past their Transformer teacher models under fixed time\nbudgets, opening a new direction for scaling inference compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements have demonstrated that the performance of large language\nmodels (LLMs) can be significantly enhanced by scaling computational resources\nat test time. A common strategy involves generating multiple Chain-of-Thought\n(CoT) trajectories and aggregating their outputs through various selection\nmechanisms. This raises a fundamental question: can models with lower\ncomplexity leverage their superior generation throughput to outperform\nsimilarly sized Transformers for a fixed computational budget? To address this\nquestion and overcome the lack of strong subquadratic reasoners, we distill\npure and hybrid Mamba models from pretrained Transformers. Trained on only 8\nbillion tokens, our distilled models show strong performance and scaling on\nmathematical reasoning datasets while being much faster at inference for large\nbatches and long sequences. Despite the zero-shot performance hit due to\ndistillation, both pure and hybrid Mamba models can scale their coverage and\naccuracy performance past their Transformer teacher models under fixed time\nbudgets, opening a new direction for scaling inference compute."
                },
                "authors": [
                    {
                        "name": "Daniele Paliotta"
                    },
                    {
                        "name": "Junxiong Wang"
                    },
                    {
                        "name": "Matteo Pagliardini"
                    },
                    {
                        "name": "Kevin Y. Li"
                    },
                    {
                        "name": "Aviv Bick"
                    },
                    {
                        "name": "J. Zico Kolter"
                    },
                    {
                        "name": "Albert Gu"
                    },
                    {
                        "name": "François Fleuret"
                    },
                    {
                        "name": "Tri Dao"
                    }
                ],
                "author_detail": {
                    "name": "Tri Dao"
                },
                "author": "Tri Dao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20335v1",
                "updated": "2025-02-27T18:05:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    5,
                    15,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T18:05:15Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    5,
                    15,
                    3,
                    58,
                    0
                ],
                "title": "Expertise Is What We Want",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expertise Is What We Want"
                },
                "summary": "Clinical decision-making depends on expert reasoning, which is guided by\nstandardized, evidence-based guidelines. However, translating these guidelines\ninto automated clinical decision support systems risks inaccuracy and\nimportantly, loss of nuance. We share an application architecture, the Large\nLanguage Expert (LLE), that combines the flexibility and power of Large\nLanguage Models (LLMs) with the interpretability, explainability, and\nreliability of Expert Systems. LLMs help address key challenges of Expert\nSystems, such as integrating and codifying knowledge, and data normalization.\nConversely, an Expert System-like approach helps overcome challenges with LLMs,\nincluding hallucinations, atomic and inexpensive updates, and testability.\n  To highlight the power of the Large Language Expert (LLE) system, we built an\nLLE to assist with the workup of patients newly diagnosed with cancer. Timely\ninitiation of cancer treatment is critical for optimal patient outcomes.\nHowever, increasing complexity in diagnostic recommendations has made it\ndifficult for primary care physicians to ensure their patients have completed\nthe necessary workup before their first visit with an oncologist. As with many\nreal-world clinical tasks, these workups require the analysis of unstructured\nhealth records and the application of nuanced clinical decision logic. In this\nstudy, we describe the design & evaluation of an LLE system built to rapidly\nidentify and suggest the correct diagnostic workup. The system demonstrated a\nhigh degree of clinical-level accuracy (>95%) and effectively addressed gaps\nidentified in real-world data from breast and colon cancer patients at a large\nacademic center.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical decision-making depends on expert reasoning, which is guided by\nstandardized, evidence-based guidelines. However, translating these guidelines\ninto automated clinical decision support systems risks inaccuracy and\nimportantly, loss of nuance. We share an application architecture, the Large\nLanguage Expert (LLE), that combines the flexibility and power of Large\nLanguage Models (LLMs) with the interpretability, explainability, and\nreliability of Expert Systems. LLMs help address key challenges of Expert\nSystems, such as integrating and codifying knowledge, and data normalization.\nConversely, an Expert System-like approach helps overcome challenges with LLMs,\nincluding hallucinations, atomic and inexpensive updates, and testability.\n  To highlight the power of the Large Language Expert (LLE) system, we built an\nLLE to assist with the workup of patients newly diagnosed with cancer. Timely\ninitiation of cancer treatment is critical for optimal patient outcomes.\nHowever, increasing complexity in diagnostic recommendations has made it\ndifficult for primary care physicians to ensure their patients have completed\nthe necessary workup before their first visit with an oncologist. As with many\nreal-world clinical tasks, these workups require the analysis of unstructured\nhealth records and the application of nuanced clinical decision logic. In this\nstudy, we describe the design & evaluation of an LLE system built to rapidly\nidentify and suggest the correct diagnostic workup. The system demonstrated a\nhigh degree of clinical-level accuracy (>95%) and effectively addressed gaps\nidentified in real-world data from breast and colon cancer patients at a large\nacademic center."
                },
                "authors": [
                    {
                        "name": "Alan Ashworth"
                    },
                    {
                        "name": "Munir Al-Dajani"
                    },
                    {
                        "name": "Keegan Duchicela"
                    },
                    {
                        "name": "Kiril Kafadarov"
                    },
                    {
                        "name": "Allison Kurian"
                    },
                    {
                        "name": "Othman Laraki"
                    },
                    {
                        "name": "Amina Lazrak"
                    },
                    {
                        "name": "Divneet Mandair"
                    },
                    {
                        "name": "Wendy McKennon"
                    },
                    {
                        "name": "Rebecca Miksad"
                    },
                    {
                        "name": "Jayodita Sanghvi"
                    },
                    {
                        "name": "Travis Zack"
                    }
                ],
                "author_detail": {
                    "name": "Travis Zack"
                },
                "author": "Travis Zack",
                "arxiv_comment": "18 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20330v1",
                "updated": "2025-02-27T17:59:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "title": "Long-Context Inference with Retrieval-Augmented Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Inference with Retrieval-Augmented Speculative Decoding"
                },
                "summary": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications."
                },
                "authors": [
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Qilong Feng"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Qizhe Shieh"
                },
                "author": "Michael Qizhe Shieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00075v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00075v4",
                "updated": "2025-02-27T17:49:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    49,
                    33,
                    3,
                    58,
                    0
                ],
                "published": "2024-06-21T19:18:16Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    19,
                    18,
                    16,
                    4,
                    173,
                    0
                ],
                "title": "Logicbreaks: A Framework for Understanding Subversion of Rule-based\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logicbreaks: A Framework for Understanding Subversion of Rule-based\n  Inference"
                },
                "summary": "We study how to subvert large language models (LLMs) from following\nprompt-specified rules. We first formalize rule-following as inference in\npropositional Horn logic, a mathematical system in which rules have the form\n\"if $P$ and $Q$, then $R$\" for some propositions $P$, $Q$, and $R$. Next, we\nprove that although small transformers can faithfully follow such rules,\nmaliciously crafted prompts can still mislead both theoretical constructions\nand models learned from data. Furthermore, we demonstrate that popular attack\nalgorithms on LLMs find adversarial prompts and induce attention patterns that\nalign with our theory. Our novel logic-based framework provides a foundation\nfor studying LLMs in rule-based settings, enabling a formal analysis of tasks\nlike logical reasoning and jailbreak attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study how to subvert large language models (LLMs) from following\nprompt-specified rules. We first formalize rule-following as inference in\npropositional Horn logic, a mathematical system in which rules have the form\n\"if $P$ and $Q$, then $R$\" for some propositions $P$, $Q$, and $R$. Next, we\nprove that although small transformers can faithfully follow such rules,\nmaliciously crafted prompts can still mislead both theoretical constructions\nand models learned from data. Furthermore, we demonstrate that popular attack\nalgorithms on LLMs find adversarial prompts and induce attention patterns that\nalign with our theory. Our novel logic-based framework provides a foundation\nfor studying LLMs in rule-based settings, enabling a formal analysis of tasks\nlike logical reasoning and jailbreak attacks."
                },
                "authors": [
                    {
                        "name": "Anton Xue"
                    },
                    {
                        "name": "Avishree Khare"
                    },
                    {
                        "name": "Rajeev Alur"
                    },
                    {
                        "name": "Surbhi Goel"
                    },
                    {
                        "name": "Eric Wong"
                    }
                ],
                "author_detail": {
                    "name": "Eric Wong"
                },
                "author": "Eric Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00075v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00075v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20320v1",
                "updated": "2025-02-27T17:45:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    45,
                    20,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:45:20Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    45,
                    20,
                    3,
                    58,
                    0
                ],
                "title": "ACCORD: Application Context-aware Cross-layer Optimization and Resource\n  Design for 5G/NextG Machine-centric Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACCORD: Application Context-aware Cross-layer Optimization and Resource\n  Design for 5G/NextG Machine-centric Applications"
                },
                "summary": "Recent advancements in AI and edge computing have accelerated the development\nof machine-centric applications (MCAs), such as smart surveillance systems. In\nthese applications, video cameras and sensors offload inference tasks like\nlicense plate recognition and vehicle tracking to remote servers due to local\ncomputing and energy constraints. However, legacy network solutions, designed\nprimarily for human-centric applications, struggle to reliably support these\nMCAs, which demand heterogeneous and fluctuating QoS (due to diverse\napplication inference tasks), further challenged by dynamic wireless network\nconditions and limited spectrum resources. To tackle these challenges, we\npropose an Application Context-aware Cross-layer Optimization and Resource\nDesign (ACCORD) framework. This innovative framework anticipates the evolving\ndemands of MCAs in real time, quickly adapting to provide customized QoS and\noptimal performance, even for the most dynamic and unpredictable MCAs. This\nalso leads to improved network resource management and spectrum utilization.\nACCORD operates as a closed feedback-loop system between the application client\nand network and consists of two key components: (1) Building Application\nContext: It focuses on understanding the specific context of MCA requirements.\nContextual factors include device capabilities, user behavior (e.g., mobility\nspeed), and network channel conditions. (2) Cross-layer Network Parameter\nConfiguration: Utilizing a DRL approach, this component leverages the\ncontextual information to optimize network configuration parameters across\nvarious layers, including PHY, MAC, and RLC, as well as the application layer,\nto meet the desired QoS requirement in real-time. Extensive evaluation with the\n3GPP-compliant MATLAB 5G toolbox demonstrates the practicality and\neffectiveness of our proposed ACCORD framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in AI and edge computing have accelerated the development\nof machine-centric applications (MCAs), such as smart surveillance systems. In\nthese applications, video cameras and sensors offload inference tasks like\nlicense plate recognition and vehicle tracking to remote servers due to local\ncomputing and energy constraints. However, legacy network solutions, designed\nprimarily for human-centric applications, struggle to reliably support these\nMCAs, which demand heterogeneous and fluctuating QoS (due to diverse\napplication inference tasks), further challenged by dynamic wireless network\nconditions and limited spectrum resources. To tackle these challenges, we\npropose an Application Context-aware Cross-layer Optimization and Resource\nDesign (ACCORD) framework. This innovative framework anticipates the evolving\ndemands of MCAs in real time, quickly adapting to provide customized QoS and\noptimal performance, even for the most dynamic and unpredictable MCAs. This\nalso leads to improved network resource management and spectrum utilization.\nACCORD operates as a closed feedback-loop system between the application client\nand network and consists of two key components: (1) Building Application\nContext: It focuses on understanding the specific context of MCA requirements.\nContextual factors include device capabilities, user behavior (e.g., mobility\nspeed), and network channel conditions. (2) Cross-layer Network Parameter\nConfiguration: Utilizing a DRL approach, this component leverages the\ncontextual information to optimize network configuration parameters across\nvarious layers, including PHY, MAC, and RLC, as well as the application layer,\nto meet the desired QoS requirement in real-time. Extensive evaluation with the\n3GPP-compliant MATLAB 5G toolbox demonstrates the practicality and\neffectiveness of our proposed ACCORD framework."
                },
                "authors": [
                    {
                        "name": "Azuka Chiejina"
                    },
                    {
                        "name": "Subhramoy Mohanti"
                    },
                    {
                        "name": "Vijay K. Shah"
                    }
                ],
                "author_detail": {
                    "name": "Vijay K. Shah"
                },
                "author": "Vijay K. Shah",
                "arxiv_comment": "Accepted for publications at ICC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20313v1",
                "updated": "2025-02-27T17:39:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    39,
                    17,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:39:17Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    39,
                    17,
                    3,
                    58,
                    0
                ],
                "title": "FlexVAR: Flexible Visual Autoregressive Modeling without Residual\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexVAR: Flexible Visual Autoregressive Modeling without Residual\n  Prediction"
                },
                "summary": "This work challenges the residual prediction paradigm in visual\nautoregressive modeling and presents FlexVAR, a new Flexible Visual\nAutoRegressive image generation paradigm. FlexVAR facilitates autoregressive\nlearning with ground-truth prediction, enabling each step to independently\nproduce plausible images. This simple, intuitive approach swiftly learns visual\ndistributions and makes the generation process more flexible and adaptable.\nTrained solely on low-resolution images ($\\leq$ 256px), FlexVAR can: (1)\nGenerate images of various resolutions and aspect ratios, even exceeding the\nresolution of the training images. (2) Support various image-to-image tasks,\nincluding image refinement, in/out-painting, and image expansion. (3) Adapt to\nvarious autoregressive steps, allowing for faster inference with fewer steps or\nenhancing image quality with more steps. Our 1.0B model outperforms its VAR\ncounterpart on the ImageNet 256$\\times$256 benchmark. Moreover, when zero-shot\ntransfer the image generation process with 13 steps, the performance further\nimproves to 2.08 FID, outperforming state-of-the-art autoregressive models\nAiM/VAR by 0.25/0.28 FID and popular diffusion models LDM/DiT by 1.52/0.19 FID,\nrespectively. When transferring our 1.0B model to the ImageNet 512$\\times$512\nbenchmark in a zero-shot manner, FlexVAR achieves competitive results compared\nto the VAR 2.3B model, which is a fully supervised model trained at\n512$\\times$512 resolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work challenges the residual prediction paradigm in visual\nautoregressive modeling and presents FlexVAR, a new Flexible Visual\nAutoRegressive image generation paradigm. FlexVAR facilitates autoregressive\nlearning with ground-truth prediction, enabling each step to independently\nproduce plausible images. This simple, intuitive approach swiftly learns visual\ndistributions and makes the generation process more flexible and adaptable.\nTrained solely on low-resolution images ($\\leq$ 256px), FlexVAR can: (1)\nGenerate images of various resolutions and aspect ratios, even exceeding the\nresolution of the training images. (2) Support various image-to-image tasks,\nincluding image refinement, in/out-painting, and image expansion. (3) Adapt to\nvarious autoregressive steps, allowing for faster inference with fewer steps or\nenhancing image quality with more steps. Our 1.0B model outperforms its VAR\ncounterpart on the ImageNet 256$\\times$256 benchmark. Moreover, when zero-shot\ntransfer the image generation process with 13 steps, the performance further\nimproves to 2.08 FID, outperforming state-of-the-art autoregressive models\nAiM/VAR by 0.25/0.28 FID and popular diffusion models LDM/DiT by 1.52/0.19 FID,\nrespectively. When transferring our 1.0B model to the ImageNet 512$\\times$512\nbenchmark in a zero-shot manner, FlexVAR achieves competitive results compared\nto the VAR 2.3B model, which is a fully supervised model trained at\n512$\\times$512 resolution."
                },
                "authors": [
                    {
                        "name": "Siyu Jiao"
                    },
                    {
                        "name": "Gengwei Zhang"
                    },
                    {
                        "name": "Yinlong Qian"
                    },
                    {
                        "name": "Jiancheng Huang"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Humphrey Shi"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Yunchao Wei"
                    },
                    {
                        "name": "Zequn Jie"
                    }
                ],
                "author_detail": {
                    "name": "Zequn Jie"
                },
                "author": "Zequn Jie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20312v1",
                "updated": "2025-02-27T17:38:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    38,
                    38,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:38:38Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    38,
                    38,
                    3,
                    58,
                    0
                ],
                "title": "Inferring Black Hole Spin from Interferometric Measurements of the First\n  Photon Ring: A Geometric Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Black Hole Spin from Interferometric Measurements of the First\n  Photon Ring: A Geometric Approach"
                },
                "summary": "Accurately inferring black hole spin is crucial for understanding black hole\ndynamics and their astrophysical environments. In this work, we outline a\ngeometric method for spin estimation by using the interferometric shape of the\nfirst photon ring ($n=1$) as an approximation to the critical curve, which,\ngiven an assumed value of the black hole inclination, is then mapped to a spin\nvalue. While future space-based missions will capture a wealth of data on the\nfirst photon ring--including the full angle-dependent diameter, angular\nbrightness profile, and astrometric offset from the $n=0$ ring--our analysis is\nrestricted to using only two angle-dependent diameters to compute its shape\nasymmetry and infer spin. Focusing on low inclinations and moderate-to-high\nspins, we test the method across various emission models, baselines, and noise\nsources, including a mock space-based observation. Although the size of the\n$n=1$ ring depends on the emission model, its interferometric shape remains a\nrobust spin probe at low inclinations. We find that the inferred asymmetry of\nthe $n=1$ image may be biased by the critical curve morphology, and it can be\nheavily skewed by the presence of noise, whether astrophysical or instrumental.\nIn low-noise limits at low viewing inclination, significant contributions from\nthe n=0 image at short baselines may lead to a downward bias in asymmetry\nestimates. While our method can estimate high spins in noise-free time-averaged\nimages, increasing the noise and astrophysical variability degrades the\nresulting constraints, providing only lower bounds on the spin when applied to\nsynthetic observed data. Remarkably, even using only the ring's asymmetry, we\ncan establish lower bounds on the spin, underscoring the promise of photon\nring-based spin inference in future space-based very long baseline\ninterferometry (VLBI) missions, such as the proposed Black Hole Explorer\n(BHEX).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately inferring black hole spin is crucial for understanding black hole\ndynamics and their astrophysical environments. In this work, we outline a\ngeometric method for spin estimation by using the interferometric shape of the\nfirst photon ring ($n=1$) as an approximation to the critical curve, which,\ngiven an assumed value of the black hole inclination, is then mapped to a spin\nvalue. While future space-based missions will capture a wealth of data on the\nfirst photon ring--including the full angle-dependent diameter, angular\nbrightness profile, and astrometric offset from the $n=0$ ring--our analysis is\nrestricted to using only two angle-dependent diameters to compute its shape\nasymmetry and infer spin. Focusing on low inclinations and moderate-to-high\nspins, we test the method across various emission models, baselines, and noise\nsources, including a mock space-based observation. Although the size of the\n$n=1$ ring depends on the emission model, its interferometric shape remains a\nrobust spin probe at low inclinations. We find that the inferred asymmetry of\nthe $n=1$ image may be biased by the critical curve morphology, and it can be\nheavily skewed by the presence of noise, whether astrophysical or instrumental.\nIn low-noise limits at low viewing inclination, significant contributions from\nthe n=0 image at short baselines may lead to a downward bias in asymmetry\nestimates. While our method can estimate high spins in noise-free time-averaged\nimages, increasing the noise and astrophysical variability degrades the\nresulting constraints, providing only lower bounds on the spin when applied to\nsynthetic observed data. Remarkably, even using only the ring's asymmetry, we\ncan establish lower bounds on the spin, underscoring the promise of photon\nring-based spin inference in future space-based very long baseline\ninterferometry (VLBI) missions, such as the proposed Black Hole Explorer\n(BHEX)."
                },
                "authors": [
                    {
                        "name": "Lennox S. Keeble"
                    },
                    {
                        "name": "Alejandro Cárdenas-Avendaño"
                    },
                    {
                        "name": "Daniel C. M. Palumbo"
                    }
                ],
                "author_detail": {
                    "name": "Daniel C. M. Palumbo"
                },
                "author": "Daniel C. M. Palumbo",
                "arxiv_comment": "16+2 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20309v1",
                "updated": "2025-02-27T17:35:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    35,
                    57,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:35:57Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    35,
                    57,
                    3,
                    58,
                    0
                ],
                "title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific\n  Research Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific\n  Research Assistants"
                },
                "summary": "Recent advancements have positioned AI, and particularly Large Language\nModels (LLMs), as transformative tools for scientific research, capable of\naddressing complex tasks that require reasoning, problem-solving, and\ndecision-making. Their exceptional capabilities suggest their potential as\nscientific research assistants but also highlight the need for holistic,\nrigorous, and domain-specific evaluation to assess effectiveness in real-world\nscientific applications. This paper describes a multifaceted methodology for\nEvaluating AI models as scientific Research Assistants (EAIRA) developed at\nArgonne National Laboratory. This methodology incorporates four primary classes\nof evaluations. 1) Multiple Choice Questions to assess factual recall; 2) Open\nResponse to evaluate advanced reasoning and problem-solving skills; 3)\nLab-Style Experiments involving detailed analysis of capabilities as research\nassistants in controlled environments; and 4) Field-Style Experiments to\ncapture researcher-LLM interactions at scale in a wide range of scientific\ndomains and applications. These complementary methods enable a comprehensive\nanalysis of LLM strengths and weaknesses with respect to their scientific\nknowledge, reasoning abilities, and adaptability. Recognizing the rapid pace of\nLLM advancements, we designed the methodology to evolve and adapt so as to\nensure its continued relevance and applicability. This paper describes the\nmethodology state at the end of February 2025. Although developed within a\nsubset of scientific domains, the methodology is designed to be generalizable\nto a wide range of scientific domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements have positioned AI, and particularly Large Language\nModels (LLMs), as transformative tools for scientific research, capable of\naddressing complex tasks that require reasoning, problem-solving, and\ndecision-making. Their exceptional capabilities suggest their potential as\nscientific research assistants but also highlight the need for holistic,\nrigorous, and domain-specific evaluation to assess effectiveness in real-world\nscientific applications. This paper describes a multifaceted methodology for\nEvaluating AI models as scientific Research Assistants (EAIRA) developed at\nArgonne National Laboratory. This methodology incorporates four primary classes\nof evaluations. 1) Multiple Choice Questions to assess factual recall; 2) Open\nResponse to evaluate advanced reasoning and problem-solving skills; 3)\nLab-Style Experiments involving detailed analysis of capabilities as research\nassistants in controlled environments; and 4) Field-Style Experiments to\ncapture researcher-LLM interactions at scale in a wide range of scientific\ndomains and applications. These complementary methods enable a comprehensive\nanalysis of LLM strengths and weaknesses with respect to their scientific\nknowledge, reasoning abilities, and adaptability. Recognizing the rapid pace of\nLLM advancements, we designed the methodology to evolve and adapt so as to\nensure its continued relevance and applicability. This paper describes the\nmethodology state at the end of February 2025. Although developed within a\nsubset of scientific domains, the methodology is designed to be generalizable\nto a wide range of scientific domains."
                },
                "authors": [
                    {
                        "name": "Franck Cappello"
                    },
                    {
                        "name": "Sandeep Madireddy"
                    },
                    {
                        "name": "Robert Underwood"
                    },
                    {
                        "name": "Neil Getty"
                    },
                    {
                        "name": "Nicholas Lee-Ping Chia"
                    },
                    {
                        "name": "Nesar Ramachandra"
                    },
                    {
                        "name": "Josh Nguyen"
                    },
                    {
                        "name": "Murat Keceli"
                    },
                    {
                        "name": "Tanwi Mallick"
                    },
                    {
                        "name": "Zilinghan Li"
                    },
                    {
                        "name": "Marieme Ngom"
                    },
                    {
                        "name": "Chenhui Zhang"
                    },
                    {
                        "name": "Angel Yanguas-Gil"
                    },
                    {
                        "name": "Evan Antoniuk"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    },
                    {
                        "name": "Minyang Tian"
                    },
                    {
                        "name": "Yufeng Du"
                    },
                    {
                        "name": "Yuan-Sen Ting"
                    },
                    {
                        "name": "Azton Wells"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    },
                    {
                        "name": "Avinash Maurya"
                    },
                    {
                        "name": "M. Mustafa Rafique"
                    },
                    {
                        "name": "Eliu Huerta"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Ian Foster"
                    },
                    {
                        "name": "Rick Stevens"
                    }
                ],
                "author_detail": {
                    "name": "Rick Stevens"
                },
                "author": "Rick Stevens",
                "arxiv_comment": "33 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20307v1",
                "updated": "2025-02-27T17:33:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    33,
                    51,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:33:51Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    33,
                    51,
                    3,
                    58,
                    0
                ],
                "title": "Mobius: Text to Seamless Looping Video Generation via Latent Shift",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobius: Text to Seamless Looping Video Generation via Latent Shift"
                },
                "summary": "We present Mobius, a novel method to generate seamlessly looping videos from\ntext descriptions directly without any user annotations, thereby creating new\nvisual materials for the multi-media presentation. Our method repurposes the\npre-trained video latent diffusion model for generating looping videos from\ntext prompts without any training. During inference, we first construct a\nlatent cycle by connecting the starting and ending noise of the videos. Given\nthat the temporal consistency can be maintained by the context of the video\ndiffusion model, we perform multi-frame latent denoising by gradually shifting\nthe first-frame latent to the end in each step. As a result, the denoising\ncontext varies in each step while maintaining consistency throughout the\ninference process. Moreover, the latent cycle in our method can be of any\nlength. This extends our latent-shifting approach to generate seamless looping\nvideos beyond the scope of the video diffusion model's context. Unlike previous\ncinemagraphs, the proposed method does not require an image as appearance,\nwhich will restrict the motions of the generated results. Instead, our method\ncan produce more dynamic motion and better visual quality. We conduct multiple\nexperiments and comparisons to verify the effectiveness of the proposed method,\ndemonstrating its efficacy in different scenarios. All the code will be made\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Mobius, a novel method to generate seamlessly looping videos from\ntext descriptions directly without any user annotations, thereby creating new\nvisual materials for the multi-media presentation. Our method repurposes the\npre-trained video latent diffusion model for generating looping videos from\ntext prompts without any training. During inference, we first construct a\nlatent cycle by connecting the starting and ending noise of the videos. Given\nthat the temporal consistency can be maintained by the context of the video\ndiffusion model, we perform multi-frame latent denoising by gradually shifting\nthe first-frame latent to the end in each step. As a result, the denoising\ncontext varies in each step while maintaining consistency throughout the\ninference process. Moreover, the latent cycle in our method can be of any\nlength. This extends our latent-shifting approach to generate seamless looping\nvideos beyond the scope of the video diffusion model's context. Unlike previous\ncinemagraphs, the proposed method does not require an image as appearance,\nwhich will restrict the motions of the generated results. Instead, our method\ncan produce more dynamic motion and better visual quality. We conduct multiple\nexperiments and comparisons to verify the effectiveness of the proposed method,\ndemonstrating its efficacy in different scenarios. All the code will be made\navailable."
                },
                "authors": [
                    {
                        "name": "Xiuli Bi"
                    },
                    {
                        "name": "Jianfei Yuan"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Xiaodong Cun"
                    },
                    {
                        "name": "Chi-Man Pun"
                    },
                    {
                        "name": "Bin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Bin Xiao"
                },
                "author": "Bin Xiao",
                "arxiv_comment": "Project page: https://mobius-diffusion.github.io/ ; GitHub\n  repository: https://github.com/YisuiTT/Mobius",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20299v1",
                "updated": "2025-02-27T17:26:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    26,
                    56,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:26:56Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    26,
                    56,
                    3,
                    58,
                    0
                ],
                "title": "An exploration of features to improve the generalisability of fake news\n  detection models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An exploration of features to improve the generalisability of fake news\n  detection models"
                },
                "summary": "Fake news poses global risks by influencing elections and spreading\nmisinformation, making detection critical. Existing NLP and supervised Machine\nLearning methods perform well under cross-validation but struggle to generalise\nacross datasets, even within the same domain. This issue stems from coarsely\nlabelled training data, where articles are labelled based on their publisher,\nintroducing biases that token-based models like TF-IDF and BERT are sensitive\nto. While Large Language Models (LLMs) offer promise, their application in fake\nnews detection remains limited. This study demonstrates that meaningful\nfeatures can still be extracted from coarsely labelled data to improve\nreal-world robustness. Stylistic features-lexical, syntactic, and semantic-are\nexplored due to their reduced sensitivity to dataset biases. Additionally,\nnovel social-monetisation features are introduced, capturing economic\nincentives behind fake news, such as advertisements, external links, and social\nmedia elements. The study trains on the coarsely labelled NELA 2020-21 dataset\nand evaluates using the manually labelled Facebook URLs dataset, a gold\nstandard for generalisability. Results highlight the limitations of token-based\nmodels trained on biased data and contribute to the scarce evidence on LLMs\nlike LLaMa in this field. Findings indicate that stylistic and\nsocial-monetisation features offer more generalisable predictions than\ntoken-based methods and LLMs. Statistical and permutation feature importance\nanalyses further reveal their potential to enhance performance and mitigate\ndataset biases, providing a path forward for improving fake news detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fake news poses global risks by influencing elections and spreading\nmisinformation, making detection critical. Existing NLP and supervised Machine\nLearning methods perform well under cross-validation but struggle to generalise\nacross datasets, even within the same domain. This issue stems from coarsely\nlabelled training data, where articles are labelled based on their publisher,\nintroducing biases that token-based models like TF-IDF and BERT are sensitive\nto. While Large Language Models (LLMs) offer promise, their application in fake\nnews detection remains limited. This study demonstrates that meaningful\nfeatures can still be extracted from coarsely labelled data to improve\nreal-world robustness. Stylistic features-lexical, syntactic, and semantic-are\nexplored due to their reduced sensitivity to dataset biases. Additionally,\nnovel social-monetisation features are introduced, capturing economic\nincentives behind fake news, such as advertisements, external links, and social\nmedia elements. The study trains on the coarsely labelled NELA 2020-21 dataset\nand evaluates using the manually labelled Facebook URLs dataset, a gold\nstandard for generalisability. Results highlight the limitations of token-based\nmodels trained on biased data and contribute to the scarce evidence on LLMs\nlike LLaMa in this field. Findings indicate that stylistic and\nsocial-monetisation features offer more generalisable predictions than\ntoken-based methods and LLMs. Statistical and permutation feature importance\nanalyses further reveal their potential to enhance performance and mitigate\ndataset biases, providing a path forward for improving fake news detection."
                },
                "authors": [
                    {
                        "name": "Nathaniel Hoy"
                    },
                    {
                        "name": "Theodora Koulouri"
                    }
                ],
                "author_detail": {
                    "name": "Theodora Koulouri"
                },
                "author": "Theodora Koulouri",
                "arxiv_doi": "10.1016/j.eswa.2025.126949",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.eswa.2025.126949",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.20299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at Expert Systems with Applications (Elsevier)",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17925v2",
                "updated": "2025-02-27T17:26:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    26,
                    11,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-25T07:46:36Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    7,
                    46,
                    36,
                    1,
                    56,
                    0
                ],
                "title": "LeanProgress: Guiding Search for Neural Theorem Proving via Proof\n  Progress Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeanProgress: Guiding Search for Neural Theorem Proving via Proof\n  Progress Prediction"
                },
                "summary": "Mathematical reasoning remains a significant challenge for Large Language\nModels (LLMs) due to hallucinations. When combined with formal proof assistants\nlike Lean, these hallucinations can be eliminated through rigorous\nverification, making theorem proving reliable. However, even with formal\nverification, LLMs still struggle with long proofs and complex mathematical\nformalizations. While Lean with LLMs offers valuable assistance with retrieving\nlemmas, generating tactics, or even complete proofs, it lacks a crucial\ncapability: providing a sense of proof progress. This limitation particularly\nimpacts the overall development efficiency in large formalization projects. We\nintroduce LeanProgress, a method that predicts the progress in the proof.\nTraining and evaluating our models made on a large corpus of Lean proofs from\nLean Workbook Plus and Mathlib4 and how many steps remain to complete it, we\nemploy data preprocessing and balancing techniques to handle the skewed\ndistribution of proof lengths. Our experiments show that LeanProgress achieves\nan overall prediction accuracy of 75.1\\% in predicting the amount of progress\nand, hence, the remaining number of steps. When integrated into a best-first\nsearch framework using Reprover, our method shows a 3.8\\% improvement on\nMathlib4 compared to baseline performances of 41.2\\%, particularly for longer\nproofs. These results demonstrate how proof progress prediction can enhance\nboth automated and interactive theorem proving, enabling users to make more\ninformed decisions about proof strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical reasoning remains a significant challenge for Large Language\nModels (LLMs) due to hallucinations. When combined with formal proof assistants\nlike Lean, these hallucinations can be eliminated through rigorous\nverification, making theorem proving reliable. However, even with formal\nverification, LLMs still struggle with long proofs and complex mathematical\nformalizations. While Lean with LLMs offers valuable assistance with retrieving\nlemmas, generating tactics, or even complete proofs, it lacks a crucial\ncapability: providing a sense of proof progress. This limitation particularly\nimpacts the overall development efficiency in large formalization projects. We\nintroduce LeanProgress, a method that predicts the progress in the proof.\nTraining and evaluating our models made on a large corpus of Lean proofs from\nLean Workbook Plus and Mathlib4 and how many steps remain to complete it, we\nemploy data preprocessing and balancing techniques to handle the skewed\ndistribution of proof lengths. Our experiments show that LeanProgress achieves\nan overall prediction accuracy of 75.1\\% in predicting the amount of progress\nand, hence, the remaining number of steps. When integrated into a best-first\nsearch framework using Reprover, our method shows a 3.8\\% improvement on\nMathlib4 compared to baseline performances of 41.2\\%, particularly for longer\nproofs. These results demonstrate how proof progress prediction can enhance\nboth automated and interactive theorem proving, enabling users to make more\ninformed decisions about proof strategies."
                },
                "authors": [
                    {
                        "name": "Suozhi Huang"
                    },
                    {
                        "name": "Peiyang Song"
                    },
                    {
                        "name": "Robert Joseph George"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20295v1",
                "updated": "2025-02-27T17:21:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    21,
                    18,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:21:18Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    21,
                    18,
                    3,
                    58,
                    0
                ],
                "title": "Judge a Book by its Cover: Investigating Multi-Modal LLMs for Multi-Page\n  Handwritten Document Transcription",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judge a Book by its Cover: Investigating Multi-Modal LLMs for Multi-Page\n  Handwritten Document Transcription"
                },
                "summary": "Handwritten text recognition (HTR) remains a challenging task, particularly\nfor multi-page documents where pages share common formatting and contextual\nfeatures. While modern optical character recognition (OCR) engines are\nproficient with printed text, their performance on handwriting is limited,\noften requiring costly labeled data for fine-tuning. In this paper, we explore\nthe use of multi-modal large language models (MLLMs) for transcribing\nmulti-page handwritten documents in a zero-shot setting. We investigate various\nconfigurations of commercial OCR engines and MLLMs, utilizing the latter both\nas end-to-end transcribers and as post-processors, with and without image\ncomponents. We propose a novel method, '+first page', which enhances MLLM\ntranscription by providing the OCR output of the entire document along with\njust the first page image. This approach leverages shared document features\nwithout incurring the high cost of processing all images. Experiments on a\nmulti-page version of the IAM Handwriting Database demonstrate that '+first\npage' improves transcription accuracy, balances cost with performance, and even\nenhances results on out-of-sample text by extrapolating formatting and OCR\nerror patterns from a single page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handwritten text recognition (HTR) remains a challenging task, particularly\nfor multi-page documents where pages share common formatting and contextual\nfeatures. While modern optical character recognition (OCR) engines are\nproficient with printed text, their performance on handwriting is limited,\noften requiring costly labeled data for fine-tuning. In this paper, we explore\nthe use of multi-modal large language models (MLLMs) for transcribing\nmulti-page handwritten documents in a zero-shot setting. We investigate various\nconfigurations of commercial OCR engines and MLLMs, utilizing the latter both\nas end-to-end transcribers and as post-processors, with and without image\ncomponents. We propose a novel method, '+first page', which enhances MLLM\ntranscription by providing the OCR output of the entire document along with\njust the first page image. This approach leverages shared document features\nwithout incurring the high cost of processing all images. Experiments on a\nmulti-page version of the IAM Handwriting Database demonstrate that '+first\npage' improves transcription accuracy, balances cost with performance, and even\nenhances results on out-of-sample text by extrapolating formatting and OCR\nerror patterns from a single page."
                },
                "authors": [
                    {
                        "name": "Benjamin Gutteridge"
                    },
                    {
                        "name": "Matthew Thomas Jackson"
                    },
                    {
                        "name": "Toni Kukurin"
                    },
                    {
                        "name": "Xiaowen Dong"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Dong"
                },
                "author": "Xiaowen Dong",
                "arxiv_comment": "11 pages (including references and appendix), 14 figures, accepted at\n  AAAI-25 Workshop on Document Understanding and Intelligence, non-archival",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16660v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16660v2",
                "updated": "2025-02-27T17:17:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    17,
                    8,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-23T17:38:10Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    17,
                    38,
                    10,
                    6,
                    54,
                    0
                ],
                "title": "BioMaze: Benchmarking and Enhancing Large Language Models for Biological\n  Pathway Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BioMaze: Benchmarking and Enhancing Large Language Models for Biological\n  Pathway Reasoning"
                },
                "summary": "The applications of large language models (LLMs) in various biological\ndomains have been explored recently, but their reasoning ability in complex\nbiological systems, such as pathways, remains underexplored, which is crucial\nfor predicting biological phenomena, formulating hypotheses, and designing\nexperiments. This work explores the potential of LLMs in pathway reasoning. We\nintroduce BioMaze, a dataset with 5.1K complex pathway problems derived from\nreal research, covering various biological contexts including natural dynamic\nchanges, disturbances, additional intervention conditions, and multi-scale\nresearch targets. Our evaluation of methods such as CoT and graph-augmented\nreasoning, shows that LLMs struggle with pathway reasoning, especially in\nperturbed systems. To address this, we propose PathSeeker, an LLM agent that\nenhances reasoning through interactive subgraph-based navigation, enabling a\nmore effective approach to handling the complexities of biological systems in a\nscientifically aligned manner. The dataset and code are available at\nhttps://github.com/zhao-ht/BioMaze.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The applications of large language models (LLMs) in various biological\ndomains have been explored recently, but their reasoning ability in complex\nbiological systems, such as pathways, remains underexplored, which is crucial\nfor predicting biological phenomena, formulating hypotheses, and designing\nexperiments. This work explores the potential of LLMs in pathway reasoning. We\nintroduce BioMaze, a dataset with 5.1K complex pathway problems derived from\nreal research, covering various biological contexts including natural dynamic\nchanges, disturbances, additional intervention conditions, and multi-scale\nresearch targets. Our evaluation of methods such as CoT and graph-augmented\nreasoning, shows that LLMs struggle with pathway reasoning, especially in\nperturbed systems. To address this, we propose PathSeeker, an LLM agent that\nenhances reasoning through interactive subgraph-based navigation, enabling a\nmore effective approach to handling the complexities of biological systems in a\nscientifically aligned manner. The dataset and code are available at\nhttps://github.com/zhao-ht/BioMaze."
                },
                "authors": [
                    {
                        "name": "Haiteng Zhao"
                    },
                    {
                        "name": "Chang Ma"
                    },
                    {
                        "name": "Fangzhi Xu"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Zhi-Hong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhi-Hong Deng"
                },
                "author": "Zhi-Hong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16660v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16660v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06423v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06423v4",
                "updated": "2025-02-27T17:15:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    15,
                    49,
                    3,
                    58,
                    0
                ],
                "published": "2024-07-08T22:06:09Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    22,
                    6,
                    9,
                    0,
                    190,
                    0
                ],
                "title": "InsightBench: Evaluating Business Analytics Agents Through Multi-Step\n  Insight Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InsightBench: Evaluating Business Analytics Agents Through Multi-Step\n  Insight Generation"
                },
                "summary": "Data analytics is essential for extracting valuable insights from data that\ncan assist organizations in making effective decisions. We introduce\nInsightBench, a benchmark dataset with three key features. First, it consists\nof 100 datasets representing diverse business use cases such as finance and\nincident management, each accompanied by a carefully curated set of insights\nplanted in the datasets. Second, unlike existing benchmarks focusing on\nanswering single queries, InsightBench evaluates agents based on their ability\nto perform end-to-end data analytics, including formulating questions,\ninterpreting answers, and generating a summary of insights and actionable\nsteps. Third, we conducted comprehensive quality assurance to ensure that each\ndataset in the benchmark had clear goals and included relevant and meaningful\nquestions and analysis. Furthermore, we implement a two-way evaluation\nmechanism using LLaMA-3 as an effective, open-source evaluator to assess\nagents' ability to extract insights. We also propose AgentPoirot, our baseline\ndata analysis agent capable of performing end-to-end data analytics. Our\nevaluation on InsightBench shows that AgentPoirot outperforms existing\napproaches (such as Pandas Agent) that focus on resolving single queries. We\nalso compare the performance of open- and closed-source LLMs and various\nevaluation strategies. Overall, this benchmark serves as a testbed to motivate\nfurther development in comprehensive automated data analytics and can be\naccessed here: https://github.com/ServiceNow/insight-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data analytics is essential for extracting valuable insights from data that\ncan assist organizations in making effective decisions. We introduce\nInsightBench, a benchmark dataset with three key features. First, it consists\nof 100 datasets representing diverse business use cases such as finance and\nincident management, each accompanied by a carefully curated set of insights\nplanted in the datasets. Second, unlike existing benchmarks focusing on\nanswering single queries, InsightBench evaluates agents based on their ability\nto perform end-to-end data analytics, including formulating questions,\ninterpreting answers, and generating a summary of insights and actionable\nsteps. Third, we conducted comprehensive quality assurance to ensure that each\ndataset in the benchmark had clear goals and included relevant and meaningful\nquestions and analysis. Furthermore, we implement a two-way evaluation\nmechanism using LLaMA-3 as an effective, open-source evaluator to assess\nagents' ability to extract insights. We also propose AgentPoirot, our baseline\ndata analysis agent capable of performing end-to-end data analytics. Our\nevaluation on InsightBench shows that AgentPoirot outperforms existing\napproaches (such as Pandas Agent) that focus on resolving single queries. We\nalso compare the performance of open- and closed-source LLMs and various\nevaluation strategies. Overall, this benchmark serves as a testbed to motivate\nfurther development in comprehensive automated data analytics and can be\naccessed here: https://github.com/ServiceNow/insight-bench."
                },
                "authors": [
                    {
                        "name": "Gaurav Sahu"
                    },
                    {
                        "name": "Abhay Puri"
                    },
                    {
                        "name": "Juan Rodriguez"
                    },
                    {
                        "name": "Amirhossein Abaskohi"
                    },
                    {
                        "name": "Mohammad Chegini"
                    },
                    {
                        "name": "Alexandre Drouin"
                    },
                    {
                        "name": "Perouz Taslakian"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "Alexandre Lacoste"
                    },
                    {
                        "name": "David Vazquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Sai Rajeswar Mudumba"
                    },
                    {
                        "name": "Issam Hadj Laradji"
                    }
                ],
                "author_detail": {
                    "name": "Issam Hadj Laradji"
                },
                "author": "Issam Hadj Laradji",
                "arxiv_comment": "Accepted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06423v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06423v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20285v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20285v1",
                "updated": "2025-02-27T17:10:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    10,
                    54,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:10:54Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    10,
                    54,
                    3,
                    58,
                    0
                ],
                "title": "Conformal Tail Risk Control for Large Language Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Tail Risk Control for Large Language Model Alignment"
                },
                "summary": "Recent developments in large language models (LLMs) have led to their\nwidespread usage for various tasks. The prevalence of LLMs in society implores\nthe assurance on the reliability of their performance. In particular,\nrisk-sensitive applications demand meticulous attention to unexpectedly poor\noutcomes, i.e., tail events, for instance, toxic answers, humiliating language,\nand offensive outputs. Due to the costly nature of acquiring human annotations,\ngeneral-purpose scoring models have been created to automate the process of\nquantifying these tail events. This phenomenon introduces potential\nhuman-machine misalignment between the respective scoring mechanisms. In this\nwork, we present a lightweight calibration framework for blackbox models that\nensures the alignment of humans and machines with provable guarantees. Our\nframework provides a rigorous approach to controlling any distortion risk\nmeasure that is characterized by a weighted average of quantiles of the loss\nincurred by the LLM with high confidence. The theoretical foundation of our\nmethod relies on the connection between conformal risk control and a\ntraditional family of statistics, i.e., L-statistics. To demonstrate the\nutility of our framework, we conduct comprehensive experiments that address the\nissue of human-machine misalignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in large language models (LLMs) have led to their\nwidespread usage for various tasks. The prevalence of LLMs in society implores\nthe assurance on the reliability of their performance. In particular,\nrisk-sensitive applications demand meticulous attention to unexpectedly poor\noutcomes, i.e., tail events, for instance, toxic answers, humiliating language,\nand offensive outputs. Due to the costly nature of acquiring human annotations,\ngeneral-purpose scoring models have been created to automate the process of\nquantifying these tail events. This phenomenon introduces potential\nhuman-machine misalignment between the respective scoring mechanisms. In this\nwork, we present a lightweight calibration framework for blackbox models that\nensures the alignment of humans and machines with provable guarantees. Our\nframework provides a rigorous approach to controlling any distortion risk\nmeasure that is characterized by a weighted average of quantiles of the loss\nincurred by the LLM with high confidence. The theoretical foundation of our\nmethod relies on the connection between conformal risk control and a\ntraditional family of statistics, i.e., L-statistics. To demonstrate the\nutility of our framework, we conduct comprehensive experiments that address the\nissue of human-machine misalignment."
                },
                "authors": [
                    {
                        "name": "Catherine Yu-Chi Chen"
                    },
                    {
                        "name": "Jingyan Shen"
                    },
                    {
                        "name": "Zhun Deng"
                    },
                    {
                        "name": "Lihua Lei"
                    }
                ],
                "author_detail": {
                    "name": "Lihua Lei"
                },
                "author": "Lihua Lei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20285v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20284v1",
                "updated": "2025-02-27T17:10:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    10,
                    52,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:10:52Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    10,
                    52,
                    3,
                    58,
                    0
                ],
                "title": "Evaluating Human Trust in LLM-Based Planners: A Preliminary Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Human Trust in LLM-Based Planners: A Preliminary Study"
                },
                "summary": "Large Language Models (LLMs) are increasingly used for planning tasks,\noffering unique capabilities not found in classical planners such as generating\nexplanations and iterative refinement. However, trust--a critical factor in the\nadoption of planning systems--remains underexplored in the context of LLM-based\nplanning tasks. This study bridges this gap by comparing human trust in\nLLM-based planners with classical planners through a user study in a Planning\nDomain Definition Language (PDDL) domain. Combining subjective measures, such\nas trust questionnaires, with objective metrics like evaluation accuracy, our\nfindings reveal that correctness is the primary driver of trust and\nperformance. Explanations provided by the LLM improved evaluation accuracy but\nhad limited impact on trust, while plan refinement showed potential for\nincreasing trust without significantly enhancing evaluation accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used for planning tasks,\noffering unique capabilities not found in classical planners such as generating\nexplanations and iterative refinement. However, trust--a critical factor in the\nadoption of planning systems--remains underexplored in the context of LLM-based\nplanning tasks. This study bridges this gap by comparing human trust in\nLLM-based planners with classical planners through a user study in a Planning\nDomain Definition Language (PDDL) domain. Combining subjective measures, such\nas trust questionnaires, with objective metrics like evaluation accuracy, our\nfindings reveal that correctness is the primary driver of trust and\nperformance. Explanations provided by the LLM improved evaluation accuracy but\nhad limited impact on trust, while plan refinement showed potential for\nincreasing trust without significantly enhancing evaluation accuracy."
                },
                "authors": [
                    {
                        "name": "Shenghui Chen"
                    },
                    {
                        "name": "Yunhao Yang"
                    },
                    {
                        "name": "Kayla Boggess"
                    },
                    {
                        "name": "Seongkook Heo"
                    },
                    {
                        "name": "Lu Feng"
                    },
                    {
                        "name": "Ufuk Topcu"
                    }
                ],
                "author_detail": {
                    "name": "Ufuk Topcu"
                },
                "author": "Ufuk Topcu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20280v1",
                "updated": "2025-02-27T17:08:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    8,
                    31,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:08:31Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    8,
                    31,
                    3,
                    58,
                    0
                ],
                "title": "A review of Bayesian sensor-based estimation and uncertainty\n  quantification of aerodynamic flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A review of Bayesian sensor-based estimation and uncertainty\n  quantification of aerodynamic flows"
                },
                "summary": "Many applications in aerodynamics depend on the use of sensors to estimate\nthe evolving state of the flow. In particular, a wide variety of traditional\nand learning-based strategies for closed-loop control rely on some knowledge of\nthe aerodynamic state in order to decide on actions. This estimation task is\ninherently accompanied by uncertainty due to the noisy measurements of sensors\nor the non-uniqueness of the underlying mapping, and knowledge of this\nuncertainty can be as important for decision-making as that of the state\nitself. The tracking of uncertainty is challenged by the often-nonlinear\nrelationship between the sensor measurements and the flow state. For example, a\ncollection of passing vortices leaves a footprint in wall pressure sensors that\ndepends nonlinearly on the strengths and positions of the vortices. In this\npaper, we will review the recent body of work on flow estimation. We will\ndiscuss the basic tools of probability, including sampling and estimation, in\nthe powerful setting of Bayesian inference and demonstrate these tools in\nstatic flow estimation examples. We will then proceed to unsteady examples and\nillustrate the application of sequential estimation, and particularly, the\nensemble Kalman filter. Finally, we will discuss uncertainty quantification in\nneural network approximations of the mappings between sensor measurements and\nflow states. Recent aerodynamic applications of neural networks have shown that\nthe flow state can be encoded into a very low-dimensional latent space, and we\nwill discuss the implications of this encoding on uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications in aerodynamics depend on the use of sensors to estimate\nthe evolving state of the flow. In particular, a wide variety of traditional\nand learning-based strategies for closed-loop control rely on some knowledge of\nthe aerodynamic state in order to decide on actions. This estimation task is\ninherently accompanied by uncertainty due to the noisy measurements of sensors\nor the non-uniqueness of the underlying mapping, and knowledge of this\nuncertainty can be as important for decision-making as that of the state\nitself. The tracking of uncertainty is challenged by the often-nonlinear\nrelationship between the sensor measurements and the flow state. For example, a\ncollection of passing vortices leaves a footprint in wall pressure sensors that\ndepends nonlinearly on the strengths and positions of the vortices. In this\npaper, we will review the recent body of work on flow estimation. We will\ndiscuss the basic tools of probability, including sampling and estimation, in\nthe powerful setting of Bayesian inference and demonstrate these tools in\nstatic flow estimation examples. We will then proceed to unsteady examples and\nillustrate the application of sequential estimation, and particularly, the\nensemble Kalman filter. Finally, we will discuss uncertainty quantification in\nneural network approximations of the mappings between sensor measurements and\nflow states. Recent aerodynamic applications of neural networks have shown that\nthe flow state can be encoded into a very low-dimensional latent space, and we\nwill discuss the implications of this encoding on uncertainty."
                },
                "authors": [
                    {
                        "name": "Jeff D. Eldredge"
                    },
                    {
                        "name": "Hanieh Mousavi"
                    }
                ],
                "author_detail": {
                    "name": "Hanieh Mousavi"
                },
                "author": "Hanieh Mousavi",
                "arxiv_comment": "57 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15120v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15120v2",
                "updated": "2025-02-27T17:01:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    1,
                    51,
                    3,
                    58,
                    0
                ],
                "published": "2024-05-24T00:26:54Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    0,
                    26,
                    54,
                    4,
                    145,
                    0
                ],
                "title": "A Counterfactual Analysis of the Dishonest Casino",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Counterfactual Analysis of the Dishonest Casino"
                },
                "summary": "The dishonest casino is a well-known hidden Markov model (HMM) used in\neducational settings to introduce HMMs and graphical models. Here, a sequence\nof die rolls is observed, with the casino switching between a fair and a loaded\ndie. Typically, the goal is to use the observed rolls to infer the pattern of\nfair and loaded dice, leading to filtering, smoothing, and Viterbi algorithms.\nThis paper, however, explores how much of the winnings is attributable to the\ncasino's cheating, a counterfactual question beyond the scope of HMM\nprimitives. To address this, we introduce a structural causal model (SCM)\nconsistent with the HMM and show the expected winnings attributable to cheating\n(EWAC) (which is only partially identifiable) can be bounded using linear\nprograms (LPs). Through numerical experiments, we compute these bounds and\ndevelop intuition using benchmark SCMs based on independence, comonotonic, and\ncounter-monotonic copulas. We show that tighter bounds are obtained with a\ntime-homogeneity condition on the SCM, while looser bounds allow for an almost\nexplicit LP solution. Domain-specific knowledge such as pathwise monotonicity\nor counterfactual stability can be incorporated via linear constraints. We also\nshow the time-average EWAC is fully identifiable in the limit as the number of\ntime periods goes to infinity. Our work contributes to bounding counterfactuals\nin causal inference and is the first to develop LP bounds in a dynamic HMM\nsetting, benefiting educational contexts where counterfactual inference is\ntaught.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dishonest casino is a well-known hidden Markov model (HMM) used in\neducational settings to introduce HMMs and graphical models. Here, a sequence\nof die rolls is observed, with the casino switching between a fair and a loaded\ndie. Typically, the goal is to use the observed rolls to infer the pattern of\nfair and loaded dice, leading to filtering, smoothing, and Viterbi algorithms.\nThis paper, however, explores how much of the winnings is attributable to the\ncasino's cheating, a counterfactual question beyond the scope of HMM\nprimitives. To address this, we introduce a structural causal model (SCM)\nconsistent with the HMM and show the expected winnings attributable to cheating\n(EWAC) (which is only partially identifiable) can be bounded using linear\nprograms (LPs). Through numerical experiments, we compute these bounds and\ndevelop intuition using benchmark SCMs based on independence, comonotonic, and\ncounter-monotonic copulas. We show that tighter bounds are obtained with a\ntime-homogeneity condition on the SCM, while looser bounds allow for an almost\nexplicit LP solution. Domain-specific knowledge such as pathwise monotonicity\nor counterfactual stability can be incorporated via linear constraints. We also\nshow the time-average EWAC is fully identifiable in the limit as the number of\ntime periods goes to infinity. Our work contributes to bounding counterfactuals\nin causal inference and is the first to develop LP bounds in a dynamic HMM\nsetting, benefiting educational contexts where counterfactual inference is\ntaught."
                },
                "authors": [
                    {
                        "name": "Martin Haugh"
                    },
                    {
                        "name": "Raghav Singal"
                    }
                ],
                "author_detail": {
                    "name": "Raghav Singal"
                },
                "author": "Raghav Singal",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2205.13832",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15120v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15120v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14739v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14739v2",
                "updated": "2025-02-27T17:01:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    1,
                    9,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-20T17:05:58Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    5,
                    58,
                    3,
                    51,
                    0
                ],
                "title": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable proficiency in\nmainstream academic disciplines such as mathematics, physics, and computer\nscience. However, human knowledge encompasses over 200 specialized disciplines,\nfar exceeding the scope of existing benchmarks. The capabilities of LLMs in\nmany of these specialized fields-particularly in light industry, agriculture,\nand service-oriented disciplines-remain inadequately evaluated. To address this\ngap, we present SuperGPQA, a comprehensive benchmark that evaluates\ngraduate-level knowledge and reasoning capabilities across 285 disciplines. Our\nbenchmark employs a novel Human-LLM collaborative filtering mechanism to\neliminate trivial or ambiguous questions through iterative refinement based on\nboth LLM responses and expert feedback. Our experimental results reveal\nsignificant room for improvement in the performance of current state-of-the-art\nLLMs across diverse knowledge domains (e.g., the reasoning-focused model\nDeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting\nthe considerable gap between current model capabilities and artificial general\nintelligence. Additionally, we present comprehensive insights from our\nmanagement of a large-scale annotation process, involving over 80 expert\nannotators and an interactive Human-LLM collaborative system, offering valuable\nmethodological guidance for future research initiatives of comparable scope.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable proficiency in\nmainstream academic disciplines such as mathematics, physics, and computer\nscience. However, human knowledge encompasses over 200 specialized disciplines,\nfar exceeding the scope of existing benchmarks. The capabilities of LLMs in\nmany of these specialized fields-particularly in light industry, agriculture,\nand service-oriented disciplines-remain inadequately evaluated. To address this\ngap, we present SuperGPQA, a comprehensive benchmark that evaluates\ngraduate-level knowledge and reasoning capabilities across 285 disciplines. Our\nbenchmark employs a novel Human-LLM collaborative filtering mechanism to\neliminate trivial or ambiguous questions through iterative refinement based on\nboth LLM responses and expert feedback. Our experimental results reveal\nsignificant room for improvement in the performance of current state-of-the-art\nLLMs across diverse knowledge domains (e.g., the reasoning-focused model\nDeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting\nthe considerable gap between current model capabilities and artificial general\nintelligence. Additionally, we present comprehensive insights from our\nmanagement of a large-scale annotation process, involving over 80 expert\nannotators and an interactive Human-LLM collaborative system, offering valuable\nmethodological guidance for future research initiatives of comparable scope."
                },
                "authors": [
                    {
                        "name": "M-A-P Team"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Yifan Yao"
                    },
                    {
                        "name": "Kaijing Ma"
                    },
                    {
                        "name": "Bingli Wang"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Kang Zhu"
                    },
                    {
                        "name": "Minghao Liu"
                    },
                    {
                        "name": "Yiming Liang"
                    },
                    {
                        "name": "Xiaolong Jin"
                    },
                    {
                        "name": "Zhenlin Wei"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Kaixin Deng"
                    },
                    {
                        "name": "Shian Jia"
                    },
                    {
                        "name": "Sichao Jiang"
                    },
                    {
                        "name": "Yiyan Liao"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Qinrui Li"
                    },
                    {
                        "name": "Sirun Li"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Yunwen Li"
                    },
                    {
                        "name": "Dehua Ma"
                    },
                    {
                        "name": "Yuansheng Ni"
                    },
                    {
                        "name": "Haoran Que"
                    },
                    {
                        "name": "Qiyao Wang"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Siwei Wu"
                    },
                    {
                        "name": "Tianshun Xing"
                    },
                    {
                        "name": "Ming Xu"
                    },
                    {
                        "name": "Zhenzhu Yang"
                    },
                    {
                        "name": "Zekun Moore Wang"
                    },
                    {
                        "name": "Junting Zhou"
                    },
                    {
                        "name": "Yuelin Bai"
                    },
                    {
                        "name": "Xingyuan Bu"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Yifan Chen"
                    },
                    {
                        "name": "Chengtuo Cheng"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Keyi Ding"
                    },
                    {
                        "name": "Siming Huang"
                    },
                    {
                        "name": "Yun Huang"
                    },
                    {
                        "name": "Yaoru Li"
                    },
                    {
                        "name": "Yizhe Li"
                    },
                    {
                        "name": "Zhaoqun Li"
                    },
                    {
                        "name": "Tianhao Liang"
                    },
                    {
                        "name": "Chengdong Lin"
                    },
                    {
                        "name": "Hongquan Lin"
                    },
                    {
                        "name": "Yinghao Ma"
                    },
                    {
                        "name": "Tianyang Pang"
                    },
                    {
                        "name": "Zhongyuan Peng"
                    },
                    {
                        "name": "Zifan Peng"
                    },
                    {
                        "name": "Qige Qi"
                    },
                    {
                        "name": "Shi Qiu"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Shanghaoran Quan"
                    },
                    {
                        "name": "Yizhou Tan"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Chenqing Wang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Yiya Wang"
                    },
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Jiajun Xu"
                    },
                    {
                        "name": "Kexin Yang"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Yuanhao Yue"
                    },
                    {
                        "name": "Tianyang Zhan"
                    },
                    {
                        "name": "Chun Zhang"
                    },
                    {
                        "name": "Jinyang Zhang"
                    },
                    {
                        "name": "Xiyue Zhang"
                    },
                    {
                        "name": "Xingjian Zhang"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yongchi Zhao"
                    },
                    {
                        "name": "Xiangyu Zheng"
                    },
                    {
                        "name": "Chenghua Zhong"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Zhoujun Li"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Shiwen Ni"
                    },
                    {
                        "name": "Junran Peng"
                    },
                    {
                        "name": "Yujia Qin"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Shi Wang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Qunshu Lin"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Ge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Zhang"
                },
                "author": "Ge Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14739v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14739v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20268v1",
                "updated": "2025-02-27T16:55:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    55,
                    18,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T16:55:18Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    55,
                    18,
                    3,
                    58,
                    0
                ],
                "title": "Large Language Models as Attribution Regularizers for Efficient Model\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Attribution Regularizers for Efficient Model\n  Training"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse domains. However, effectively leveraging their vast knowledge for\ntraining smaller downstream models remains an open challenge, especially in\ndomains like tabular data learning, where simpler models are often preferred\ndue to interpretability and efficiency.\n  In this paper, we introduce a novel yet straightforward method for\nincorporating LLM-generated global task feature attributions into the training\nprocess of smaller networks. Specifically, we propose an attribution-matching\nregularization term that aligns the training dynamics of the smaller model with\nthe insights provided by the LLM. By doing so, our approach yields superior\nperformance in few-shot learning scenarios. Notably, our method requires only\nblack-box API access to the LLM, making it easy to integrate into existing\ntraining pipelines with minimal computational overhead.\n  Furthermore, we demonstrate how this method can be used to address common\nissues in real-world datasets, such as skewness and bias. By integrating\nhigh-level knowledge from LLMs, our approach improves generalization, even when\ntraining data is limited or imbalanced. We validate its effectiveness through\nextensive experiments across multiple tasks, demonstrating improved learning\nefficiency and model robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse domains. However, effectively leveraging their vast knowledge for\ntraining smaller downstream models remains an open challenge, especially in\ndomains like tabular data learning, where simpler models are often preferred\ndue to interpretability and efficiency.\n  In this paper, we introduce a novel yet straightforward method for\nincorporating LLM-generated global task feature attributions into the training\nprocess of smaller networks. Specifically, we propose an attribution-matching\nregularization term that aligns the training dynamics of the smaller model with\nthe insights provided by the LLM. By doing so, our approach yields superior\nperformance in few-shot learning scenarios. Notably, our method requires only\nblack-box API access to the LLM, making it easy to integrate into existing\ntraining pipelines with minimal computational overhead.\n  Furthermore, we demonstrate how this method can be used to address common\nissues in real-world datasets, such as skewness and bias. By integrating\nhigh-level knowledge from LLMs, our approach improves generalization, even when\ntraining data is limited or imbalanced. We validate its effectiveness through\nextensive experiments across multiple tasks, demonstrating improved learning\nefficiency and model robustness."
                },
                "authors": [
                    {
                        "name": "Davor Vukadin"
                    },
                    {
                        "name": "Marin Šilić"
                    },
                    {
                        "name": "Goran Delač"
                    }
                ],
                "author_detail": {
                    "name": "Goran Delač"
                },
                "author": "Goran Delač",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02408v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02408v2",
                "updated": "2025-02-27T16:53:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    53,
                    40,
                    3,
                    58,
                    0
                ],
                "published": "2024-10-18T15:22:07Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    15,
                    22,
                    7,
                    4,
                    292,
                    0
                ],
                "title": "AI on My Shoulder: Supporting Emotional Labor in Front-Office Roles with\n  an LLM-based Empathetic Coworker",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI on My Shoulder: Supporting Emotional Labor in Front-Office Roles with\n  an LLM-based Empathetic Coworker"
                },
                "summary": "Client-Service Representatives (CSRs) are vital to organizations. Frequent\ninteractions with disgruntled clients, however, disrupt their mental\nwell-being. To help CSRs regulate their emotions while interacting with uncivil\nclients, we designed Care-Pilot, an LLM-powered assistant, and evaluated its\nefficacy, perception, and use. Our comparative analyses between 665 human and\nCare-Pilot-generated support messages highlight Care-Pilot's ability to adapt\nto and demonstrate empathy in various incivility incidents. Additionally, 143\nCSRs assessed Care-Pilot's empathy as more sincere and actionable than human\nmessages. Finally, we interviewed 20 CSRs who interacted with Care-Pilot in a\nsimulation exercise. They reported that Care-Pilot helped them avoid negative\nthinking, recenter thoughts, and humanize clients; showing potential for\nbridging gaps in coworker support. Yet, they also noted deployment challenges\nand emphasized the indispensability of shared experiences. We discuss future\ndesigns and societal implications of AI-mediated emotional labor, underscoring\nempathy as a critical function for AI assistants for worker mental health.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-Service Representatives (CSRs) are vital to organizations. Frequent\ninteractions with disgruntled clients, however, disrupt their mental\nwell-being. To help CSRs regulate their emotions while interacting with uncivil\nclients, we designed Care-Pilot, an LLM-powered assistant, and evaluated its\nefficacy, perception, and use. Our comparative analyses between 665 human and\nCare-Pilot-generated support messages highlight Care-Pilot's ability to adapt\nto and demonstrate empathy in various incivility incidents. Additionally, 143\nCSRs assessed Care-Pilot's empathy as more sincere and actionable than human\nmessages. Finally, we interviewed 20 CSRs who interacted with Care-Pilot in a\nsimulation exercise. They reported that Care-Pilot helped them avoid negative\nthinking, recenter thoughts, and humanize clients; showing potential for\nbridging gaps in coworker support. Yet, they also noted deployment challenges\nand emphasized the indispensability of shared experiences. We discuss future\ndesigns and societal implications of AI-mediated emotional labor, underscoring\nempathy as a critical function for AI assistants for worker mental health."
                },
                "authors": [
                    {
                        "name": "Vedant Das Swain"
                    },
                    {
                        "name": "Qiuyue \"Joy\" Zhong"
                    },
                    {
                        "name": "Jash Rajesh Parekh"
                    },
                    {
                        "name": "Yechan Jeon"
                    },
                    {
                        "name": "Roy Zimmermann"
                    },
                    {
                        "name": "Mary Czerwinski"
                    },
                    {
                        "name": "Jina Suh"
                    },
                    {
                        "name": "Varun Mishra"
                    },
                    {
                        "name": "Koustuv Saha"
                    },
                    {
                        "name": "Javier Hernandez"
                    }
                ],
                "author_detail": {
                    "name": "Javier Hernandez"
                },
                "author": "Javier Hernandez",
                "arxiv_doi": "10.1145/3706598.3713705",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713705",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.02408v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02408v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "CHI Conference on Human Factors in Computing Systems (CHI '25),\n  April 26-May 1, 2025, Yokohama, Japan",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15291v2",
                "updated": "2025-02-27T16:47:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    47,
                    40,
                    3,
                    58,
                    0
                ],
                "published": "2024-12-19T07:10:51Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    7,
                    10,
                    51,
                    3,
                    354,
                    0
                ],
                "title": "A Large-Scale Simulation on Large Language Models for Decision-Making in\n  Political Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Large-Scale Simulation on Large Language Models for Decision-Making in\n  Political Science"
                },
                "summary": "While LLMs have demonstrated remarkable capabilities in text generation and\nreasoning, their ability to simulate human decision-making -- particularly in\npolitical contexts -- remains an open question. However, modeling voter\nbehavior presents unique challenges due to limited voter-level data, evolving\npolitical landscapes, and the complexity of human reasoning. In this study, we\ndevelop a theory-driven, multi-step reasoning framework that integrates\ndemographic, temporal and ideological factors to simulate voter decision-making\nat scale. Using synthetic personas calibrated to real-world voter data, we\nconduct large-scale simulations of recent U.S. presidential elections. Our\nmethod significantly improves simulation accuracy while mitigating model\nbiases. We examine its robustness by comparing performance across different\nLLMs. We further investigate the challenges and constraints that arise from\nLLM-based political simulations. Our work provides both a scalable framework\nfor modeling political decision-making behavior and insights into the promise\nand limitations of using LLMs in political science research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLMs have demonstrated remarkable capabilities in text generation and\nreasoning, their ability to simulate human decision-making -- particularly in\npolitical contexts -- remains an open question. However, modeling voter\nbehavior presents unique challenges due to limited voter-level data, evolving\npolitical landscapes, and the complexity of human reasoning. In this study, we\ndevelop a theory-driven, multi-step reasoning framework that integrates\ndemographic, temporal and ideological factors to simulate voter decision-making\nat scale. Using synthetic personas calibrated to real-world voter data, we\nconduct large-scale simulations of recent U.S. presidential elections. Our\nmethod significantly improves simulation accuracy while mitigating model\nbiases. We examine its robustness by comparing performance across different\nLLMs. We further investigate the challenges and constraints that arise from\nLLM-based political simulations. Our work provides both a scalable framework\nfor modeling political decision-making behavior and insights into the promise\nand limitations of using LLMs in political science research."
                },
                "authors": [
                    {
                        "name": "Chenxiao Yu"
                    },
                    {
                        "name": "Jinyi Ye"
                    },
                    {
                        "name": "Yuangang Li"
                    },
                    {
                        "name": "Zhaotian Weng"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Emilio Ferrara"
                    },
                    {
                        "name": "Xiyang Hu"
                    },
                    {
                        "name": "Yue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhao"
                },
                "author": "Yue Zhao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2411.03321 This\n  version adds a new model to our experimental setup, modifies the paper's main\n  discussion, and updates the authorship list",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20258v1",
                "updated": "2025-02-27T16:46:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    46,
                    23,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T16:46:23Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    46,
                    23,
                    3,
                    58,
                    0
                ],
                "title": "LLM as a Broken Telephone: Iterative Generation Distorts Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM as a Broken Telephone: Iterative Generation Distorts Information"
                },
                "summary": "As large language models are increasingly responsible for online content,\nconcerns arise about the impact of repeatedly processing their own outputs.\nInspired by the \"broken telephone\" effect in chained human communication, this\nstudy investigates whether LLMs similarly distort information through iterative\ngeneration. Through translation-based experiments, we find that distortion\naccumulates over time, influenced by language choice and chain complexity.\nWhile degradation is inevitable, it can be mitigated through strategic\nprompting techniques. These findings contribute to discussions on the long-term\neffects of AI-mediated information propagation, raising important questions\nabout the reliability of LLM-generated content in iterative workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models are increasingly responsible for online content,\nconcerns arise about the impact of repeatedly processing their own outputs.\nInspired by the \"broken telephone\" effect in chained human communication, this\nstudy investigates whether LLMs similarly distort information through iterative\ngeneration. Through translation-based experiments, we find that distortion\naccumulates over time, influenced by language choice and chain complexity.\nWhile degradation is inevitable, it can be mitigated through strategic\nprompting techniques. These findings contribute to discussions on the long-term\neffects of AI-mediated information propagation, raising important questions\nabout the reliability of LLM-generated content in iterative workflows."
                },
                "authors": [
                    {
                        "name": "Amr Mohamed"
                    },
                    {
                        "name": "Mingmeng Geng"
                    },
                    {
                        "name": "Michalis Vazirgiannis"
                    },
                    {
                        "name": "Guokan Shang"
                    }
                ],
                "author_detail": {
                    "name": "Guokan Shang"
                },
                "author": "Guokan Shang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20154v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20154v5",
                "updated": "2025-02-27T16:43:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    43,
                    27,
                    3,
                    58,
                    0
                ],
                "published": "2024-09-30T10:02:42Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    10,
                    2,
                    42,
                    0,
                    274,
                    0
                ],
                "title": "GravMAD: Grounded Spatial Value Maps Guided Action Diffusion for\n  Generalized 3D Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GravMAD: Grounded Spatial Value Maps Guided Action Diffusion for\n  Generalized 3D Manipulation"
                },
                "summary": "Robots' ability to follow language instructions and execute diverse 3D\nmanipulation tasks is vital in robot learning. Traditional imitation\nlearning-based methods perform well on seen tasks but struggle with novel,\nunseen ones due to variability. Recent approaches leverage large foundation\nmodels to assist in understanding novel tasks, thereby mitigating this issue.\nHowever, these methods lack a task-specific learning process, which is\nessential for an accurate understanding of 3D environments, often leading to\nexecution failures. In this paper, we introduce GravMAD, a sub-goal-driven,\nlanguage-conditioned action diffusion framework that combines the strengths of\nimitation learning and foundation models. Our approach breaks tasks into\nsub-goals based on language instructions, allowing auxiliary guidance during\nboth training and inference. During training, we introduce Sub-goal Keypose\nDiscovery to identify key sub-goals from demonstrations. Inference differs from\ntraining, as there are no demonstrations available, so we use pre-trained\nfoundation models to bridge the gap and identify sub-goals for the current\ntask. In both phases, GravMaps are generated from sub-goals, providing GravMAD\nwith more flexible 3D spatial guidance compared to fixed 3D positions.\nEmpirical evaluations on RLBench show that GravMAD significantly outperforms\nstate-of-the-art methods, with a 28.63% improvement on novel tasks and a 13.36%\ngain on tasks encountered during training. Evaluations on real-world robotic\ntasks further show that GravMAD can reason about real-world tasks, associate\nthem with relevant visual information, and generalize to novel tasks. These\nresults demonstrate GravMAD's strong multi-task learning and generalization in\n3D manipulation. Video demonstrations are available at:\nhttps://gravmad.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robots' ability to follow language instructions and execute diverse 3D\nmanipulation tasks is vital in robot learning. Traditional imitation\nlearning-based methods perform well on seen tasks but struggle with novel,\nunseen ones due to variability. Recent approaches leverage large foundation\nmodels to assist in understanding novel tasks, thereby mitigating this issue.\nHowever, these methods lack a task-specific learning process, which is\nessential for an accurate understanding of 3D environments, often leading to\nexecution failures. In this paper, we introduce GravMAD, a sub-goal-driven,\nlanguage-conditioned action diffusion framework that combines the strengths of\nimitation learning and foundation models. Our approach breaks tasks into\nsub-goals based on language instructions, allowing auxiliary guidance during\nboth training and inference. During training, we introduce Sub-goal Keypose\nDiscovery to identify key sub-goals from demonstrations. Inference differs from\ntraining, as there are no demonstrations available, so we use pre-trained\nfoundation models to bridge the gap and identify sub-goals for the current\ntask. In both phases, GravMaps are generated from sub-goals, providing GravMAD\nwith more flexible 3D spatial guidance compared to fixed 3D positions.\nEmpirical evaluations on RLBench show that GravMAD significantly outperforms\nstate-of-the-art methods, with a 28.63% improvement on novel tasks and a 13.36%\ngain on tasks encountered during training. Evaluations on real-world robotic\ntasks further show that GravMAD can reason about real-world tasks, associate\nthem with relevant visual information, and generalize to novel tasks. These\nresults demonstrate GravMAD's strong multi-task learning and generalization in\n3D manipulation. Video demonstrations are available at:\nhttps://gravmad.github.io."
                },
                "authors": [
                    {
                        "name": "Yangtao Chen"
                    },
                    {
                        "name": "Zixuan Chen"
                    },
                    {
                        "name": "Junhui Yin"
                    },
                    {
                        "name": "Jing Huo"
                    },
                    {
                        "name": "Pinzhuo Tian"
                    },
                    {
                        "name": "Jieqi Shi"
                    },
                    {
                        "name": "Yang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Gao"
                },
                "author": "Yang Gao",
                "arxiv_comment": "ICLR 2025. The first two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20154v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20154v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17787v2",
                "updated": "2025-02-27T16:42:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    42,
                    10,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-25T02:39:57Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    2,
                    39,
                    57,
                    1,
                    56,
                    0
                ],
                "title": "AIR: Complex Instruction Generation via Automatic Iterative Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIR: Complex Instruction Generation via Automatic Iterative Refinement"
                },
                "summary": "With the development of large language models, their ability to follow simple\ninstructions has significantly improved. However, adhering to complex\ninstructions remains a major challenge. Current approaches to generating\ncomplex instructions are often irrelevant to the current instruction\nrequirements or suffer from limited scalability and diversity. Moreover,\nmethods such as back-translation, while effective for simple instruction\ngeneration, fail to leverage the rich contents and structures in large web\ncorpora. In this paper, we propose a novel automatic iterative refinement\nframework to generate complex instructions with constraints, which not only\nbetter reflects the requirements of real scenarios but also significantly\nenhances LLMs' ability to follow complex instructions. The AIR framework\nconsists of two stages: (1)Generate an initial instruction from a document;\n(2)Iteratively refine instructions with LLM-as-judge guidance by comparing the\nmodel's output with the document to incorporate valuable constraints. Finally,\nwe construct the AIR-10K dataset with 10K complex instructions and demonstrate\nthat instructions generated with our approach significantly improve the model's\nability to follow complex instructions, outperforming existing methods for\ninstruction generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models, their ability to follow simple\ninstructions has significantly improved. However, adhering to complex\ninstructions remains a major challenge. Current approaches to generating\ncomplex instructions are often irrelevant to the current instruction\nrequirements or suffer from limited scalability and diversity. Moreover,\nmethods such as back-translation, while effective for simple instruction\ngeneration, fail to leverage the rich contents and structures in large web\ncorpora. In this paper, we propose a novel automatic iterative refinement\nframework to generate complex instructions with constraints, which not only\nbetter reflects the requirements of real scenarios but also significantly\nenhances LLMs' ability to follow complex instructions. The AIR framework\nconsists of two stages: (1)Generate an initial instruction from a document;\n(2)Iteratively refine instructions with LLM-as-judge guidance by comparing the\nmodel's output with the document to incorporate valuable constraints. Finally,\nwe construct the AIR-10K dataset with 10K complex instructions and demonstrate\nthat instructions generated with our approach significantly improve the model's\nability to follow complex instructions, outperforming existing methods for\ninstruction generation."
                },
                "authors": [
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Hui Huang"
                    },
                    {
                        "name": "Chengwei Hu"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Shilong Li"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "The first three authors contributed equally, 20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.01314v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.01314v3",
                "updated": "2025-02-27T16:36:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    36,
                    8,
                    3,
                    58,
                    0
                ],
                "published": "2023-11-02T15:31:12Z",
                "published_parsed": [
                    2023,
                    11,
                    2,
                    15,
                    31,
                    12,
                    3,
                    306,
                    0
                ],
                "title": "Recommendations by Concise User Profiles from Review Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommendations by Concise User Profiles from Review Text"
                },
                "summary": "Recommender systems perform well for popular items and users with ample\ninteractions (likes, ratings etc.). This work addresses the difficult and\nunderexplored case of users who have very sparse interactions but post\ninformative review texts. This setting naturally calls for encoding\nuser-specific text with large language models (LLM). However, feeding the full\ntext of all reviews through an LLM has a weak signal-to-noise ratio and incurs\nhigh costs of processed tokens. This paper addresses these two issues. It\npresents a light-weight framework, called CUP, which first computes concise\nuser profiles and feeds only these into the training of transformer-based\nrecommenders. For user profiles, we devise various techniques to select the\nmost informative cues from noisy reviews. Experiments, with book reviews data,\nshow that fine-tuning a small language model with judiciously constructed\nprofiles achieves the best performance, even in comparison to LLM-generated\nrankings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems perform well for popular items and users with ample\ninteractions (likes, ratings etc.). This work addresses the difficult and\nunderexplored case of users who have very sparse interactions but post\ninformative review texts. This setting naturally calls for encoding\nuser-specific text with large language models (LLM). However, feeding the full\ntext of all reviews through an LLM has a weak signal-to-noise ratio and incurs\nhigh costs of processed tokens. This paper addresses these two issues. It\npresents a light-weight framework, called CUP, which first computes concise\nuser profiles and feeds only these into the training of transformer-based\nrecommenders. For user profiles, we devise various techniques to select the\nmost informative cues from noisy reviews. Experiments, with book reviews data,\nshow that fine-tuning a small language model with judiciously constructed\nprofiles achieves the best performance, even in comparison to LLM-generated\nrankings."
                },
                "authors": [
                    {
                        "name": "Ghazaleh Haratinezhad Torbati"
                    },
                    {
                        "name": "Anna Tigunova"
                    },
                    {
                        "name": "Andrew Yates"
                    },
                    {
                        "name": "Gerhard Weikum"
                    }
                ],
                "author_detail": {
                    "name": "Gerhard Weikum"
                },
                "author": "Gerhard Weikum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.01314v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.01314v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19361v2",
                "updated": "2025-02-27T16:34:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    34,
                    4,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-26T17:59:27Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    59,
                    27,
                    2,
                    57,
                    0
                ],
                "title": "Can Large Language Models Detect Errors in Long Chain-of-Thought\n  Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Detect Errors in Long Chain-of-Thought\n  Reasoning?"
                },
                "summary": "Recently, o1-like models have drawn significant attention, where these models\nproduce the long Chain-of-Thought (CoT) reasoning steps to improve the\nreasoning abilities of existing Large Language Models (LLMs). In this paper, to\nunderstand the qualities of these long CoTs and measure the critique abilities\nof existing LLMs on these long CoTs, we introduce the DeltaBench, including the\ngenerated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for\ndifferent reasoning tasks (e.g., Math, Code, General Reasoning), to measure the\nability to detect errors in long CoT reasoning. Based on DeltaBench, we first\nperform fine-grained analysis of the generated long CoTs to discover the\neffectiveness and efficiency of different o1-like models. Then, we conduct\nextensive evaluations of existing process reward models (PRMs) and critic\nmodels to detect the errors of each annotated process, which aims to\ninvestigate the boundaries and limitations of existing PRMs and critic models.\nFinally, we hope that DeltaBench could guide developers to better understand\nthe long CoT reasoning abilities of their models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, o1-like models have drawn significant attention, where these models\nproduce the long Chain-of-Thought (CoT) reasoning steps to improve the\nreasoning abilities of existing Large Language Models (LLMs). In this paper, to\nunderstand the qualities of these long CoTs and measure the critique abilities\nof existing LLMs on these long CoTs, we introduce the DeltaBench, including the\ngenerated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for\ndifferent reasoning tasks (e.g., Math, Code, General Reasoning), to measure the\nability to detect errors in long CoT reasoning. Based on DeltaBench, we first\nperform fine-grained analysis of the generated long CoTs to discover the\neffectiveness and efficiency of different o1-like models. Then, we conduct\nextensive evaluations of existing process reward models (PRMs) and critic\nmodels to detect the errors of each annotated process, which aims to\ninvestigate the boundaries and limitations of existing PRMs and critic models.\nFinally, we hope that DeltaBench could guide developers to better understand\nthe long CoT reasoning abilities of their models."
                },
                "authors": [
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Shilong Li"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Weixun Wang"
                    },
                    {
                        "name": "Xingyuan Bu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Zhongyuan Peng"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Zhicheng Zheng"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "The first four authors contributed equally, 27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13156v2",
                "updated": "2025-02-27T16:30:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    30,
                    42,
                    3,
                    58,
                    0
                ],
                "published": "2024-09-20T01:46:07Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    1,
                    46,
                    7,
                    4,
                    264,
                    0
                ],
                "title": "RRM: Robust Reward Model Training Mitigates Reward Hacking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RRM: Robust Reward Model Training Mitigates Reward Hacking"
                },
                "summary": "Reward models (RMs) play a pivotal role in aligning large language models\n(LLMs) with human preferences. However, traditional RM training, which relies\non response pairs tied to specific prompts, struggles to disentangle\nprompt-driven preferences from prompt-independent artifacts, such as response\nlength and format. In this work, we expose a fundamental limitation of current\nRM training methods, where RMs fail to effectively distinguish between\ncontextual signals and irrelevant artifacts when determining preferences. To\naddress this, we introduce a causal framework that learns preferences\nindependent of these artifacts and propose a novel data augmentation technique\ndesigned to eliminate them. Extensive experiments show that our approach\nsuccessfully filters out undesirable artifacts, yielding a more robust reward\nmodel (RRM). Our RRM improves the performance of a pairwise reward model\ntrained on Gemma-2-9b-it, on RewardBench, increasing accuracy from 80.61% to\n84.15%. Additionally, we train two DPO policies using both the RM and RRM,\ndemonstrating that the RRM significantly enhances DPO-aligned policies,\nimproving MT-Bench scores from 7.27 to 8.31 and length-controlled win-rates in\nAlpacaEval-2 from 33.46% to 52.49%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models (RMs) play a pivotal role in aligning large language models\n(LLMs) with human preferences. However, traditional RM training, which relies\non response pairs tied to specific prompts, struggles to disentangle\nprompt-driven preferences from prompt-independent artifacts, such as response\nlength and format. In this work, we expose a fundamental limitation of current\nRM training methods, where RMs fail to effectively distinguish between\ncontextual signals and irrelevant artifacts when determining preferences. To\naddress this, we introduce a causal framework that learns preferences\nindependent of these artifacts and propose a novel data augmentation technique\ndesigned to eliminate them. Extensive experiments show that our approach\nsuccessfully filters out undesirable artifacts, yielding a more robust reward\nmodel (RRM). Our RRM improves the performance of a pairwise reward model\ntrained on Gemma-2-9b-it, on RewardBench, increasing accuracy from 80.61% to\n84.15%. Additionally, we train two DPO policies using both the RM and RRM,\ndemonstrating that the RRM significantly enhances DPO-aligned policies,\nimproving MT-Bench scores from 7.27 to 8.31 and length-controlled win-rates in\nAlpacaEval-2 from 33.46% to 52.49%."
                },
                "authors": [
                    {
                        "name": "Tianqi Liu"
                    },
                    {
                        "name": "Wei Xiong"
                    },
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Lichang Chen"
                    },
                    {
                        "name": "Junru Wu"
                    },
                    {
                        "name": "Rishabh Joshi"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Tianhe Yu"
                    },
                    {
                        "name": "Daniel Sohn"
                    },
                    {
                        "name": "Anastasiia Makarova"
                    },
                    {
                        "name": "Jeremiah Liu"
                    },
                    {
                        "name": "Yuan Liu"
                    },
                    {
                        "name": "Bilal Piot"
                    },
                    {
                        "name": "Abe Ittycheriah"
                    },
                    {
                        "name": "Aviral Kumar"
                    },
                    {
                        "name": "Mohammad Saleh"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Saleh"
                },
                "author": "Mohammad Saleh",
                "arxiv_comment": "Accepted in ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20246v1",
                "updated": "2025-02-27T16:30:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    30,
                    0,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T16:30:00Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    30,
                    0,
                    3,
                    58,
                    0
                ],
                "title": "Beyond Natural Language Perplexity: Detecting Dead Code Poisoning in\n  Code Generation Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Natural Language Perplexity: Detecting Dead Code Poisoning in\n  Code Generation Datasets"
                },
                "summary": "The increasing adoption of large language models (LLMs) for code-related\ntasks has raised concerns about the security of their training datasets. One\ncritical threat is dead code poisoning, where syntactically valid but\nfunctionally redundant code is injected into training data to manipulate model\nbehavior. Such attacks can degrade the performance of neural code search\nsystems, leading to biased or insecure code suggestions. Existing detection\nmethods, such as token-level perplexity analysis, fail to effectively identify\ndead code due to the structural and contextual characteristics of programming\nlanguages. In this paper, we propose DePA (Dead Code Perplexity Analysis), a\nnovel line-level detection and cleansing method tailored to the structural\nproperties of code. DePA computes line-level perplexity by leveraging the\ncontextual relationships between code lines and identifies anomalous lines by\ncomparing their perplexity to the overall distribution within the file. Our\nexperiments on benchmark datasets demonstrate that DePA significantly\noutperforms existing methods, achieving 0.14-0.19 improvement in detection\nF1-score and a 44-65% increase in poisoned segment localization precision.\nFurthermore, DePA enhances detection speed by 0.62-23x, making it practical for\nlarge-scale dataset cleansing. Overall, by addressing the unique challenges of\ndead code poisoning, DePA provides a robust and efficient solution for\nsafeguarding the integrity of code generation model training datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of large language models (LLMs) for code-related\ntasks has raised concerns about the security of their training datasets. One\ncritical threat is dead code poisoning, where syntactically valid but\nfunctionally redundant code is injected into training data to manipulate model\nbehavior. Such attacks can degrade the performance of neural code search\nsystems, leading to biased or insecure code suggestions. Existing detection\nmethods, such as token-level perplexity analysis, fail to effectively identify\ndead code due to the structural and contextual characteristics of programming\nlanguages. In this paper, we propose DePA (Dead Code Perplexity Analysis), a\nnovel line-level detection and cleansing method tailored to the structural\nproperties of code. DePA computes line-level perplexity by leveraging the\ncontextual relationships between code lines and identifies anomalous lines by\ncomparing their perplexity to the overall distribution within the file. Our\nexperiments on benchmark datasets demonstrate that DePA significantly\noutperforms existing methods, achieving 0.14-0.19 improvement in detection\nF1-score and a 44-65% increase in poisoned segment localization precision.\nFurthermore, DePA enhances detection speed by 0.62-23x, making it practical for\nlarge-scale dataset cleansing. Overall, by addressing the unique challenges of\ndead code poisoning, DePA provides a robust and efficient solution for\nsafeguarding the integrity of code generation model training datasets."
                },
                "authors": [
                    {
                        "name": "Chichien Tsai"
                    },
                    {
                        "name": "Chiamu Yu"
                    },
                    {
                        "name": "Yingdar Lin"
                    },
                    {
                        "name": "Yusung Wu"
                    },
                    {
                        "name": "Weibin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Weibin Lee"
                },
                "author": "Weibin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20238v1",
                "updated": "2025-02-27T16:23:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    23,
                    25,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T16:23:25Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    23,
                    25,
                    3,
                    58,
                    0
                ],
                "title": "FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through\n  Reflective Puzzle Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through\n  Reflective Puzzle Solving"
                },
                "summary": "Many challenging reasoning tasks require not just rapid, intuitive responses,\nbut a more deliberate, multi-step approach. Recent progress in large language\nmodels (LLMs) highlights an important shift from the \"System 1\" way of quick\nreactions to the \"System 2\" style of reflection-and-correction problem solving.\nHowever, current benchmarks heavily rely on the final-answer accuracy, leaving\nmuch of a model's intermediate reasoning steps unexamined. This fails to assess\nthe model's ability to reflect and rectify mistakes within the reasoning\nprocess. To bridge this gap, we introduce FINEREASON, a logic-puzzle benchmark\nfor fine-grained evaluation of LLMs' reasoning capabilities. Each puzzle can be\ndecomposed into atomic steps, making it ideal for rigorous validation of\nintermediate correctness. Building on this, we introduce two tasks: state\nchecking, and state transition, for a comprehensive evaluation of how models\nassess the current situation and plan the next move. To support broader\nresearch, we also provide a puzzle training set aimed at enhancing performance\non general mathematical tasks. We show that models trained on our state\nchecking and transition data demonstrate gains in math reasoning by up to 5.1%\non GSM8K.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many challenging reasoning tasks require not just rapid, intuitive responses,\nbut a more deliberate, multi-step approach. Recent progress in large language\nmodels (LLMs) highlights an important shift from the \"System 1\" way of quick\nreactions to the \"System 2\" style of reflection-and-correction problem solving.\nHowever, current benchmarks heavily rely on the final-answer accuracy, leaving\nmuch of a model's intermediate reasoning steps unexamined. This fails to assess\nthe model's ability to reflect and rectify mistakes within the reasoning\nprocess. To bridge this gap, we introduce FINEREASON, a logic-puzzle benchmark\nfor fine-grained evaluation of LLMs' reasoning capabilities. Each puzzle can be\ndecomposed into atomic steps, making it ideal for rigorous validation of\nintermediate correctness. Building on this, we introduce two tasks: state\nchecking, and state transition, for a comprehensive evaluation of how models\nassess the current situation and plan the next move. To support broader\nresearch, we also provide a puzzle training set aimed at enhancing performance\non general mathematical tasks. We show that models trained on our state\nchecking and transition data demonstrate gains in math reasoning by up to 5.1%\non GSM8K."
                },
                "authors": [
                    {
                        "name": "Guizhen Chen"
                    },
                    {
                        "name": "Weiwen Xu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Hou Pong Chan"
                    },
                    {
                        "name": "Chaoqun Liu"
                    },
                    {
                        "name": "Lidong Bing"
                    },
                    {
                        "name": "Deli Zhao"
                    },
                    {
                        "name": "Anh Tuan Luu"
                    },
                    {
                        "name": "Yu Rong"
                    }
                ],
                "author_detail": {
                    "name": "Yu Rong"
                },
                "author": "Yu Rong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20231v1",
                "updated": "2025-02-27T16:16:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    16,
                    37,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T16:16:37Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    16,
                    37,
                    3,
                    58,
                    0
                ],
                "title": "AI Will Always Love You: Studying Implicit Biases in Romantic AI\n  Companions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Will Always Love You: Studying Implicit Biases in Romantic AI\n  Companions"
                },
                "summary": "While existing studies have recognised explicit biases in generative models,\nincluding occupational gender biases, the nuances of gender stereotypes and\nexpectations of relationships between users and AI companions remain\nunderexplored. In the meantime, AI companions have become increasingly popular\nas friends or gendered romantic partners to their users. This study bridges the\ngap by devising three experiments tailored for romantic, gender-assigned AI\ncompanions and their users, effectively evaluating implicit biases across\nvarious-sized LLMs. Each experiment looks at a different dimension: implicit\nassociations, emotion responses, and sycophancy. This study aims to measure and\ncompare biases manifested in different companion systems by quantitatively\nanalysing persona-assigned model responses to a baseline through newly devised\nmetrics. The results are noteworthy: they show that assigning gendered,\nrelationship personas to Large Language Models significantly alters the\nresponses of these models, and in certain situations in a biased, stereotypical\nway.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While existing studies have recognised explicit biases in generative models,\nincluding occupational gender biases, the nuances of gender stereotypes and\nexpectations of relationships between users and AI companions remain\nunderexplored. In the meantime, AI companions have become increasingly popular\nas friends or gendered romantic partners to their users. This study bridges the\ngap by devising three experiments tailored for romantic, gender-assigned AI\ncompanions and their users, effectively evaluating implicit biases across\nvarious-sized LLMs. Each experiment looks at a different dimension: implicit\nassociations, emotion responses, and sycophancy. This study aims to measure and\ncompare biases manifested in different companion systems by quantitatively\nanalysing persona-assigned model responses to a baseline through newly devised\nmetrics. The results are noteworthy: they show that assigning gendered,\nrelationship personas to Large Language Models significantly alters the\nresponses of these models, and in certain situations in a biased, stereotypical\nway."
                },
                "authors": [
                    {
                        "name": "Clare Grogan"
                    },
                    {
                        "name": "Jackie Kay"
                    },
                    {
                        "name": "María Pérez-Ortiz"
                    }
                ],
                "author_detail": {
                    "name": "María Pérez-Ortiz"
                },
                "author": "María Pérez-Ortiz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20226v1",
                "updated": "2025-02-27T16:12:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    12,
                    52,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T16:12:52Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    12,
                    52,
                    3,
                    58,
                    0
                ],
                "title": "Inferring the Equation of State from Neutron Star Observables via\n  Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring the Equation of State from Neutron Star Observables via\n  Machine Learning"
                },
                "summary": "We have conducted an extensive study using a diverse set of equations of\nstate (EoSs) to uncover strong relationships between neutron star (NS)\nobservables and the underlying EoS parameters using symbolic regression method.\nThese EoS models, derived from a mix of agnostic and physics-based approaches,\nconsidered neutron stars composed of nucleons, hyperons, and other exotic\ndegrees of freedom in beta equilibrium. The maximum mass of a NS is found to be\nstrongly correlated with the pressure and baryon density at an energy density\nof approximately 800 MeV.fm$^{-3}$. We have also demonstrated that the EoS can\nbe expressed as a function of radius and tidal deformability within the NS mass\nrange 1-2$M_\\odot$. These insights offer a promising and efficient framework to\ndecode the dense matter EoS directly from the accurate knowledge of NS\nobservables.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have conducted an extensive study using a diverse set of equations of\nstate (EoSs) to uncover strong relationships between neutron star (NS)\nobservables and the underlying EoS parameters using symbolic regression method.\nThese EoS models, derived from a mix of agnostic and physics-based approaches,\nconsidered neutron stars composed of nucleons, hyperons, and other exotic\ndegrees of freedom in beta equilibrium. The maximum mass of a NS is found to be\nstrongly correlated with the pressure and baryon density at an energy density\nof approximately 800 MeV.fm$^{-3}$. We have also demonstrated that the EoS can\nbe expressed as a function of radius and tidal deformability within the NS mass\nrange 1-2$M_\\odot$. These insights offer a promising and efficient framework to\ndecode the dense matter EoS directly from the accurate knowledge of NS\nobservables."
                },
                "authors": [
                    {
                        "name": "N. K. Patra"
                    },
                    {
                        "name": "Tuhin Malik"
                    },
                    {
                        "name": "Helena Pais"
                    },
                    {
                        "name": "Kai Zhou"
                    },
                    {
                        "name": "B. K. Agrawal"
                    },
                    {
                        "name": "Constança Providência"
                    }
                ],
                "author_detail": {
                    "name": "Constança Providência"
                },
                "author": "Constança Providência",
                "arxiv_comment": "10 pages, 5 figures, 2 tables (All comments are welcome)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04332v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04332v3",
                "updated": "2025-02-27T16:08:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    8,
                    51,
                    3,
                    58,
                    0
                ],
                "published": "2024-12-05T16:48:16Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    48,
                    16,
                    3,
                    340,
                    0
                ],
                "title": "Liquid: Language Models are Scalable and Unified Multi-modal Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Liquid: Language Models are Scalable and Unified Multi-modal Generators"
                },
                "summary": "We present Liquid, an auto-regressive generation paradigm that seamlessly\nintegrates visual comprehension and generation by tokenizing images into\ndiscrete codes and learning these code embeddings alongside text tokens within\na shared feature space for both vision and language. Unlike previous multimodal\nlarge language model (MLLM), Liquid achieves this integration using a single\nlarge language model (LLM), eliminating the need for external pretrained visual\nembeddings such as CLIP. For the first time, Liquid uncovers a scaling law that\nperformance drop unavoidably brought by the unified training of visual and\nlanguage tasks diminishes as the model size increases. Furthermore, the unified\ntoken space enables visual generation and comprehension tasks to mutually\nenhance each other, effectively removing the typical interference seen in\nearlier models. We show that existing LLMs can serve as strong foundations for\nLiquid, saving 100x in training costs while outperforming Chameleon in\nmultimodal capabilities and maintaining language performance comparable to\nmainstream LLMs like LLAMA2. Liquid also outperforms models like SD v2.1 and\nSD-XL (FID of 5.47 on MJHQ-30K), excelling in both vision-language and\ntext-only tasks. This work demonstrates that LLMs such as Qwen2.5 and GEMMA2\nare powerful multimodal generators, offering a scalable solution for enhancing\nboth vision-language understanding and generation. The code and models will be\nreleased at https://github.com/FoundationVision/Liquid.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Liquid, an auto-regressive generation paradigm that seamlessly\nintegrates visual comprehension and generation by tokenizing images into\ndiscrete codes and learning these code embeddings alongside text tokens within\na shared feature space for both vision and language. Unlike previous multimodal\nlarge language model (MLLM), Liquid achieves this integration using a single\nlarge language model (LLM), eliminating the need for external pretrained visual\nembeddings such as CLIP. For the first time, Liquid uncovers a scaling law that\nperformance drop unavoidably brought by the unified training of visual and\nlanguage tasks diminishes as the model size increases. Furthermore, the unified\ntoken space enables visual generation and comprehension tasks to mutually\nenhance each other, effectively removing the typical interference seen in\nearlier models. We show that existing LLMs can serve as strong foundations for\nLiquid, saving 100x in training costs while outperforming Chameleon in\nmultimodal capabilities and maintaining language performance comparable to\nmainstream LLMs like LLAMA2. Liquid also outperforms models like SD v2.1 and\nSD-XL (FID of 5.47 on MJHQ-30K), excelling in both vision-language and\ntext-only tasks. This work demonstrates that LLMs such as Qwen2.5 and GEMMA2\nare powerful multimodal generators, offering a scalable solution for enhancing\nboth vision-language understanding and generation. The code and models will be\nreleased at https://github.com/FoundationVision/Liquid."
                },
                "authors": [
                    {
                        "name": "Junfeng Wu"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Chuofan Ma"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Zehuan Yuan"
                    },
                    {
                        "name": "Song Bai"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "arxiv_comment": "Technical report. Project page:\n  https://foundationvision.github.io/Liquid/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04332v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04332v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05014v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05014v4",
                "updated": "2025-02-27T16:06:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    6,
                    25,
                    3,
                    58,
                    0
                ],
                "published": "2023-10-08T05:13:25Z",
                "published_parsed": [
                    2023,
                    10,
                    8,
                    5,
                    13,
                    25,
                    6,
                    281,
                    0
                ],
                "title": "Congruence Closure Modulo Groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Congruence Closure Modulo Groups"
                },
                "summary": "This paper presents a new framework for constructing congruence closure of a\nfinite set of ground equations over uninterpreted symbols and interpreted\nsymbols for the group axioms. In this framework, ground equations are flattened\ninto certain forms by introducing new constants, and a completion procedure is\nperformed on ground flat equations. The proposed completion procedure uses\nequational inference rules and constructs a ground convergent rewrite system\nfor congruence closure with such interpreted symbols. If the completion\nprocedure terminates, then it yields a decision procedure for the word problem\nfor a finite set of ground equations with respect to the group axioms. This\npaper also provides a sufficient terminating condition of the completion\nprocedure for constructing a ground convergent rewrite system from ground flat\nequations containing interpreted symbols for the group axioms. In addition,\nthis paper presents a new method for constructing congruence closure of a\nfinite set of ground equations containing interpreted symbols for the\nsemigroup, monoid, and the multiple disjoint sets of group axioms,\nrespectively, using the proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new framework for constructing congruence closure of a\nfinite set of ground equations over uninterpreted symbols and interpreted\nsymbols for the group axioms. In this framework, ground equations are flattened\ninto certain forms by introducing new constants, and a completion procedure is\nperformed on ground flat equations. The proposed completion procedure uses\nequational inference rules and constructs a ground convergent rewrite system\nfor congruence closure with such interpreted symbols. If the completion\nprocedure terminates, then it yields a decision procedure for the word problem\nfor a finite set of ground equations with respect to the group axioms. This\npaper also provides a sufficient terminating condition of the completion\nprocedure for constructing a ground convergent rewrite system from ground flat\nequations containing interpreted symbols for the group axioms. In addition,\nthis paper presents a new method for constructing congruence closure of a\nfinite set of ground equations containing interpreted symbols for the\nsemigroup, monoid, and the multiple disjoint sets of group axioms,\nrespectively, using the proposed framework."
                },
                "authors": [
                    {
                        "name": "Dohan Kim"
                    }
                ],
                "author_detail": {
                    "name": "Dohan Kim"
                },
                "author": "Dohan Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.05014v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05014v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17732v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17732v2",
                "updated": "2025-02-27T16:05:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    5,
                    40,
                    3,
                    58,
                    0
                ],
                "published": "2024-11-22T11:18:26Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    11,
                    18,
                    26,
                    4,
                    327,
                    0
                ],
                "title": "CheckMate: LLM-Powered Approximate Intermittent Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CheckMate: LLM-Powered Approximate Intermittent Computing"
                },
                "summary": "Batteryless IoT systems face energy constraints exacerbated by checkpointing\noverhead. Approximate computing offers solutions but demands manual expertise,\nlimiting scalability. This paper presents CheckMate, an automated framework\nleveraging LLMs for context-aware code approximations. CheckMate integrates\nvalidation of LLM-generated approximations to ensure correct execution and\nemploys Bayesian optimization to fine-tune approximation parameters\nautonomously, eliminating the need for developer input. Tested across six IoT\napplications, it reduces power cycles by up to 60% with an accuracy loss of\njust 8%, outperforming semi-automated tools like ACCEPT in speedup and\naccuracy. CheckMate's results establish it as a robust, user-friendly tool and\na foundational step toward automated approximation frameworks for intermittent\ncomputing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batteryless IoT systems face energy constraints exacerbated by checkpointing\noverhead. Approximate computing offers solutions but demands manual expertise,\nlimiting scalability. This paper presents CheckMate, an automated framework\nleveraging LLMs for context-aware code approximations. CheckMate integrates\nvalidation of LLM-generated approximations to ensure correct execution and\nemploys Bayesian optimization to fine-tune approximation parameters\nautonomously, eliminating the need for developer input. Tested across six IoT\napplications, it reduces power cycles by up to 60% with an accuracy loss of\njust 8%, outperforming semi-automated tools like ACCEPT in speedup and\naccuracy. CheckMate's results establish it as a robust, user-friendly tool and\na foundational step toward automated approximation frameworks for intermittent\ncomputing."
                },
                "authors": [
                    {
                        "name": "Abdur-Rahman Ibrahim Sayyid-Ali"
                    },
                    {
                        "name": "Abdul Rafay"
                    },
                    {
                        "name": "Muhammad Abdullah Soomro"
                    },
                    {
                        "name": "Muhammad Hamad Alizai"
                    },
                    {
                        "name": "Naveed Anwar Bhatti"
                    }
                ],
                "author_detail": {
                    "name": "Naveed Anwar Bhatti"
                },
                "author": "Naveed Anwar Bhatti",
                "arxiv_comment": "Accepted in SenSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17732v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17732v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20220v1",
                "updated": "2025-02-27T16:00:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    0,
                    11,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T16:00:11Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    0,
                    11,
                    3,
                    58,
                    0
                ],
                "title": "Avat3r: Large Animatable Gaussian Reconstruction Model for High-fidelity\n  3D Head Avatars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Avat3r: Large Animatable Gaussian Reconstruction Model for High-fidelity\n  3D Head Avatars"
                },
                "summary": "Traditionally, creating photo-realistic 3D head avatars requires a\nstudio-level multi-view capture setup and expensive optimization during\ntest-time, limiting the use of digital human doubles to the VFX industry or\noffline renderings.\n  To address this shortcoming, we present Avat3r, which regresses a\nhigh-quality and animatable 3D head avatar from just a few input images, vastly\nreducing compute requirements during inference. More specifically, we make\nLarge Reconstruction Models animatable and learn a powerful prior over 3D human\nheads from a large multi-view video dataset. For better 3D head\nreconstructions, we employ position maps from DUSt3R and generalized feature\nmaps from the human foundation model Sapiens. To animate the 3D head, our key\ndiscovery is that simple cross-attention to an expression code is already\nsufficient. Finally, we increase robustness by feeding input images with\ndifferent expressions to our model during training, enabling the reconstruction\nof 3D head avatars from inconsistent inputs, e.g., an imperfect phone capture\nwith accidental movement, or frames from a monocular video.\n  We compare Avat3r with current state-of-the-art methods for few-input and\nsingle-input scenarios, and find that our method has a competitive advantage in\nboth tasks. Finally, we demonstrate the wide applicability of our proposed\nmodel, creating 3D head avatars from images of different sources, smartphone\ncaptures, single images, and even out-of-domain inputs like antique busts.\n  Project website: https://tobias-kirschstein.github.io/avat3r/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditionally, creating photo-realistic 3D head avatars requires a\nstudio-level multi-view capture setup and expensive optimization during\ntest-time, limiting the use of digital human doubles to the VFX industry or\noffline renderings.\n  To address this shortcoming, we present Avat3r, which regresses a\nhigh-quality and animatable 3D head avatar from just a few input images, vastly\nreducing compute requirements during inference. More specifically, we make\nLarge Reconstruction Models animatable and learn a powerful prior over 3D human\nheads from a large multi-view video dataset. For better 3D head\nreconstructions, we employ position maps from DUSt3R and generalized feature\nmaps from the human foundation model Sapiens. To animate the 3D head, our key\ndiscovery is that simple cross-attention to an expression code is already\nsufficient. Finally, we increase robustness by feeding input images with\ndifferent expressions to our model during training, enabling the reconstruction\nof 3D head avatars from inconsistent inputs, e.g., an imperfect phone capture\nwith accidental movement, or frames from a monocular video.\n  We compare Avat3r with current state-of-the-art methods for few-input and\nsingle-input scenarios, and find that our method has a competitive advantage in\nboth tasks. Finally, we demonstrate the wide applicability of our proposed\nmodel, creating 3D head avatars from images of different sources, smartphone\ncaptures, single images, and even out-of-domain inputs like antique busts.\n  Project website: https://tobias-kirschstein.github.io/avat3r/"
                },
                "authors": [
                    {
                        "name": "Tobias Kirschstein"
                    },
                    {
                        "name": "Javier Romero"
                    },
                    {
                        "name": "Artem Sevastopolsky"
                    },
                    {
                        "name": "Matthias Nießner"
                    },
                    {
                        "name": "Shunsuke Saito"
                    }
                ],
                "author_detail": {
                    "name": "Shunsuke Saito"
                },
                "author": "Shunsuke Saito",
                "arxiv_comment": "Project website: https://tobias-kirschstein.github.io/avat3r/, Video:\n  https://youtu.be/P3zNVx15gYs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20196v1",
                "updated": "2025-02-27T15:36:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    36,
                    0,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T15:36:00Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    36,
                    0,
                    3,
                    58,
                    0
                ],
                "title": "ChineseEcomQA: A Scalable E-commerce Concept Evaluation Benchmark for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChineseEcomQA: A Scalable E-commerce Concept Evaluation Benchmark for\n  Large Language Models"
                },
                "summary": "With the increasing use of Large Language Models (LLMs) in fields such as\ne-commerce, domain-specific concept evaluation benchmarks are crucial for\nassessing their domain capabilities. Existing LLMs may generate factually\nincorrect information within the complex e-commerce applications. Therefore, it\nis necessary to build an e-commerce concept benchmark. Existing benchmarks\nencounter two primary challenges: (1) handle the heterogeneous and diverse\nnature of tasks, (2) distinguish between generality and specificity within the\ne-commerce field. To address these problems, we propose \\textbf{ChineseEcomQA},\na scalable question-answering benchmark focused on fundamental e-commerce\nconcepts. ChineseEcomQA is built on three core characteristics: \\textbf{Focus\non Fundamental Concept}, \\textbf{E-commerce Generality} and \\textbf{E-commerce\nExpertise}. Fundamental concepts are designed to be applicable across a diverse\narray of e-commerce tasks, thus addressing the challenge of heterogeneity and\ndiversity. Additionally, by carefully balancing generality and specificity,\nChineseEcomQA effectively differentiates between broad e-commerce concepts,\nallowing for precise validation of domain capabilities. We achieve this through\na scalable benchmark construction process that combines LLM validation,\nRetrieval-Augmented Generation (RAG) validation, and rigorous manual\nannotation. Based on ChineseEcomQA, we conduct extensive evaluations on\nmainstream LLMs and provide some valuable insights. We hope that ChineseEcomQA\ncould guide future domain-specific evaluations, and facilitate broader LLM\nadoption in e-commerce applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing use of Large Language Models (LLMs) in fields such as\ne-commerce, domain-specific concept evaluation benchmarks are crucial for\nassessing their domain capabilities. Existing LLMs may generate factually\nincorrect information within the complex e-commerce applications. Therefore, it\nis necessary to build an e-commerce concept benchmark. Existing benchmarks\nencounter two primary challenges: (1) handle the heterogeneous and diverse\nnature of tasks, (2) distinguish between generality and specificity within the\ne-commerce field. To address these problems, we propose \\textbf{ChineseEcomQA},\na scalable question-answering benchmark focused on fundamental e-commerce\nconcepts. ChineseEcomQA is built on three core characteristics: \\textbf{Focus\non Fundamental Concept}, \\textbf{E-commerce Generality} and \\textbf{E-commerce\nExpertise}. Fundamental concepts are designed to be applicable across a diverse\narray of e-commerce tasks, thus addressing the challenge of heterogeneity and\ndiversity. Additionally, by carefully balancing generality and specificity,\nChineseEcomQA effectively differentiates between broad e-commerce concepts,\nallowing for precise validation of domain capabilities. We achieve this through\na scalable benchmark construction process that combines LLM validation,\nRetrieval-Augmented Generation (RAG) validation, and rigorous manual\nannotation. Based on ChineseEcomQA, we conduct extensive evaluations on\nmainstream LLMs and provide some valuable insights. We hope that ChineseEcomQA\ncould guide future domain-specific evaluations, and facilitate broader LLM\nadoption in e-commerce applications."
                },
                "authors": [
                    {
                        "name": "Haibin Chen"
                    },
                    {
                        "name": "Kangtao Lv"
                    },
                    {
                        "name": "Chengwei Hu"
                    },
                    {
                        "name": "Yanshi Li"
                    },
                    {
                        "name": "Yujin Yuan"
                    },
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Xingyao Zhang"
                    },
                    {
                        "name": "Langming Liu"
                    },
                    {
                        "name": "Shilei Liu"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15495v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15495v3",
                "updated": "2025-02-27T15:30:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    30,
                    47,
                    3,
                    58,
                    0
                ],
                "published": "2024-08-28T02:45:41Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    2,
                    45,
                    41,
                    2,
                    241,
                    0
                ],
                "title": "Remove Symmetries to Control Model Expressivity and Improve Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remove Symmetries to Control Model Expressivity and Improve Optimization"
                },
                "summary": "When symmetry is present in the loss function, the model is likely to be\ntrapped in a low-capacity state that is sometimes known as a \"collapse\". Being\ntrapped in these low-capacity states can be a major obstacle to training across\nmany scenarios where deep learning technology is applied. We first prove two\nconcrete mechanisms through which symmetries lead to reduced capacities and\nignored features during training and inference. We then propose a simple and\ntheoretically justified algorithm, syre, to remove almost all symmetry-induced\nlow-capacity states in neural networks. When this type of entrapment is\nespecially a concern, removing symmetries with the proposed method is shown to\ncorrelate well with improved optimization or performance. A remarkable merit of\nthe proposed method is that it is model-agnostic and does not require any\nknowledge of the symmetry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When symmetry is present in the loss function, the model is likely to be\ntrapped in a low-capacity state that is sometimes known as a \"collapse\". Being\ntrapped in these low-capacity states can be a major obstacle to training across\nmany scenarios where deep learning technology is applied. We first prove two\nconcrete mechanisms through which symmetries lead to reduced capacities and\nignored features during training and inference. We then propose a simple and\ntheoretically justified algorithm, syre, to remove almost all symmetry-induced\nlow-capacity states in neural networks. When this type of entrapment is\nespecially a concern, removing symmetries with the proposed method is shown to\ncorrelate well with improved optimization or performance. A remarkable merit of\nthe proposed method is that it is model-agnostic and does not require any\nknowledge of the symmetry."
                },
                "authors": [
                    {
                        "name": "Liu Ziyin"
                    },
                    {
                        "name": "Yizhou Xu"
                    },
                    {
                        "name": "Isaac Chuang"
                    }
                ],
                "author_detail": {
                    "name": "Isaac Chuang"
                },
                "author": "Isaac Chuang",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15495v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15495v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v2",
                "updated": "2025-02-27T15:29:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    29,
                    3,
                    3,
                    58,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09886v2",
                "updated": "2025-02-27T15:24:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    24,
                    27,
                    3,
                    58,
                    0
                ],
                "published": "2024-08-19T11:01:00Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    11,
                    1,
                    0,
                    0,
                    232,
                    0
                ],
                "title": "Improved Baselines with Synchronized Encoding for Universal Medical\n  Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Baselines with Synchronized Encoding for Universal Medical\n  Image Segmentation"
                },
                "summary": "Large foundation models, known for their strong zero-shot generalization\ncapabilities, can be applied to a wide range of downstream tasks. However,\ndeveloping foundation models for medical image segmentation poses a significant\nchallenge due to the domain gap between natural and medical images. While\nfine-tuning techniques based on the Segment Anything Model (SAM) have been\nexplored, they primarily focus on scaling up data or refining inference\nstrategies without incorporating domain-specific architectural designs,\nlimiting their zero-shot performance. To optimize segmentation performance\nunder standard inference settings and provide a strong baseline for future\nresearch, we introduce SyncSAM, which employs a synchronized dual-branch\nencoder that integrates convolution and Transformer features in a synchronized\nmanner to enhance medical image encoding, and a multi-scale dual-branch decoder\nto preserve image details. SyncSAM is trained on two of the largest medical\nimage segmentation datasets, SA-Med2D-20M and IMed-361M, resulting in a series\nof pre-trained models for universal medical image segmentation. Experimental\nresults demonstrate that SyncSAM not only achieves state-of-the-art performance\non test sets but also exhibits strong zero-shot capabilities on unseen\ndatasets. The code and model weights are available at\nhttps://github.com/Hhankyangg/SyncSAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large foundation models, known for their strong zero-shot generalization\ncapabilities, can be applied to a wide range of downstream tasks. However,\ndeveloping foundation models for medical image segmentation poses a significant\nchallenge due to the domain gap between natural and medical images. While\nfine-tuning techniques based on the Segment Anything Model (SAM) have been\nexplored, they primarily focus on scaling up data or refining inference\nstrategies without incorporating domain-specific architectural designs,\nlimiting their zero-shot performance. To optimize segmentation performance\nunder standard inference settings and provide a strong baseline for future\nresearch, we introduce SyncSAM, which employs a synchronized dual-branch\nencoder that integrates convolution and Transformer features in a synchronized\nmanner to enhance medical image encoding, and a multi-scale dual-branch decoder\nto preserve image details. SyncSAM is trained on two of the largest medical\nimage segmentation datasets, SA-Med2D-20M and IMed-361M, resulting in a series\nof pre-trained models for universal medical image segmentation. Experimental\nresults demonstrate that SyncSAM not only achieves state-of-the-art performance\non test sets but also exhibits strong zero-shot capabilities on unseen\ndatasets. The code and model weights are available at\nhttps://github.com/Hhankyangg/SyncSAM."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Xuande Mi"
                    },
                    {
                        "name": "Jiadong Feng"
                    },
                    {
                        "name": "Haixia Bi"
                    },
                    {
                        "name": "Hai Zhang"
                    },
                    {
                        "name": "Jian Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jian Sun"
                },
                "author": "Jian Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20188v1",
                "updated": "2025-02-27T15:23:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    23,
                    18,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T15:23:18Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    23,
                    18,
                    3,
                    58,
                    0
                ],
                "title": "Bisecting K-Means in RAG for Enhancing Question-Answering Tasks\n  Performance in Telecommunications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bisecting K-Means in RAG for Enhancing Question-Answering Tasks\n  Performance in Telecommunications"
                },
                "summary": "Question-answering tasks in the telecom domain are still reasonably\nunexplored in the literature, primarily due to the field's rapid changes and\nevolving standards. This work presents a novel Retrieval-Augmented Generation\nframework explicitly designed for the telecommunication domain, focusing on\ndatasets composed of 3GPP documents. The framework introduces the use of the\nBisecting K-Means clustering technique to organize the embedding vectors by\ncontents, facilitating more efficient information retrieval. By leveraging this\nclustering technique, the system pre-selects a subset of clusters that are most\nsimilar to the user's query, enhancing the relevance of the retrieved\ninformation. Aiming for models with lower computational cost for inference, the\nframework was tested using Small Language Models, demonstrating improved\nperformance with an accuracy of 66.12% on phi-2 and 72.13% on phi-3 fine-tuned\nmodels, and reduced training time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question-answering tasks in the telecom domain are still reasonably\nunexplored in the literature, primarily due to the field's rapid changes and\nevolving standards. This work presents a novel Retrieval-Augmented Generation\nframework explicitly designed for the telecommunication domain, focusing on\ndatasets composed of 3GPP documents. The framework introduces the use of the\nBisecting K-Means clustering technique to organize the embedding vectors by\ncontents, facilitating more efficient information retrieval. By leveraging this\nclustering technique, the system pre-selects a subset of clusters that are most\nsimilar to the user's query, enhancing the relevance of the retrieved\ninformation. Aiming for models with lower computational cost for inference, the\nframework was tested using Small Language Models, demonstrating improved\nperformance with an accuracy of 66.12% on phi-2 and 72.13% on phi-3 fine-tuned\nmodels, and reduced training time."
                },
                "authors": [
                    {
                        "name": "Pedro Sousa"
                    },
                    {
                        "name": "Cláudio Klautau Mello"
                    },
                    {
                        "name": "Frank B. Morte"
                    },
                    {
                        "name": "Luis F. Solis Navarro"
                    }
                ],
                "author_detail": {
                    "name": "Luis F. Solis Navarro"
                },
                "author": "Luis F. Solis Navarro",
                "arxiv_comment": "6 pages, 8 figures, accepted at GLOBECOM WORKSHOPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20186v1",
                "updated": "2025-02-27T15:22:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    22,
                    14,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T15:22:14Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    22,
                    14,
                    3,
                    58,
                    0
                ],
                "title": "Layer-Aware Task Arithmetic: Disentangling Task-Specific and\n  Instruction-Following Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layer-Aware Task Arithmetic: Disentangling Task-Specific and\n  Instruction-Following Knowledge"
                },
                "summary": "Large language models (LLMs) demonstrate strong task-specific capabilities\nthrough fine-tuning, but merging multiple fine-tuned models often leads to\ndegraded performance due to overlapping instruction-following components. Task\nArithmetic (TA), which combines task vectors derived from fine-tuning, enables\nmulti-task learning and task forgetting but struggles to isolate task-specific\nknowledge from general instruction-following behavior. To address this, we\npropose Layer-Aware Task Arithmetic (LATA), a novel approach that assigns\nlayer-specific weights to task vectors based on their alignment with\ninstruction-following or task-specific components. By amplifying task-relevant\nlayers and attenuating instruction-following layers, LATA improves task\nlearning and forgetting performance while preserving overall model utility.\nExperiments on multiple benchmarks, including WikiText-2, GSM8K, and HumanEval,\ndemonstrate that LATA outperforms existing methods in both multi-task learning\nand selective task forgetting, achieving higher task accuracy and alignment\nwith minimal degradation in output quality. Our findings highlight the\nimportance of layer-wise analysis in disentangling task-specific and\ngeneral-purpose knowledge, offering a robust framework for efficient model\nmerging and editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate strong task-specific capabilities\nthrough fine-tuning, but merging multiple fine-tuned models often leads to\ndegraded performance due to overlapping instruction-following components. Task\nArithmetic (TA), which combines task vectors derived from fine-tuning, enables\nmulti-task learning and task forgetting but struggles to isolate task-specific\nknowledge from general instruction-following behavior. To address this, we\npropose Layer-Aware Task Arithmetic (LATA), a novel approach that assigns\nlayer-specific weights to task vectors based on their alignment with\ninstruction-following or task-specific components. By amplifying task-relevant\nlayers and attenuating instruction-following layers, LATA improves task\nlearning and forgetting performance while preserving overall model utility.\nExperiments on multiple benchmarks, including WikiText-2, GSM8K, and HumanEval,\ndemonstrate that LATA outperforms existing methods in both multi-task learning\nand selective task forgetting, achieving higher task accuracy and alignment\nwith minimal degradation in output quality. Our findings highlight the\nimportance of layer-wise analysis in disentangling task-specific and\ngeneral-purpose knowledge, offering a robust framework for efficient model\nmerging and editing."
                },
                "authors": [
                    {
                        "name": "Yan-Lun Chen"
                    },
                    {
                        "name": "Yi-Ru Wei"
                    },
                    {
                        "name": "Chia-Yi Hsu"
                    },
                    {
                        "name": "Chia-Mu Yu"
                    },
                    {
                        "name": "Chun-Ying Huang"
                    },
                    {
                        "name": "Ying-Dar Lin"
                    },
                    {
                        "name": "Yu-Sung Wu"
                    },
                    {
                        "name": "Wei-Bin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Wei-Bin Lee"
                },
                "author": "Wei-Bin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20177v1",
                "updated": "2025-02-27T15:17:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    17,
                    21,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T15:17:21Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    17,
                    21,
                    3,
                    58,
                    0
                ],
                "title": "From the marginal likelihood of a two-way table to ecological inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From the marginal likelihood of a two-way table to ecological inference"
                },
                "summary": "The paper extends in two directions the work of \\cite{Plackett77} who studied\nhow, in a $2\\times 2$ table, the likelihood of the column totals depends on the\nodds ratio. First, we study the marginal likelihood of a single $R\\times C$\nfrequency table when only the marginal frequencies are observed and then\nconsider a collection of, say, $s$ $R\\times C$ tables, where only the row and\ncolumn totals can be observed, which is the basic framework which in\napplications of Ecological Inference. In the simpler context, we derive the\nlikelihood equations and show that the likelihood has a collection of local\nmaxima which, after a suitable rearrangement of the row and column categories,\nexhibit the strongest positive association compatible with the marginals, a\nkind of paradox, considering that the available data are so poor. Next, we\nderive the likelihood equations for the marginal likelihood of a collection of\ntow-way tables, under the assumption that they share the same row conditional\ndistributions and derive a necessary condition for the information matrix to be\nwell defined. We also describe a Fisher-scoring algorithm for maximizing the\nmarginal likelihood which, however, can be used only if the number of available\nreplications reaches a given threshold.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paper extends in two directions the work of \\cite{Plackett77} who studied\nhow, in a $2\\times 2$ table, the likelihood of the column totals depends on the\nodds ratio. First, we study the marginal likelihood of a single $R\\times C$\nfrequency table when only the marginal frequencies are observed and then\nconsider a collection of, say, $s$ $R\\times C$ tables, where only the row and\ncolumn totals can be observed, which is the basic framework which in\napplications of Ecological Inference. In the simpler context, we derive the\nlikelihood equations and show that the likelihood has a collection of local\nmaxima which, after a suitable rearrangement of the row and column categories,\nexhibit the strongest positive association compatible with the marginals, a\nkind of paradox, considering that the available data are so poor. Next, we\nderive the likelihood equations for the marginal likelihood of a collection of\ntow-way tables, under the assumption that they share the same row conditional\ndistributions and derive a necessary condition for the information matrix to be\nwell defined. We also describe a Fisher-scoring algorithm for maximizing the\nmarginal likelihood which, however, can be used only if the number of available\nreplications reaches a given threshold."
                },
                "authors": [
                    {
                        "name": "Antonio Forcina"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Forcina"
                },
                "author": "Antonio Forcina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20175v1",
                "updated": "2025-02-27T15:13:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    13,
                    7,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T15:13:07Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    13,
                    7,
                    3,
                    58,
                    0
                ],
                "title": "An Extensive Evaluation of PDDL Capabilities in off-the-shelf LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Extensive Evaluation of PDDL Capabilities in off-the-shelf LLMs"
                },
                "summary": "In recent advancements, large language models (LLMs) have exhibited\nproficiency in code generation and chain-of-thought reasoning, laying the\ngroundwork for tackling automatic formal planning tasks. This study evaluates\nthe potential of LLMs to understand and generate Planning Domain Definition\nLanguage (PDDL), an essential representation in artificial intelligence\nplanning. We conduct an extensive analysis across 20 distinct models spanning 7\nmajor LLM families, both commercial and open-source. Our comprehensive\nevaluation sheds light on the zero-shot LLM capabilities of parsing,\ngenerating, and reasoning with PDDL. Our findings indicate that while some\nmodels demonstrate notable effectiveness in handling PDDL, others pose\nlimitations in more complex scenarios requiring nuanced planning knowledge.\nThese results highlight the promise and current limitations of LLMs in formal\nplanning tasks, offering insights into their application and guiding future\nefforts in AI-driven planning paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent advancements, large language models (LLMs) have exhibited\nproficiency in code generation and chain-of-thought reasoning, laying the\ngroundwork for tackling automatic formal planning tasks. This study evaluates\nthe potential of LLMs to understand and generate Planning Domain Definition\nLanguage (PDDL), an essential representation in artificial intelligence\nplanning. We conduct an extensive analysis across 20 distinct models spanning 7\nmajor LLM families, both commercial and open-source. Our comprehensive\nevaluation sheds light on the zero-shot LLM capabilities of parsing,\ngenerating, and reasoning with PDDL. Our findings indicate that while some\nmodels demonstrate notable effectiveness in handling PDDL, others pose\nlimitations in more complex scenarios requiring nuanced planning knowledge.\nThese results highlight the promise and current limitations of LLMs in formal\nplanning tasks, offering insights into their application and guiding future\nefforts in AI-driven planning paradigms."
                },
                "authors": [
                    {
                        "name": "Kaustubh Vyas"
                    },
                    {
                        "name": "Damien Graux"
                    },
                    {
                        "name": "Sébastien Montella"
                    },
                    {
                        "name": "Pavlos Vougiouklis"
                    },
                    {
                        "name": "Ruofei Lai"
                    },
                    {
                        "name": "Keshuang Li"
                    },
                    {
                        "name": "Yang Ren"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03471v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03471v4",
                "updated": "2025-02-27T15:11:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    11,
                    54,
                    3,
                    58,
                    0
                ],
                "published": "2024-04-04T14:24:06Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    14,
                    24,
                    6,
                    3,
                    95,
                    0
                ],
                "title": "The Impact of Unstated Norms in Bias Analysis of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Unstated Norms in Bias Analysis of Language Models"
                },
                "summary": "Bias in large language models (LLMs) has many forms, from overt\ndiscrimination to implicit stereotypes. Counterfactual bias evaluation is a\nwidely used approach to quantifying bias and often relies on template-based\nprobes that explicitly state group membership. It measures whether the outcome\nof a task performed by an LLM is invariant to a change in group membership. In\nthis work, we find that template-based probes can lead to unrealistic bias\nmeasurements. For example, LLMs appear to mistakenly cast text associated with\nWhite race as negative at higher rates than other groups. We hypothesize that\nthis arises artificially via a mismatch between commonly unstated norms, in the\nform of markedness, in the pretraining text of LLMs (e.g., Black president vs.\npresident) and templates used for bias measurement (e.g., Black president vs.\nWhite president). The findings highlight the potential misleading impact of\nvarying group membership through explicit mention in counterfactual bias\nquantification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias in large language models (LLMs) has many forms, from overt\ndiscrimination to implicit stereotypes. Counterfactual bias evaluation is a\nwidely used approach to quantifying bias and often relies on template-based\nprobes that explicitly state group membership. It measures whether the outcome\nof a task performed by an LLM is invariant to a change in group membership. In\nthis work, we find that template-based probes can lead to unrealistic bias\nmeasurements. For example, LLMs appear to mistakenly cast text associated with\nWhite race as negative at higher rates than other groups. We hypothesize that\nthis arises artificially via a mismatch between commonly unstated norms, in the\nform of markedness, in the pretraining text of LLMs (e.g., Black president vs.\npresident) and templates used for bias measurement (e.g., Black president vs.\nWhite president). The findings highlight the potential misleading impact of\nvarying group membership through explicit mention in counterfactual bias\nquantification."
                },
                "authors": [
                    {
                        "name": "Farnaz Kohankhaki"
                    },
                    {
                        "name": "D. B. Emerson"
                    },
                    {
                        "name": "Jacob-Junqi Tian"
                    },
                    {
                        "name": "Laleh Seyyed-Kalantari"
                    },
                    {
                        "name": "Faiza Khan Khattak"
                    }
                ],
                "author_detail": {
                    "name": "Faiza Khan Khattak"
                },
                "author": "Faiza Khan Khattak",
                "arxiv_comment": "15 Pages, 4 Figures, 4 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03471v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03471v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00468v2",
                "updated": "2025-02-27T15:10:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    10,
                    56,
                    3,
                    58,
                    0
                ],
                "published": "2024-06-29T15:28:45Z",
                "published_parsed": [
                    2024,
                    6,
                    29,
                    15,
                    28,
                    45,
                    5,
                    181,
                    0
                ],
                "title": "MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and\n  Efficient Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and\n  Efficient Evaluation"
                },
                "summary": "Large Multimodal Models (LMMs) exhibit impressive cross-modal understanding\nand reasoning abilities, often assessed through multiple-choice questions\n(MCQs) that include an image, a question, and several options. However, many\nbenchmarks used for such evaluations suffer from systematic biases. Remarkably,\nLarge Language Models (LLMs) without any visual perception capabilities achieve\nnon-trivial performance, undermining the credibility of these evaluations. To\naddress this issue while maintaining the efficiency of MCQ evaluations, we\npropose MMEvalPro, a benchmark designed to avoid Type-I errors through a\ntrilogy evaluation pipeline and more rigorous metrics. For each original\nquestion from existing benchmarks, human annotators augment it by creating one\nperception question and one knowledge anchor question through a meticulous\nannotation process. MMEvalPro comprises $2,138$ question triplets, totaling\n$6,414$ distinct questions. Two-thirds of these questions are manually labeled\nby human experts, while the rest are sourced from existing benchmarks (MMMU,\nScienceQA, and MathVista). Compared with the existing benchmarks, our\nexperiments with the latest LLMs and LMMs demonstrate that MMEvalPro is more\nchallenging (the best LMM lags behind human performance by $31.73\\%$, compared\nto an average gap of $8.03\\%$ in previous benchmarks) and more trustworthy (the\nbest LLM trails the best LMM by $23.09\\%$, whereas the gap for previous\nbenchmarks is just $14.64\\%$). Our in-depth analysis explains the reason for\nthe large performance gap and justifies the trustworthiness of evaluation,\nunderscoring its significant potential for advancing future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) exhibit impressive cross-modal understanding\nand reasoning abilities, often assessed through multiple-choice questions\n(MCQs) that include an image, a question, and several options. However, many\nbenchmarks used for such evaluations suffer from systematic biases. Remarkably,\nLarge Language Models (LLMs) without any visual perception capabilities achieve\nnon-trivial performance, undermining the credibility of these evaluations. To\naddress this issue while maintaining the efficiency of MCQ evaluations, we\npropose MMEvalPro, a benchmark designed to avoid Type-I errors through a\ntrilogy evaluation pipeline and more rigorous metrics. For each original\nquestion from existing benchmarks, human annotators augment it by creating one\nperception question and one knowledge anchor question through a meticulous\nannotation process. MMEvalPro comprises $2,138$ question triplets, totaling\n$6,414$ distinct questions. Two-thirds of these questions are manually labeled\nby human experts, while the rest are sourced from existing benchmarks (MMMU,\nScienceQA, and MathVista). Compared with the existing benchmarks, our\nexperiments with the latest LLMs and LMMs demonstrate that MMEvalPro is more\nchallenging (the best LMM lags behind human performance by $31.73\\%$, compared\nto an average gap of $8.03\\%$ in previous benchmarks) and more trustworthy (the\nbest LLM trails the best LMM by $23.09\\%$, whereas the gap for previous\nbenchmarks is just $14.64\\%$). Our in-depth analysis explains the reason for\nthe large performance gap and justifies the trustworthiness of evaluation,\nunderscoring its significant potential for advancing future research."
                },
                "authors": [
                    {
                        "name": "Jinsheng Huang"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Taian Guo"
                    },
                    {
                        "name": "Fu Zeng"
                    },
                    {
                        "name": "Yusheng Zhao"
                    },
                    {
                        "name": "Bohan Wu"
                    },
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Haozhe Zhao"
                    },
                    {
                        "name": "Zhihui Guo"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Jingyang Yuan"
                    },
                    {
                        "name": "Wei Ju"
                    },
                    {
                        "name": "Luchen Liu"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Baobao Chang"
                    },
                    {
                        "name": "Ming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhang"
                },
                "author": "Ming Zhang",
                "arxiv_comment": "18 pages, code released at https://github.com/chenllliang/MMEvalPro,\n  Homepage at https://mmevalpro.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20170v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20170v1",
                "updated": "2025-02-27T15:07:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    7,
                    47,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T15:07:47Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    7,
                    47,
                    3,
                    58,
                    0
                ],
                "title": "Re-evaluating Open-ended Evaluation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-evaluating Open-ended Evaluation of Large Language Models"
                },
                "summary": "Evaluation has traditionally focused on ranking candidates for a specific\nskill. Modern generalist models, such as Large Language Models (LLMs),\ndecidedly outpace this paradigm. Open-ended evaluation systems, where candidate\nmodels are compared on user-submitted prompts, have emerged as a popular\nsolution. Despite their many advantages, we show that the current Elo-based\nrating systems can be susceptible to and even reinforce biases in data,\nintentional or accidental, due to their sensitivity to redundancies. To address\nthis issue, we propose evaluation as a 3-player game, and introduce novel\ngame-theoretic solution concepts to ensure robustness to redundancy. We show\nthat our method leads to intuitive ratings and provide insights into the\ncompetitive landscape of LLM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation has traditionally focused on ranking candidates for a specific\nskill. Modern generalist models, such as Large Language Models (LLMs),\ndecidedly outpace this paradigm. Open-ended evaluation systems, where candidate\nmodels are compared on user-submitted prompts, have emerged as a popular\nsolution. Despite their many advantages, we show that the current Elo-based\nrating systems can be susceptible to and even reinforce biases in data,\nintentional or accidental, due to their sensitivity to redundancies. To address\nthis issue, we propose evaluation as a 3-player game, and introduce novel\ngame-theoretic solution concepts to ensure robustness to redundancy. We show\nthat our method leads to intuitive ratings and provide insights into the\ncompetitive landscape of LLM development."
                },
                "authors": [
                    {
                        "name": "Siqi Liu"
                    },
                    {
                        "name": "Ian Gemp"
                    },
                    {
                        "name": "Luke Marris"
                    },
                    {
                        "name": "Georgios Piliouras"
                    },
                    {
                        "name": "Nicolas Heess"
                    },
                    {
                        "name": "Marc Lanctot"
                    }
                ],
                "author_detail": {
                    "name": "Marc Lanctot"
                },
                "author": "Marc Lanctot",
                "arxiv_comment": "Published at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20170v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20170v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20167v1",
                "updated": "2025-02-27T15:05:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    5,
                    0,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T15:05:00Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    5,
                    0,
                    3,
                    58,
                    0
                ],
                "title": "Similarity-Distance-Magnitude Universal Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity-Distance-Magnitude Universal Verification"
                },
                "summary": "We solve the neural network robustness problem by adding Similarity (i.e.,\ncorrectly predicted depth-matches into training)-awareness and\nDistance-to-training-distribution-awareness to the existing output Magnitude\n(i.e., decision-boundary)-awareness of the softmax function. The resulting sdm\nactivation function provides strong signals of the relative epistemic\n(reducible) predictive uncertainty. We use this novel behavior to further\naddress the complementary HCI problem of mapping the output to\nhuman-interpretable summary statistics over relevant partitions of a held-out\ncalibration set. Estimates of prediction-conditional uncertainty are obtained\nvia a parsimonious learned transform over the class-conditional empirical CDFs\nof the output of a final-layer sdm activation function. For decision-making and\nas an intrinsic model check, estimates of class-conditional accuracy are\nobtained by further partitioning the high-probability regions of this\ncalibrated output into class-conditional, region-specific CDFs. The uncertainty\nestimates from sdm calibration are remarkably robust to test-time distribution\nshifts and out-of-distribution inputs; incorporate awareness of the effective\nsample size; provide estimates of uncertainty from the learning and data\nsplitting processes; and are well-suited for selective classification and\nconditional branching for additional test-time compute based on the predictive\nuncertainty, as for selective LLM generation, routing, and composition over\nmultiple models and retrieval. Finally, we construct sdm networks, LLMs with\nuncertainty-aware verification and interpretability-by-exemplar as intrinsic\nproperties. We provide open-source software implementing these results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We solve the neural network robustness problem by adding Similarity (i.e.,\ncorrectly predicted depth-matches into training)-awareness and\nDistance-to-training-distribution-awareness to the existing output Magnitude\n(i.e., decision-boundary)-awareness of the softmax function. The resulting sdm\nactivation function provides strong signals of the relative epistemic\n(reducible) predictive uncertainty. We use this novel behavior to further\naddress the complementary HCI problem of mapping the output to\nhuman-interpretable summary statistics over relevant partitions of a held-out\ncalibration set. Estimates of prediction-conditional uncertainty are obtained\nvia a parsimonious learned transform over the class-conditional empirical CDFs\nof the output of a final-layer sdm activation function. For decision-making and\nas an intrinsic model check, estimates of class-conditional accuracy are\nobtained by further partitioning the high-probability regions of this\ncalibrated output into class-conditional, region-specific CDFs. The uncertainty\nestimates from sdm calibration are remarkably robust to test-time distribution\nshifts and out-of-distribution inputs; incorporate awareness of the effective\nsample size; provide estimates of uncertainty from the learning and data\nsplitting processes; and are well-suited for selective classification and\nconditional branching for additional test-time compute based on the predictive\nuncertainty, as for selective LLM generation, routing, and composition over\nmultiple models and retrieval. Finally, we construct sdm networks, LLMs with\nuncertainty-aware verification and interpretability-by-exemplar as intrinsic\nproperties. We provide open-source software implementing these results."
                },
                "authors": [
                    {
                        "name": "Allen Schmaltz"
                    }
                ],
                "author_detail": {
                    "name": "Allen Schmaltz"
                },
                "author": "Allen Schmaltz",
                "arxiv_comment": "35 pages (8 Tables, 4 Algorithms, 5 Listings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20153v1",
                "updated": "2025-02-27T14:52:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    52,
                    23,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T14:52:23Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    52,
                    23,
                    3,
                    58,
                    0
                ],
                "title": "Transfer Learning in Latent Contextual Bandits with Covariate Shift\n  Through Causal Transportability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transfer Learning in Latent Contextual Bandits with Covariate Shift\n  Through Causal Transportability"
                },
                "summary": "Transferring knowledge from one environment to another is an essential\nability of intelligent systems. Nevertheless, when two environments are\ndifferent, naively transferring all knowledge may deteriorate the performance,\na phenomenon known as negative transfer. In this paper, we address this issue\nwithin the framework of multi-armed bandits from the perspective of causal\ninference. Specifically, we consider transfer learning in latent contextual\nbandits, where the actual context is hidden, but a potentially high-dimensional\nproxy is observable. We further consider a covariate shift in the context\nacross environments. We show that naively transferring all knowledge for\nclassical bandit algorithms in this setting led to negative transfer. We then\nleverage transportability theory from causal inference to develop algorithms\nthat explicitly transfer effective knowledge for estimating the causal effects\nof interest in the target environment. Besides, we utilize variational\nautoencoders to approximate causal effects under the presence of a\nhigh-dimensional proxy. We test our algorithms on synthetic and semi-synthetic\ndatasets, empirically demonstrating consistently improved learning efficiency\nacross different proxies compared to baseline algorithms, showing the\neffectiveness of our causal framework in transferring knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transferring knowledge from one environment to another is an essential\nability of intelligent systems. Nevertheless, when two environments are\ndifferent, naively transferring all knowledge may deteriorate the performance,\na phenomenon known as negative transfer. In this paper, we address this issue\nwithin the framework of multi-armed bandits from the perspective of causal\ninference. Specifically, we consider transfer learning in latent contextual\nbandits, where the actual context is hidden, but a potentially high-dimensional\nproxy is observable. We further consider a covariate shift in the context\nacross environments. We show that naively transferring all knowledge for\nclassical bandit algorithms in this setting led to negative transfer. We then\nleverage transportability theory from causal inference to develop algorithms\nthat explicitly transfer effective knowledge for estimating the causal effects\nof interest in the target environment. Besides, we utilize variational\nautoencoders to approximate causal effects under the presence of a\nhigh-dimensional proxy. We test our algorithms on synthetic and semi-synthetic\ndatasets, empirically demonstrating consistently improved learning efficiency\nacross different proxies compared to baseline algorithms, showing the\neffectiveness of our causal framework in transferring knowledge."
                },
                "authors": [
                    {
                        "name": "Mingwei Deng"
                    },
                    {
                        "name": "Ville Kyrki"
                    },
                    {
                        "name": "Dominik Baumann"
                    }
                ],
                "author_detail": {
                    "name": "Dominik Baumann"
                },
                "author": "Dominik Baumann",
                "arxiv_comment": "Accepted at the Conference of Causal Learning and Reasoning (CLeaR\n  2025), will be published in the Proceedings of Machine Learning Research",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16860v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16860v2",
                "updated": "2025-02-27T14:50:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    50,
                    10,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-24T05:51:53Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    5,
                    51,
                    53,
                    0,
                    55,
                    0
                ],
                "title": "LongAttn: Selecting Long-context Training Data via Token-level Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongAttn: Selecting Long-context Training Data via Token-level Attention"
                },
                "summary": "With the development of large language models (LLMs), there has been an\nincreasing need for significant advancements in handling long contexts. To\nenhance long-context capabilities, constructing high-quality training data with\nlong-range dependencies is crucial. Existing methods to select long-context\ndata often rely on sentence-level analysis, which can be greatly optimized in\nboth performance and efficiency. In this paper, we propose a novel token-level\nframework, LongAttn, which leverages the self-attention mechanism of LLMs to\nmeasure the long-range dependencies for the data. By calculating token-level\ndependency strength and distribution uniformity of token scores, LongAttn\neffectively quantifies long-range dependencies, enabling more accurate and\nefficient data selection. We filter LongABC-32K from open-source long-context\ndatasets (ArXiv, Book, and Code). Through our comprehensive experiments,\nLongAttn has demonstrated its excellent effectiveness, scalability, and\nefficiency. To facilitate future research in long-context data, we released our\ncode and the high-quality long-context training data LongABC-32K.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), there has been an\nincreasing need for significant advancements in handling long contexts. To\nenhance long-context capabilities, constructing high-quality training data with\nlong-range dependencies is crucial. Existing methods to select long-context\ndata often rely on sentence-level analysis, which can be greatly optimized in\nboth performance and efficiency. In this paper, we propose a novel token-level\nframework, LongAttn, which leverages the self-attention mechanism of LLMs to\nmeasure the long-range dependencies for the data. By calculating token-level\ndependency strength and distribution uniformity of token scores, LongAttn\neffectively quantifies long-range dependencies, enabling more accurate and\nefficient data selection. We filter LongABC-32K from open-source long-context\ndatasets (ArXiv, Book, and Code). Through our comprehensive experiments,\nLongAttn has demonstrated its excellent effectiveness, scalability, and\nefficiency. To facilitate future research in long-context data, we released our\ncode and the high-quality long-context training data LongABC-32K."
                },
                "authors": [
                    {
                        "name": "Longyun Wu"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Guangxiang Zhao"
                    },
                    {
                        "name": "Zhuocheng Yu"
                    },
                    {
                        "name": "Junfeng Ran"
                    },
                    {
                        "name": "Xiangyu Wong"
                    },
                    {
                        "name": "Lin Sun"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "17 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16860v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16860v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20149v1",
                "updated": "2025-02-27T14:44:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    44,
                    15,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T14:44:15Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    44,
                    15,
                    3,
                    58,
                    0
                ],
                "title": "Inferring a Cell Structure on the Space of Cyclooctane Conformations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring a Cell Structure on the Space of Cyclooctane Conformations"
                },
                "summary": "The conformation space of cyclooctane, a ringlike organic molecule comprising\neight carbon atoms, is a two-dimensional algebraic variety, which has been\nstudied extensively for more than 90 years. We propose a cell structure\nrepresenting this space, which arises naturally by partitioning the space into\nsubsets of conformations that admit particular symmetries. We do so both for\nthe labeled conformation space, in which the carbon atoms are considered as\ndistinct, and for the actual, unlabeled, conformation space. The proposed cell\nstructure is obtained by identifying subspaces of conformations based on\nsymmetry patterns and studying the geometry and topology of these subsets using\nmethods from dimensionality reduction and topological data analysis. Our\nfindings suggest that, in contrast to the labeled variant, the conformation\nspace of cyclooctane is contractible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The conformation space of cyclooctane, a ringlike organic molecule comprising\neight carbon atoms, is a two-dimensional algebraic variety, which has been\nstudied extensively for more than 90 years. We propose a cell structure\nrepresenting this space, which arises naturally by partitioning the space into\nsubsets of conformations that admit particular symmetries. We do so both for\nthe labeled conformation space, in which the carbon atoms are considered as\ndistinct, and for the actual, unlabeled, conformation space. The proposed cell\nstructure is obtained by identifying subspaces of conformations based on\nsymmetry patterns and studying the geometry and topology of these subsets using\nmethods from dimensionality reduction and topological data analysis. Our\nfindings suggest that, in contrast to the labeled variant, the conformation\nspace of cyclooctane is contractible."
                },
                "authors": [
                    {
                        "name": "Ulrich Bauer"
                    },
                    {
                        "name": "Fabian Lenzen"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Lenzen"
                },
                "author": "Fabian Lenzen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "57-08, 14-04",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20140v1",
                "updated": "2025-02-27T14:31:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    31,
                    42,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T14:31:42Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    31,
                    42,
                    3,
                    58,
                    0
                ],
                "title": "Telephone Surveys Meet Conversational AI: Evaluating a LLM-Based\n  Telephone Survey System at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Telephone Surveys Meet Conversational AI: Evaluating a LLM-Based\n  Telephone Survey System at Scale"
                },
                "summary": "Telephone surveys remain a valuable tool for gathering insights but typically\nrequire substantial resources in training and coordinating human interviewers.\nThis work presents an AI-driven telephone survey system integrating\ntext-to-speech (TTS), a large language model (LLM), and speech-to-text (STT)\nthat mimics the versatility of human-led interviews on scale.\n  We tested the system across two populations, a pilot study in the United\nStates (n = 75) and a large-scale deployment in Peru (n = 2,739), inviting\nparticipants via web-based links and contacting them via direct phone calls.\nThe AI agent successfully administered open-ended and closed-ended questions,\nhandled basic clarifications, and dynamically navigated branching logic,\nallowing fast large-scale survey deployment without interviewer recruitment or\ntraining.\n  Our findings demonstrate that while the AI system's probing for qualitative\ndepth was more limited than human interviewers, overall data quality approached\nhuman-led standards for structured items. This study represents one of the\nfirst successful large-scale deployments of an LLM-based telephone interviewer\nin a real-world survey context. The AI-powered telephone survey system has the\npotential for expanding scalable, consistent data collecting across market\nresearch, social science, and public opinion studies, thus improving\noperational efficiency while maintaining appropriate data quality for research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Telephone surveys remain a valuable tool for gathering insights but typically\nrequire substantial resources in training and coordinating human interviewers.\nThis work presents an AI-driven telephone survey system integrating\ntext-to-speech (TTS), a large language model (LLM), and speech-to-text (STT)\nthat mimics the versatility of human-led interviews on scale.\n  We tested the system across two populations, a pilot study in the United\nStates (n = 75) and a large-scale deployment in Peru (n = 2,739), inviting\nparticipants via web-based links and contacting them via direct phone calls.\nThe AI agent successfully administered open-ended and closed-ended questions,\nhandled basic clarifications, and dynamically navigated branching logic,\nallowing fast large-scale survey deployment without interviewer recruitment or\ntraining.\n  Our findings demonstrate that while the AI system's probing for qualitative\ndepth was more limited than human interviewers, overall data quality approached\nhuman-led standards for structured items. This study represents one of the\nfirst successful large-scale deployments of an LLM-based telephone interviewer\nin a real-world survey context. The AI-powered telephone survey system has the\npotential for expanding scalable, consistent data collecting across market\nresearch, social science, and public opinion studies, thus improving\noperational efficiency while maintaining appropriate data quality for research."
                },
                "authors": [
                    {
                        "name": "Max M. Lang"
                    },
                    {
                        "name": "Sol Eskenazi"
                    }
                ],
                "author_detail": {
                    "name": "Sol Eskenazi"
                },
                "author": "Sol Eskenazi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13474v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13474v2",
                "updated": "2025-02-27T14:29:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    29,
                    8,
                    3,
                    58,
                    0
                ],
                "published": "2024-06-19T11:53:21Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    11,
                    53,
                    21,
                    2,
                    171,
                    0
                ],
                "title": "BoA: Attention-aware Post-training Quantization without Backpropagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BoA: Attention-aware Post-training Quantization without Backpropagation"
                },
                "summary": "Post-training quantization (PTQ) is a promising solution for deploying large\nlanguage models (LLMs) on resource-constrained devices. Early methods developed\nfor smaller networks like ResNet rely on gradient-based optimization, which\nbecomes impractical for hyper-scale LLMs with billions of parameters. While\nrecently proposed backpropagation-free or transformation-based methods\nalleviate this issue, their performance remains limited by either a lack of\ninter-layer dependency consideration or the use of naive nearest-rounding-based\ninteger weight assignment to save the heavy computational cost of weight\noptimization. We thus introduce a novel backpropagation-free PTQ algorithm that\noptimizes integer weights by considering inter-layer dependencies. The key\ninnovation is the development of attention-aware Hessian matrices that capture\ninter-layer interactions within the attention module. Extensive experiments\ndemonstrate that our approach not only outperforms existing weight quantization\nmethods but also shows good synergy with conventional methods to suppress\nactivation outliers, leading to state-of-the-art weight-activation quantization\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) is a promising solution for deploying large\nlanguage models (LLMs) on resource-constrained devices. Early methods developed\nfor smaller networks like ResNet rely on gradient-based optimization, which\nbecomes impractical for hyper-scale LLMs with billions of parameters. While\nrecently proposed backpropagation-free or transformation-based methods\nalleviate this issue, their performance remains limited by either a lack of\ninter-layer dependency consideration or the use of naive nearest-rounding-based\ninteger weight assignment to save the heavy computational cost of weight\noptimization. We thus introduce a novel backpropagation-free PTQ algorithm that\noptimizes integer weights by considering inter-layer dependencies. The key\ninnovation is the development of attention-aware Hessian matrices that capture\ninter-layer interactions within the attention module. Extensive experiments\ndemonstrate that our approach not only outperforms existing weight quantization\nmethods but also shows good synergy with conventional methods to suppress\nactivation outliers, leading to state-of-the-art weight-activation quantization\nperformance."
                },
                "authors": [
                    {
                        "name": "Junhan Kim"
                    },
                    {
                        "name": "Ho-young Kim"
                    },
                    {
                        "name": "Eulrang Cho"
                    },
                    {
                        "name": "Chungman Lee"
                    },
                    {
                        "name": "Joonyoung Kim"
                    },
                    {
                        "name": "Yongkweon Jeon"
                    }
                ],
                "author_detail": {
                    "name": "Yongkweon Jeon"
                },
                "author": "Yongkweon Jeon",
                "arxiv_comment": "19 pages, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13474v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13474v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20129v1",
                "updated": "2025-02-27T14:24:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    24,
                    51,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T14:24:51Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    24,
                    51,
                    3,
                    58,
                    0
                ],
                "title": "Finite State Automata Inside Transformers with Chain-of-Thought: A\n  Mechanistic Study on State Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finite State Automata Inside Transformers with Chain-of-Thought: A\n  Mechanistic Study on State Tracking"
                },
                "summary": "Chain-of-Thought (CoT) significantly enhances the performance of large\nlanguage models (LLMs) across a wide range of tasks, and prior research shows\nthat CoT can theoretically increase expressiveness. However, there is limited\nmechanistic understanding of the algorithms that Transformer+CoT can learn. In\nthis work, we (1) evaluate the state tracking capabilities of Transformer+CoT\nand its variants, confirming the effectiveness of CoT. (2) Next, we identify\nthe circuit, a subset of model components, responsible for tracking the world\nstate, finding that late-layer MLP neurons play a key role. We propose two\nmetrics, compression and distinction, and show that the neuron sets for each\nstate achieve nearly 100% accuracy, providing evidence of an implicit finite\nstate automaton (FSA) embedded within the model. (3) Additionally, we explore\nthree realistic settings: skipping intermediate steps, introducing data noise,\nand testing length generalization. Our results demonstrate that Transformer+CoT\nlearns robust algorithms (FSA), highlighting its resilience in challenging\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) significantly enhances the performance of large\nlanguage models (LLMs) across a wide range of tasks, and prior research shows\nthat CoT can theoretically increase expressiveness. However, there is limited\nmechanistic understanding of the algorithms that Transformer+CoT can learn. In\nthis work, we (1) evaluate the state tracking capabilities of Transformer+CoT\nand its variants, confirming the effectiveness of CoT. (2) Next, we identify\nthe circuit, a subset of model components, responsible for tracking the world\nstate, finding that late-layer MLP neurons play a key role. We propose two\nmetrics, compression and distinction, and show that the neuron sets for each\nstate achieve nearly 100% accuracy, providing evidence of an implicit finite\nstate automaton (FSA) embedded within the model. (3) Additionally, we explore\nthree realistic settings: skipping intermediate steps, introducing data noise,\nand testing length generalization. Our results demonstrate that Transformer+CoT\nlearns robust algorithms (FSA), highlighting its resilience in challenging\nscenarios."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Wenyu Du"
                    },
                    {
                        "name": "Dongming Jin"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Zhi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Jin"
                },
                "author": "Zhi Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16577v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16577v3",
                "updated": "2025-02-27T14:23:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    23,
                    40,
                    3,
                    58,
                    0
                ],
                "published": "2024-10-21T23:30:36Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    23,
                    30,
                    36,
                    0,
                    295,
                    0
                ],
                "title": "Bayesian High-dimensional Linear Regression with Sparse\n  Projection-posterior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian High-dimensional Linear Regression with Sparse\n  Projection-posterior"
                },
                "summary": "We consider a novel Bayesian approach to estimation, uncertainty\nquantification, and variable selection for a high-dimensional linear regression\nmodel under sparsity. The number of predictors can be nearly exponentially\nlarge relative to the sample size. We put a conjugate normal prior initially\ndisregarding sparsity, but for making an inference, instead of the original\nmultivariate normal posterior, we use the posterior distribution induced by a\nmap transforming the vector of regression coefficients to a sparse vector\nobtained by minimizing the sum of squares of deviations plus a suitably scaled\n$\\ell_1$-penalty on the vector. We show that the resulting sparse\nprojection-posterior distribution contracts around the true value of the\nparameter at the optimal rate adapted to the sparsity of the vector. We show\nthat the true sparsity structure gets a large sparse projection-posterior\nprobability. We further show that an appropriately recentred credible ball has\nthe correct asymptotic frequentist coverage. Finally, we describe how the\ncomputational burden can be distributed to many machines, each dealing with\nonly a small fraction of the whole dataset. We conduct a comprehensive\nsimulation study under a variety of settings and found that the proposed method\nperforms well for finite sample sizes. We also apply the method to several real\ndatasets, including the ADNI data, and compare its performance with the\nstate-of-the-art methods. We implemented the method in the \\texttt{R} package\ncalled \\texttt{sparseProj}, and all computations have been carried out using\nthis package.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a novel Bayesian approach to estimation, uncertainty\nquantification, and variable selection for a high-dimensional linear regression\nmodel under sparsity. The number of predictors can be nearly exponentially\nlarge relative to the sample size. We put a conjugate normal prior initially\ndisregarding sparsity, but for making an inference, instead of the original\nmultivariate normal posterior, we use the posterior distribution induced by a\nmap transforming the vector of regression coefficients to a sparse vector\nobtained by minimizing the sum of squares of deviations plus a suitably scaled\n$\\ell_1$-penalty on the vector. We show that the resulting sparse\nprojection-posterior distribution contracts around the true value of the\nparameter at the optimal rate adapted to the sparsity of the vector. We show\nthat the true sparsity structure gets a large sparse projection-posterior\nprobability. We further show that an appropriately recentred credible ball has\nthe correct asymptotic frequentist coverage. Finally, we describe how the\ncomputational burden can be distributed to many machines, each dealing with\nonly a small fraction of the whole dataset. We conduct a comprehensive\nsimulation study under a variety of settings and found that the proposed method\nperforms well for finite sample sizes. We also apply the method to several real\ndatasets, including the ADNI data, and compare its performance with the\nstate-of-the-art methods. We implemented the method in the \\texttt{R} package\ncalled \\texttt{sparseProj}, and all computations have been carried out using\nthis package."
                },
                "authors": [
                    {
                        "name": "Samhita Pal"
                    },
                    {
                        "name": "Subhashis Ghoshal"
                    }
                ],
                "author_detail": {
                    "name": "Subhashis Ghoshal"
                },
                "author": "Subhashis Ghoshal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16577v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16577v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15091v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15091v3",
                "updated": "2025-02-27T14:21:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    21,
                    43,
                    3,
                    58,
                    0
                ],
                "published": "2024-08-27T14:22:02Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    22,
                    2,
                    1,
                    240,
                    0
                ],
                "title": "Relation Also Knows: Rethinking the Recall and Editing of Factual\n  Associations in Auto-Regressive Transformer Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relation Also Knows: Rethinking the Recall and Editing of Factual\n  Associations in Auto-Regressive Transformer Language Models"
                },
                "summary": "The storage and recall of factual associations in auto-regressive transformer\nlanguage models (LMs) have drawn a great deal of attention, inspiring knowledge\nediting by directly modifying the located model weights. Most editing works\nachieve knowledge editing under the guidance of existing interpretations of\nknowledge recall that mainly focus on subject knowledge. However, these\ninterpretations are seriously flawed, neglecting relation information and\nleading to the over-generalizing problem for editing. In this work, we discover\na novel relation-focused perspective to interpret the knowledge recall of\ntransformer LMs during inference and apply it on single knowledge editing to\navoid over-generalizing. Experimental results on the dataset supplemented with\na new R-Specificity criterion demonstrate that our editing approach\nsignificantly alleviates over-generalizing while remaining competitive on other\ncriteria, breaking the domination of subject-focused editing for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The storage and recall of factual associations in auto-regressive transformer\nlanguage models (LMs) have drawn a great deal of attention, inspiring knowledge\nediting by directly modifying the located model weights. Most editing works\nachieve knowledge editing under the guidance of existing interpretations of\nknowledge recall that mainly focus on subject knowledge. However, these\ninterpretations are seriously flawed, neglecting relation information and\nleading to the over-generalizing problem for editing. In this work, we discover\na novel relation-focused perspective to interpret the knowledge recall of\ntransformer LMs during inference and apply it on single knowledge editing to\navoid over-generalizing. Experimental results on the dataset supplemented with\na new R-Specificity criterion demonstrate that our editing approach\nsignificantly alleviates over-generalizing while remaining competitive on other\ncriteria, breaking the domination of subject-focused editing for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Xiyu Liu"
                    },
                    {
                        "name": "Zhengxiao Liu"
                    },
                    {
                        "name": "Naibin Gu"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Wanli Ma"
                    },
                    {
                        "name": "Ji Xiang"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "arxiv_comment": "Accepted by AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15091v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15091v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20127v1",
                "updated": "2025-02-27T14:19:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    19,
                    45,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T14:19:45Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    19,
                    45,
                    3,
                    58,
                    0
                ],
                "title": "SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning"
                },
                "summary": "Mainstream issue-resolving frameworks predominantly rely on commercial\nmodels, leading to high costs and privacy concerns. Existing training\napproaches for issue resolving struggle with poor generalization and fail to\nfully leverage open-source development resources. We propose Subtask-oriented\nReinforced Fine-Tuning (SoRFT), a novel training approach to enhance the issue\nresolving capability of LLMs. We decomposes issue resolving into structured\nsubtasks: file localization, function localization, line localization, and code\nedit generation. SoRFT consists of two training stages: (1) rejection-sampled\nsupervised fine-tuning, Chain of Thought (CoT) data is filtered using\nground-truth before fine-tuning the LLM, and (2) rule-based reinforcement\nlearning, which leverages PPO with ground-truth based rewards. We evaluate the\nSoRFT-trained model on SWE-Bench Verified and SWE-Bench Lite, achieving\nstate-of-the-art (SOTA) performance among open-source models (e.g., resolve\n21.4% issues on SWE-Bench Verified with SoRFT-Qwen-7B). The experimental\nresults demonstrate that SoRFT significantly enhances issue-resolving\nperformance, improves model generalization, and provides a cost-efficient\nalternative to commercial models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainstream issue-resolving frameworks predominantly rely on commercial\nmodels, leading to high costs and privacy concerns. Existing training\napproaches for issue resolving struggle with poor generalization and fail to\nfully leverage open-source development resources. We propose Subtask-oriented\nReinforced Fine-Tuning (SoRFT), a novel training approach to enhance the issue\nresolving capability of LLMs. We decomposes issue resolving into structured\nsubtasks: file localization, function localization, line localization, and code\nedit generation. SoRFT consists of two training stages: (1) rejection-sampled\nsupervised fine-tuning, Chain of Thought (CoT) data is filtered using\nground-truth before fine-tuning the LLM, and (2) rule-based reinforcement\nlearning, which leverages PPO with ground-truth based rewards. We evaluate the\nSoRFT-trained model on SWE-Bench Verified and SWE-Bench Lite, achieving\nstate-of-the-art (SOTA) performance among open-source models (e.g., resolve\n21.4% issues on SWE-Bench Verified with SoRFT-Qwen-7B). The experimental\nresults demonstrate that SoRFT significantly enhances issue-resolving\nperformance, improves model generalization, and provides a cost-efficient\nalternative to commercial models."
                },
                "authors": [
                    {
                        "name": "Zexiong Ma"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Pengfei Gao"
                    },
                    {
                        "name": "Xiangxin Meng"
                    },
                    {
                        "name": "Yanzhen Zou"
                    },
                    {
                        "name": "Bing Xie"
                    }
                ],
                "author_detail": {
                    "name": "Bing Xie"
                },
                "author": "Bing Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20126v1",
                "updated": "2025-02-27T14:16:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    16,
                    56,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T14:16:56Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    16,
                    56,
                    3,
                    58,
                    0
                ],
                "title": "FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality\n  Samples with Less Compute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality\n  Samples with Less Compute"
                },
                "summary": "Despite their remarkable performance, modern Diffusion Transformers are\nhindered by substantial resource requirements during inference, stemming from\nthe fixed and large amount of compute needed for each denoising step. In this\nwork, we revisit the conventional static paradigm that allocates a fixed\ncompute budget per denoising iteration and propose a dynamic strategy instead.\nOur simple and sample-efficient framework enables pre-trained DiT models to be\nconverted into \\emph{flexible} ones -- dubbed FlexiDiT -- allowing them to\nprocess inputs at varying compute budgets. We demonstrate how a single\n\\emph{flexible} model can generate images without any drop in quality, while\nreducing the required FLOPs by more than $40$\\% compared to their static\ncounterparts, for both class-conditioned and text-conditioned image generation.\nOur method is general and agnostic to input and conditioning modalities. We\nshow how our approach can be readily extended for video generation, where\nFlexiDiT models generate samples with up to $75$\\% less compute without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable performance, modern Diffusion Transformers are\nhindered by substantial resource requirements during inference, stemming from\nthe fixed and large amount of compute needed for each denoising step. In this\nwork, we revisit the conventional static paradigm that allocates a fixed\ncompute budget per denoising iteration and propose a dynamic strategy instead.\nOur simple and sample-efficient framework enables pre-trained DiT models to be\nconverted into \\emph{flexible} ones -- dubbed FlexiDiT -- allowing them to\nprocess inputs at varying compute budgets. We demonstrate how a single\n\\emph{flexible} model can generate images without any drop in quality, while\nreducing the required FLOPs by more than $40$\\% compared to their static\ncounterparts, for both class-conditioned and text-conditioned image generation.\nOur method is general and agnostic to input and conditioning modalities. We\nshow how our approach can be readily extended for video generation, where\nFlexiDiT models generate samples with up to $75$\\% less compute without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Sotiris Anagnostidis"
                    },
                    {
                        "name": "Gregor Bachmann"
                    },
                    {
                        "name": "Yeongmin Kim"
                    },
                    {
                        "name": "Jonas Kohler"
                    },
                    {
                        "name": "Markos Georgopoulos"
                    },
                    {
                        "name": "Artsiom Sanakoyeu"
                    },
                    {
                        "name": "Yuming Du"
                    },
                    {
                        "name": "Albert Pumarola"
                    },
                    {
                        "name": "Ali Thabet"
                    },
                    {
                        "name": "Edgar Schönfeld"
                    }
                ],
                "author_detail": {
                    "name": "Edgar Schönfeld"
                },
                "author": "Edgar Schönfeld",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20124v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20124v1",
                "updated": "2025-02-27T14:16:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    16,
                    1,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T14:16:01Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    16,
                    1,
                    3,
                    58,
                    0
                ],
                "title": "Exploring Open-world Continual Learning with Knowns-Unknowns Knowledge\n  Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Open-world Continual Learning with Knowns-Unknowns Knowledge\n  Transfer"
                },
                "summary": "Open-World Continual Learning (OWCL) is a challenging paradigm where models\nmust incrementally learn new knowledge without forgetting while operating under\nan open-world assumption. This requires handling incomplete training data and\nrecognizing unknown samples during inference. However, existing OWCL methods\noften treat open detection and continual learning as separate tasks, limiting\ntheir ability to integrate open-set detection and incremental classification in\nOWCL. Moreover, current approaches primarily focus on transferring knowledge\nfrom known samples, neglecting the insights derived from unknown/open samples.\nTo address these limitations, we formalize four distinct OWCL scenarios and\nconduct comprehensive empirical experiments to explore potential challenges in\nOWCL. Our findings reveal a significant interplay between the open detection of\nunknowns and incremental classification of knowns, challenging a widely held\nassumption that unknown detection and known classification are orthogonal\nprocesses. Building on our insights, we propose \\textbf{HoliTrans} (Holistic\nKnowns-Unknowns Knowledge Transfer), a novel OWCL framework that integrates\nnonlinear random projection (NRP) to create a more linearly separable embedding\nspace and distribution-aware prototypes (DAPs) to construct an adaptive\nknowledge space. Particularly, our HoliTrans effectively supports knowledge\ntransfer for both known and unknown samples while dynamically updating\nrepresentations of open samples during OWCL. Extensive experiments across\nvarious OWCL scenarios demonstrate that HoliTrans outperforms 22 competitive\nbaselines, bridging the gap between OWCL theory and practice and providing a\nrobust, scalable framework for advancing open-world learning paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-World Continual Learning (OWCL) is a challenging paradigm where models\nmust incrementally learn new knowledge without forgetting while operating under\nan open-world assumption. This requires handling incomplete training data and\nrecognizing unknown samples during inference. However, existing OWCL methods\noften treat open detection and continual learning as separate tasks, limiting\ntheir ability to integrate open-set detection and incremental classification in\nOWCL. Moreover, current approaches primarily focus on transferring knowledge\nfrom known samples, neglecting the insights derived from unknown/open samples.\nTo address these limitations, we formalize four distinct OWCL scenarios and\nconduct comprehensive empirical experiments to explore potential challenges in\nOWCL. Our findings reveal a significant interplay between the open detection of\nunknowns and incremental classification of knowns, challenging a widely held\nassumption that unknown detection and known classification are orthogonal\nprocesses. Building on our insights, we propose \\textbf{HoliTrans} (Holistic\nKnowns-Unknowns Knowledge Transfer), a novel OWCL framework that integrates\nnonlinear random projection (NRP) to create a more linearly separable embedding\nspace and distribution-aware prototypes (DAPs) to construct an adaptive\nknowledge space. Particularly, our HoliTrans effectively supports knowledge\ntransfer for both known and unknown samples while dynamically updating\nrepresentations of open samples during OWCL. Extensive experiments across\nvarious OWCL scenarios demonstrate that HoliTrans outperforms 22 competitive\nbaselines, bridging the gap between OWCL theory and practice and providing a\nrobust, scalable framework for advancing open-world learning paradigms."
                },
                "authors": [
                    {
                        "name": "Yujie Li"
                    },
                    {
                        "name": "Guannan Lai"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Yonghao Li"
                    },
                    {
                        "name": "Marcello Bonsangue"
                    },
                    {
                        "name": "Tianrui Li"
                    }
                ],
                "author_detail": {
                    "name": "Tianrui Li"
                },
                "author": "Tianrui Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20124v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20124v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20122v1",
                "updated": "2025-02-27T14:14:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    14,
                    50,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T14:14:50Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    14,
                    50,
                    3,
                    58,
                    0
                ],
                "title": "Self-Training Elicits Concise Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Training Elicits Concise Reasoning in Large Language Models"
                },
                "summary": "Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to\nutilize additional computation through intermediate tokens to solve complex\ntasks. However, we posit that typical reasoning traces contain many redundant\ntokens, incurring extraneous inference costs. Upon examination of the output\ndistribution of current LLMs, we find evidence on their latent ability to\nreason more concisely, relative to their default behavior. To elicit this\ncapability, we propose simple fine-tuning methods which leverage self-generated\nconcise reasoning paths obtained by best-of-N sampling and few-shot\nconditioning, in task-specific settings. Our combined method achieves a 30%\nreduction in output tokens on average, across five model families on GSM8K and\nMATH, while maintaining average accuracy. By exploiting the fundamental\nstochasticity and in-context learning capabilities of LLMs, our self-training\napproach robustly elicits concise reasoning on a wide range of models,\nincluding those with extensive post-training. Code is available at\nhttps://github.com/TergelMunkhbat/concise-reasoning",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to\nutilize additional computation through intermediate tokens to solve complex\ntasks. However, we posit that typical reasoning traces contain many redundant\ntokens, incurring extraneous inference costs. Upon examination of the output\ndistribution of current LLMs, we find evidence on their latent ability to\nreason more concisely, relative to their default behavior. To elicit this\ncapability, we propose simple fine-tuning methods which leverage self-generated\nconcise reasoning paths obtained by best-of-N sampling and few-shot\nconditioning, in task-specific settings. Our combined method achieves a 30%\nreduction in output tokens on average, across five model families on GSM8K and\nMATH, while maintaining average accuracy. By exploiting the fundamental\nstochasticity and in-context learning capabilities of LLMs, our self-training\napproach robustly elicits concise reasoning on a wide range of models,\nincluding those with extensive post-training. Code is available at\nhttps://github.com/TergelMunkhbat/concise-reasoning"
                },
                "authors": [
                    {
                        "name": "Tergel Munkhbat"
                    },
                    {
                        "name": "Namgyu Ho"
                    },
                    {
                        "name": "Seohyun Kim"
                    },
                    {
                        "name": "Yongjin Yang"
                    },
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "23 pages, 10 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20119v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20119v1",
                "updated": "2025-02-27T14:11:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    11,
                    58,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T14:11:58Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    11,
                    58,
                    3,
                    58,
                    0
                ],
                "title": "Sketch & Paint: Stroke-by-Stroke Evolution of Visual Artworks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sketch & Paint: Stroke-by-Stroke Evolution of Visual Artworks"
                },
                "summary": "Understanding the stroke-based evolution of visual artworks is useful for\nadvancing artwork learning, appreciation, and interactive display. While the\nstroke sequence of renowned artworks remains largely unknown, formulating this\nsequence for near-natural image drawing processes can significantly enhance our\nunderstanding of artistic techniques. This paper introduces a novel method for\napproximating artwork stroke evolution through a proximity-based clustering\nmechanism. We first convert pixel images into vector images via parametric\ncurves and then explore the clustering approach to determine the sequence order\nof extracted strokes. Our proposed algorithm demonstrates the potential to\ninfer stroke sequences in unknown artworks. We evaluate the performance of our\nmethod using WikiArt data and qualitatively demonstrate the plausible stroke\nsequences. Additionally, we demonstrate the robustness of our approach to\nhandle a wide variety of input image types such as line art, face sketches,\npaintings, and photographic images. By exploring stroke extraction and sequence\nconstruction, we aim to improve our understanding of the intricacies of the art\ndevelopment techniques and the step-by-step reconstruction process behind\nvisual artworks, thereby enriching our understanding of the creative journey\nfrom the initial sketch to the final artwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the stroke-based evolution of visual artworks is useful for\nadvancing artwork learning, appreciation, and interactive display. While the\nstroke sequence of renowned artworks remains largely unknown, formulating this\nsequence for near-natural image drawing processes can significantly enhance our\nunderstanding of artistic techniques. This paper introduces a novel method for\napproximating artwork stroke evolution through a proximity-based clustering\nmechanism. We first convert pixel images into vector images via parametric\ncurves and then explore the clustering approach to determine the sequence order\nof extracted strokes. Our proposed algorithm demonstrates the potential to\ninfer stroke sequences in unknown artworks. We evaluate the performance of our\nmethod using WikiArt data and qualitatively demonstrate the plausible stroke\nsequences. Additionally, we demonstrate the robustness of our approach to\nhandle a wide variety of input image types such as line art, face sketches,\npaintings, and photographic images. By exploring stroke extraction and sequence\nconstruction, we aim to improve our understanding of the intricacies of the art\ndevelopment techniques and the step-by-step reconstruction process behind\nvisual artworks, thereby enriching our understanding of the creative journey\nfrom the initial sketch to the final artwork."
                },
                "authors": [
                    {
                        "name": "Jeripothula Prudviraj"
                    },
                    {
                        "name": "Vikram Jamwal"
                    }
                ],
                "author_detail": {
                    "name": "Vikram Jamwal"
                },
                "author": "Vikram Jamwal",
                "arxiv_comment": "ECCV 2024 Workshop: AI for Visual Arts Workshop and Challenges\n  (AI4VA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20119v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20119v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.3.3; I.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09648v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09648v2",
                "updated": "2025-02-27T14:10:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    10,
                    3,
                    3,
                    58,
                    0
                ],
                "published": "2025-01-16T16:43:05Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    43,
                    5,
                    3,
                    16,
                    0
                ],
                "title": "Statistical inference for interacting innovation processes and related\n  general results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical inference for interacting innovation processes and related\n  general results"
                },
                "summary": "Given the importance of understanding how different innovation processes\naffect each other, we have introduced a model for a finite system of\ninteracting innovation processes. The present work focuses on the second-order\nasymptotic properties of the model and illustrates how to leverage the\ntheoretical results in order to make statistical inference on the intensity of\nthe interaction. This methodology is presented within a general framework in\nthe supplementary material to ensure its broad applicability across various\ncontexts. We apply the proposed tools to two real data sets (from Reddit and\nGutenberg).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the importance of understanding how different innovation processes\naffect each other, we have introduced a model for a finite system of\ninteracting innovation processes. The present work focuses on the second-order\nasymptotic properties of the model and illustrates how to leverage the\ntheoretical results in order to make statistical inference on the intensity of\nthe interaction. This methodology is presented within a general framework in\nthe supplementary material to ensure its broad applicability across various\ncontexts. We apply the proposed tools to two real data sets (from Reddit and\nGutenberg)."
                },
                "authors": [
                    {
                        "name": "Giacomo Aletti"
                    },
                    {
                        "name": "Irene Crimaldi"
                    },
                    {
                        "name": "Andrea Ghiglietti"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Ghiglietti"
                },
                "author": "Andrea Ghiglietti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09648v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09648v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08753v2",
                "updated": "2025-02-27T14:07:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    7,
                    23,
                    3,
                    58,
                    0
                ],
                "published": "2024-03-13T17:51:47Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    17,
                    51,
                    47,
                    2,
                    73,
                    0
                ],
                "title": "Invalid proxies and volatility changes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invalid proxies and volatility changes"
                },
                "summary": "When in proxy-SVARs the covariance matrix of VAR disturbances is subject to\nexogenous, permanent breaks that cause IRFs to change across volatility\nregimes, even strong, exogenous external instruments yield inconsistent\nestimates of the dynamic causal effects. However, if these volatility shifts\nare properly incorporated into the analysis through (testable) \"stability\nrestrictions\", we demonstrate that the target IRFs are point-identified and can\nbe estimated consistently under a necessary and sufficient rank condition. If\nthe shifts in volatility are sufficiently informative, standard asymptotic\ninference remains valid even with (i) local-to-zero covariance between the\nproxies and the instrumented structural shocks, and (ii) potential failures of\ninstrument exogeneity. Intuitively, shifts in volatility act similarly to\nstrong instruments that are correlated with both the target and non-target\nshocks. We illustrate the effectiveness of our approach by revisiting a seminal\nfiscal proxy-SVAR for the US economy. We detect a sharp change in the size of\nthe tax multiplier when the narrative tax instrument is complemented with the\ndecline in unconditional volatility observed during the transition from the\nGreat Inflation to the Great Moderation. The narrative tax instrument\ncontributes to identify the tax shock in both regimes, despite our empirical\nanalysis raises concerns about its \"statistical\" validity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When in proxy-SVARs the covariance matrix of VAR disturbances is subject to\nexogenous, permanent breaks that cause IRFs to change across volatility\nregimes, even strong, exogenous external instruments yield inconsistent\nestimates of the dynamic causal effects. However, if these volatility shifts\nare properly incorporated into the analysis through (testable) \"stability\nrestrictions\", we demonstrate that the target IRFs are point-identified and can\nbe estimated consistently under a necessary and sufficient rank condition. If\nthe shifts in volatility are sufficiently informative, standard asymptotic\ninference remains valid even with (i) local-to-zero covariance between the\nproxies and the instrumented structural shocks, and (ii) potential failures of\ninstrument exogeneity. Intuitively, shifts in volatility act similarly to\nstrong instruments that are correlated with both the target and non-target\nshocks. We illustrate the effectiveness of our approach by revisiting a seminal\nfiscal proxy-SVAR for the US economy. We detect a sharp change in the size of\nthe tax multiplier when the narrative tax instrument is complemented with the\ndecline in unconditional volatility observed during the transition from the\nGreat Inflation to the Great Moderation. The narrative tax instrument\ncontributes to identify the tax shock in both regimes, despite our empirical\nanalysis raises concerns about its \"statistical\" validity."
                },
                "authors": [
                    {
                        "name": "Giovanni Angelini"
                    },
                    {
                        "name": "Luca Fanelli"
                    },
                    {
                        "name": "Luca Neri"
                    }
                ],
                "author_detail": {
                    "name": "Luca Neri"
                },
                "author": "Luca Neri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20110v1",
                "updated": "2025-02-27T14:03:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    3,
                    15,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T14:03:15Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    3,
                    15,
                    3,
                    58,
                    0
                ],
                "title": "UniDepthV2: Universal Monocular Metric Depth Estimation Made Simpler",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniDepthV2: Universal Monocular Metric Depth Estimation Made Simpler"
                },
                "summary": "Accurate monocular metric depth estimation (MMDE) is crucial to solving\ndownstream tasks in 3D perception and modeling. However, the remarkable\naccuracy of recent MMDE methods is confined to their training domains. These\nmethods fail to generalize to unseen domains even in the presence of moderate\ndomain gaps, which hinders their practical applicability. We propose a new\nmodel, UniDepthV2, capable of reconstructing metric 3D scenes from solely\nsingle images across domains. Departing from the existing MMDE paradigm,\nUniDepthV2 directly predicts metric 3D points from the input image at inference\ntime without any additional information, striving for a universal and flexible\nMMDE solution. In particular, UniDepthV2 implements a self-promptable camera\nmodule predicting a dense camera representation to condition depth features.\nOur model exploits a pseudo-spherical output representation, which disentangles\nthe camera and depth representations. In addition, we propose a geometric\ninvariance loss that promotes the invariance of camera-prompted depth features.\nUniDepthV2 improves its predecessor UniDepth model via a new edge-guided loss\nwhich enhances the localization and sharpness of edges in the metric depth\noutputs, a revisited, simplified and more efficient architectural design, and\nan additional uncertainty-level output which enables downstream tasks requiring\nconfidence. Thorough evaluations on ten depth datasets in a zero-shot regime\nconsistently demonstrate the superior performance and generalization of\nUniDepthV2. Code and models are available at\nhttps://github.com/lpiccinelli-eth/UniDepth",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate monocular metric depth estimation (MMDE) is crucial to solving\ndownstream tasks in 3D perception and modeling. However, the remarkable\naccuracy of recent MMDE methods is confined to their training domains. These\nmethods fail to generalize to unseen domains even in the presence of moderate\ndomain gaps, which hinders their practical applicability. We propose a new\nmodel, UniDepthV2, capable of reconstructing metric 3D scenes from solely\nsingle images across domains. Departing from the existing MMDE paradigm,\nUniDepthV2 directly predicts metric 3D points from the input image at inference\ntime without any additional information, striving for a universal and flexible\nMMDE solution. In particular, UniDepthV2 implements a self-promptable camera\nmodule predicting a dense camera representation to condition depth features.\nOur model exploits a pseudo-spherical output representation, which disentangles\nthe camera and depth representations. In addition, we propose a geometric\ninvariance loss that promotes the invariance of camera-prompted depth features.\nUniDepthV2 improves its predecessor UniDepth model via a new edge-guided loss\nwhich enhances the localization and sharpness of edges in the metric depth\noutputs, a revisited, simplified and more efficient architectural design, and\nan additional uncertainty-level output which enables downstream tasks requiring\nconfidence. Thorough evaluations on ten depth datasets in a zero-shot regime\nconsistently demonstrate the superior performance and generalization of\nUniDepthV2. Code and models are available at\nhttps://github.com/lpiccinelli-eth/UniDepth"
                },
                "authors": [
                    {
                        "name": "Luigi Piccinelli"
                    },
                    {
                        "name": "Christos Sakaridis"
                    },
                    {
                        "name": "Yung-Hsu Yang"
                    },
                    {
                        "name": "Mattia Segu"
                    },
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Wim Abbeloos"
                    },
                    {
                        "name": "Luc Van Gool"
                    }
                ],
                "author_detail": {
                    "name": "Luc Van Gool"
                },
                "author": "Luc Van Gool",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2403.18913",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11807v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11807v6",
                "updated": "2025-02-27T13:57:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    13,
                    57,
                    52,
                    3,
                    58,
                    0
                ],
                "published": "2024-03-18T14:04:47Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    14,
                    4,
                    47,
                    0,
                    78,
                    0
                ],
                "title": "How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming\n  Ability in Multi-Agent Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming\n  Ability in Multi-Agent Environments"
                },
                "summary": "Decision-making is a complex process requiring diverse abilities, making it\nan excellent framework for evaluating Large Language Models (LLMs). Researchers\nhave examined LLMs' decision-making through the lens of Game Theory. However,\nexisting evaluation mainly focus on two-player scenarios where an LLM competes\nagainst another. Additionally, previous benchmarks suffer from test set leakage\ndue to their static design. We introduce GAMA($\\gamma$)-Bench, a new framework\nfor evaluating LLMs' Gaming Ability in Multi-Agent environments. It includes\neight classical game theory scenarios and a dynamic scoring scheme specially\ndesigned to quantitatively assess LLMs' performance. $\\gamma$-Bench allows\nflexible game settings and adapts the scoring system to different game\nparameters, enabling comprehensive evaluation of robustness, generalizability,\nand strategies for improvement. Our results indicate that GPT-3.5 demonstrates\nstrong robustness but limited generalizability, which can be enhanced using\nmethods like Chain-of-Thought. We also evaluate 13 LLMs from 6 model families,\nincluding GPT-3.5, GPT-4, Gemini, LLaMA-3.1, Mixtral, and Qwen-2.\nGemini-1.5-Pro outperforms others, scoring of $69.8$ out of $100$, followed by\nLLaMA-3.1-70B ($65.9$) and Mixtral-8x22B ($62.4$). Our code and experimental\nresults are publicly available at https://github.com/CUHK-ARISE/GAMABench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-making is a complex process requiring diverse abilities, making it\nan excellent framework for evaluating Large Language Models (LLMs). Researchers\nhave examined LLMs' decision-making through the lens of Game Theory. However,\nexisting evaluation mainly focus on two-player scenarios where an LLM competes\nagainst another. Additionally, previous benchmarks suffer from test set leakage\ndue to their static design. We introduce GAMA($\\gamma$)-Bench, a new framework\nfor evaluating LLMs' Gaming Ability in Multi-Agent environments. It includes\neight classical game theory scenarios and a dynamic scoring scheme specially\ndesigned to quantitatively assess LLMs' performance. $\\gamma$-Bench allows\nflexible game settings and adapts the scoring system to different game\nparameters, enabling comprehensive evaluation of robustness, generalizability,\nand strategies for improvement. Our results indicate that GPT-3.5 demonstrates\nstrong robustness but limited generalizability, which can be enhanced using\nmethods like Chain-of-Thought. We also evaluate 13 LLMs from 6 model families,\nincluding GPT-3.5, GPT-4, Gemini, LLaMA-3.1, Mixtral, and Qwen-2.\nGemini-1.5-Pro outperforms others, scoring of $69.8$ out of $100$, followed by\nLLaMA-3.1-70B ($65.9$) and Mixtral-8x22B ($62.4$). Our code and experimental\nresults are publicly available at https://github.com/CUHK-ARISE/GAMABench."
                },
                "authors": [
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Eric John Li"
                    },
                    {
                        "name": "Man Ho Lam"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Youliang Yuan"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "arxiv_comment": "Accepted to ICLR 2025; 11 pages of main text; 26 pages of appendices;\n  Included models: GPT-3.5-{0613, 1106, 0125}, GPT-4-0125, GPT-4o-0806,\n  Gemini-{1.0, 1.5)-Pro, LLaMA-3.1-{7, 70, 405}B, Mixtral-8x{7, 22}B,\n  Qwen-2-72B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.11807v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11807v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10781v2",
                "updated": "2025-02-27T13:51:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    13,
                    51,
                    10,
                    3,
                    58,
                    0
                ],
                "published": "2024-11-16T11:51:33Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    11,
                    51,
                    33,
                    5,
                    321,
                    0
                ],
                "title": "Bag of Design Choices for Inference of High-Resolution Masked Generative\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bag of Design Choices for Inference of High-Resolution Masked Generative\n  Transformer"
                },
                "summary": "Text-to-image diffusion models (DMs) develop at an unprecedented pace,\nsupported by thorough theoretical exploration and empirical analysis.\nUnfortunately, the discrepancy between DMs and autoregressive models (ARMs)\ncomplicates the path toward achieving the goal of unified vision and language\ngeneration. Recently, the masked generative Transformer (MGT) serves as a\npromising intermediary between DM and ARM by predicting randomly masked image\ntokens (i.e., masked image modeling), combining the efficiency of DM with the\ndiscrete token nature of ARM. However, we find that the comprehensive analyses\nregarding the inference for MGT are virtually non-existent, and thus we aim to\npresent positive design choices to fill this gap. We propose and redesign a set\nof enhanced inference techniques tailored for MGT, providing a detailed\nanalysis of their performance. Additionally, we explore several DM-based\napproaches aimed at accelerating the sampling process on MGT. Extensive\nexperiments and empirical analyses on the recent SOTA MGT, such as MaskGIT and\nMeissonic lead to concrete and effective design choices, and these design\nchoices can be merged to achieve further performance gains. For instance, in\nterms of enhanced inference, we achieve winning rates of approximately 70%\ncompared to vanilla sampling on HPS v2 with Meissonic-1024x1024.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image diffusion models (DMs) develop at an unprecedented pace,\nsupported by thorough theoretical exploration and empirical analysis.\nUnfortunately, the discrepancy between DMs and autoregressive models (ARMs)\ncomplicates the path toward achieving the goal of unified vision and language\ngeneration. Recently, the masked generative Transformer (MGT) serves as a\npromising intermediary between DM and ARM by predicting randomly masked image\ntokens (i.e., masked image modeling), combining the efficiency of DM with the\ndiscrete token nature of ARM. However, we find that the comprehensive analyses\nregarding the inference for MGT are virtually non-existent, and thus we aim to\npresent positive design choices to fill this gap. We propose and redesign a set\nof enhanced inference techniques tailored for MGT, providing a detailed\nanalysis of their performance. Additionally, we explore several DM-based\napproaches aimed at accelerating the sampling process on MGT. Extensive\nexperiments and empirical analyses on the recent SOTA MGT, such as MaskGIT and\nMeissonic lead to concrete and effective design choices, and these design\nchoices can be merged to achieve further performance gains. For instance, in\nterms of enhanced inference, we achieve winning rates of approximately 70%\ncompared to vanilla sampling on HPS v2 with Meissonic-1024x1024."
                },
                "authors": [
                    {
                        "name": "Shitong Shao"
                    },
                    {
                        "name": "Zikai Zhou"
                    },
                    {
                        "name": "Tian Ye"
                    },
                    {
                        "name": "Lichen Bai"
                    },
                    {
                        "name": "Zhiqiang Xu"
                    },
                    {
                        "name": "Zeke Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zeke Xie"
                },
                "author": "Zeke Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03982v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03982v3",
                "updated": "2025-02-27T13:50:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    13,
                    50,
                    49,
                    3,
                    58,
                    0
                ],
                "published": "2025-01-07T18:39:28Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    39,
                    28,
                    1,
                    7,
                    0
                ],
                "title": "Free Anytime Validity by Sequentializing a Test and Optional\n  Continuation with Tests as Future Significance Levels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free Anytime Validity by Sequentializing a Test and Optional\n  Continuation with Tests as Future Significance Levels"
                },
                "summary": "Anytime valid sequential tests permit us to stop and continue testing based\non the current data, without invalidating the inference. Given a maximum number\nof observations $N$, one may believe this must come at the cost of power when\ncompared to a conventional test that waits until all $N$ observations have\narrived. Our first contribution is to show that this is false: for any valid\ntest based on $N$ observations, we derive an anytime valid sequential test that\nmatches it after $N$ observations. Our second contribution is that the outcome\nof a continuously-interpreted test can be used as a significance level in\nsubsequent testing, leading to an overall procedure that is valid at the\noriginal significance level. Combined this shows both the value of\ncontinuously-interpreted tests, and the fact that anytime validity and optional\ncontinuation are readily available in traditional testing, without requiring\nexplicit use of e-values. We illustrate this by deriving the anytime valid\nsequentialized $z$-test and $t$-test, which at time $N$ coincide with the\ntraditional $z$-test and $t$-test. Lastly, we show the popular log-optimal\nsequential $z$-test can be interpreted as desiring a rejection by the\ntraditional $z$-test at some tiny significance level in the distant future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anytime valid sequential tests permit us to stop and continue testing based\non the current data, without invalidating the inference. Given a maximum number\nof observations $N$, one may believe this must come at the cost of power when\ncompared to a conventional test that waits until all $N$ observations have\narrived. Our first contribution is to show that this is false: for any valid\ntest based on $N$ observations, we derive an anytime valid sequential test that\nmatches it after $N$ observations. Our second contribution is that the outcome\nof a continuously-interpreted test can be used as a significance level in\nsubsequent testing, leading to an overall procedure that is valid at the\noriginal significance level. Combined this shows both the value of\ncontinuously-interpreted tests, and the fact that anytime validity and optional\ncontinuation are readily available in traditional testing, without requiring\nexplicit use of e-values. We illustrate this by deriving the anytime valid\nsequentialized $z$-test and $t$-test, which at time $N$ coincide with the\ntraditional $z$-test and $t$-test. Lastly, we show the popular log-optimal\nsequential $z$-test can be interpreted as desiring a rejection by the\ntraditional $z$-test at some tiny significance level in the distant future."
                },
                "authors": [
                    {
                        "name": "Nick W. Koning"
                    },
                    {
                        "name": "Sam van Meer"
                    }
                ],
                "author_detail": {
                    "name": "Sam van Meer"
                },
                "author": "Sam van Meer",
                "arxiv_comment": "Changes: added one-sided sequentialised t-test plus general strategy\n  for monotone likelihood ratios, added connection to tracking strong p-value,\n  added switching significance levels, more explicitly discourage randomization\n  and clarified relationship to randomization, generally improved writing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03982v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03982v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20086v1",
                "updated": "2025-02-27T13:44:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    13,
                    44,
                    15,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T13:44:15Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    13,
                    44,
                    15,
                    3,
                    58,
                    0
                ],
                "title": "Subspace accelerated measure transport methods for fast and scalable\n  sequential experimental design, with application to photoacoustic imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subspace accelerated measure transport methods for fast and scalable\n  sequential experimental design, with application to photoacoustic imaging"
                },
                "summary": "We propose a novel approach for sequential optimal experimental design (sOED)\nfor Bayesian inverse problems involving expensive models with large-dimensional\nunknown parameters. The focus of this work is on designs that maximize the\nexpected information gain (EIG) from prior to posterior, which is a\ncomputationally challenging task in the non-Gaussian setting. This challenge is\namplified in sOED, as the incremental expected information gain (iEIG) must be\napproximated multiple times in distinct stages, with both prior and posterior\ndistributions often being intractable. To address this, we derive a\nderivative-based upper bound for the iEIG, which not only guides design\nplacement but also enables the construction of projectors onto\nlikelihood-informed subspaces, facilitating parameter dimension reduction. By\ncombining this approach with conditional measure transport maps for the\nsequence of posteriors, we develop a unified framework for sOED, together with\namortized inference, scalable to high- and infinite-dimensional problems.\nNumerical experiments for two inverse problems governed by partial differential\nequations (PDEs) demonstrate the effectiveness of designs that maximize our\nproposed upper bound.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel approach for sequential optimal experimental design (sOED)\nfor Bayesian inverse problems involving expensive models with large-dimensional\nunknown parameters. The focus of this work is on designs that maximize the\nexpected information gain (EIG) from prior to posterior, which is a\ncomputationally challenging task in the non-Gaussian setting. This challenge is\namplified in sOED, as the incremental expected information gain (iEIG) must be\napproximated multiple times in distinct stages, with both prior and posterior\ndistributions often being intractable. To address this, we derive a\nderivative-based upper bound for the iEIG, which not only guides design\nplacement but also enables the construction of projectors onto\nlikelihood-informed subspaces, facilitating parameter dimension reduction. By\ncombining this approach with conditional measure transport maps for the\nsequence of posteriors, we develop a unified framework for sOED, together with\namortized inference, scalable to high- and infinite-dimensional problems.\nNumerical experiments for two inverse problems governed by partial differential\nequations (PDEs) demonstrate the effectiveness of designs that maximize our\nproposed upper bound."
                },
                "authors": [
                    {
                        "name": "Tiangang Cui"
                    },
                    {
                        "name": "Karina Koval"
                    },
                    {
                        "name": "Roland Herzog"
                    },
                    {
                        "name": "Robert Scheichl"
                    }
                ],
                "author_detail": {
                    "name": "Robert Scheichl"
                },
                "author": "Robert Scheichl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62K05, 62F15, 65D40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20082v1",
                "updated": "2025-02-27T13:41:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    13,
                    41,
                    7,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T13:41:07Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    13,
                    41,
                    7,
                    3,
                    58,
                    0
                ],
                "title": "LongRoPE2: Near-Lossless LLM Context Window Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongRoPE2: Near-Lossless LLM Context Window Scaling"
                },
                "summary": "LongRoPE2 is a novel approach that extends the effective context window of\npre-trained large language models (LLMs) to the target length, while preserving\nthe performance on the original shorter context window. This is achieved by\nthree contributions: (1) a hypothesis that insufficient training in higher RoPE\ndimensions contributes to the persistent out-of-distribution (OOD) issues\nobserved in existing methods; (2) an effective RoPE rescaling algorithm that\nadopts evolutionary search guided by \"needle-driven\" perplexity to address the\ninsufficient training problem; (3) a mixed context window training approach\nthat fine-tunes model weights to adopt rescaled RoPE for long-context sequences\nwhile preserving the short-context performance with the original RoPE.\nExtensive experiments on LLaMA3-8B and Phi3-mini-3.8B across various benchmarks\nvalidate the hypothesis and demonstrate the effectiveness of LongRoPE2.\nRemarkably, LongRoPE2 extends LLaMA3-8B to achieve a 128K effective context\nlength while retaining over 98.5% of short-context performance, using only 10B\ntokens -- 80x fewer than Meta's approach, which fails to reach the target\neffective context length. Code will be available at\nhttps://github.com/microsoft/LongRoPE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongRoPE2 is a novel approach that extends the effective context window of\npre-trained large language models (LLMs) to the target length, while preserving\nthe performance on the original shorter context window. This is achieved by\nthree contributions: (1) a hypothesis that insufficient training in higher RoPE\ndimensions contributes to the persistent out-of-distribution (OOD) issues\nobserved in existing methods; (2) an effective RoPE rescaling algorithm that\nadopts evolutionary search guided by \"needle-driven\" perplexity to address the\ninsufficient training problem; (3) a mixed context window training approach\nthat fine-tunes model weights to adopt rescaled RoPE for long-context sequences\nwhile preserving the short-context performance with the original RoPE.\nExtensive experiments on LLaMA3-8B and Phi3-mini-3.8B across various benchmarks\nvalidate the hypothesis and demonstrate the effectiveness of LongRoPE2.\nRemarkably, LongRoPE2 extends LLaMA3-8B to achieve a 128K effective context\nlength while retaining over 98.5% of short-context performance, using only 10B\ntokens -- 80x fewer than Meta's approach, which fails to reach the target\neffective context length. Code will be available at\nhttps://github.com/microsoft/LongRoPE."
                },
                "authors": [
                    {
                        "name": "Ning Shang"
                    },
                    {
                        "name": "Li Lyna Zhang"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Gaokai Zhang"
                    },
                    {
                        "name": "Gilsinia Lopez"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Weizhu Chen"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06153v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06153v3",
                "updated": "2025-02-27T13:33:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    13,
                    33,
                    55,
                    3,
                    58,
                    0
                ],
                "published": "2024-10-08T15:52:42Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    15,
                    52,
                    42,
                    1,
                    282,
                    0
                ],
                "title": "AgentSquare: Automatic LLM Agent Search in Modular Design Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentSquare: Automatic LLM Agent Search in Modular Design Space"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have led to a rapid\ngrowth of agentic systems capable of handling a wide range of complex tasks.\nHowever, current research largely relies on manual, task-specific design,\nlimiting their adaptability to novel tasks. In this paper, we introduce a new\nresearch problem: Modularized LLM Agent Search (MoLAS). We propose a modular\ndesign space that abstracts existing LLM agent designs into four fundamental\nmodules with uniform IO interface: Planning, Reasoning, Tool Use, and Memory.\nBuilding on this design space, we present a novel LLM agent search framework\ncalled AgentSquare, which introduces two core mechanisms, i.e., module\nevolution and recombination, to efficiently search for optimized LLM agents. To\nfurther accelerate the process, we design a performance predictor that uses\nin-context surrogate models to skip unpromising agent designs. Extensive\nexperiments across six benchmarks, covering the diverse scenarios of web,\nembodied, tool use and game applications, show that AgentSquare substantially\noutperforms hand-crafted agents, achieving an average performance gain of 17.2%\nagainst best-known human designs. Moreover, AgentSquare can generate\ninterpretable design insights, enabling a deeper understanding of agentic\narchitecture and its impact on task performance. We believe that the modular\ndesign space and AgentSquare search framework offer a platform for fully\nexploiting the potential of prior successful designs and consolidating the\ncollective efforts of research community. Code repo is available at\nhttps://github.com/tsinghua-fib-lab/AgentSquare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have led to a rapid\ngrowth of agentic systems capable of handling a wide range of complex tasks.\nHowever, current research largely relies on manual, task-specific design,\nlimiting their adaptability to novel tasks. In this paper, we introduce a new\nresearch problem: Modularized LLM Agent Search (MoLAS). We propose a modular\ndesign space that abstracts existing LLM agent designs into four fundamental\nmodules with uniform IO interface: Planning, Reasoning, Tool Use, and Memory.\nBuilding on this design space, we present a novel LLM agent search framework\ncalled AgentSquare, which introduces two core mechanisms, i.e., module\nevolution and recombination, to efficiently search for optimized LLM agents. To\nfurther accelerate the process, we design a performance predictor that uses\nin-context surrogate models to skip unpromising agent designs. Extensive\nexperiments across six benchmarks, covering the diverse scenarios of web,\nembodied, tool use and game applications, show that AgentSquare substantially\noutperforms hand-crafted agents, achieving an average performance gain of 17.2%\nagainst best-known human designs. Moreover, AgentSquare can generate\ninterpretable design insights, enabling a deeper understanding of agentic\narchitecture and its impact on task performance. We believe that the modular\ndesign space and AgentSquare search framework offer a platform for fully\nexploiting the potential of prior successful designs and consolidating the\ncollective efforts of research community. Code repo is available at\nhttps://github.com/tsinghua-fib-lab/AgentSquare."
                },
                "authors": [
                    {
                        "name": "Yu Shang"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Keyu Zhao"
                    },
                    {
                        "name": "Likai Ma"
                    },
                    {
                        "name": "Jiahe Liu"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06153v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06153v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20076v1",
                "updated": "2025-02-27T13:33:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    13,
                    33,
                    27,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T13:33:27Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    13,
                    33,
                    27,
                    3,
                    58,
                    0
                ],
                "title": "Purported quantitative support for multiple introductions of SARS-CoV-2\n  into humans is an artefact of an imbalanced hypothesis testing framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purported quantitative support for multiple introductions of SARS-CoV-2\n  into humans is an artefact of an imbalanced hypothesis testing framework"
                },
                "summary": "A prominent report claimed substantial support for two introductions of\nSARS-CoV-2 into humans using a calculation that combined phylodynamic\ninferences and epidemic models. Inspection of the calculation identifies an\nimbalance in the hypothesis testing framework that confounds this result; the\nsingle-introduction model was tested against more stringent conditions than the\ntwo-introduction model. Here, I show that when the two-introduction model is\ntested against the same conditions, the support disappears.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A prominent report claimed substantial support for two introductions of\nSARS-CoV-2 into humans using a calculation that combined phylodynamic\ninferences and epidemic models. Inspection of the calculation identifies an\nimbalance in the hypothesis testing framework that confounds this result; the\nsingle-introduction model was tested against more stringent conditions than the\ntwo-introduction model. Here, I show that when the two-introduction model is\ntested against the same conditions, the support disappears."
                },
                "authors": [
                    {
                        "name": "Angus McCowan"
                    }
                ],
                "author_detail": {
                    "name": "Angus McCowan"
                },
                "author": "Angus McCowan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20073v1",
                "updated": "2025-02-27T13:31:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    13,
                    31,
                    13,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T13:31:13Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    13,
                    31,
                    13,
                    3,
                    58,
                    0
                ],
                "title": "Collab-Overcooked: Benchmarking and Evaluating Large Language Models as\n  Collaborative Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collab-Overcooked: Benchmarking and Evaluating Large Language Models as\n  Collaborative Agents"
                },
                "summary": "Large language models (LLMs) based agent systems have made great strides in\nreal-world applications beyond traditional NLP tasks. This paper proposes a new\nLLM-powered Multi-Agent System (LLM-MAS) benchmark, Collab-Overcooked, built on\nthe popular Overcooked-AI game with more applicable and challenging tasks in\ninteractive environments. Collab-Overcooked extends existing benchmarks from\ntwo novel perspectives. First, it provides a multi-agent framework supporting\ndiverse tasks and objectives and encourages collaboration through natural\nlanguage communication. Second, it introduces a spectrum of process-oriented\nevaluation metrics to assess the fine-grained collaboration capabilities of\ndifferent LLM agents, a dimension often overlooked in prior work. We conduct\nextensive experiments over 10 popular LLMs and show that, while the LLMs\npresent a strong ability in goal interpretation, there is a significant\ndiscrepancy in active collaboration and continuous adaption that are critical\nfor efficiently fulfilling complicated tasks. Notably, we highlight the\nstrengths and weaknesses in LLM-MAS and provide insights for improving and\nevaluating LLM-MAS on a unified and open-sourced benchmark. Environments, 30\nopen-ended tasks, and an integrated evaluation package are now publicly\navailable at https://github.com/YusaeMeow/Collab-Overcooked.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) based agent systems have made great strides in\nreal-world applications beyond traditional NLP tasks. This paper proposes a new\nLLM-powered Multi-Agent System (LLM-MAS) benchmark, Collab-Overcooked, built on\nthe popular Overcooked-AI game with more applicable and challenging tasks in\ninteractive environments. Collab-Overcooked extends existing benchmarks from\ntwo novel perspectives. First, it provides a multi-agent framework supporting\ndiverse tasks and objectives and encourages collaboration through natural\nlanguage communication. Second, it introduces a spectrum of process-oriented\nevaluation metrics to assess the fine-grained collaboration capabilities of\ndifferent LLM agents, a dimension often overlooked in prior work. We conduct\nextensive experiments over 10 popular LLMs and show that, while the LLMs\npresent a strong ability in goal interpretation, there is a significant\ndiscrepancy in active collaboration and continuous adaption that are critical\nfor efficiently fulfilling complicated tasks. Notably, we highlight the\nstrengths and weaknesses in LLM-MAS and provide insights for improving and\nevaluating LLM-MAS on a unified and open-sourced benchmark. Environments, 30\nopen-ended tasks, and an integrated evaluation package are now publicly\navailable at https://github.com/YusaeMeow/Collab-Overcooked."
                },
                "authors": [
                    {
                        "name": "Haochen Sun"
                    },
                    {
                        "name": "Shuwen Zhang"
                    },
                    {
                        "name": "Lei Ren"
                    },
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Hao Fu"
                    },
                    {
                        "name": "Caixia Yuan"
                    },
                    {
                        "name": "Xiaojie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojie Wang"
                },
                "author": "Xiaojie Wang",
                "arxiv_comment": "25 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18934v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18934v2",
                "updated": "2025-02-27T13:20:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    13,
                    20,
                    53,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-26T08:36:20Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    8,
                    36,
                    20,
                    2,
                    57,
                    0
                ],
                "title": "Kanana: Compute-efficient Bilingual Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kanana: Compute-efficient Bilingual Language Models"
                },
                "summary": "We introduce Kanana, a series of bilingual language models that demonstrate\nexceeding performance in Korean and competitive performance in English. The\ncomputational cost of Kanana is significantly lower than that of\nstate-of-the-art models of similar size. The report details the techniques\nemployed during pre-training to achieve compute-efficient yet competitive\nmodels, including high quality data filtering, staged pre-training, depth\nup-scaling, and pruning and distillation. Furthermore, the report outlines the\nmethodologies utilized during the post-training of the Kanana models,\nencompassing supervised fine-tuning and preference optimization, aimed at\nenhancing their capability for seamless interaction with users. Lastly, the\nreport elaborates on plausible approaches used for language model adaptation to\nspecific scenarios, such as embedding, retrieval augmented generation, and\nfunction calling. The Kanana model series spans from 2.1B to 32.5B parameters\nwith 2.1B models (base, instruct, embedding) publicly released to promote\nresearch on Korean language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Kanana, a series of bilingual language models that demonstrate\nexceeding performance in Korean and competitive performance in English. The\ncomputational cost of Kanana is significantly lower than that of\nstate-of-the-art models of similar size. The report details the techniques\nemployed during pre-training to achieve compute-efficient yet competitive\nmodels, including high quality data filtering, staged pre-training, depth\nup-scaling, and pruning and distillation. Furthermore, the report outlines the\nmethodologies utilized during the post-training of the Kanana models,\nencompassing supervised fine-tuning and preference optimization, aimed at\nenhancing their capability for seamless interaction with users. Lastly, the\nreport elaborates on plausible approaches used for language model adaptation to\nspecific scenarios, such as embedding, retrieval augmented generation, and\nfunction calling. The Kanana model series spans from 2.1B to 32.5B parameters\nwith 2.1B models (base, instruct, embedding) publicly released to promote\nresearch on Korean language models."
                },
                "authors": [
                    {
                        "name": "Kanana LLM Team"
                    },
                    {
                        "name": "Yunju Bak"
                    },
                    {
                        "name": "Hojin Lee"
                    },
                    {
                        "name": "Minho Ryu"
                    },
                    {
                        "name": "Jiyeon Ham"
                    },
                    {
                        "name": "Seungjae Jung"
                    },
                    {
                        "name": "Daniel Wontae Nam"
                    },
                    {
                        "name": "Taegyeong Eo"
                    },
                    {
                        "name": "Donghun Lee"
                    },
                    {
                        "name": "Doohae Jung"
                    },
                    {
                        "name": "Boseop Kim"
                    },
                    {
                        "name": "Nayeon Kim"
                    },
                    {
                        "name": "Jaesun Park"
                    },
                    {
                        "name": "Hyunho Kim"
                    },
                    {
                        "name": "Hyunwoong Ko"
                    },
                    {
                        "name": "Changmin Lee"
                    },
                    {
                        "name": "Kyoung-Woon On"
                    },
                    {
                        "name": "Seulye Baeg"
                    },
                    {
                        "name": "Junrae Cho"
                    },
                    {
                        "name": "Sunghee Jung"
                    },
                    {
                        "name": "Jieun Kang"
                    },
                    {
                        "name": "EungGyun Kim"
                    },
                    {
                        "name": "Eunhwa Kim"
                    },
                    {
                        "name": "Byeongil Ko"
                    },
                    {
                        "name": "Daniel Lee"
                    },
                    {
                        "name": "Minchul Lee"
                    },
                    {
                        "name": "Miok Lee"
                    },
                    {
                        "name": "Shinbok Lee"
                    },
                    {
                        "name": "Gaeun Seo"
                    }
                ],
                "author_detail": {
                    "name": "Gaeun Seo"
                },
                "author": "Gaeun Seo",
                "arxiv_comment": "40 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18934v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18934v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06899v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06899v2",
                "updated": "2025-02-27T13:08:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    13,
                    8,
                    46,
                    3,
                    58,
                    0
                ],
                "published": "2024-11-11T11:57:37Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    11,
                    57,
                    37,
                    0,
                    316,
                    0
                ],
                "title": "LongSafety: Enhance Safety for Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongSafety: Enhance Safety for Long-Context LLMs"
                },
                "summary": "Recent advancements in model architectures and length extrapolation\ntechniques have significantly extended the context length of large language\nmodels (LLMs), paving the way for their application in increasingly complex\ntasks. However, despite the growing capabilities of long-context LLMs, the\nsafety issues in long-context scenarios remain underexplored. While safety\nalignment in short context has been widely studied, the safety concerns of\nlong-context LLMs have not been adequately addressed. In this work, we\nintroduce \\textbf{LongSafety}, a comprehensive safety alignment dataset for\nlong-context LLMs, containing 10 tasks and 17k samples, with an average length\nof 40.9k tokens. Our experiments demonstrate that training with LongSafety can\nenhance long-context safety performance while enhancing short-context safety\nand preserving general capabilities. Furthermore, we demonstrate that\nlong-context safety does not equal long-context alignment with short-context\nsafety data and LongSafety has generalizing capabilities in context length and\nlong-context safety scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in model architectures and length extrapolation\ntechniques have significantly extended the context length of large language\nmodels (LLMs), paving the way for their application in increasingly complex\ntasks. However, despite the growing capabilities of long-context LLMs, the\nsafety issues in long-context scenarios remain underexplored. While safety\nalignment in short context has been widely studied, the safety concerns of\nlong-context LLMs have not been adequately addressed. In this work, we\nintroduce \\textbf{LongSafety}, a comprehensive safety alignment dataset for\nlong-context LLMs, containing 10 tasks and 17k samples, with an average length\nof 40.9k tokens. Our experiments demonstrate that training with LongSafety can\nenhance long-context safety performance while enhancing short-context safety\nand preserving general capabilities. Furthermore, we demonstrate that\nlong-context safety does not equal long-context alignment with short-context\nsafety data and LongSafety has generalizing capabilities in context length and\nlong-context safety scenarios."
                },
                "authors": [
                    {
                        "name": "Mianqiu Huang"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Shaojun Zhou"
                    },
                    {
                        "name": "Mozhi Zhang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Linyang Li"
                    },
                    {
                        "name": "Chenkun Tan"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Pengyu Wang"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Yaqian Zhou"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06899v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06899v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17839v2",
                "updated": "2025-02-27T12:55:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    55,
                    8,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-25T04:38:38Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    4,
                    38,
                    38,
                    1,
                    56,
                    0
                ],
                "title": "Say Less, Mean More: Leveraging Pragmatics in Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Say Less, Mean More: Leveraging Pragmatics in Retrieval-Augmented\n  Generation"
                },
                "summary": "We propose a simple, unsupervised method that injects pragmatic principles in\nretrieval-augmented generation (RAG) frameworks such as Dense Passage Retrieval\nto enhance the utility of retrieved contexts. Our approach first identifies\nwhich sentences in a pool of documents retrieved by RAG are most relevant to\nthe question at hand, cover all the topics addressed in the input question and\nno more, and then highlights these sentences within their context, before they\nare provided to the LLM, without truncating or altering the context in any\nother way. We show that this simple idea brings consistent improvements in\nexperiments on three question answering tasks (ARC-Challenge, PubHealth and\nPopQA) using five different LLMs. It notably enhances relative accuracy by up\nto 19.7% on PubHealth and 10% on ARC-Challenge compared to a conventional RAG\nsystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple, unsupervised method that injects pragmatic principles in\nretrieval-augmented generation (RAG) frameworks such as Dense Passage Retrieval\nto enhance the utility of retrieved contexts. Our approach first identifies\nwhich sentences in a pool of documents retrieved by RAG are most relevant to\nthe question at hand, cover all the topics addressed in the input question and\nno more, and then highlights these sentences within their context, before they\nare provided to the LLM, without truncating or altering the context in any\nother way. We show that this simple idea brings consistent improvements in\nexperiments on three question answering tasks (ARC-Challenge, PubHealth and\nPopQA) using five different LLMs. It notably enhances relative accuracy by up\nto 19.7% on PubHealth and 10% on ARC-Challenge compared to a conventional RAG\nsystem."
                },
                "authors": [
                    {
                        "name": "Haris Riaz"
                    },
                    {
                        "name": "Ellen Riloff"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    }
                ],
                "author_detail": {
                    "name": "Mihai Surdeanu"
                },
                "author": "Mihai Surdeanu",
                "arxiv_comment": "16 pages, 2 figures, 8 tables. Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.00693v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.00693v3",
                "updated": "2025-02-27T12:49:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    49,
                    5,
                    3,
                    58,
                    0
                ],
                "published": "2023-06-01T14:02:45Z",
                "published_parsed": [
                    2023,
                    6,
                    1,
                    14,
                    2,
                    45,
                    3,
                    152,
                    0
                ],
                "title": "GPT4Image: Large Pre-trained Models Help Vision Models Learn Better on\n  Perception Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT4Image: Large Pre-trained Models Help Vision Models Learn Better on\n  Perception Task"
                },
                "summary": "The upsurge in pre-trained large models started by ChatGPT has swept across\nthe entire deep learning community. Such powerful models demonstrate advanced\ngenerative ability and multimodal understanding capability, which quickly set\nnew state of the arts on a variety of benchmarks. The pre-trained LLM usually\nplays the role as a universal AI model that can conduct various tasks like\narticle analysis and image comprehension. However, due to the prohibitively\nhigh memory and computational cost of implementing such a large model, the\nconventional models (such as CNN and ViT) are still essential for many visual\nperception tasks. In this paper, we propose to enhance the representation\nability of ordinary vision models on perception tasks (e.g. image\nclassification) by taking advantage of the off-the-shelf large pre-trained\nmodels. We present a new learning framework, dubbed GPT4Image, where the\nknowledge of the large pre-trained models are extracted to help CNNs and ViTs\nlearn better representations and achieve higher performance. Firstly, we curate\na high quality description set by prompting a multimodal LLM to generate\ndescriptions for training images. Then, these detailed descriptions are fed\ninto a pre-trained encoder to extract text embeddings that encodes the rich\nsemantics of images. During training, text embeddings will serve as extra\nsupervising signal and be aligned with image representations learned by vision\nmodels. The alignment process helps vision models achieve better performance\nwith the aid of pre-trained LLMs. We conduct extensive experiments to verify\nthe effectiveness of the proposed algorithm on various visual perception tasks\nfor heterogeneous model architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The upsurge in pre-trained large models started by ChatGPT has swept across\nthe entire deep learning community. Such powerful models demonstrate advanced\ngenerative ability and multimodal understanding capability, which quickly set\nnew state of the arts on a variety of benchmarks. The pre-trained LLM usually\nplays the role as a universal AI model that can conduct various tasks like\narticle analysis and image comprehension. However, due to the prohibitively\nhigh memory and computational cost of implementing such a large model, the\nconventional models (such as CNN and ViT) are still essential for many visual\nperception tasks. In this paper, we propose to enhance the representation\nability of ordinary vision models on perception tasks (e.g. image\nclassification) by taking advantage of the off-the-shelf large pre-trained\nmodels. We present a new learning framework, dubbed GPT4Image, where the\nknowledge of the large pre-trained models are extracted to help CNNs and ViTs\nlearn better representations and achieve higher performance. Firstly, we curate\na high quality description set by prompting a multimodal LLM to generate\ndescriptions for training images. Then, these detailed descriptions are fed\ninto a pre-trained encoder to extract text embeddings that encodes the rich\nsemantics of images. During training, text embeddings will serve as extra\nsupervising signal and be aligned with image representations learned by vision\nmodels. The alignment process helps vision models achieve better performance\nwith the aid of pre-trained LLMs. We conduct extensive experiments to verify\nthe effectiveness of the proposed algorithm on various visual perception tasks\nfor heterogeneous model architectures."
                },
                "authors": [
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Zhongqian Fu"
                    },
                    {
                        "name": "Chao Xu"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "arxiv_comment": "GitHub:\n  https://github.com/huawei-noah/Efficient-Computing/tree/master/GPT4Image/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.00693v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.00693v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04149v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04149v2",
                "updated": "2025-02-27T12:43:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    43,
                    29,
                    3,
                    58,
                    0
                ],
                "published": "2024-12-05T13:23:06Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    13,
                    23,
                    6,
                    3,
                    340,
                    0
                ],
                "title": "Frequency-Adaptive Low-Latency Object Detection Using Events and Frames",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frequency-Adaptive Low-Latency Object Detection Using Events and Frames"
                },
                "summary": "Fusing Events and RGB images for object detection leverages the robustness of\nEvent cameras in adverse environments and the rich semantic information\nprovided by RGB cameras. However, two critical mismatches: low-latency Events\n\\textit{vs.}~high-latency RGB frames; temporally sparse labels in training\n\\textit{vs.}~continuous flow in inference, significantly hinder the\nhigh-frequency fusion-based object detection. To address these challenges, we\npropose the \\textbf{F}requency-\\textbf{A}daptive Low-Latency \\textbf{O}bject\n\\textbf{D}etector (FAOD). FAOD aligns low-frequency RGB frames with\nhigh-frequency Events through an Align Module, which reinforces cross-modal\nstyle and spatial proximity to address the Event-RGB Mismatch. We further\npropose a training strategy, Time Shift, which enforces the module to align the\nprediction from temporally shifted Event-RGB pairs and their original\nrepresentation, that is, consistent with Event-aligned annotations. This\nstrategy enables the network to use high-frequency Event data as the primary\nreference while treating low-frequency RGB images as supplementary information,\nretaining the low-latency nature of the Event stream toward high-frequency\ndetection. Furthermore, we observe that these corrected Event-RGB pairs\ndemonstrate better generalization from low training frequency to higher\ninference frequencies compared to using Event data alone. Extensive experiments\non the PKU-DAVIS-SOD and DSEC-Detection datasets demonstrate that our FAOD\nachieves SOTA performance. Specifically, in the PKU-DAVIS-SOD Dataset, FAOD\nachieves 9.8 points improvement in terms of the mAP in fully paired Event-RGB\ndata with only a quarter of the parameters compared to SODFormer, and even\nmaintains robust performance (only a 3 points drop in mAP) under 80$\\times$\nEvent-RGB frequency mismatch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusing Events and RGB images for object detection leverages the robustness of\nEvent cameras in adverse environments and the rich semantic information\nprovided by RGB cameras. However, two critical mismatches: low-latency Events\n\\textit{vs.}~high-latency RGB frames; temporally sparse labels in training\n\\textit{vs.}~continuous flow in inference, significantly hinder the\nhigh-frequency fusion-based object detection. To address these challenges, we\npropose the \\textbf{F}requency-\\textbf{A}daptive Low-Latency \\textbf{O}bject\n\\textbf{D}etector (FAOD). FAOD aligns low-frequency RGB frames with\nhigh-frequency Events through an Align Module, which reinforces cross-modal\nstyle and spatial proximity to address the Event-RGB Mismatch. We further\npropose a training strategy, Time Shift, which enforces the module to align the\nprediction from temporally shifted Event-RGB pairs and their original\nrepresentation, that is, consistent with Event-aligned annotations. This\nstrategy enables the network to use high-frequency Event data as the primary\nreference while treating low-frequency RGB images as supplementary information,\nretaining the low-latency nature of the Event stream toward high-frequency\ndetection. Furthermore, we observe that these corrected Event-RGB pairs\ndemonstrate better generalization from low training frequency to higher\ninference frequencies compared to using Event data alone. Extensive experiments\non the PKU-DAVIS-SOD and DSEC-Detection datasets demonstrate that our FAOD\nachieves SOTA performance. Specifically, in the PKU-DAVIS-SOD Dataset, FAOD\nachieves 9.8 points improvement in terms of the mAP in fully paired Event-RGB\ndata with only a quarter of the parameters compared to SODFormer, and even\nmaintains robust performance (only a 3 points drop in mAP) under 80$\\times$\nEvent-RGB frequency mismatch."
                },
                "authors": [
                    {
                        "name": "Haitian Zhang"
                    },
                    {
                        "name": "Xiangyuan Wang"
                    },
                    {
                        "name": "Chang Xu"
                    },
                    {
                        "name": "Xinya Wang"
                    },
                    {
                        "name": "Fang Xu"
                    },
                    {
                        "name": "Huai Yu"
                    },
                    {
                        "name": "Lei Yu"
                    },
                    {
                        "name": "Wen Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wen Yang"
                },
                "author": "Wen Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04149v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04149v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v3",
                "updated": "2025-02-27T12:30:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    30,
                    43,
                    3,
                    58,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "ICLR 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20041v1",
                "updated": "2025-02-27T12:29:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    29,
                    44,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T12:29:44Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    29,
                    44,
                    3,
                    58,
                    0
                ],
                "title": "3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary\n  Affordance Detection in 3D Worlds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary\n  Affordance Detection in 3D Worlds"
                },
                "summary": "3D Affordance detection is a challenging problem with broad applications on\nvarious robotic tasks. Existing methods typically formulate the detection\nparadigm as a label-based semantic segmentation task. This paradigm relies on\npredefined labels and lacks the ability to comprehend complex natural language,\nresulting in limited generalization in open-world scene. To address these\nlimitations, we reformulate the traditional affordance detection paradigm into\n\\textit{Instruction Reasoning Affordance Segmentation} (IRAS) task. This task\nis designed to output a affordance mask region given a query reasoning text,\nwhich avoids fixed categories of input labels. We accordingly propose the\n\\textit{3D-AffordanceLLM} (3D-ADLLM), a framework designed for reasoning\naffordance detection in 3D open-scene. Specifically, 3D-ADLLM introduces large\nlanguage models (LLMs) to 3D affordance perception with a custom-designed\ndecoder for generating affordance masks, thus achieving open-world reasoning\naffordance detection. In addition, given the scarcity of 3D affordance datasets\nfor training large models, we seek to extract knowledge from general\nsegmentation data and transfer it to affordance detection. Thus, we propose a\nmulti-stage training strategy that begins with a novel pre-training task, i.e.,\n\\textit{Referring Object Part Segmentation}~(ROPS). This stage is designed to\nequip the model with general recognition and segmentation capabilities at the\nobject-part level. Then followed by fine-tuning with the IRAS task, 3D-ADLLM\nobtains the reasoning ability for affordance detection. In summary, 3D-ADLLM\nleverages the rich world knowledge and human-object interaction reasoning\nability of LLMs, achieving approximately an 8\\% improvement in mIoU on\nopen-vocabulary affordance detection tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Affordance detection is a challenging problem with broad applications on\nvarious robotic tasks. Existing methods typically formulate the detection\nparadigm as a label-based semantic segmentation task. This paradigm relies on\npredefined labels and lacks the ability to comprehend complex natural language,\nresulting in limited generalization in open-world scene. To address these\nlimitations, we reformulate the traditional affordance detection paradigm into\n\\textit{Instruction Reasoning Affordance Segmentation} (IRAS) task. This task\nis designed to output a affordance mask region given a query reasoning text,\nwhich avoids fixed categories of input labels. We accordingly propose the\n\\textit{3D-AffordanceLLM} (3D-ADLLM), a framework designed for reasoning\naffordance detection in 3D open-scene. Specifically, 3D-ADLLM introduces large\nlanguage models (LLMs) to 3D affordance perception with a custom-designed\ndecoder for generating affordance masks, thus achieving open-world reasoning\naffordance detection. In addition, given the scarcity of 3D affordance datasets\nfor training large models, we seek to extract knowledge from general\nsegmentation data and transfer it to affordance detection. Thus, we propose a\nmulti-stage training strategy that begins with a novel pre-training task, i.e.,\n\\textit{Referring Object Part Segmentation}~(ROPS). This stage is designed to\nequip the model with general recognition and segmentation capabilities at the\nobject-part level. Then followed by fine-tuning with the IRAS task, 3D-ADLLM\nobtains the reasoning ability for affordance detection. In summary, 3D-ADLLM\nleverages the rich world knowledge and human-object interaction reasoning\nability of LLMs, achieving approximately an 8\\% improvement in mIoU on\nopen-vocabulary affordance detection tasks."
                },
                "authors": [
                    {
                        "name": "Hengshuo Chu"
                    },
                    {
                        "name": "Xiang Deng"
                    },
                    {
                        "name": "Xiaoyang Chen"
                    },
                    {
                        "name": "Yinchuan Li"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "ICLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14117v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14117v2",
                "updated": "2025-02-27T12:29:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    29,
                    11,
                    3,
                    58,
                    0
                ],
                "published": "2024-05-23T02:44:12Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    2,
                    44,
                    12,
                    3,
                    144,
                    0
                ],
                "title": "Knowledge Localization: Mission Not Accomplished? Enter Query\n  Localization!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Localization: Mission Not Accomplished? Enter Query\n  Localization!"
                },
                "summary": "Large language models (LLMs) store extensive factual knowledge, but the\nmechanisms behind how they store and express this knowledge remain unclear. The\nKnowledge Neuron (KN) thesis is a prominent theory for explaining these\nmechanisms. This theory is based on the Knowledge Localization (KL) assumption,\nwhich suggests that a fact can be localized to a few knowledge storage units,\nnamely knowledge neurons.\n  However, this assumption has two limitations: first, it may be too rigid\nregarding knowledge storage, and second, it neglects the role of the attention\nmodule in knowledge expression.\n  In this paper, we first re-examine the KL assumption and demonstrate that its\nlimitations do indeed exist. To address these, we then present two new\nfindings, each targeting one of the limitations: one focusing on knowledge\nstorage and the other on knowledge expression. We summarize these findings as\n\\textbf{Query Localization} (QL) assumption and argue that the KL assumption\ncan be viewed as a simplification of the QL assumption. Based on QL assumption,\nwe further propose the Consistency-Aware KN modification method, which improves\nthe performance of knowledge modification, further validating our new\nassumption. We conduct 39 sets of experiments, along with additional\nvisualization experiments, to rigorously confirm our conclusions. Code is\navailable at https://github.com/heng840/KnowledgeLocalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) store extensive factual knowledge, but the\nmechanisms behind how they store and express this knowledge remain unclear. The\nKnowledge Neuron (KN) thesis is a prominent theory for explaining these\nmechanisms. This theory is based on the Knowledge Localization (KL) assumption,\nwhich suggests that a fact can be localized to a few knowledge storage units,\nnamely knowledge neurons.\n  However, this assumption has two limitations: first, it may be too rigid\nregarding knowledge storage, and second, it neglects the role of the attention\nmodule in knowledge expression.\n  In this paper, we first re-examine the KL assumption and demonstrate that its\nlimitations do indeed exist. To address these, we then present two new\nfindings, each targeting one of the limitations: one focusing on knowledge\nstorage and the other on knowledge expression. We summarize these findings as\n\\textbf{Query Localization} (QL) assumption and argue that the KL assumption\ncan be viewed as a simplification of the QL assumption. Based on QL assumption,\nwe further propose the Consistency-Aware KN modification method, which improves\nthe performance of knowledge modification, further validating our new\nassumption. We conduct 39 sets of experiments, along with additional\nvisualization experiments, to rigorously confirm our conclusions. Code is\navailable at https://github.com/heng840/KnowledgeLocalization."
                },
                "authors": [
                    {
                        "name": "Yuheng Chen"
                    },
                    {
                        "name": "Pengfei Cao"
                    },
                    {
                        "name": "Yubo Chen"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "ICLR 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14117v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14117v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00915v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00915v2",
                "updated": "2025-02-27T12:28:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    28,
                    21,
                    3,
                    58,
                    0
                ],
                "published": "2024-05-02T00:04:02Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    0,
                    4,
                    2,
                    3,
                    123,
                    0
                ],
                "title": "EchoScene: Indoor Scene Generation via Information Echo over Scene Graph\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoScene: Indoor Scene Generation via Information Echo over Scene Graph\n  Diffusion"
                },
                "summary": "We present EchoScene, an interactive and controllable generative model that\ngenerates 3D indoor scenes on scene graphs. EchoScene leverages a dual-branch\ndiffusion model that dynamically adapts to scene graphs. Existing methods\nstruggle to handle scene graphs due to varying numbers of nodes, multiple edge\ncombinations, and manipulator-induced node-edge operations. EchoScene overcomes\nthis by associating each node with a denoising process and enables\ncollaborative information exchange, enhancing controllable and consistent\ngeneration aware of global constraints. This is achieved through an information\necho scheme in both shape and layout branches. At every denoising step, all\nprocesses share their denoising data with an information exchange unit that\ncombines these updates using graph convolution. The scheme ensures that the\ndenoising processes are influenced by a holistic understanding of the scene\ngraph, facilitating the generation of globally coherent scenes. The resulting\nscenes can be manipulated during inference by editing the input scene graph and\nsampling the noise in the diffusion model. Extensive experiments validate our\napproach, which maintains scene controllability and surpasses previous methods\nin generation fidelity. Moreover, the generated scenes are of high quality and\nthus directly compatible with off-the-shelf texture generation. Code and\ntrained models are open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present EchoScene, an interactive and controllable generative model that\ngenerates 3D indoor scenes on scene graphs. EchoScene leverages a dual-branch\ndiffusion model that dynamically adapts to scene graphs. Existing methods\nstruggle to handle scene graphs due to varying numbers of nodes, multiple edge\ncombinations, and manipulator-induced node-edge operations. EchoScene overcomes\nthis by associating each node with a denoising process and enables\ncollaborative information exchange, enhancing controllable and consistent\ngeneration aware of global constraints. This is achieved through an information\necho scheme in both shape and layout branches. At every denoising step, all\nprocesses share their denoising data with an information exchange unit that\ncombines these updates using graph convolution. The scheme ensures that the\ndenoising processes are influenced by a holistic understanding of the scene\ngraph, facilitating the generation of globally coherent scenes. The resulting\nscenes can be manipulated during inference by editing the input scene graph and\nsampling the noise in the diffusion model. Extensive experiments validate our\napproach, which maintains scene controllability and surpasses previous methods\nin generation fidelity. Moreover, the generated scenes are of high quality and\nthus directly compatible with off-the-shelf texture generation. Code and\ntrained models are open-sourced."
                },
                "authors": [
                    {
                        "name": "Guangyao Zhai"
                    },
                    {
                        "name": "Evin Pınar Örnek"
                    },
                    {
                        "name": "Dave Zhenyu Chen"
                    },
                    {
                        "name": "Ruotong Liao"
                    },
                    {
                        "name": "Yan Di"
                    },
                    {
                        "name": "Nassir Navab"
                    },
                    {
                        "name": "Federico Tombari"
                    },
                    {
                        "name": "Benjamin Busam"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Busam"
                },
                "author": "Benjamin Busam",
                "arxiv_comment": "Nectar Track at 3DV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00915v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00915v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18300v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18300v3",
                "updated": "2025-02-27T12:22:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    22,
                    35,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-25T15:39:33Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    39,
                    33,
                    1,
                    56,
                    0
                ],
                "title": "Bayesian Computation in Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Computation in Deep Learning"
                },
                "summary": "This review paper is intended for the 2nd edition of the Handbook of Markov\nchain Monte Carlo. We provide an introduction to approximate inference\ntechniques as Bayesian computation methods applied to deep learning models. We\norganize the chapter by presenting popular computational methods for Bayesian\nneural networks and deep generative models, explaining their unique challenges\nin posterior inference as well as the solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This review paper is intended for the 2nd edition of the Handbook of Markov\nchain Monte Carlo. We provide an introduction to approximate inference\ntechniques as Bayesian computation methods applied to deep learning models. We\norganize the chapter by presenting popular computational methods for Bayesian\nneural networks and deep generative models, explaining their unique challenges\nin posterior inference as well as the solutions."
                },
                "authors": [
                    {
                        "name": "Wenlong Chen"
                    },
                    {
                        "name": "Bolian Li"
                    },
                    {
                        "name": "Ruqi Zhang"
                    },
                    {
                        "name": "Yingzhen Li"
                    }
                ],
                "author_detail": {
                    "name": "Yingzhen Li"
                },
                "author": "Yingzhen Li",
                "arxiv_comment": "43 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18300v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18300v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20722v2",
                "updated": "2025-02-27T12:15:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    15,
                    38,
                    3,
                    58,
                    0
                ],
                "published": "2024-07-30T10:34:40Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    10,
                    34,
                    40,
                    1,
                    212,
                    0
                ],
                "title": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo"
                },
                "summary": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks."
                },
                "authors": [
                    {
                        "name": "Minas Karamanis"
                    },
                    {
                        "name": "Uroš Seljak"
                    }
                ],
                "author_detail": {
                    "name": "Uroš Seljak"
                },
                "author": "Uroš Seljak",
                "arxiv_comment": "36 pages, 9 figures. Submitted to Statistics & Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05760v2",
                "updated": "2025-02-27T12:13:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    13,
                    38,
                    3,
                    58,
                    0
                ],
                "published": "2024-10-08T07:33:49Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    7,
                    33,
                    49,
                    1,
                    282,
                    0
                ],
                "title": "Training-free Diffusion Model Alignment with Sampling Demons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free Diffusion Model Alignment with Sampling Demons"
                },
                "summary": "Aligning diffusion models with user preferences has been a key challenge.\nExisting methods for aligning diffusion models either require retraining or are\nlimited to differentiable reward functions. To address these limitations, we\npropose a stochastic optimization approach, dubbed Demon, to guide the\ndenoising process at inference time without backpropagation through reward\nfunctions or model retraining. Our approach works by controlling noise\ndistribution in denoising steps to concentrate density on regions corresponding\nto high rewards through stochastic optimization. We provide comprehensive\ntheoretical and empirical evidence to support and validate our approach,\nincluding experiments that use non-differentiable sources of rewards such as\nVisual-Language Model (VLM) APIs and human judgements. To the best of our\nknowledge, the proposed approach is the first inference-time,\nbackpropagation-free preference alignment method for diffusion models. Our\nmethod can be easily integrated with existing diffusion models without further\ntraining. Our experiments show that the proposed approach significantly\nimproves the average aesthetics scores for text-to-image generation.\nImplementation is available at https://github.com/aiiu-lab/DemonSampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning diffusion models with user preferences has been a key challenge.\nExisting methods for aligning diffusion models either require retraining or are\nlimited to differentiable reward functions. To address these limitations, we\npropose a stochastic optimization approach, dubbed Demon, to guide the\ndenoising process at inference time without backpropagation through reward\nfunctions or model retraining. Our approach works by controlling noise\ndistribution in denoising steps to concentrate density on regions corresponding\nto high rewards through stochastic optimization. We provide comprehensive\ntheoretical and empirical evidence to support and validate our approach,\nincluding experiments that use non-differentiable sources of rewards such as\nVisual-Language Model (VLM) APIs and human judgements. To the best of our\nknowledge, the proposed approach is the first inference-time,\nbackpropagation-free preference alignment method for diffusion models. Our\nmethod can be easily integrated with existing diffusion models without further\ntraining. Our experiments show that the proposed approach significantly\nimproves the average aesthetics scores for text-to-image generation.\nImplementation is available at https://github.com/aiiu-lab/DemonSampling."
                },
                "authors": [
                    {
                        "name": "Po-Hung Yeh"
                    },
                    {
                        "name": "Kuang-Huei Lee"
                    },
                    {
                        "name": "Jun-Cheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jun-Cheng Chen"
                },
                "author": "Jun-Cheng Chen",
                "arxiv_comment": "35 pages",
                "arxiv_journal_ref": "Proceedings of the Thirteenth International Conference on Learning\n  Representations (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08774v2",
                "updated": "2025-02-27T11:57:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    11,
                    57,
                    16,
                    3,
                    58,
                    0
                ],
                "published": "2024-12-11T20:55:21Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    20,
                    55,
                    21,
                    2,
                    346,
                    0
                ],
                "title": "ProtoOcc: Accurate, Efficient 3D Occupancy Prediction Using Dual Branch\n  Encoder-Prototype Query Decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProtoOcc: Accurate, Efficient 3D Occupancy Prediction Using Dual Branch\n  Encoder-Prototype Query Decoder"
                },
                "summary": "In this paper, we introduce ProtoOcc, a novel 3D occupancy prediction model\ndesigned to predict the occupancy states and semantic classes of 3D voxels\nthrough a deep semantic understanding of scenes. ProtoOcc consists of two main\ncomponents: the Dual Branch Encoder (DBE) and the Prototype Query Decoder\n(PQD). The DBE produces a new 3D voxel representation by combining 3D voxel and\nBEV representations across multiple scales through a dual branch structure.\nThis design enhances both performance and computational efficiency by providing\na large receptive field for the BEV representation while maintaining a smaller\nreceptive field for the voxel representation. The PQD introduces Prototype\nQueries to accelerate the decoding process. Scene-Adaptive Prototypes are\nderived from the 3D voxel features of input sample, while Scene-Agnostic\nPrototypes are computed by applying Scene-Adaptive Prototypes to an Exponential\nMoving Average during the training phase. By using these prototype-based\nqueries for decoding, we can directly predict 3D occupancy in a single step,\neliminating the need for iterative Transformer decoding. Additionally, we\npropose the Robust Prototype Learning, which injects noise into prototype\ngeneration process and trains the model to denoise during the training phase.\nProtoOcc achieves state-of-the-art performance with 45.02% mIoU on the\nOcc3D-nuScenes benchmark. For single-frame method, it reaches 39.56% mIoU with\nan inference speed of 12.83 FPS on an NVIDIA RTX 3090. Our code can be found at\nhttps://github.com/SPA-junghokim/ProtoOcc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce ProtoOcc, a novel 3D occupancy prediction model\ndesigned to predict the occupancy states and semantic classes of 3D voxels\nthrough a deep semantic understanding of scenes. ProtoOcc consists of two main\ncomponents: the Dual Branch Encoder (DBE) and the Prototype Query Decoder\n(PQD). The DBE produces a new 3D voxel representation by combining 3D voxel and\nBEV representations across multiple scales through a dual branch structure.\nThis design enhances both performance and computational efficiency by providing\na large receptive field for the BEV representation while maintaining a smaller\nreceptive field for the voxel representation. The PQD introduces Prototype\nQueries to accelerate the decoding process. Scene-Adaptive Prototypes are\nderived from the 3D voxel features of input sample, while Scene-Agnostic\nPrototypes are computed by applying Scene-Adaptive Prototypes to an Exponential\nMoving Average during the training phase. By using these prototype-based\nqueries for decoding, we can directly predict 3D occupancy in a single step,\neliminating the need for iterative Transformer decoding. Additionally, we\npropose the Robust Prototype Learning, which injects noise into prototype\ngeneration process and trains the model to denoise during the training phase.\nProtoOcc achieves state-of-the-art performance with 45.02% mIoU on the\nOcc3D-nuScenes benchmark. For single-frame method, it reaches 39.56% mIoU with\nan inference speed of 12.83 FPS on an NVIDIA RTX 3090. Our code can be found at\nhttps://github.com/SPA-junghokim/ProtoOcc."
                },
                "authors": [
                    {
                        "name": "Jungho Kim"
                    },
                    {
                        "name": "Changwon Kang"
                    },
                    {
                        "name": "Dongyoung Lee"
                    },
                    {
                        "name": "Sehwan Choi"
                    },
                    {
                        "name": "Jun Won Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jun Won Choi"
                },
                "author": "Jun Won Choi",
                "arxiv_comment": "Accepted to AAAI Conference on Artificial Intelligence 2025, 15\n  pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20011v1",
                "updated": "2025-02-27T11:48:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    11,
                    48,
                    58,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T11:48:58Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    11,
                    48,
                    58,
                    3,
                    58,
                    0
                ],
                "title": "On window mean survival time with interval-censored data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On window mean survival time with interval-censored data"
                },
                "summary": "In recent years, cancer clinical trials have increasingly encountered non\nproportional hazards (NPH) scenarios, particularly with the emergence of\nimmunotherapy. In randomized controlled trials comparing immunotherapy with\nconventional chemotherapy or placebo, late difference and early crossing\nsurvivals scenarios are commonly observed. In such cases, window mean survival\ntime (WMST), the area under the survival curve within a pre-specified interval\n$[\\tau_0, \\tau_1]$, has gained increasing attention due to its superior power\ncompared to restricted mean survival time (RMST), the area under the survival\ncurve up to a pre-specified time point. Considering the increasing use of\nprogression-free survival as a co-primary endpoint alongside overall survival,\nthere is a critical need to establish a WMST estimation method for\ninterval-censored data; however, sufficient research has yet to be conducted.\nTo bridge this gap, this study proposes a WMST inference method utilizing\none-point imputations and Turnbull's method. Extensive numerical simulations\ndemonstrate that the WMST estimation method using mid-point imputation for\ninterval-censored data exhibits comparable performance to that using Turnbull's\nmethod. Since the former facilitates standard error calculation, we adopt it as\nthe standard method. Numerical simulations on two-sample tests confirm that the\nproposed WMST testing method have higher power than RMST in late difference and\nearly crossing survival scenarios, while having compatible power to the\nlog-rank test under the PH. Furthermore, even when pre-specified $\\tau_0$\ndeviated from the clinically desirable time point, WMST consistently maintains\nhigher power than RMST in late difference and early crossing survivals\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, cancer clinical trials have increasingly encountered non\nproportional hazards (NPH) scenarios, particularly with the emergence of\nimmunotherapy. In randomized controlled trials comparing immunotherapy with\nconventional chemotherapy or placebo, late difference and early crossing\nsurvivals scenarios are commonly observed. In such cases, window mean survival\ntime (WMST), the area under the survival curve within a pre-specified interval\n$[\\tau_0, \\tau_1]$, has gained increasing attention due to its superior power\ncompared to restricted mean survival time (RMST), the area under the survival\ncurve up to a pre-specified time point. Considering the increasing use of\nprogression-free survival as a co-primary endpoint alongside overall survival,\nthere is a critical need to establish a WMST estimation method for\ninterval-censored data; however, sufficient research has yet to be conducted.\nTo bridge this gap, this study proposes a WMST inference method utilizing\none-point imputations and Turnbull's method. Extensive numerical simulations\ndemonstrate that the WMST estimation method using mid-point imputation for\ninterval-censored data exhibits comparable performance to that using Turnbull's\nmethod. Since the former facilitates standard error calculation, we adopt it as\nthe standard method. Numerical simulations on two-sample tests confirm that the\nproposed WMST testing method have higher power than RMST in late difference and\nearly crossing survival scenarios, while having compatible power to the\nlog-rank test under the PH. Furthermore, even when pre-specified $\\tau_0$\ndeviated from the clinically desirable time point, WMST consistently maintains\nhigher power than RMST in late difference and early crossing survivals\nscenarios."
                },
                "authors": [
                    {
                        "name": "Takuto Iijima"
                    },
                    {
                        "name": "Tomotaka Momozaki"
                    },
                    {
                        "name": "Shuji Ando"
                    }
                ],
                "author_detail": {
                    "name": "Shuji Ando"
                },
                "author": "Shuji Ando",
                "arxiv_comment": "21 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20000v1",
                "updated": "2025-02-27T11:26:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    11,
                    26,
                    36,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T11:26:36Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    11,
                    26,
                    36,
                    3,
                    58,
                    0
                ],
                "title": "Bayesian inferences on covariant density functionals from multimessenger\n  astrophysical data. I. Nucleonic models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inferences on covariant density functionals from multimessenger\n  astrophysical data. I. Nucleonic models"
                },
                "summary": "[Background] Bayesian inference frameworks incorporating multi-messenger\nastrophysical constraints have recently been applied to covariant density\nfunctional (CDF) models to constrain their parameters. Among these, frameworks\nutilizing CDFs with density-dependent meson-nucleon couplings furnishing the\nequation of state (EoS) of compact star (CS) matter have been explored.\n[Purpose] The aforementioned inference framework has not yet incorporated\nastrophysical objects with potentially extreme high masses or ultra-small radii\namong its constraints, leaving its flexibility and predictive power under such\nextreme parameters still unknown. [Method] We apply the Bayesian inference\nframework based on CDFs with density dependent couplings. The astrophysical\ndata is expanded to include not only the latest multi-messenger constraints\nfrom NICER and gravitational wave events but also the highest measured mass to\ndate for the ``black widow\" pulsar PSR J0952-0607 and the mass-radius estimates\nfor the ultra-compact, low-mass object HESS J1731-347. [Results] Our systematic\nBayesian analysis indicates that our CDF models can support higher maximum\nmasses for CSs, reaching up to $2.4$-$2.5\\,M_{\\odot}$. However, achieving\nsufficient softening of the EoS in the low-density regime to accommodate the\nHESS J1731-347 data remains challenging. Nonetheless, we are able to impose\ntighter constraints on the parameter space of CDF models, ensuring consistency\nwith current nuclear experimental and astrophysical data. [Conclusions] CDF\nmodels with density-dependent meson-nucleon couplings encompass a wide range of\nnuclear and astrophysical phenomena, providing a robust theoretical framework\nfor interpreting compact objects. However, the predicted lower limit for the\nradii of low-mass stars is approximately 12 km, which stems from the restricted\ndegrees of freedom in the isovector sector.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[Background] Bayesian inference frameworks incorporating multi-messenger\nastrophysical constraints have recently been applied to covariant density\nfunctional (CDF) models to constrain their parameters. Among these, frameworks\nutilizing CDFs with density-dependent meson-nucleon couplings furnishing the\nequation of state (EoS) of compact star (CS) matter have been explored.\n[Purpose] The aforementioned inference framework has not yet incorporated\nastrophysical objects with potentially extreme high masses or ultra-small radii\namong its constraints, leaving its flexibility and predictive power under such\nextreme parameters still unknown. [Method] We apply the Bayesian inference\nframework based on CDFs with density dependent couplings. The astrophysical\ndata is expanded to include not only the latest multi-messenger constraints\nfrom NICER and gravitational wave events but also the highest measured mass to\ndate for the ``black widow\" pulsar PSR J0952-0607 and the mass-radius estimates\nfor the ultra-compact, low-mass object HESS J1731-347. [Results] Our systematic\nBayesian analysis indicates that our CDF models can support higher maximum\nmasses for CSs, reaching up to $2.4$-$2.5\\,M_{\\odot}$. However, achieving\nsufficient softening of the EoS in the low-density regime to accommodate the\nHESS J1731-347 data remains challenging. Nonetheless, we are able to impose\ntighter constraints on the parameter space of CDF models, ensuring consistency\nwith current nuclear experimental and astrophysical data. [Conclusions] CDF\nmodels with density-dependent meson-nucleon couplings encompass a wide range of\nnuclear and astrophysical phenomena, providing a robust theoretical framework\nfor interpreting compact objects. However, the predicted lower limit for the\nradii of low-mass stars is approximately 12 km, which stems from the restricted\ndegrees of freedom in the isovector sector."
                },
                "authors": [
                    {
                        "name": "Jia-Jie Li"
                    },
                    {
                        "name": "Yu Tian"
                    },
                    {
                        "name": "Armen Sedrakian"
                    }
                ],
                "author_detail": {
                    "name": "Armen Sedrakian"
                },
                "arxiv_affiliation": "FIAS, Frankfurt and Wroclaw U.",
                "author": "Armen Sedrakian",
                "arxiv_comment": "25 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04671v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04671v2",
                "updated": "2025-02-27T11:26:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    11,
                    26,
                    20,
                    3,
                    58,
                    0
                ],
                "published": "2024-11-07T12:55:17Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    12,
                    55,
                    17,
                    3,
                    312,
                    0
                ],
                "title": "CUIfy the XR: An Open-Source Package to Embed LLM-powered Conversational\n  Agents in XR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CUIfy the XR: An Open-Source Package to Embed LLM-powered Conversational\n  Agents in XR"
                },
                "summary": "Recent developments in computer graphics, machine learning, and sensor\ntechnologies enable numerous opportunities for extended reality (XR) setups for\neveryday life, from skills training to entertainment. With large corporations\noffering affordable consumer-grade head-mounted displays (HMDs), XR will likely\nbecome pervasive, and HMDs will develop as personal devices like smartphones\nand tablets. However, having intelligent spaces and naturalistic interactions\nin XR is as important as technological advances so that users grow their\nengagement in virtual and augmented spaces. To this end, large language model\n(LLM)--powered non-player characters (NPCs) with speech-to-text (STT) and\ntext-to-speech (TTS) models bring significant advantages over conventional or\npre-scripted NPCs for facilitating more natural conversational user interfaces\n(CUIs) in XR. This paper provides the community with an open-source,\ncustomizable, extendable, and privacy-aware Unity package, CUIfy, that\nfacilitates speech-based NPC-user interaction with widely used LLMs, STT, and\nTTS models. Our package also supports multiple LLM-powered NPCs per environment\nand minimizes latency between different computational models through streaming\nto achieve usable interactions between users and NPCs. We publish our source\ncode in the following repository: https://gitlab.lrz.de/hctl/cuify",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in computer graphics, machine learning, and sensor\ntechnologies enable numerous opportunities for extended reality (XR) setups for\neveryday life, from skills training to entertainment. With large corporations\noffering affordable consumer-grade head-mounted displays (HMDs), XR will likely\nbecome pervasive, and HMDs will develop as personal devices like smartphones\nand tablets. However, having intelligent spaces and naturalistic interactions\nin XR is as important as technological advances so that users grow their\nengagement in virtual and augmented spaces. To this end, large language model\n(LLM)--powered non-player characters (NPCs) with speech-to-text (STT) and\ntext-to-speech (TTS) models bring significant advantages over conventional or\npre-scripted NPCs for facilitating more natural conversational user interfaces\n(CUIs) in XR. This paper provides the community with an open-source,\ncustomizable, extendable, and privacy-aware Unity package, CUIfy, that\nfacilitates speech-based NPC-user interaction with widely used LLMs, STT, and\nTTS models. Our package also supports multiple LLM-powered NPCs per environment\nand minimizes latency between different computational models through streaming\nto achieve usable interactions between users and NPCs. We publish our source\ncode in the following repository: https://gitlab.lrz.de/hctl/cuify"
                },
                "authors": [
                    {
                        "name": "Kadir Burak Buldu"
                    },
                    {
                        "name": "Süleyman Özdel"
                    },
                    {
                        "name": "Ka Hei Carrie Lau"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Daniel Saad"
                    },
                    {
                        "name": "Sofie Schönborn"
                    },
                    {
                        "name": "Auxane Boch"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    },
                    {
                        "name": "Efe Bozkir"
                    }
                ],
                "author_detail": {
                    "name": "Efe Bozkir"
                },
                "author": "Efe Bozkir",
                "arxiv_comment": "7th IEEE International Conference on Artificial Intelligence &\n  eXtended and Virtual Reality (IEEE AIxVR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04671v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04671v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01928v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01928v2",
                "updated": "2025-02-27T11:25:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    11,
                    25,
                    2,
                    3,
                    58,
                    0
                ],
                "published": "2024-12-02T19:30:36Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    19,
                    30,
                    36,
                    0,
                    337,
                    0
                ],
                "title": "MALT: Improving Reasoning with Multi-Agent LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MALT: Improving Reasoning with Multi-Agent LLM Training"
                },
                "summary": "Large Language Models (LLMs) often produce answers with a single\nchain-of-thought, which restricts their ability to explore reasoning paths or\nself-correct flawed outputs in complex tasks. In this paper, we introduce MALT\n(Multi-Agent LLM Training), a novel post-training strategy that divides the\nreasoning process into generation, verification, and refinement steps using a\nsequential pipeline of heterogeneous agents. During data generation, each agent\nis repeatedly sampled to form a multi-agent search tree, where final outputs\nare graded against ground-truth data. We then apply value iteration to\npropagate reward signals back to each role-conditioned model, automatically\nproducing multi-agent post-training data without human or teacher-model\nsupervision. Our off-policy approach allows each agent to specialize by\nlearning from correct and incorrect trajectories, ultimately improving the\nend-to-end reasoning chain. On MATH, GSM8K, and CSQA, MALT surpasses the same\nbaseline LLM with a relative improvement of 15.66%, 7.42%, and 9.40%\nrespectively, making it an important advance towards multi-agent cooperative\ntraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often produce answers with a single\nchain-of-thought, which restricts their ability to explore reasoning paths or\nself-correct flawed outputs in complex tasks. In this paper, we introduce MALT\n(Multi-Agent LLM Training), a novel post-training strategy that divides the\nreasoning process into generation, verification, and refinement steps using a\nsequential pipeline of heterogeneous agents. During data generation, each agent\nis repeatedly sampled to form a multi-agent search tree, where final outputs\nare graded against ground-truth data. We then apply value iteration to\npropagate reward signals back to each role-conditioned model, automatically\nproducing multi-agent post-training data without human or teacher-model\nsupervision. Our off-policy approach allows each agent to specialize by\nlearning from correct and incorrect trajectories, ultimately improving the\nend-to-end reasoning chain. On MATH, GSM8K, and CSQA, MALT surpasses the same\nbaseline LLM with a relative improvement of 15.66%, 7.42%, and 9.40%\nrespectively, making it an important advance towards multi-agent cooperative\ntraining."
                },
                "authors": [
                    {
                        "name": "Sumeet Ramesh Motwani"
                    },
                    {
                        "name": "Chandler Smith"
                    },
                    {
                        "name": "Rocktim Jyoti Das"
                    },
                    {
                        "name": "Rafael Rafailov"
                    },
                    {
                        "name": "Ivan Laptev"
                    },
                    {
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "name": "Fabio Pizzati"
                    },
                    {
                        "name": "Ronald Clark"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Schroeder de Witt"
                },
                "author": "Christian Schroeder de Witt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01928v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01928v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17247v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17247v2",
                "updated": "2025-02-27T11:16:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    11,
                    16,
                    33,
                    3,
                    58,
                    0
                ],
                "published": "2024-10-22T17:59:53Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    17,
                    59,
                    53,
                    1,
                    296,
                    0
                ],
                "title": "PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid\n  Visual Redundancy Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid\n  Visual Redundancy Reduction"
                },
                "summary": "In large vision-language models (LVLMs), images serve as inputs that carry a\nwealth of information. As the idiom \"A picture is worth a thousand words\"\nimplies, representing a single image in current LVLMs can require hundreds or\neven thousands of tokens. This results in significant computational costs,\nwhich grow quadratically as input image resolution increases, thereby severely\nimpacting the efficiency of both training and inference. Previous approaches\nhave attempted to reduce the number of image tokens either before or within the\nearly layers of LVLMs. However, these strategies inevitably result in the loss\nof crucial image information, ultimately diminishing model performance. To\naddress this challenge, we conduct an empirical study revealing that all visual\ntokens are necessary for LVLMs in the shallow layers, and token redundancy\nprogressively increases in the deeper layers of the model. To this end, we\npropose PyramidDrop, a visual redundancy reduction strategy for LVLMs to boost\ntheir efficiency in both training and inference with neglectable performance\nloss. Specifically, we partition the LVLM into several stages and drop part of\nthe image tokens at the end of each stage with a pre-defined ratio, creating\npyramid-like visual tokens across model layers. The dropping is based on a\nlightweight similarity calculation with a negligible time overhead. Extensive\nexperiments demonstrate that PyramidDrop can achieve a 40% training time and\n55% inference FLOPs acceleration of LLaVA-NeXT with comparable performance.\nBesides, the PyramidDrop could also serve as a plug-and-play strategy for\ninference acceleration without training, with better performance and lower\ninference cost than counterparts. Code is available at\nhttps://github.com/Cooperx521/PyramidDrop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large vision-language models (LVLMs), images serve as inputs that carry a\nwealth of information. As the idiom \"A picture is worth a thousand words\"\nimplies, representing a single image in current LVLMs can require hundreds or\neven thousands of tokens. This results in significant computational costs,\nwhich grow quadratically as input image resolution increases, thereby severely\nimpacting the efficiency of both training and inference. Previous approaches\nhave attempted to reduce the number of image tokens either before or within the\nearly layers of LVLMs. However, these strategies inevitably result in the loss\nof crucial image information, ultimately diminishing model performance. To\naddress this challenge, we conduct an empirical study revealing that all visual\ntokens are necessary for LVLMs in the shallow layers, and token redundancy\nprogressively increases in the deeper layers of the model. To this end, we\npropose PyramidDrop, a visual redundancy reduction strategy for LVLMs to boost\ntheir efficiency in both training and inference with neglectable performance\nloss. Specifically, we partition the LVLM into several stages and drop part of\nthe image tokens at the end of each stage with a pre-defined ratio, creating\npyramid-like visual tokens across model layers. The dropping is based on a\nlightweight similarity calculation with a negligible time overhead. Extensive\nexperiments demonstrate that PyramidDrop can achieve a 40% training time and\n55% inference FLOPs acceleration of LLaVA-NeXT with comparable performance.\nBesides, the PyramidDrop could also serve as a plug-and-play strategy for\ninference acceleration without training, with better performance and lower\ninference cost than counterparts. Code is available at\nhttps://github.com/Cooperx521/PyramidDrop."
                },
                "authors": [
                    {
                        "name": "Long Xing"
                    },
                    {
                        "name": "Qidong Huang"
                    },
                    {
                        "name": "Xiaoyi Dong"
                    },
                    {
                        "name": "Jiajie Lu"
                    },
                    {
                        "name": "Pan Zhang"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Yuhang Cao"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Feng Wu"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "arxiv_comment": "CVPR 2025, code is available at\n  https://github.com/Cooperx521/PyramidDrop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17247v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17247v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.20395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20395v1",
                "updated": "2025-02-27T18:59:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    59,
                    32,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T18:59:32Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    59,
                    32,
                    3,
                    58,
                    0
                ],
                "title": "R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts"
                },
                "summary": "In large multimodal models (LMMs), the perception of non-language modalities\n(e.g., visual representations) is usually not on par with the large language\nmodels (LLMs)' powerful reasoning capabilities, deterring LMMs' performance on\nchallenging downstream tasks. This weakness has been recently mitigated by\nreplacing the vision encoder with a mixture-of-experts (MoE), which provides\nrich, multi-granularity, and diverse representations required by diverse\ndownstream tasks. The performance of multimodal MoE largely depends on its\nrouter, which reweights and mixes the representations of different experts for\neach input. However, we find that the end-to-end trained router does not always\nproduce the optimal routing weights for every test sample. To bridge the gap,\nwe propose a novel and efficient method \"Re-Routing in Test-Time(R2-T2) that\nlocally optimizes the vector of routing weights in test-time by moving it\ntoward those vectors of the correctly predicted samples in a neighborhood of\nthe test sample. We propose three R2-T2 strategies with different optimization\nobjectives and neighbor-search spaces. R2-T2 consistently and greatly improves\nstate-of-the-art LMMs' performance on challenging benchmarks of diverse tasks,\nwithout training any base-model parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large multimodal models (LMMs), the perception of non-language modalities\n(e.g., visual representations) is usually not on par with the large language\nmodels (LLMs)' powerful reasoning capabilities, deterring LMMs' performance on\nchallenging downstream tasks. This weakness has been recently mitigated by\nreplacing the vision encoder with a mixture-of-experts (MoE), which provides\nrich, multi-granularity, and diverse representations required by diverse\ndownstream tasks. The performance of multimodal MoE largely depends on its\nrouter, which reweights and mixes the representations of different experts for\neach input. However, we find that the end-to-end trained router does not always\nproduce the optimal routing weights for every test sample. To bridge the gap,\nwe propose a novel and efficient method \"Re-Routing in Test-Time(R2-T2) that\nlocally optimizes the vector of routing weights in test-time by moving it\ntoward those vectors of the correctly predicted samples in a neighborhood of\nthe test sample. We propose three R2-T2 strategies with different optimization\nobjectives and neighbor-search spaces. R2-T2 consistently and greatly improves\nstate-of-the-art LMMs' performance on challenging benchmarks of diverse tasks,\nwithout training any base-model parameters."
                },
                "authors": [
                    {
                        "name": "Zhongyang Li"
                    },
                    {
                        "name": "Ziyue Li"
                    },
                    {
                        "name": "Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhou"
                },
                "author": "Tianyi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20383v1",
                "updated": "2025-02-27T18:56:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    56,
                    26,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T18:56:26Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    56,
                    26,
                    3,
                    58,
                    0
                ],
                "title": "Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security\n  Analysis"
                },
                "summary": "Recent advancements in Web AI agents have demonstrated remarkable\ncapabilities in addressing complex web navigation tasks. However, emerging\nresearch shows that these agents exhibit greater vulnerability compared to\nstandalone Large Language Models (LLMs), despite both being built upon the same\nsafety-aligned models. This discrepancy is particularly concerning given the\ngreater flexibility of Web AI Agent compared to standalone LLMs, which may\nexpose them to a wider range of adversarial user inputs. To build a scaffold\nthat addresses these concerns, this study investigates the underlying factors\nthat contribute to the increased vulnerability of Web AI agents. Notably, this\ndisparity stems from the multifaceted differences between Web AI agents and\nstandalone LLMs, as well as the complex signals - nuances that simple\nevaluation metrics, such as success rate, often fail to capture. To tackle\nthese challenges, we propose a component-level analysis and a more granular,\nsystematic evaluation framework. Through this fine-grained investigation, we\nidentify three critical factors that amplify the vulnerability of Web AI\nagents; (1) embedding user goals into the system prompt, (2) multi-step action\ngeneration, and (3) observational capabilities. Our findings highlights the\npressing need to enhance security and robustness in AI agent design and provide\nactionable insights for targeted defense strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Web AI agents have demonstrated remarkable\ncapabilities in addressing complex web navigation tasks. However, emerging\nresearch shows that these agents exhibit greater vulnerability compared to\nstandalone Large Language Models (LLMs), despite both being built upon the same\nsafety-aligned models. This discrepancy is particularly concerning given the\ngreater flexibility of Web AI Agent compared to standalone LLMs, which may\nexpose them to a wider range of adversarial user inputs. To build a scaffold\nthat addresses these concerns, this study investigates the underlying factors\nthat contribute to the increased vulnerability of Web AI agents. Notably, this\ndisparity stems from the multifaceted differences between Web AI agents and\nstandalone LLMs, as well as the complex signals - nuances that simple\nevaluation metrics, such as success rate, often fail to capture. To tackle\nthese challenges, we propose a component-level analysis and a more granular,\nsystematic evaluation framework. Through this fine-grained investigation, we\nidentify three critical factors that amplify the vulnerability of Web AI\nagents; (1) embedding user goals into the system prompt, (2) multi-step action\ngeneration, and (3) observational capabilities. Our findings highlights the\npressing need to enhance security and robustness in AI agent design and provide\nactionable insights for targeted defense strategies."
                },
                "authors": [
                    {
                        "name": "Jeffrey Yang Fan Chiang"
                    },
                    {
                        "name": "Seungjae Lee"
                    },
                    {
                        "name": "Jia-Bin Huang"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Yizheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yizheng Chen"
                },
                "author": "Yizheng Chen",
                "arxiv_comment": "Project website: http://vulnerable-ai-agents.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20379v1",
                "updated": "2025-02-27T18:53:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    53,
                    30,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T18:53:30Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    53,
                    30,
                    3,
                    58,
                    0
                ],
                "title": "Multi-Agent Verification: Scaling Test-Time Compute with Multiple\n  Verifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Verification: Scaling Test-Time Compute with Multiple\n  Verifiers"
                },
                "summary": "By utilizing more computational resources at test-time, large language models\n(LLMs) can improve without additional training. One common strategy uses\nverifiers to evaluate candidate outputs. In this work, we propose a novel\nscaling dimension for test-time compute: scaling the number of verifiers. We\nintroduce Multi-Agent Verification (MAV) as a test-time compute paradigm that\ncombines multiple verifiers to improve performance. We propose using Aspect\nVerifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of\noutputs, as one possible choice for the verifiers in a MAV system. AVs are a\nconvenient building block for MAV since they can be easily combined without\nadditional training. Moreover, we introduce BoN-MAV, a simple multi-agent\nverification algorithm that combines best-of-n sampling with multiple\nverifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency\nand reward model verification, and we demonstrate both weak-to-strong\ngeneralization, where combining weak verifiers improves even stronger LLMs, and\nself-improvement, where the same base model is used to both generate and verify\noutputs. Our results establish scaling the number of verifiers as a promising\nnew dimension for improving language model performance at test-time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By utilizing more computational resources at test-time, large language models\n(LLMs) can improve without additional training. One common strategy uses\nverifiers to evaluate candidate outputs. In this work, we propose a novel\nscaling dimension for test-time compute: scaling the number of verifiers. We\nintroduce Multi-Agent Verification (MAV) as a test-time compute paradigm that\ncombines multiple verifiers to improve performance. We propose using Aspect\nVerifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of\noutputs, as one possible choice for the verifiers in a MAV system. AVs are a\nconvenient building block for MAV since they can be easily combined without\nadditional training. Moreover, we introduce BoN-MAV, a simple multi-agent\nverification algorithm that combines best-of-n sampling with multiple\nverifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency\nand reward model verification, and we demonstrate both weak-to-strong\ngeneralization, where combining weak verifiers improves even stronger LLMs, and\nself-improvement, where the same base model is used to both generate and verify\noutputs. Our results establish scaling the number of verifiers as a promising\nnew dimension for improving language model performance at test-time."
                },
                "authors": [
                    {
                        "name": "Shalev Lifshitz"
                    },
                    {
                        "name": "Sheila A. McIlraith"
                    },
                    {
                        "name": "Yilun Du"
                    }
                ],
                "author_detail": {
                    "name": "Yilun Du"
                },
                "author": "Yilun Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20377v1",
                "updated": "2025-02-27T18:51:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    51,
                    22,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T18:51:22Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    51,
                    22,
                    3,
                    58,
                    0
                ],
                "title": "PhantomWiki: On-Demand Datasets for Reasoning and Retrieval Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhantomWiki: On-Demand Datasets for Reasoning and Retrieval Evaluation"
                },
                "summary": "High-quality benchmarks are essential for evaluating reasoning and retrieval\ncapabilities of large language models (LLMs). However, curating datasets for\nthis purpose is not a permanent solution as they are prone to data leakage and\ninflated performance results. To address these challenges, we propose\nPhantomWiki: a pipeline to generate unique, factually consistent document\ncorpora with diverse question-answer pairs. Unlike prior work, PhantomWiki is\nneither a fixed dataset, nor is it based on any existing data. Instead, a new\nPhantomWiki instance is generated on demand for each evaluation. We vary the\nquestion difficulty and corpus size to disentangle reasoning and retrieval\ncapabilities respectively, and find that PhantomWiki datasets are surprisingly\nchallenging for frontier LLMs. Thus, we contribute a scalable and data\nleakage-resistant framework for disentangled evaluation of reasoning,\nretrieval, and tool-use abilities. Our code is available at\nhttps://github.com/kilian-group/phantom-wiki.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality benchmarks are essential for evaluating reasoning and retrieval\ncapabilities of large language models (LLMs). However, curating datasets for\nthis purpose is not a permanent solution as they are prone to data leakage and\ninflated performance results. To address these challenges, we propose\nPhantomWiki: a pipeline to generate unique, factually consistent document\ncorpora with diverse question-answer pairs. Unlike prior work, PhantomWiki is\nneither a fixed dataset, nor is it based on any existing data. Instead, a new\nPhantomWiki instance is generated on demand for each evaluation. We vary the\nquestion difficulty and corpus size to disentangle reasoning and retrieval\ncapabilities respectively, and find that PhantomWiki datasets are surprisingly\nchallenging for frontier LLMs. Thus, we contribute a scalable and data\nleakage-resistant framework for disentangled evaluation of reasoning,\nretrieval, and tool-use abilities. Our code is available at\nhttps://github.com/kilian-group/phantom-wiki."
                },
                "authors": [
                    {
                        "name": "Albert Gong"
                    },
                    {
                        "name": "Kamilė Stankevičiūtė"
                    },
                    {
                        "name": "Chao Wan"
                    },
                    {
                        "name": "Anmol Kabra"
                    },
                    {
                        "name": "Raphael Thesmar"
                    },
                    {
                        "name": "Johann Lee"
                    },
                    {
                        "name": "Julius Klenke"
                    },
                    {
                        "name": "Carla P. Gomes"
                    },
                    {
                        "name": "Kilian Q. Weinberger"
                    }
                ],
                "author_detail": {
                    "name": "Kilian Q. Weinberger"
                },
                "author": "Kilian Q. Weinberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20364v1",
                "updated": "2025-02-27T18:35:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    35,
                    39,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T18:35:39Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    35,
                    39,
                    3,
                    58,
                    0
                ],
                "title": "Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with\n  Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix\n  Factorization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with\n  Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix\n  Factorization"
                },
                "summary": "Agentic Generative AI, powered by Large Language Models (LLMs) with\nRetrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores\n(VSs), represents a transformative technology applicable to specialized domains\nsuch as legal systems, research, recommender systems, cybersecurity, and global\nsecurity, including proliferation research. This technology excels at inferring\nrelationships within vast unstructured or semi-structured datasets. The legal\ndomain here comprises complex data characterized by extensive, interrelated,\nand semi-structured knowledge systems with complex relations. It comprises\nconstitutions, statutes, regulations, and case law. Extracting insights and\nnavigating the intricate networks of legal documents and their relations is\ncrucial for effective legal research. Here, we introduce a generative AI system\nthat integrates RAG, VS, and KG, constructed via Non-Negative Matrix\nFactorization (NMF), to enhance legal information retrieval and AI reasoning\nand minimize hallucinations. In the legal system, these technologies empower AI\nagents to identify and analyze complex connections among cases, statutes, and\nlegal precedents, uncovering hidden relationships and predicting legal\ntrends-challenging tasks that are essential for ensuring justice and improving\noperational efficiency. Our system employs web scraping techniques to\nsystematically collect legal texts, such as statutes, constitutional\nprovisions, and case law, from publicly accessible platforms like Justia. It\nbridges the gap between traditional keyword-based searches and contextual\nunderstanding by leveraging advanced semantic representations, hierarchical\nrelationships, and latent topic discovery. This framework supports legal\ndocument clustering, summarization, and cross-referencing, for scalable,\ninterpretable, and accurate retrieval for semi-structured data while advancing\ncomputational law and AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Generative AI, powered by Large Language Models (LLMs) with\nRetrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores\n(VSs), represents a transformative technology applicable to specialized domains\nsuch as legal systems, research, recommender systems, cybersecurity, and global\nsecurity, including proliferation research. This technology excels at inferring\nrelationships within vast unstructured or semi-structured datasets. The legal\ndomain here comprises complex data characterized by extensive, interrelated,\nand semi-structured knowledge systems with complex relations. It comprises\nconstitutions, statutes, regulations, and case law. Extracting insights and\nnavigating the intricate networks of legal documents and their relations is\ncrucial for effective legal research. Here, we introduce a generative AI system\nthat integrates RAG, VS, and KG, constructed via Non-Negative Matrix\nFactorization (NMF), to enhance legal information retrieval and AI reasoning\nand minimize hallucinations. In the legal system, these technologies empower AI\nagents to identify and analyze complex connections among cases, statutes, and\nlegal precedents, uncovering hidden relationships and predicting legal\ntrends-challenging tasks that are essential for ensuring justice and improving\noperational efficiency. Our system employs web scraping techniques to\nsystematically collect legal texts, such as statutes, constitutional\nprovisions, and case law, from publicly accessible platforms like Justia. It\nbridges the gap between traditional keyword-based searches and contextual\nunderstanding by leveraging advanced semantic representations, hierarchical\nrelationships, and latent topic discovery. This framework supports legal\ndocument clustering, summarization, and cross-referencing, for scalable,\ninterpretable, and accurate retrieval for semi-structured data while advancing\ncomputational law and AI."
                },
                "authors": [
                    {
                        "name": "Ryan C. Barron"
                    },
                    {
                        "name": "Maksim E. Eren"
                    },
                    {
                        "name": "Olga M. Serafimova"
                    },
                    {
                        "name": "Cynthia Matuszek"
                    },
                    {
                        "name": "Boian S. Alexandrov"
                    }
                ],
                "author_detail": {
                    "name": "Boian S. Alexandrov"
                },
                "author": "Boian S. Alexandrov",
                "arxiv_comment": "10 pages, 6 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05332v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05332v2",
                "updated": "2025-02-27T18:31:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    31,
                    8,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-07T21:13:31Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    21,
                    13,
                    31,
                    4,
                    38,
                    0
                ],
                "title": "Removing Neural Signal Artifacts with Autoencoder-Targeted Adversarial\n  Transformers (AT-AT)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Removing Neural Signal Artifacts with Autoencoder-Targeted Adversarial\n  Transformers (AT-AT)"
                },
                "summary": "Electromyogenic (EMG) noise is a major contamination source in EEG data that\ncan impede accurate analysis of brain-specific neural activity. Recent\nliterature on EMG artifact removal has moved beyond traditional linear\nalgorithms in favor of machine learning-based systems. However, existing deep\nlearning-based filtration methods often have large compute footprints and\nprohibitively long training times. In this study, we present a new machine\nlearning-based system for filtering EMG interference from EEG data using an\nautoencoder-targeted adversarial transformer (AT-AT). By leveraging the\nlightweight expressivity of an autoencoder to determine optimal time-series\ntransformer application sites, our AT-AT architecture achieves a >90% model\nsize reduction compared to published artifact removal models. The addition of\nadversarial training ensures that filtered signals adhere to the fundamental\ncharacteristics of EEG data. We trained AT-AT using published neural data from\n67 subjects and found that the system was able to achieve comparable test\nperformance to larger models; AT-AT posted a mean reconstructive correlation\ncoefficient above 0.95 at an initial signal-to-noise ratio (SNR) of 2 dB and\n0.70 at -7 dB SNR. Further research generalizing these results to broader\nsample sizes beyond these isolated test cases will be crucial; while outside\nthe scope of this study, we also include results from a real-world deployment\nof AT-AT in the Appendix.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electromyogenic (EMG) noise is a major contamination source in EEG data that\ncan impede accurate analysis of brain-specific neural activity. Recent\nliterature on EMG artifact removal has moved beyond traditional linear\nalgorithms in favor of machine learning-based systems. However, existing deep\nlearning-based filtration methods often have large compute footprints and\nprohibitively long training times. In this study, we present a new machine\nlearning-based system for filtering EMG interference from EEG data using an\nautoencoder-targeted adversarial transformer (AT-AT). By leveraging the\nlightweight expressivity of an autoencoder to determine optimal time-series\ntransformer application sites, our AT-AT architecture achieves a >90% model\nsize reduction compared to published artifact removal models. The addition of\nadversarial training ensures that filtered signals adhere to the fundamental\ncharacteristics of EEG data. We trained AT-AT using published neural data from\n67 subjects and found that the system was able to achieve comparable test\nperformance to larger models; AT-AT posted a mean reconstructive correlation\ncoefficient above 0.95 at an initial signal-to-noise ratio (SNR) of 2 dB and\n0.70 at -7 dB SNR. Further research generalizing these results to broader\nsample sizes beyond these isolated test cases will be crucial; while outside\nthe scope of this study, we also include results from a real-world deployment\nof AT-AT in the Appendix."
                },
                "authors": [
                    {
                        "name": "Benjamin J. Choi"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin J. Choi"
                },
                "author": "Benjamin J. Choi",
                "arxiv_comment": "Accepted at CNS 2025, Boston, MA, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05332v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05332v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20356v1",
                "updated": "2025-02-27T18:29:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    29,
                    9,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T18:29:09Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    29,
                    9,
                    3,
                    58,
                    0
                ],
                "title": "Bridging the Creativity Understanding Gap: Small-Scale Human Alignment\n  Enables Expert-Level Humor Ranking in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Creativity Understanding Gap: Small-Scale Human Alignment\n  Enables Expert-Level Humor Ranking in LLMs"
                },
                "summary": "Large Language Models (LLMs) have shown significant limitations in\nunderstanding creative content, as demonstrated by Hessel et al. (2023)'s\ninfluential work on the New Yorker Cartoon Caption Contest (NYCCC). Their study\nexposed a substantial gap between LLMs and humans in humor comprehension,\nestablishing that understanding and evaluating creative content is key\nchallenge in AI development. We revisit this challenge by decomposing humor\nunderstanding into three components and systematically improve each: enhancing\nvisual understanding through improved annotation, utilizing LLM-generated humor\nreasoning and explanations, and implementing targeted alignment with human\npreference data. Our refined approach achieves 82.4% accuracy in caption\nranking, singificantly improving upon the previous 67% benchmark and matching\nthe performance of world-renowned human experts in this domain. Notably, while\nattempts to mimic subgroup preferences through various persona prompts showed\nminimal impact, model finetuning with crowd preferences proved remarkably\neffective. These findings reveal that LLM limitations in creative judgment can\nbe effectively addressed through focused alignment to specific subgroups and\nindividuals. Lastly, we propose the position that achieving artificial general\nintelligence necessitates systematic collection of human preference data across\ncreative domains. We advocate that just as human creativity is deeply\ninfluenced by individual and cultural preferences, training LLMs with diverse\nhuman preference data may be essential for developing true creative\nunderstanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown significant limitations in\nunderstanding creative content, as demonstrated by Hessel et al. (2023)'s\ninfluential work on the New Yorker Cartoon Caption Contest (NYCCC). Their study\nexposed a substantial gap between LLMs and humans in humor comprehension,\nestablishing that understanding and evaluating creative content is key\nchallenge in AI development. We revisit this challenge by decomposing humor\nunderstanding into three components and systematically improve each: enhancing\nvisual understanding through improved annotation, utilizing LLM-generated humor\nreasoning and explanations, and implementing targeted alignment with human\npreference data. Our refined approach achieves 82.4% accuracy in caption\nranking, singificantly improving upon the previous 67% benchmark and matching\nthe performance of world-renowned human experts in this domain. Notably, while\nattempts to mimic subgroup preferences through various persona prompts showed\nminimal impact, model finetuning with crowd preferences proved remarkably\neffective. These findings reveal that LLM limitations in creative judgment can\nbe effectively addressed through focused alignment to specific subgroups and\nindividuals. Lastly, we propose the position that achieving artificial general\nintelligence necessitates systematic collection of human preference data across\ncreative domains. We advocate that just as human creativity is deeply\ninfluenced by individual and cultural preferences, training LLMs with diverse\nhuman preference data may be essential for developing true creative\nunderstanding."
                },
                "authors": [
                    {
                        "name": "Kuan Lok Zhou"
                    },
                    {
                        "name": "Jiayi Chen"
                    },
                    {
                        "name": "Siddharth Suresh"
                    },
                    {
                        "name": "Reuben Narad"
                    },
                    {
                        "name": "Timothy T. Rogers"
                    },
                    {
                        "name": "Lalit K Jain"
                    },
                    {
                        "name": "Robert D Nowak"
                    },
                    {
                        "name": "Bob Mankoff"
                    },
                    {
                        "name": "Jifan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jifan Zhang"
                },
                "author": "Jifan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20350v1",
                "updated": "2025-02-27T18:22:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    22,
                    33,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T18:22:33Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    22,
                    33,
                    3,
                    58,
                    0
                ],
                "title": "KEDRec-LM: A Knowledge-distilled Explainable Drug Recommendation Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KEDRec-LM: A Knowledge-distilled Explainable Drug Recommendation Large\n  Language Model"
                },
                "summary": "Drug discovery is a critical task in biomedical natural language processing\n(NLP), yet explainable drug discovery remains underexplored. Meanwhile, large\nlanguage models (LLMs) have shown remarkable abilities in natural language\nunderstanding and generation. Leveraging LLMs for explainable drug discovery\nhas the potential to improve downstream tasks and real-world applications. In\nthis study, we utilize open-source drug knowledge graphs, clinical trial data,\nand PubMed publications to construct a comprehensive dataset for the\nexplainable drug discovery task, named \\textbf{expRxRec}. Furthermore, we\nintroduce \\textbf{KEDRec-LM}, an instruction-tuned LLM which distills knowledge\nfrom rich medical knowledge corpus for drug recommendation and rationale\ngeneration. To encourage further research in this area, we will publicly\nrelease\\footnote{A copy is attached with this submission} both the dataset and\nKEDRec-LM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drug discovery is a critical task in biomedical natural language processing\n(NLP), yet explainable drug discovery remains underexplored. Meanwhile, large\nlanguage models (LLMs) have shown remarkable abilities in natural language\nunderstanding and generation. Leveraging LLMs for explainable drug discovery\nhas the potential to improve downstream tasks and real-world applications. In\nthis study, we utilize open-source drug knowledge graphs, clinical trial data,\nand PubMed publications to construct a comprehensive dataset for the\nexplainable drug discovery task, named \\textbf{expRxRec}. Furthermore, we\nintroduce \\textbf{KEDRec-LM}, an instruction-tuned LLM which distills knowledge\nfrom rich medical knowledge corpus for drug recommendation and rationale\ngeneration. To encourage further research in this area, we will publicly\nrelease\\footnote{A copy is attached with this submission} both the dataset and\nKEDRec-LM."
                },
                "authors": [
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Rui Zhu"
                    },
                    {
                        "name": "Shutian Ma"
                    },
                    {
                        "name": "Jingwei Xiong"
                    },
                    {
                        "name": "Yejin Kim"
                    },
                    {
                        "name": "Fabricio Murai"
                    },
                    {
                        "name": "Xiaozhong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaozhong Liu"
                },
                "author": "Xiaozhong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20344v1",
                "updated": "2025-02-27T18:16:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    16,
                    47,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T18:16:47Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    16,
                    47,
                    3,
                    58,
                    0
                ],
                "title": "Sparse Auto-Encoder Interprets Linguistic Features in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Auto-Encoder Interprets Linguistic Features in Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) excel in tasks that require complex linguistic\nabilities, such as reference disambiguation and metaphor\nrecognition/generation. Although LLMs possess impressive capabilities, their\ninternal mechanisms for processing and representing linguistic knowledge remain\nlargely opaque. Previous work on linguistic mechanisms has been limited by\ncoarse granularity, insufficient causal analysis, and a narrow focus. In this\nstudy, we present a systematic and comprehensive causal investigation using\nsparse auto-encoders (SAEs). We extract a wide range of linguistic features\nfrom six dimensions: phonetics, phonology, morphology, syntax, semantics, and\npragmatics. We extract, evaluate, and intervene on these features by\nconstructing minimal contrast datasets and counterfactual sentence datasets. We\nintroduce two indices-Feature Representation Confidence (FRC) and Feature\nIntervention Confidence (FIC)-to measure the ability of linguistic features to\ncapture and control linguistic phenomena. Our results reveal inherent\nrepresentations of linguistic knowledge in LLMs and demonstrate the potential\nfor controlling model outputs. This work provides strong evidence that LLMs\npossess genuine linguistic knowledge and lays the foundation for more\ninterpretable and controllable language modeling in future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in tasks that require complex linguistic\nabilities, such as reference disambiguation and metaphor\nrecognition/generation. Although LLMs possess impressive capabilities, their\ninternal mechanisms for processing and representing linguistic knowledge remain\nlargely opaque. Previous work on linguistic mechanisms has been limited by\ncoarse granularity, insufficient causal analysis, and a narrow focus. In this\nstudy, we present a systematic and comprehensive causal investigation using\nsparse auto-encoders (SAEs). We extract a wide range of linguistic features\nfrom six dimensions: phonetics, phonology, morphology, syntax, semantics, and\npragmatics. We extract, evaluate, and intervene on these features by\nconstructing minimal contrast datasets and counterfactual sentence datasets. We\nintroduce two indices-Feature Representation Confidence (FRC) and Feature\nIntervention Confidence (FIC)-to measure the ability of linguistic features to\ncapture and control linguistic phenomena. Our results reveal inherent\nrepresentations of linguistic knowledge in LLMs and demonstrate the potential\nfor controlling model outputs. This work provides strong evidence that LLMs\npossess genuine linguistic knowledge and lays the foundation for more\ninterpretable and controllable language modeling in future research."
                },
                "authors": [
                    {
                        "name": "Yi Jing"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Lingxu Ran"
                    },
                    {
                        "name": "Hongzhu Guo"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20339v1",
                "updated": "2025-02-27T18:08:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    8,
                    16,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T18:08:16Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    8,
                    16,
                    3,
                    58,
                    0
                ],
                "title": "Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners"
                },
                "summary": "Recent advancements have demonstrated that the performance of large language\nmodels (LLMs) can be significantly enhanced by scaling computational resources\nat test time. A common strategy involves generating multiple Chain-of-Thought\n(CoT) trajectories and aggregating their outputs through various selection\nmechanisms. This raises a fundamental question: can models with lower\ncomplexity leverage their superior generation throughput to outperform\nsimilarly sized Transformers for a fixed computational budget? To address this\nquestion and overcome the lack of strong subquadratic reasoners, we distill\npure and hybrid Mamba models from pretrained Transformers. Trained on only 8\nbillion tokens, our distilled models show strong performance and scaling on\nmathematical reasoning datasets while being much faster at inference for large\nbatches and long sequences. Despite the zero-shot performance hit due to\ndistillation, both pure and hybrid Mamba models can scale their coverage and\naccuracy performance past their Transformer teacher models under fixed time\nbudgets, opening a new direction for scaling inference compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements have demonstrated that the performance of large language\nmodels (LLMs) can be significantly enhanced by scaling computational resources\nat test time. A common strategy involves generating multiple Chain-of-Thought\n(CoT) trajectories and aggregating their outputs through various selection\nmechanisms. This raises a fundamental question: can models with lower\ncomplexity leverage their superior generation throughput to outperform\nsimilarly sized Transformers for a fixed computational budget? To address this\nquestion and overcome the lack of strong subquadratic reasoners, we distill\npure and hybrid Mamba models from pretrained Transformers. Trained on only 8\nbillion tokens, our distilled models show strong performance and scaling on\nmathematical reasoning datasets while being much faster at inference for large\nbatches and long sequences. Despite the zero-shot performance hit due to\ndistillation, both pure and hybrid Mamba models can scale their coverage and\naccuracy performance past their Transformer teacher models under fixed time\nbudgets, opening a new direction for scaling inference compute."
                },
                "authors": [
                    {
                        "name": "Daniele Paliotta"
                    },
                    {
                        "name": "Junxiong Wang"
                    },
                    {
                        "name": "Matteo Pagliardini"
                    },
                    {
                        "name": "Kevin Y. Li"
                    },
                    {
                        "name": "Aviv Bick"
                    },
                    {
                        "name": "J. Zico Kolter"
                    },
                    {
                        "name": "Albert Gu"
                    },
                    {
                        "name": "François Fleuret"
                    },
                    {
                        "name": "Tri Dao"
                    }
                ],
                "author_detail": {
                    "name": "Tri Dao"
                },
                "author": "Tri Dao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20335v1",
                "updated": "2025-02-27T18:05:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    5,
                    15,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T18:05:15Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    5,
                    15,
                    3,
                    58,
                    0
                ],
                "title": "Expertise Is What We Want",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expertise Is What We Want"
                },
                "summary": "Clinical decision-making depends on expert reasoning, which is guided by\nstandardized, evidence-based guidelines. However, translating these guidelines\ninto automated clinical decision support systems risks inaccuracy and\nimportantly, loss of nuance. We share an application architecture, the Large\nLanguage Expert (LLE), that combines the flexibility and power of Large\nLanguage Models (LLMs) with the interpretability, explainability, and\nreliability of Expert Systems. LLMs help address key challenges of Expert\nSystems, such as integrating and codifying knowledge, and data normalization.\nConversely, an Expert System-like approach helps overcome challenges with LLMs,\nincluding hallucinations, atomic and inexpensive updates, and testability.\n  To highlight the power of the Large Language Expert (LLE) system, we built an\nLLE to assist with the workup of patients newly diagnosed with cancer. Timely\ninitiation of cancer treatment is critical for optimal patient outcomes.\nHowever, increasing complexity in diagnostic recommendations has made it\ndifficult for primary care physicians to ensure their patients have completed\nthe necessary workup before their first visit with an oncologist. As with many\nreal-world clinical tasks, these workups require the analysis of unstructured\nhealth records and the application of nuanced clinical decision logic. In this\nstudy, we describe the design & evaluation of an LLE system built to rapidly\nidentify and suggest the correct diagnostic workup. The system demonstrated a\nhigh degree of clinical-level accuracy (>95%) and effectively addressed gaps\nidentified in real-world data from breast and colon cancer patients at a large\nacademic center.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical decision-making depends on expert reasoning, which is guided by\nstandardized, evidence-based guidelines. However, translating these guidelines\ninto automated clinical decision support systems risks inaccuracy and\nimportantly, loss of nuance. We share an application architecture, the Large\nLanguage Expert (LLE), that combines the flexibility and power of Large\nLanguage Models (LLMs) with the interpretability, explainability, and\nreliability of Expert Systems. LLMs help address key challenges of Expert\nSystems, such as integrating and codifying knowledge, and data normalization.\nConversely, an Expert System-like approach helps overcome challenges with LLMs,\nincluding hallucinations, atomic and inexpensive updates, and testability.\n  To highlight the power of the Large Language Expert (LLE) system, we built an\nLLE to assist with the workup of patients newly diagnosed with cancer. Timely\ninitiation of cancer treatment is critical for optimal patient outcomes.\nHowever, increasing complexity in diagnostic recommendations has made it\ndifficult for primary care physicians to ensure their patients have completed\nthe necessary workup before their first visit with an oncologist. As with many\nreal-world clinical tasks, these workups require the analysis of unstructured\nhealth records and the application of nuanced clinical decision logic. In this\nstudy, we describe the design & evaluation of an LLE system built to rapidly\nidentify and suggest the correct diagnostic workup. The system demonstrated a\nhigh degree of clinical-level accuracy (>95%) and effectively addressed gaps\nidentified in real-world data from breast and colon cancer patients at a large\nacademic center."
                },
                "authors": [
                    {
                        "name": "Alan Ashworth"
                    },
                    {
                        "name": "Munir Al-Dajani"
                    },
                    {
                        "name": "Keegan Duchicela"
                    },
                    {
                        "name": "Kiril Kafadarov"
                    },
                    {
                        "name": "Allison Kurian"
                    },
                    {
                        "name": "Othman Laraki"
                    },
                    {
                        "name": "Amina Lazrak"
                    },
                    {
                        "name": "Divneet Mandair"
                    },
                    {
                        "name": "Wendy McKennon"
                    },
                    {
                        "name": "Rebecca Miksad"
                    },
                    {
                        "name": "Jayodita Sanghvi"
                    },
                    {
                        "name": "Travis Zack"
                    }
                ],
                "author_detail": {
                    "name": "Travis Zack"
                },
                "author": "Travis Zack",
                "arxiv_comment": "18 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20330v1",
                "updated": "2025-02-27T17:59:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "title": "Long-Context Inference with Retrieval-Augmented Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Inference with Retrieval-Augmented Speculative Decoding"
                },
                "summary": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications."
                },
                "authors": [
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Qilong Feng"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Qizhe Shieh"
                },
                "author": "Michael Qizhe Shieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00075v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00075v4",
                "updated": "2025-02-27T17:49:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    49,
                    33,
                    3,
                    58,
                    0
                ],
                "published": "2024-06-21T19:18:16Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    19,
                    18,
                    16,
                    4,
                    173,
                    0
                ],
                "title": "Logicbreaks: A Framework for Understanding Subversion of Rule-based\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logicbreaks: A Framework for Understanding Subversion of Rule-based\n  Inference"
                },
                "summary": "We study how to subvert large language models (LLMs) from following\nprompt-specified rules. We first formalize rule-following as inference in\npropositional Horn logic, a mathematical system in which rules have the form\n\"if $P$ and $Q$, then $R$\" for some propositions $P$, $Q$, and $R$. Next, we\nprove that although small transformers can faithfully follow such rules,\nmaliciously crafted prompts can still mislead both theoretical constructions\nand models learned from data. Furthermore, we demonstrate that popular attack\nalgorithms on LLMs find adversarial prompts and induce attention patterns that\nalign with our theory. Our novel logic-based framework provides a foundation\nfor studying LLMs in rule-based settings, enabling a formal analysis of tasks\nlike logical reasoning and jailbreak attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study how to subvert large language models (LLMs) from following\nprompt-specified rules. We first formalize rule-following as inference in\npropositional Horn logic, a mathematical system in which rules have the form\n\"if $P$ and $Q$, then $R$\" for some propositions $P$, $Q$, and $R$. Next, we\nprove that although small transformers can faithfully follow such rules,\nmaliciously crafted prompts can still mislead both theoretical constructions\nand models learned from data. Furthermore, we demonstrate that popular attack\nalgorithms on LLMs find adversarial prompts and induce attention patterns that\nalign with our theory. Our novel logic-based framework provides a foundation\nfor studying LLMs in rule-based settings, enabling a formal analysis of tasks\nlike logical reasoning and jailbreak attacks."
                },
                "authors": [
                    {
                        "name": "Anton Xue"
                    },
                    {
                        "name": "Avishree Khare"
                    },
                    {
                        "name": "Rajeev Alur"
                    },
                    {
                        "name": "Surbhi Goel"
                    },
                    {
                        "name": "Eric Wong"
                    }
                ],
                "author_detail": {
                    "name": "Eric Wong"
                },
                "author": "Eric Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00075v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00075v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16097v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16097v2",
                "updated": "2025-02-27T17:43:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    43,
                    42,
                    3,
                    58,
                    0
                ],
                "published": "2024-12-20T17:41:02Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    41,
                    2,
                    4,
                    355,
                    0
                ],
                "title": "Dual-Polarized Beyond Diagonal RIS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual-Polarized Beyond Diagonal RIS"
                },
                "summary": "Beyond diagonal reconfigurable intelligent surface (BD-RIS) is a family of\nRIS architectures more flexible than conventional RIS. While BD-RIS has been\nprimarily analyzed assuming uni-polarized systems, modern wireless deployments\nare dual-polarized. To address this gap, this paper investigates the\nfundamental limits of dual-polarized BD-RIS-aided systems. We derive the\nscaling laws governing the performance of BD-RIS and the Pareto frontier of the\ntrade-off between performance and circuit complexity enabled by BD-RIS.\nTheoretical results show that the group-connected RIS with group size 2\nprovides remarkable gains over conventional RIS in both Rayleigh and\nline-of-sight (LoS) channels, while maintaining a reduced circuit complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond diagonal reconfigurable intelligent surface (BD-RIS) is a family of\nRIS architectures more flexible than conventional RIS. While BD-RIS has been\nprimarily analyzed assuming uni-polarized systems, modern wireless deployments\nare dual-polarized. To address this gap, this paper investigates the\nfundamental limits of dual-polarized BD-RIS-aided systems. We derive the\nscaling laws governing the performance of BD-RIS and the Pareto frontier of the\ntrade-off between performance and circuit complexity enabled by BD-RIS.\nTheoretical results show that the group-connected RIS with group size 2\nprovides remarkable gains over conventional RIS in both Rayleigh and\nline-of-sight (LoS) channels, while maintaining a reduced circuit complexity."
                },
                "authors": [
                    {
                        "name": "Matteo Nerini"
                    },
                    {
                        "name": "Bruno Clerckx"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Clerckx"
                },
                "author": "Bruno Clerckx",
                "arxiv_comment": "Accepted by IEEE for publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16097v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16097v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20309v1",
                "updated": "2025-02-27T17:35:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    35,
                    57,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:35:57Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    35,
                    57,
                    3,
                    58,
                    0
                ],
                "title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific\n  Research Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific\n  Research Assistants"
                },
                "summary": "Recent advancements have positioned AI, and particularly Large Language\nModels (LLMs), as transformative tools for scientific research, capable of\naddressing complex tasks that require reasoning, problem-solving, and\ndecision-making. Their exceptional capabilities suggest their potential as\nscientific research assistants but also highlight the need for holistic,\nrigorous, and domain-specific evaluation to assess effectiveness in real-world\nscientific applications. This paper describes a multifaceted methodology for\nEvaluating AI models as scientific Research Assistants (EAIRA) developed at\nArgonne National Laboratory. This methodology incorporates four primary classes\nof evaluations. 1) Multiple Choice Questions to assess factual recall; 2) Open\nResponse to evaluate advanced reasoning and problem-solving skills; 3)\nLab-Style Experiments involving detailed analysis of capabilities as research\nassistants in controlled environments; and 4) Field-Style Experiments to\ncapture researcher-LLM interactions at scale in a wide range of scientific\ndomains and applications. These complementary methods enable a comprehensive\nanalysis of LLM strengths and weaknesses with respect to their scientific\nknowledge, reasoning abilities, and adaptability. Recognizing the rapid pace of\nLLM advancements, we designed the methodology to evolve and adapt so as to\nensure its continued relevance and applicability. This paper describes the\nmethodology state at the end of February 2025. Although developed within a\nsubset of scientific domains, the methodology is designed to be generalizable\nto a wide range of scientific domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements have positioned AI, and particularly Large Language\nModels (LLMs), as transformative tools for scientific research, capable of\naddressing complex tasks that require reasoning, problem-solving, and\ndecision-making. Their exceptional capabilities suggest their potential as\nscientific research assistants but also highlight the need for holistic,\nrigorous, and domain-specific evaluation to assess effectiveness in real-world\nscientific applications. This paper describes a multifaceted methodology for\nEvaluating AI models as scientific Research Assistants (EAIRA) developed at\nArgonne National Laboratory. This methodology incorporates four primary classes\nof evaluations. 1) Multiple Choice Questions to assess factual recall; 2) Open\nResponse to evaluate advanced reasoning and problem-solving skills; 3)\nLab-Style Experiments involving detailed analysis of capabilities as research\nassistants in controlled environments; and 4) Field-Style Experiments to\ncapture researcher-LLM interactions at scale in a wide range of scientific\ndomains and applications. These complementary methods enable a comprehensive\nanalysis of LLM strengths and weaknesses with respect to their scientific\nknowledge, reasoning abilities, and adaptability. Recognizing the rapid pace of\nLLM advancements, we designed the methodology to evolve and adapt so as to\nensure its continued relevance and applicability. This paper describes the\nmethodology state at the end of February 2025. Although developed within a\nsubset of scientific domains, the methodology is designed to be generalizable\nto a wide range of scientific domains."
                },
                "authors": [
                    {
                        "name": "Franck Cappello"
                    },
                    {
                        "name": "Sandeep Madireddy"
                    },
                    {
                        "name": "Robert Underwood"
                    },
                    {
                        "name": "Neil Getty"
                    },
                    {
                        "name": "Nicholas Lee-Ping Chia"
                    },
                    {
                        "name": "Nesar Ramachandra"
                    },
                    {
                        "name": "Josh Nguyen"
                    },
                    {
                        "name": "Murat Keceli"
                    },
                    {
                        "name": "Tanwi Mallick"
                    },
                    {
                        "name": "Zilinghan Li"
                    },
                    {
                        "name": "Marieme Ngom"
                    },
                    {
                        "name": "Chenhui Zhang"
                    },
                    {
                        "name": "Angel Yanguas-Gil"
                    },
                    {
                        "name": "Evan Antoniuk"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    },
                    {
                        "name": "Minyang Tian"
                    },
                    {
                        "name": "Yufeng Du"
                    },
                    {
                        "name": "Yuan-Sen Ting"
                    },
                    {
                        "name": "Azton Wells"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    },
                    {
                        "name": "Avinash Maurya"
                    },
                    {
                        "name": "M. Mustafa Rafique"
                    },
                    {
                        "name": "Eliu Huerta"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Ian Foster"
                    },
                    {
                        "name": "Rick Stevens"
                    }
                ],
                "author_detail": {
                    "name": "Rick Stevens"
                },
                "author": "Rick Stevens",
                "arxiv_comment": "33 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20299v1",
                "updated": "2025-02-27T17:26:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    26,
                    56,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:26:56Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    26,
                    56,
                    3,
                    58,
                    0
                ],
                "title": "An exploration of features to improve the generalisability of fake news\n  detection models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An exploration of features to improve the generalisability of fake news\n  detection models"
                },
                "summary": "Fake news poses global risks by influencing elections and spreading\nmisinformation, making detection critical. Existing NLP and supervised Machine\nLearning methods perform well under cross-validation but struggle to generalise\nacross datasets, even within the same domain. This issue stems from coarsely\nlabelled training data, where articles are labelled based on their publisher,\nintroducing biases that token-based models like TF-IDF and BERT are sensitive\nto. While Large Language Models (LLMs) offer promise, their application in fake\nnews detection remains limited. This study demonstrates that meaningful\nfeatures can still be extracted from coarsely labelled data to improve\nreal-world robustness. Stylistic features-lexical, syntactic, and semantic-are\nexplored due to their reduced sensitivity to dataset biases. Additionally,\nnovel social-monetisation features are introduced, capturing economic\nincentives behind fake news, such as advertisements, external links, and social\nmedia elements. The study trains on the coarsely labelled NELA 2020-21 dataset\nand evaluates using the manually labelled Facebook URLs dataset, a gold\nstandard for generalisability. Results highlight the limitations of token-based\nmodels trained on biased data and contribute to the scarce evidence on LLMs\nlike LLaMa in this field. Findings indicate that stylistic and\nsocial-monetisation features offer more generalisable predictions than\ntoken-based methods and LLMs. Statistical and permutation feature importance\nanalyses further reveal their potential to enhance performance and mitigate\ndataset biases, providing a path forward for improving fake news detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fake news poses global risks by influencing elections and spreading\nmisinformation, making detection critical. Existing NLP and supervised Machine\nLearning methods perform well under cross-validation but struggle to generalise\nacross datasets, even within the same domain. This issue stems from coarsely\nlabelled training data, where articles are labelled based on their publisher,\nintroducing biases that token-based models like TF-IDF and BERT are sensitive\nto. While Large Language Models (LLMs) offer promise, their application in fake\nnews detection remains limited. This study demonstrates that meaningful\nfeatures can still be extracted from coarsely labelled data to improve\nreal-world robustness. Stylistic features-lexical, syntactic, and semantic-are\nexplored due to their reduced sensitivity to dataset biases. Additionally,\nnovel social-monetisation features are introduced, capturing economic\nincentives behind fake news, such as advertisements, external links, and social\nmedia elements. The study trains on the coarsely labelled NELA 2020-21 dataset\nand evaluates using the manually labelled Facebook URLs dataset, a gold\nstandard for generalisability. Results highlight the limitations of token-based\nmodels trained on biased data and contribute to the scarce evidence on LLMs\nlike LLaMa in this field. Findings indicate that stylistic and\nsocial-monetisation features offer more generalisable predictions than\ntoken-based methods and LLMs. Statistical and permutation feature importance\nanalyses further reveal their potential to enhance performance and mitigate\ndataset biases, providing a path forward for improving fake news detection."
                },
                "authors": [
                    {
                        "name": "Nathaniel Hoy"
                    },
                    {
                        "name": "Theodora Koulouri"
                    }
                ],
                "author_detail": {
                    "name": "Theodora Koulouri"
                },
                "author": "Theodora Koulouri",
                "arxiv_doi": "10.1016/j.eswa.2025.126949",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.eswa.2025.126949",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.20299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at Expert Systems with Applications (Elsevier)",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17925v2",
                "updated": "2025-02-27T17:26:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    26,
                    11,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-25T07:46:36Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    7,
                    46,
                    36,
                    1,
                    56,
                    0
                ],
                "title": "LeanProgress: Guiding Search for Neural Theorem Proving via Proof\n  Progress Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeanProgress: Guiding Search for Neural Theorem Proving via Proof\n  Progress Prediction"
                },
                "summary": "Mathematical reasoning remains a significant challenge for Large Language\nModels (LLMs) due to hallucinations. When combined with formal proof assistants\nlike Lean, these hallucinations can be eliminated through rigorous\nverification, making theorem proving reliable. However, even with formal\nverification, LLMs still struggle with long proofs and complex mathematical\nformalizations. While Lean with LLMs offers valuable assistance with retrieving\nlemmas, generating tactics, or even complete proofs, it lacks a crucial\ncapability: providing a sense of proof progress. This limitation particularly\nimpacts the overall development efficiency in large formalization projects. We\nintroduce LeanProgress, a method that predicts the progress in the proof.\nTraining and evaluating our models made on a large corpus of Lean proofs from\nLean Workbook Plus and Mathlib4 and how many steps remain to complete it, we\nemploy data preprocessing and balancing techniques to handle the skewed\ndistribution of proof lengths. Our experiments show that LeanProgress achieves\nan overall prediction accuracy of 75.1\\% in predicting the amount of progress\nand, hence, the remaining number of steps. When integrated into a best-first\nsearch framework using Reprover, our method shows a 3.8\\% improvement on\nMathlib4 compared to baseline performances of 41.2\\%, particularly for longer\nproofs. These results demonstrate how proof progress prediction can enhance\nboth automated and interactive theorem proving, enabling users to make more\ninformed decisions about proof strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical reasoning remains a significant challenge for Large Language\nModels (LLMs) due to hallucinations. When combined with formal proof assistants\nlike Lean, these hallucinations can be eliminated through rigorous\nverification, making theorem proving reliable. However, even with formal\nverification, LLMs still struggle with long proofs and complex mathematical\nformalizations. While Lean with LLMs offers valuable assistance with retrieving\nlemmas, generating tactics, or even complete proofs, it lacks a crucial\ncapability: providing a sense of proof progress. This limitation particularly\nimpacts the overall development efficiency in large formalization projects. We\nintroduce LeanProgress, a method that predicts the progress in the proof.\nTraining and evaluating our models made on a large corpus of Lean proofs from\nLean Workbook Plus and Mathlib4 and how many steps remain to complete it, we\nemploy data preprocessing and balancing techniques to handle the skewed\ndistribution of proof lengths. Our experiments show that LeanProgress achieves\nan overall prediction accuracy of 75.1\\% in predicting the amount of progress\nand, hence, the remaining number of steps. When integrated into a best-first\nsearch framework using Reprover, our method shows a 3.8\\% improvement on\nMathlib4 compared to baseline performances of 41.2\\%, particularly for longer\nproofs. These results demonstrate how proof progress prediction can enhance\nboth automated and interactive theorem proving, enabling users to make more\ninformed decisions about proof strategies."
                },
                "authors": [
                    {
                        "name": "Suozhi Huang"
                    },
                    {
                        "name": "Peiyang Song"
                    },
                    {
                        "name": "Robert Joseph George"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20295v1",
                "updated": "2025-02-27T17:21:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    21,
                    18,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:21:18Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    21,
                    18,
                    3,
                    58,
                    0
                ],
                "title": "Judge a Book by its Cover: Investigating Multi-Modal LLMs for Multi-Page\n  Handwritten Document Transcription",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judge a Book by its Cover: Investigating Multi-Modal LLMs for Multi-Page\n  Handwritten Document Transcription"
                },
                "summary": "Handwritten text recognition (HTR) remains a challenging task, particularly\nfor multi-page documents where pages share common formatting and contextual\nfeatures. While modern optical character recognition (OCR) engines are\nproficient with printed text, their performance on handwriting is limited,\noften requiring costly labeled data for fine-tuning. In this paper, we explore\nthe use of multi-modal large language models (MLLMs) for transcribing\nmulti-page handwritten documents in a zero-shot setting. We investigate various\nconfigurations of commercial OCR engines and MLLMs, utilizing the latter both\nas end-to-end transcribers and as post-processors, with and without image\ncomponents. We propose a novel method, '+first page', which enhances MLLM\ntranscription by providing the OCR output of the entire document along with\njust the first page image. This approach leverages shared document features\nwithout incurring the high cost of processing all images. Experiments on a\nmulti-page version of the IAM Handwriting Database demonstrate that '+first\npage' improves transcription accuracy, balances cost with performance, and even\nenhances results on out-of-sample text by extrapolating formatting and OCR\nerror patterns from a single page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handwritten text recognition (HTR) remains a challenging task, particularly\nfor multi-page documents where pages share common formatting and contextual\nfeatures. While modern optical character recognition (OCR) engines are\nproficient with printed text, their performance on handwriting is limited,\noften requiring costly labeled data for fine-tuning. In this paper, we explore\nthe use of multi-modal large language models (MLLMs) for transcribing\nmulti-page handwritten documents in a zero-shot setting. We investigate various\nconfigurations of commercial OCR engines and MLLMs, utilizing the latter both\nas end-to-end transcribers and as post-processors, with and without image\ncomponents. We propose a novel method, '+first page', which enhances MLLM\ntranscription by providing the OCR output of the entire document along with\njust the first page image. This approach leverages shared document features\nwithout incurring the high cost of processing all images. Experiments on a\nmulti-page version of the IAM Handwriting Database demonstrate that '+first\npage' improves transcription accuracy, balances cost with performance, and even\nenhances results on out-of-sample text by extrapolating formatting and OCR\nerror patterns from a single page."
                },
                "authors": [
                    {
                        "name": "Benjamin Gutteridge"
                    },
                    {
                        "name": "Matthew Thomas Jackson"
                    },
                    {
                        "name": "Toni Kukurin"
                    },
                    {
                        "name": "Xiaowen Dong"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Dong"
                },
                "author": "Xiaowen Dong",
                "arxiv_comment": "11 pages (including references and appendix), 14 figures, accepted at\n  AAAI-25 Workshop on Document Understanding and Intelligence, non-archival",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16660v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16660v2",
                "updated": "2025-02-27T17:17:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    17,
                    8,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-23T17:38:10Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    17,
                    38,
                    10,
                    6,
                    54,
                    0
                ],
                "title": "BioMaze: Benchmarking and Enhancing Large Language Models for Biological\n  Pathway Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BioMaze: Benchmarking and Enhancing Large Language Models for Biological\n  Pathway Reasoning"
                },
                "summary": "The applications of large language models (LLMs) in various biological\ndomains have been explored recently, but their reasoning ability in complex\nbiological systems, such as pathways, remains underexplored, which is crucial\nfor predicting biological phenomena, formulating hypotheses, and designing\nexperiments. This work explores the potential of LLMs in pathway reasoning. We\nintroduce BioMaze, a dataset with 5.1K complex pathway problems derived from\nreal research, covering various biological contexts including natural dynamic\nchanges, disturbances, additional intervention conditions, and multi-scale\nresearch targets. Our evaluation of methods such as CoT and graph-augmented\nreasoning, shows that LLMs struggle with pathway reasoning, especially in\nperturbed systems. To address this, we propose PathSeeker, an LLM agent that\nenhances reasoning through interactive subgraph-based navigation, enabling a\nmore effective approach to handling the complexities of biological systems in a\nscientifically aligned manner. The dataset and code are available at\nhttps://github.com/zhao-ht/BioMaze.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The applications of large language models (LLMs) in various biological\ndomains have been explored recently, but their reasoning ability in complex\nbiological systems, such as pathways, remains underexplored, which is crucial\nfor predicting biological phenomena, formulating hypotheses, and designing\nexperiments. This work explores the potential of LLMs in pathway reasoning. We\nintroduce BioMaze, a dataset with 5.1K complex pathway problems derived from\nreal research, covering various biological contexts including natural dynamic\nchanges, disturbances, additional intervention conditions, and multi-scale\nresearch targets. Our evaluation of methods such as CoT and graph-augmented\nreasoning, shows that LLMs struggle with pathway reasoning, especially in\nperturbed systems. To address this, we propose PathSeeker, an LLM agent that\nenhances reasoning through interactive subgraph-based navigation, enabling a\nmore effective approach to handling the complexities of biological systems in a\nscientifically aligned manner. The dataset and code are available at\nhttps://github.com/zhao-ht/BioMaze."
                },
                "authors": [
                    {
                        "name": "Haiteng Zhao"
                    },
                    {
                        "name": "Chang Ma"
                    },
                    {
                        "name": "Fangzhi Xu"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Zhi-Hong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhi-Hong Deng"
                },
                "author": "Zhi-Hong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16660v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16660v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06423v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06423v4",
                "updated": "2025-02-27T17:15:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    15,
                    49,
                    3,
                    58,
                    0
                ],
                "published": "2024-07-08T22:06:09Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    22,
                    6,
                    9,
                    0,
                    190,
                    0
                ],
                "title": "InsightBench: Evaluating Business Analytics Agents Through Multi-Step\n  Insight Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InsightBench: Evaluating Business Analytics Agents Through Multi-Step\n  Insight Generation"
                },
                "summary": "Data analytics is essential for extracting valuable insights from data that\ncan assist organizations in making effective decisions. We introduce\nInsightBench, a benchmark dataset with three key features. First, it consists\nof 100 datasets representing diverse business use cases such as finance and\nincident management, each accompanied by a carefully curated set of insights\nplanted in the datasets. Second, unlike existing benchmarks focusing on\nanswering single queries, InsightBench evaluates agents based on their ability\nto perform end-to-end data analytics, including formulating questions,\ninterpreting answers, and generating a summary of insights and actionable\nsteps. Third, we conducted comprehensive quality assurance to ensure that each\ndataset in the benchmark had clear goals and included relevant and meaningful\nquestions and analysis. Furthermore, we implement a two-way evaluation\nmechanism using LLaMA-3 as an effective, open-source evaluator to assess\nagents' ability to extract insights. We also propose AgentPoirot, our baseline\ndata analysis agent capable of performing end-to-end data analytics. Our\nevaluation on InsightBench shows that AgentPoirot outperforms existing\napproaches (such as Pandas Agent) that focus on resolving single queries. We\nalso compare the performance of open- and closed-source LLMs and various\nevaluation strategies. Overall, this benchmark serves as a testbed to motivate\nfurther development in comprehensive automated data analytics and can be\naccessed here: https://github.com/ServiceNow/insight-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data analytics is essential for extracting valuable insights from data that\ncan assist organizations in making effective decisions. We introduce\nInsightBench, a benchmark dataset with three key features. First, it consists\nof 100 datasets representing diverse business use cases such as finance and\nincident management, each accompanied by a carefully curated set of insights\nplanted in the datasets. Second, unlike existing benchmarks focusing on\nanswering single queries, InsightBench evaluates agents based on their ability\nto perform end-to-end data analytics, including formulating questions,\ninterpreting answers, and generating a summary of insights and actionable\nsteps. Third, we conducted comprehensive quality assurance to ensure that each\ndataset in the benchmark had clear goals and included relevant and meaningful\nquestions and analysis. Furthermore, we implement a two-way evaluation\nmechanism using LLaMA-3 as an effective, open-source evaluator to assess\nagents' ability to extract insights. We also propose AgentPoirot, our baseline\ndata analysis agent capable of performing end-to-end data analytics. Our\nevaluation on InsightBench shows that AgentPoirot outperforms existing\napproaches (such as Pandas Agent) that focus on resolving single queries. We\nalso compare the performance of open- and closed-source LLMs and various\nevaluation strategies. Overall, this benchmark serves as a testbed to motivate\nfurther development in comprehensive automated data analytics and can be\naccessed here: https://github.com/ServiceNow/insight-bench."
                },
                "authors": [
                    {
                        "name": "Gaurav Sahu"
                    },
                    {
                        "name": "Abhay Puri"
                    },
                    {
                        "name": "Juan Rodriguez"
                    },
                    {
                        "name": "Amirhossein Abaskohi"
                    },
                    {
                        "name": "Mohammad Chegini"
                    },
                    {
                        "name": "Alexandre Drouin"
                    },
                    {
                        "name": "Perouz Taslakian"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "Alexandre Lacoste"
                    },
                    {
                        "name": "David Vazquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Sai Rajeswar Mudumba"
                    },
                    {
                        "name": "Issam Hadj Laradji"
                    }
                ],
                "author_detail": {
                    "name": "Issam Hadj Laradji"
                },
                "author": "Issam Hadj Laradji",
                "arxiv_comment": "Accepted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06423v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06423v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20285v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20285v1",
                "updated": "2025-02-27T17:10:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    10,
                    54,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:10:54Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    10,
                    54,
                    3,
                    58,
                    0
                ],
                "title": "Conformal Tail Risk Control for Large Language Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Tail Risk Control for Large Language Model Alignment"
                },
                "summary": "Recent developments in large language models (LLMs) have led to their\nwidespread usage for various tasks. The prevalence of LLMs in society implores\nthe assurance on the reliability of their performance. In particular,\nrisk-sensitive applications demand meticulous attention to unexpectedly poor\noutcomes, i.e., tail events, for instance, toxic answers, humiliating language,\nand offensive outputs. Due to the costly nature of acquiring human annotations,\ngeneral-purpose scoring models have been created to automate the process of\nquantifying these tail events. This phenomenon introduces potential\nhuman-machine misalignment between the respective scoring mechanisms. In this\nwork, we present a lightweight calibration framework for blackbox models that\nensures the alignment of humans and machines with provable guarantees. Our\nframework provides a rigorous approach to controlling any distortion risk\nmeasure that is characterized by a weighted average of quantiles of the loss\nincurred by the LLM with high confidence. The theoretical foundation of our\nmethod relies on the connection between conformal risk control and a\ntraditional family of statistics, i.e., L-statistics. To demonstrate the\nutility of our framework, we conduct comprehensive experiments that address the\nissue of human-machine misalignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in large language models (LLMs) have led to their\nwidespread usage for various tasks. The prevalence of LLMs in society implores\nthe assurance on the reliability of their performance. In particular,\nrisk-sensitive applications demand meticulous attention to unexpectedly poor\noutcomes, i.e., tail events, for instance, toxic answers, humiliating language,\nand offensive outputs. Due to the costly nature of acquiring human annotations,\ngeneral-purpose scoring models have been created to automate the process of\nquantifying these tail events. This phenomenon introduces potential\nhuman-machine misalignment between the respective scoring mechanisms. In this\nwork, we present a lightweight calibration framework for blackbox models that\nensures the alignment of humans and machines with provable guarantees. Our\nframework provides a rigorous approach to controlling any distortion risk\nmeasure that is characterized by a weighted average of quantiles of the loss\nincurred by the LLM with high confidence. The theoretical foundation of our\nmethod relies on the connection between conformal risk control and a\ntraditional family of statistics, i.e., L-statistics. To demonstrate the\nutility of our framework, we conduct comprehensive experiments that address the\nissue of human-machine misalignment."
                },
                "authors": [
                    {
                        "name": "Catherine Yu-Chi Chen"
                    },
                    {
                        "name": "Jingyan Shen"
                    },
                    {
                        "name": "Zhun Deng"
                    },
                    {
                        "name": "Lihua Lei"
                    }
                ],
                "author_detail": {
                    "name": "Lihua Lei"
                },
                "author": "Lihua Lei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20285v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20284v1",
                "updated": "2025-02-27T17:10:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    10,
                    52,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:10:52Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    10,
                    52,
                    3,
                    58,
                    0
                ],
                "title": "Evaluating Human Trust in LLM-Based Planners: A Preliminary Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Human Trust in LLM-Based Planners: A Preliminary Study"
                },
                "summary": "Large Language Models (LLMs) are increasingly used for planning tasks,\noffering unique capabilities not found in classical planners such as generating\nexplanations and iterative refinement. However, trust--a critical factor in the\nadoption of planning systems--remains underexplored in the context of LLM-based\nplanning tasks. This study bridges this gap by comparing human trust in\nLLM-based planners with classical planners through a user study in a Planning\nDomain Definition Language (PDDL) domain. Combining subjective measures, such\nas trust questionnaires, with objective metrics like evaluation accuracy, our\nfindings reveal that correctness is the primary driver of trust and\nperformance. Explanations provided by the LLM improved evaluation accuracy but\nhad limited impact on trust, while plan refinement showed potential for\nincreasing trust without significantly enhancing evaluation accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used for planning tasks,\noffering unique capabilities not found in classical planners such as generating\nexplanations and iterative refinement. However, trust--a critical factor in the\nadoption of planning systems--remains underexplored in the context of LLM-based\nplanning tasks. This study bridges this gap by comparing human trust in\nLLM-based planners with classical planners through a user study in a Planning\nDomain Definition Language (PDDL) domain. Combining subjective measures, such\nas trust questionnaires, with objective metrics like evaluation accuracy, our\nfindings reveal that correctness is the primary driver of trust and\nperformance. Explanations provided by the LLM improved evaluation accuracy but\nhad limited impact on trust, while plan refinement showed potential for\nincreasing trust without significantly enhancing evaluation accuracy."
                },
                "authors": [
                    {
                        "name": "Shenghui Chen"
                    },
                    {
                        "name": "Yunhao Yang"
                    },
                    {
                        "name": "Kayla Boggess"
                    },
                    {
                        "name": "Seongkook Heo"
                    },
                    {
                        "name": "Lu Feng"
                    },
                    {
                        "name": "Ufuk Topcu"
                    }
                ],
                "author_detail": {
                    "name": "Ufuk Topcu"
                },
                "author": "Ufuk Topcu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14739v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14739v2",
                "updated": "2025-02-27T17:01:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    1,
                    9,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-20T17:05:58Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    5,
                    58,
                    3,
                    51,
                    0
                ],
                "title": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable proficiency in\nmainstream academic disciplines such as mathematics, physics, and computer\nscience. However, human knowledge encompasses over 200 specialized disciplines,\nfar exceeding the scope of existing benchmarks. The capabilities of LLMs in\nmany of these specialized fields-particularly in light industry, agriculture,\nand service-oriented disciplines-remain inadequately evaluated. To address this\ngap, we present SuperGPQA, a comprehensive benchmark that evaluates\ngraduate-level knowledge and reasoning capabilities across 285 disciplines. Our\nbenchmark employs a novel Human-LLM collaborative filtering mechanism to\neliminate trivial or ambiguous questions through iterative refinement based on\nboth LLM responses and expert feedback. Our experimental results reveal\nsignificant room for improvement in the performance of current state-of-the-art\nLLMs across diverse knowledge domains (e.g., the reasoning-focused model\nDeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting\nthe considerable gap between current model capabilities and artificial general\nintelligence. Additionally, we present comprehensive insights from our\nmanagement of a large-scale annotation process, involving over 80 expert\nannotators and an interactive Human-LLM collaborative system, offering valuable\nmethodological guidance for future research initiatives of comparable scope.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable proficiency in\nmainstream academic disciplines such as mathematics, physics, and computer\nscience. However, human knowledge encompasses over 200 specialized disciplines,\nfar exceeding the scope of existing benchmarks. The capabilities of LLMs in\nmany of these specialized fields-particularly in light industry, agriculture,\nand service-oriented disciplines-remain inadequately evaluated. To address this\ngap, we present SuperGPQA, a comprehensive benchmark that evaluates\ngraduate-level knowledge and reasoning capabilities across 285 disciplines. Our\nbenchmark employs a novel Human-LLM collaborative filtering mechanism to\neliminate trivial or ambiguous questions through iterative refinement based on\nboth LLM responses and expert feedback. Our experimental results reveal\nsignificant room for improvement in the performance of current state-of-the-art\nLLMs across diverse knowledge domains (e.g., the reasoning-focused model\nDeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting\nthe considerable gap between current model capabilities and artificial general\nintelligence. Additionally, we present comprehensive insights from our\nmanagement of a large-scale annotation process, involving over 80 expert\nannotators and an interactive Human-LLM collaborative system, offering valuable\nmethodological guidance for future research initiatives of comparable scope."
                },
                "authors": [
                    {
                        "name": "M-A-P Team"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Yifan Yao"
                    },
                    {
                        "name": "Kaijing Ma"
                    },
                    {
                        "name": "Bingli Wang"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Kang Zhu"
                    },
                    {
                        "name": "Minghao Liu"
                    },
                    {
                        "name": "Yiming Liang"
                    },
                    {
                        "name": "Xiaolong Jin"
                    },
                    {
                        "name": "Zhenlin Wei"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Kaixin Deng"
                    },
                    {
                        "name": "Shian Jia"
                    },
                    {
                        "name": "Sichao Jiang"
                    },
                    {
                        "name": "Yiyan Liao"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Qinrui Li"
                    },
                    {
                        "name": "Sirun Li"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Yunwen Li"
                    },
                    {
                        "name": "Dehua Ma"
                    },
                    {
                        "name": "Yuansheng Ni"
                    },
                    {
                        "name": "Haoran Que"
                    },
                    {
                        "name": "Qiyao Wang"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Siwei Wu"
                    },
                    {
                        "name": "Tianshun Xing"
                    },
                    {
                        "name": "Ming Xu"
                    },
                    {
                        "name": "Zhenzhu Yang"
                    },
                    {
                        "name": "Zekun Moore Wang"
                    },
                    {
                        "name": "Junting Zhou"
                    },
                    {
                        "name": "Yuelin Bai"
                    },
                    {
                        "name": "Xingyuan Bu"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Yifan Chen"
                    },
                    {
                        "name": "Chengtuo Cheng"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Keyi Ding"
                    },
                    {
                        "name": "Siming Huang"
                    },
                    {
                        "name": "Yun Huang"
                    },
                    {
                        "name": "Yaoru Li"
                    },
                    {
                        "name": "Yizhe Li"
                    },
                    {
                        "name": "Zhaoqun Li"
                    },
                    {
                        "name": "Tianhao Liang"
                    },
                    {
                        "name": "Chengdong Lin"
                    },
                    {
                        "name": "Hongquan Lin"
                    },
                    {
                        "name": "Yinghao Ma"
                    },
                    {
                        "name": "Tianyang Pang"
                    },
                    {
                        "name": "Zhongyuan Peng"
                    },
                    {
                        "name": "Zifan Peng"
                    },
                    {
                        "name": "Qige Qi"
                    },
                    {
                        "name": "Shi Qiu"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Shanghaoran Quan"
                    },
                    {
                        "name": "Yizhou Tan"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Chenqing Wang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Yiya Wang"
                    },
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Jiajun Xu"
                    },
                    {
                        "name": "Kexin Yang"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Yuanhao Yue"
                    },
                    {
                        "name": "Tianyang Zhan"
                    },
                    {
                        "name": "Chun Zhang"
                    },
                    {
                        "name": "Jinyang Zhang"
                    },
                    {
                        "name": "Xiyue Zhang"
                    },
                    {
                        "name": "Xingjian Zhang"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yongchi Zhao"
                    },
                    {
                        "name": "Xiangyu Zheng"
                    },
                    {
                        "name": "Chenghua Zhong"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Zhoujun Li"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Shiwen Ni"
                    },
                    {
                        "name": "Junran Peng"
                    },
                    {
                        "name": "Yujia Qin"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Shi Wang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Qunshu Lin"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Ge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Zhang"
                },
                "author": "Ge Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14739v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14739v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20268v1",
                "updated": "2025-02-27T16:55:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    55,
                    18,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T16:55:18Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    55,
                    18,
                    3,
                    58,
                    0
                ],
                "title": "Large Language Models as Attribution Regularizers for Efficient Model\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Attribution Regularizers for Efficient Model\n  Training"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse domains. However, effectively leveraging their vast knowledge for\ntraining smaller downstream models remains an open challenge, especially in\ndomains like tabular data learning, where simpler models are often preferred\ndue to interpretability and efficiency.\n  In this paper, we introduce a novel yet straightforward method for\nincorporating LLM-generated global task feature attributions into the training\nprocess of smaller networks. Specifically, we propose an attribution-matching\nregularization term that aligns the training dynamics of the smaller model with\nthe insights provided by the LLM. By doing so, our approach yields superior\nperformance in few-shot learning scenarios. Notably, our method requires only\nblack-box API access to the LLM, making it easy to integrate into existing\ntraining pipelines with minimal computational overhead.\n  Furthermore, we demonstrate how this method can be used to address common\nissues in real-world datasets, such as skewness and bias. By integrating\nhigh-level knowledge from LLMs, our approach improves generalization, even when\ntraining data is limited or imbalanced. We validate its effectiveness through\nextensive experiments across multiple tasks, demonstrating improved learning\nefficiency and model robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse domains. However, effectively leveraging their vast knowledge for\ntraining smaller downstream models remains an open challenge, especially in\ndomains like tabular data learning, where simpler models are often preferred\ndue to interpretability and efficiency.\n  In this paper, we introduce a novel yet straightforward method for\nincorporating LLM-generated global task feature attributions into the training\nprocess of smaller networks. Specifically, we propose an attribution-matching\nregularization term that aligns the training dynamics of the smaller model with\nthe insights provided by the LLM. By doing so, our approach yields superior\nperformance in few-shot learning scenarios. Notably, our method requires only\nblack-box API access to the LLM, making it easy to integrate into existing\ntraining pipelines with minimal computational overhead.\n  Furthermore, we demonstrate how this method can be used to address common\nissues in real-world datasets, such as skewness and bias. By integrating\nhigh-level knowledge from LLMs, our approach improves generalization, even when\ntraining data is limited or imbalanced. We validate its effectiveness through\nextensive experiments across multiple tasks, demonstrating improved learning\nefficiency and model robustness."
                },
                "authors": [
                    {
                        "name": "Davor Vukadin"
                    },
                    {
                        "name": "Marin Šilić"
                    },
                    {
                        "name": "Goran Delač"
                    }
                ],
                "author_detail": {
                    "name": "Goran Delač"
                },
                "author": "Goran Delač",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02408v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02408v2",
                "updated": "2025-02-27T16:53:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    53,
                    40,
                    3,
                    58,
                    0
                ],
                "published": "2024-10-18T15:22:07Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    15,
                    22,
                    7,
                    4,
                    292,
                    0
                ],
                "title": "AI on My Shoulder: Supporting Emotional Labor in Front-Office Roles with\n  an LLM-based Empathetic Coworker",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI on My Shoulder: Supporting Emotional Labor in Front-Office Roles with\n  an LLM-based Empathetic Coworker"
                },
                "summary": "Client-Service Representatives (CSRs) are vital to organizations. Frequent\ninteractions with disgruntled clients, however, disrupt their mental\nwell-being. To help CSRs regulate their emotions while interacting with uncivil\nclients, we designed Care-Pilot, an LLM-powered assistant, and evaluated its\nefficacy, perception, and use. Our comparative analyses between 665 human and\nCare-Pilot-generated support messages highlight Care-Pilot's ability to adapt\nto and demonstrate empathy in various incivility incidents. Additionally, 143\nCSRs assessed Care-Pilot's empathy as more sincere and actionable than human\nmessages. Finally, we interviewed 20 CSRs who interacted with Care-Pilot in a\nsimulation exercise. They reported that Care-Pilot helped them avoid negative\nthinking, recenter thoughts, and humanize clients; showing potential for\nbridging gaps in coworker support. Yet, they also noted deployment challenges\nand emphasized the indispensability of shared experiences. We discuss future\ndesigns and societal implications of AI-mediated emotional labor, underscoring\nempathy as a critical function for AI assistants for worker mental health.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-Service Representatives (CSRs) are vital to organizations. Frequent\ninteractions with disgruntled clients, however, disrupt their mental\nwell-being. To help CSRs regulate their emotions while interacting with uncivil\nclients, we designed Care-Pilot, an LLM-powered assistant, and evaluated its\nefficacy, perception, and use. Our comparative analyses between 665 human and\nCare-Pilot-generated support messages highlight Care-Pilot's ability to adapt\nto and demonstrate empathy in various incivility incidents. Additionally, 143\nCSRs assessed Care-Pilot's empathy as more sincere and actionable than human\nmessages. Finally, we interviewed 20 CSRs who interacted with Care-Pilot in a\nsimulation exercise. They reported that Care-Pilot helped them avoid negative\nthinking, recenter thoughts, and humanize clients; showing potential for\nbridging gaps in coworker support. Yet, they also noted deployment challenges\nand emphasized the indispensability of shared experiences. We discuss future\ndesigns and societal implications of AI-mediated emotional labor, underscoring\nempathy as a critical function for AI assistants for worker mental health."
                },
                "authors": [
                    {
                        "name": "Vedant Das Swain"
                    },
                    {
                        "name": "Qiuyue \"Joy\" Zhong"
                    },
                    {
                        "name": "Jash Rajesh Parekh"
                    },
                    {
                        "name": "Yechan Jeon"
                    },
                    {
                        "name": "Roy Zimmermann"
                    },
                    {
                        "name": "Mary Czerwinski"
                    },
                    {
                        "name": "Jina Suh"
                    },
                    {
                        "name": "Varun Mishra"
                    },
                    {
                        "name": "Koustuv Saha"
                    },
                    {
                        "name": "Javier Hernandez"
                    }
                ],
                "author_detail": {
                    "name": "Javier Hernandez"
                },
                "author": "Javier Hernandez",
                "arxiv_doi": "10.1145/3706598.3713705",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713705",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.02408v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02408v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "CHI Conference on Human Factors in Computing Systems (CHI '25),\n  April 26-May 1, 2025, Yokohama, Japan",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15291v2",
                "updated": "2025-02-27T16:47:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    47,
                    40,
                    3,
                    58,
                    0
                ],
                "published": "2024-12-19T07:10:51Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    7,
                    10,
                    51,
                    3,
                    354,
                    0
                ],
                "title": "A Large-Scale Simulation on Large Language Models for Decision-Making in\n  Political Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Large-Scale Simulation on Large Language Models for Decision-Making in\n  Political Science"
                },
                "summary": "While LLMs have demonstrated remarkable capabilities in text generation and\nreasoning, their ability to simulate human decision-making -- particularly in\npolitical contexts -- remains an open question. However, modeling voter\nbehavior presents unique challenges due to limited voter-level data, evolving\npolitical landscapes, and the complexity of human reasoning. In this study, we\ndevelop a theory-driven, multi-step reasoning framework that integrates\ndemographic, temporal and ideological factors to simulate voter decision-making\nat scale. Using synthetic personas calibrated to real-world voter data, we\nconduct large-scale simulations of recent U.S. presidential elections. Our\nmethod significantly improves simulation accuracy while mitigating model\nbiases. We examine its robustness by comparing performance across different\nLLMs. We further investigate the challenges and constraints that arise from\nLLM-based political simulations. Our work provides both a scalable framework\nfor modeling political decision-making behavior and insights into the promise\nand limitations of using LLMs in political science research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLMs have demonstrated remarkable capabilities in text generation and\nreasoning, their ability to simulate human decision-making -- particularly in\npolitical contexts -- remains an open question. However, modeling voter\nbehavior presents unique challenges due to limited voter-level data, evolving\npolitical landscapes, and the complexity of human reasoning. In this study, we\ndevelop a theory-driven, multi-step reasoning framework that integrates\ndemographic, temporal and ideological factors to simulate voter decision-making\nat scale. Using synthetic personas calibrated to real-world voter data, we\nconduct large-scale simulations of recent U.S. presidential elections. Our\nmethod significantly improves simulation accuracy while mitigating model\nbiases. We examine its robustness by comparing performance across different\nLLMs. We further investigate the challenges and constraints that arise from\nLLM-based political simulations. Our work provides both a scalable framework\nfor modeling political decision-making behavior and insights into the promise\nand limitations of using LLMs in political science research."
                },
                "authors": [
                    {
                        "name": "Chenxiao Yu"
                    },
                    {
                        "name": "Jinyi Ye"
                    },
                    {
                        "name": "Yuangang Li"
                    },
                    {
                        "name": "Zhaotian Weng"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Emilio Ferrara"
                    },
                    {
                        "name": "Xiyang Hu"
                    },
                    {
                        "name": "Yue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhao"
                },
                "author": "Yue Zhao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2411.03321 This\n  version adds a new model to our experimental setup, modifies the paper's main\n  discussion, and updates the authorship list",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20258v1",
                "updated": "2025-02-27T16:46:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    46,
                    23,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T16:46:23Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    46,
                    23,
                    3,
                    58,
                    0
                ],
                "title": "LLM as a Broken Telephone: Iterative Generation Distorts Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM as a Broken Telephone: Iterative Generation Distorts Information"
                },
                "summary": "As large language models are increasingly responsible for online content,\nconcerns arise about the impact of repeatedly processing their own outputs.\nInspired by the \"broken telephone\" effect in chained human communication, this\nstudy investigates whether LLMs similarly distort information through iterative\ngeneration. Through translation-based experiments, we find that distortion\naccumulates over time, influenced by language choice and chain complexity.\nWhile degradation is inevitable, it can be mitigated through strategic\nprompting techniques. These findings contribute to discussions on the long-term\neffects of AI-mediated information propagation, raising important questions\nabout the reliability of LLM-generated content in iterative workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models are increasingly responsible for online content,\nconcerns arise about the impact of repeatedly processing their own outputs.\nInspired by the \"broken telephone\" effect in chained human communication, this\nstudy investigates whether LLMs similarly distort information through iterative\ngeneration. Through translation-based experiments, we find that distortion\naccumulates over time, influenced by language choice and chain complexity.\nWhile degradation is inevitable, it can be mitigated through strategic\nprompting techniques. These findings contribute to discussions on the long-term\neffects of AI-mediated information propagation, raising important questions\nabout the reliability of LLM-generated content in iterative workflows."
                },
                "authors": [
                    {
                        "name": "Amr Mohamed"
                    },
                    {
                        "name": "Mingmeng Geng"
                    },
                    {
                        "name": "Michalis Vazirgiannis"
                    },
                    {
                        "name": "Guokan Shang"
                    }
                ],
                "author_detail": {
                    "name": "Guokan Shang"
                },
                "author": "Guokan Shang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17787v2",
                "updated": "2025-02-27T16:42:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    42,
                    10,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-25T02:39:57Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    2,
                    39,
                    57,
                    1,
                    56,
                    0
                ],
                "title": "AIR: Complex Instruction Generation via Automatic Iterative Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIR: Complex Instruction Generation via Automatic Iterative Refinement"
                },
                "summary": "With the development of large language models, their ability to follow simple\ninstructions has significantly improved. However, adhering to complex\ninstructions remains a major challenge. Current approaches to generating\ncomplex instructions are often irrelevant to the current instruction\nrequirements or suffer from limited scalability and diversity. Moreover,\nmethods such as back-translation, while effective for simple instruction\ngeneration, fail to leverage the rich contents and structures in large web\ncorpora. In this paper, we propose a novel automatic iterative refinement\nframework to generate complex instructions with constraints, which not only\nbetter reflects the requirements of real scenarios but also significantly\nenhances LLMs' ability to follow complex instructions. The AIR framework\nconsists of two stages: (1)Generate an initial instruction from a document;\n(2)Iteratively refine instructions with LLM-as-judge guidance by comparing the\nmodel's output with the document to incorporate valuable constraints. Finally,\nwe construct the AIR-10K dataset with 10K complex instructions and demonstrate\nthat instructions generated with our approach significantly improve the model's\nability to follow complex instructions, outperforming existing methods for\ninstruction generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models, their ability to follow simple\ninstructions has significantly improved. However, adhering to complex\ninstructions remains a major challenge. Current approaches to generating\ncomplex instructions are often irrelevant to the current instruction\nrequirements or suffer from limited scalability and diversity. Moreover,\nmethods such as back-translation, while effective for simple instruction\ngeneration, fail to leverage the rich contents and structures in large web\ncorpora. In this paper, we propose a novel automatic iterative refinement\nframework to generate complex instructions with constraints, which not only\nbetter reflects the requirements of real scenarios but also significantly\nenhances LLMs' ability to follow complex instructions. The AIR framework\nconsists of two stages: (1)Generate an initial instruction from a document;\n(2)Iteratively refine instructions with LLM-as-judge guidance by comparing the\nmodel's output with the document to incorporate valuable constraints. Finally,\nwe construct the AIR-10K dataset with 10K complex instructions and demonstrate\nthat instructions generated with our approach significantly improve the model's\nability to follow complex instructions, outperforming existing methods for\ninstruction generation."
                },
                "authors": [
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Hui Huang"
                    },
                    {
                        "name": "Chengwei Hu"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Shilong Li"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "The first three authors contributed equally, 20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.01314v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.01314v3",
                "updated": "2025-02-27T16:36:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    36,
                    8,
                    3,
                    58,
                    0
                ],
                "published": "2023-11-02T15:31:12Z",
                "published_parsed": [
                    2023,
                    11,
                    2,
                    15,
                    31,
                    12,
                    3,
                    306,
                    0
                ],
                "title": "Recommendations by Concise User Profiles from Review Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommendations by Concise User Profiles from Review Text"
                },
                "summary": "Recommender systems perform well for popular items and users with ample\ninteractions (likes, ratings etc.). This work addresses the difficult and\nunderexplored case of users who have very sparse interactions but post\ninformative review texts. This setting naturally calls for encoding\nuser-specific text with large language models (LLM). However, feeding the full\ntext of all reviews through an LLM has a weak signal-to-noise ratio and incurs\nhigh costs of processed tokens. This paper addresses these two issues. It\npresents a light-weight framework, called CUP, which first computes concise\nuser profiles and feeds only these into the training of transformer-based\nrecommenders. For user profiles, we devise various techniques to select the\nmost informative cues from noisy reviews. Experiments, with book reviews data,\nshow that fine-tuning a small language model with judiciously constructed\nprofiles achieves the best performance, even in comparison to LLM-generated\nrankings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems perform well for popular items and users with ample\ninteractions (likes, ratings etc.). This work addresses the difficult and\nunderexplored case of users who have very sparse interactions but post\ninformative review texts. This setting naturally calls for encoding\nuser-specific text with large language models (LLM). However, feeding the full\ntext of all reviews through an LLM has a weak signal-to-noise ratio and incurs\nhigh costs of processed tokens. This paper addresses these two issues. It\npresents a light-weight framework, called CUP, which first computes concise\nuser profiles and feeds only these into the training of transformer-based\nrecommenders. For user profiles, we devise various techniques to select the\nmost informative cues from noisy reviews. Experiments, with book reviews data,\nshow that fine-tuning a small language model with judiciously constructed\nprofiles achieves the best performance, even in comparison to LLM-generated\nrankings."
                },
                "authors": [
                    {
                        "name": "Ghazaleh Haratinezhad Torbati"
                    },
                    {
                        "name": "Anna Tigunova"
                    },
                    {
                        "name": "Andrew Yates"
                    },
                    {
                        "name": "Gerhard Weikum"
                    }
                ],
                "author_detail": {
                    "name": "Gerhard Weikum"
                },
                "author": "Gerhard Weikum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.01314v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.01314v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19361v2",
                "updated": "2025-02-27T16:34:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    34,
                    4,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-26T17:59:27Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    59,
                    27,
                    2,
                    57,
                    0
                ],
                "title": "Can Large Language Models Detect Errors in Long Chain-of-Thought\n  Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Detect Errors in Long Chain-of-Thought\n  Reasoning?"
                },
                "summary": "Recently, o1-like models have drawn significant attention, where these models\nproduce the long Chain-of-Thought (CoT) reasoning steps to improve the\nreasoning abilities of existing Large Language Models (LLMs). In this paper, to\nunderstand the qualities of these long CoTs and measure the critique abilities\nof existing LLMs on these long CoTs, we introduce the DeltaBench, including the\ngenerated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for\ndifferent reasoning tasks (e.g., Math, Code, General Reasoning), to measure the\nability to detect errors in long CoT reasoning. Based on DeltaBench, we first\nperform fine-grained analysis of the generated long CoTs to discover the\neffectiveness and efficiency of different o1-like models. Then, we conduct\nextensive evaluations of existing process reward models (PRMs) and critic\nmodels to detect the errors of each annotated process, which aims to\ninvestigate the boundaries and limitations of existing PRMs and critic models.\nFinally, we hope that DeltaBench could guide developers to better understand\nthe long CoT reasoning abilities of their models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, o1-like models have drawn significant attention, where these models\nproduce the long Chain-of-Thought (CoT) reasoning steps to improve the\nreasoning abilities of existing Large Language Models (LLMs). In this paper, to\nunderstand the qualities of these long CoTs and measure the critique abilities\nof existing LLMs on these long CoTs, we introduce the DeltaBench, including the\ngenerated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for\ndifferent reasoning tasks (e.g., Math, Code, General Reasoning), to measure the\nability to detect errors in long CoT reasoning. Based on DeltaBench, we first\nperform fine-grained analysis of the generated long CoTs to discover the\neffectiveness and efficiency of different o1-like models. Then, we conduct\nextensive evaluations of existing process reward models (PRMs) and critic\nmodels to detect the errors of each annotated process, which aims to\ninvestigate the boundaries and limitations of existing PRMs and critic models.\nFinally, we hope that DeltaBench could guide developers to better understand\nthe long CoT reasoning abilities of their models."
                },
                "authors": [
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Shilong Li"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Weixun Wang"
                    },
                    {
                        "name": "Xingyuan Bu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Zhongyuan Peng"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Zhicheng Zheng"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "The first four authors contributed equally, 27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13156v2",
                "updated": "2025-02-27T16:30:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    30,
                    42,
                    3,
                    58,
                    0
                ],
                "published": "2024-09-20T01:46:07Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    1,
                    46,
                    7,
                    4,
                    264,
                    0
                ],
                "title": "RRM: Robust Reward Model Training Mitigates Reward Hacking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RRM: Robust Reward Model Training Mitigates Reward Hacking"
                },
                "summary": "Reward models (RMs) play a pivotal role in aligning large language models\n(LLMs) with human preferences. However, traditional RM training, which relies\non response pairs tied to specific prompts, struggles to disentangle\nprompt-driven preferences from prompt-independent artifacts, such as response\nlength and format. In this work, we expose a fundamental limitation of current\nRM training methods, where RMs fail to effectively distinguish between\ncontextual signals and irrelevant artifacts when determining preferences. To\naddress this, we introduce a causal framework that learns preferences\nindependent of these artifacts and propose a novel data augmentation technique\ndesigned to eliminate them. Extensive experiments show that our approach\nsuccessfully filters out undesirable artifacts, yielding a more robust reward\nmodel (RRM). Our RRM improves the performance of a pairwise reward model\ntrained on Gemma-2-9b-it, on RewardBench, increasing accuracy from 80.61% to\n84.15%. Additionally, we train two DPO policies using both the RM and RRM,\ndemonstrating that the RRM significantly enhances DPO-aligned policies,\nimproving MT-Bench scores from 7.27 to 8.31 and length-controlled win-rates in\nAlpacaEval-2 from 33.46% to 52.49%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models (RMs) play a pivotal role in aligning large language models\n(LLMs) with human preferences. However, traditional RM training, which relies\non response pairs tied to specific prompts, struggles to disentangle\nprompt-driven preferences from prompt-independent artifacts, such as response\nlength and format. In this work, we expose a fundamental limitation of current\nRM training methods, where RMs fail to effectively distinguish between\ncontextual signals and irrelevant artifacts when determining preferences. To\naddress this, we introduce a causal framework that learns preferences\nindependent of these artifacts and propose a novel data augmentation technique\ndesigned to eliminate them. Extensive experiments show that our approach\nsuccessfully filters out undesirable artifacts, yielding a more robust reward\nmodel (RRM). Our RRM improves the performance of a pairwise reward model\ntrained on Gemma-2-9b-it, on RewardBench, increasing accuracy from 80.61% to\n84.15%. Additionally, we train two DPO policies using both the RM and RRM,\ndemonstrating that the RRM significantly enhances DPO-aligned policies,\nimproving MT-Bench scores from 7.27 to 8.31 and length-controlled win-rates in\nAlpacaEval-2 from 33.46% to 52.49%."
                },
                "authors": [
                    {
                        "name": "Tianqi Liu"
                    },
                    {
                        "name": "Wei Xiong"
                    },
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Lichang Chen"
                    },
                    {
                        "name": "Junru Wu"
                    },
                    {
                        "name": "Rishabh Joshi"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Tianhe Yu"
                    },
                    {
                        "name": "Daniel Sohn"
                    },
                    {
                        "name": "Anastasiia Makarova"
                    },
                    {
                        "name": "Jeremiah Liu"
                    },
                    {
                        "name": "Yuan Liu"
                    },
                    {
                        "name": "Bilal Piot"
                    },
                    {
                        "name": "Abe Ittycheriah"
                    },
                    {
                        "name": "Aviral Kumar"
                    },
                    {
                        "name": "Mohammad Saleh"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Saleh"
                },
                "author": "Mohammad Saleh",
                "arxiv_comment": "Accepted in ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20246v1",
                "updated": "2025-02-27T16:30:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    30,
                    0,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T16:30:00Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    30,
                    0,
                    3,
                    58,
                    0
                ],
                "title": "Beyond Natural Language Perplexity: Detecting Dead Code Poisoning in\n  Code Generation Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Natural Language Perplexity: Detecting Dead Code Poisoning in\n  Code Generation Datasets"
                },
                "summary": "The increasing adoption of large language models (LLMs) for code-related\ntasks has raised concerns about the security of their training datasets. One\ncritical threat is dead code poisoning, where syntactically valid but\nfunctionally redundant code is injected into training data to manipulate model\nbehavior. Such attacks can degrade the performance of neural code search\nsystems, leading to biased or insecure code suggestions. Existing detection\nmethods, such as token-level perplexity analysis, fail to effectively identify\ndead code due to the structural and contextual characteristics of programming\nlanguages. In this paper, we propose DePA (Dead Code Perplexity Analysis), a\nnovel line-level detection and cleansing method tailored to the structural\nproperties of code. DePA computes line-level perplexity by leveraging the\ncontextual relationships between code lines and identifies anomalous lines by\ncomparing their perplexity to the overall distribution within the file. Our\nexperiments on benchmark datasets demonstrate that DePA significantly\noutperforms existing methods, achieving 0.14-0.19 improvement in detection\nF1-score and a 44-65% increase in poisoned segment localization precision.\nFurthermore, DePA enhances detection speed by 0.62-23x, making it practical for\nlarge-scale dataset cleansing. Overall, by addressing the unique challenges of\ndead code poisoning, DePA provides a robust and efficient solution for\nsafeguarding the integrity of code generation model training datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of large language models (LLMs) for code-related\ntasks has raised concerns about the security of their training datasets. One\ncritical threat is dead code poisoning, where syntactically valid but\nfunctionally redundant code is injected into training data to manipulate model\nbehavior. Such attacks can degrade the performance of neural code search\nsystems, leading to biased or insecure code suggestions. Existing detection\nmethods, such as token-level perplexity analysis, fail to effectively identify\ndead code due to the structural and contextual characteristics of programming\nlanguages. In this paper, we propose DePA (Dead Code Perplexity Analysis), a\nnovel line-level detection and cleansing method tailored to the structural\nproperties of code. DePA computes line-level perplexity by leveraging the\ncontextual relationships between code lines and identifies anomalous lines by\ncomparing their perplexity to the overall distribution within the file. Our\nexperiments on benchmark datasets demonstrate that DePA significantly\noutperforms existing methods, achieving 0.14-0.19 improvement in detection\nF1-score and a 44-65% increase in poisoned segment localization precision.\nFurthermore, DePA enhances detection speed by 0.62-23x, making it practical for\nlarge-scale dataset cleansing. Overall, by addressing the unique challenges of\ndead code poisoning, DePA provides a robust and efficient solution for\nsafeguarding the integrity of code generation model training datasets."
                },
                "authors": [
                    {
                        "name": "Chichien Tsai"
                    },
                    {
                        "name": "Chiamu Yu"
                    },
                    {
                        "name": "Yingdar Lin"
                    },
                    {
                        "name": "Yusung Wu"
                    },
                    {
                        "name": "Weibin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Weibin Lee"
                },
                "author": "Weibin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20242v1",
                "updated": "2025-02-27T16:27:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    27,
                    42,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T16:27:42Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    27,
                    42,
                    3,
                    58,
                    0
                ],
                "title": "GreenDFL: a Framework for Assessing the Sustainability of Decentralized\n  Federated Learning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GreenDFL: a Framework for Assessing the Sustainability of Decentralized\n  Federated Learning Systems"
                },
                "summary": "Decentralized Federated Learning (DFL) is an emerging paradigm that enables\ncollaborative model training without centralized data aggregation, enhancing\nprivacy and resilience. However, its sustainability remains underexplored, as\nenergy consumption and carbon emissions vary across different system\nconfigurations. Understanding the environmental impact of DFL is crucial for\noptimizing its design and deployment. This study aims to assess the\nsustainability of DFL systems by analyzing factors that influence energy\nconsumption and carbon emissions. Additionally, it proposes\nsustainability-aware optimization strategies, including a node selection\nalgorithm and an aggregation method, to reduce the environmental footprint of\nDFL without compromising model performance. The proposed framework, named\nGreenDFL, systematically evaluates the impact of hardware accelerators, model\narchitecture, communication medium, data distribution, network topology, and\nfederation size on sustainability. Empirical experiments are conducted on\nmultiple datasets using different system configurations, measuring energy\nconsumption and carbon emissions across various phases of the DFL lifecycle.\nResults indicate that local training dominates energy consumption and carbon\nemissions, while communication has a relatively minor impact. Optimizing model\ncomplexity, using GPUs instead of CPUs, and strategically selecting\nparticipating nodes significantly improve sustainability. Additionally,\ndeploying nodes in regions with lower carbon intensity and integrating early\nstopping mechanisms further reduce emissions. The proposed framework provides a\ncomprehensive and practical computational approach for assessing the\nsustainability of DFL systems. Furthermore, it offers best practices for\nimproving environmental efficiency in DFL, making sustainability considerations\nmore actionable in real-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning (DFL) is an emerging paradigm that enables\ncollaborative model training without centralized data aggregation, enhancing\nprivacy and resilience. However, its sustainability remains underexplored, as\nenergy consumption and carbon emissions vary across different system\nconfigurations. Understanding the environmental impact of DFL is crucial for\noptimizing its design and deployment. This study aims to assess the\nsustainability of DFL systems by analyzing factors that influence energy\nconsumption and carbon emissions. Additionally, it proposes\nsustainability-aware optimization strategies, including a node selection\nalgorithm and an aggregation method, to reduce the environmental footprint of\nDFL without compromising model performance. The proposed framework, named\nGreenDFL, systematically evaluates the impact of hardware accelerators, model\narchitecture, communication medium, data distribution, network topology, and\nfederation size on sustainability. Empirical experiments are conducted on\nmultiple datasets using different system configurations, measuring energy\nconsumption and carbon emissions across various phases of the DFL lifecycle.\nResults indicate that local training dominates energy consumption and carbon\nemissions, while communication has a relatively minor impact. Optimizing model\ncomplexity, using GPUs instead of CPUs, and strategically selecting\nparticipating nodes significantly improve sustainability. Additionally,\ndeploying nodes in regions with lower carbon intensity and integrating early\nstopping mechanisms further reduce emissions. The proposed framework provides a\ncomprehensive and practical computational approach for assessing the\nsustainability of DFL systems. Furthermore, it offers best practices for\nimproving environmental efficiency in DFL, making sustainability considerations\nmore actionable in real-world deployments."
                },
                "authors": [
                    {
                        "name": "Chao Feng"
                    },
                    {
                        "name": "Alberto Huertas Celdrán"
                    },
                    {
                        "name": "Xi Cheng"
                    },
                    {
                        "name": "Gérôme Bovet"
                    },
                    {
                        "name": "Burkhard Stiller"
                    }
                ],
                "author_detail": {
                    "name": "Burkhard Stiller"
                },
                "author": "Burkhard Stiller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20238v1",
                "updated": "2025-02-27T16:23:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    23,
                    25,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T16:23:25Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    23,
                    25,
                    3,
                    58,
                    0
                ],
                "title": "FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through\n  Reflective Puzzle Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through\n  Reflective Puzzle Solving"
                },
                "summary": "Many challenging reasoning tasks require not just rapid, intuitive responses,\nbut a more deliberate, multi-step approach. Recent progress in large language\nmodels (LLMs) highlights an important shift from the \"System 1\" way of quick\nreactions to the \"System 2\" style of reflection-and-correction problem solving.\nHowever, current benchmarks heavily rely on the final-answer accuracy, leaving\nmuch of a model's intermediate reasoning steps unexamined. This fails to assess\nthe model's ability to reflect and rectify mistakes within the reasoning\nprocess. To bridge this gap, we introduce FINEREASON, a logic-puzzle benchmark\nfor fine-grained evaluation of LLMs' reasoning capabilities. Each puzzle can be\ndecomposed into atomic steps, making it ideal for rigorous validation of\nintermediate correctness. Building on this, we introduce two tasks: state\nchecking, and state transition, for a comprehensive evaluation of how models\nassess the current situation and plan the next move. To support broader\nresearch, we also provide a puzzle training set aimed at enhancing performance\non general mathematical tasks. We show that models trained on our state\nchecking and transition data demonstrate gains in math reasoning by up to 5.1%\non GSM8K.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many challenging reasoning tasks require not just rapid, intuitive responses,\nbut a more deliberate, multi-step approach. Recent progress in large language\nmodels (LLMs) highlights an important shift from the \"System 1\" way of quick\nreactions to the \"System 2\" style of reflection-and-correction problem solving.\nHowever, current benchmarks heavily rely on the final-answer accuracy, leaving\nmuch of a model's intermediate reasoning steps unexamined. This fails to assess\nthe model's ability to reflect and rectify mistakes within the reasoning\nprocess. To bridge this gap, we introduce FINEREASON, a logic-puzzle benchmark\nfor fine-grained evaluation of LLMs' reasoning capabilities. Each puzzle can be\ndecomposed into atomic steps, making it ideal for rigorous validation of\nintermediate correctness. Building on this, we introduce two tasks: state\nchecking, and state transition, for a comprehensive evaluation of how models\nassess the current situation and plan the next move. To support broader\nresearch, we also provide a puzzle training set aimed at enhancing performance\non general mathematical tasks. We show that models trained on our state\nchecking and transition data demonstrate gains in math reasoning by up to 5.1%\non GSM8K."
                },
                "authors": [
                    {
                        "name": "Guizhen Chen"
                    },
                    {
                        "name": "Weiwen Xu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Hou Pong Chan"
                    },
                    {
                        "name": "Chaoqun Liu"
                    },
                    {
                        "name": "Lidong Bing"
                    },
                    {
                        "name": "Deli Zhao"
                    },
                    {
                        "name": "Anh Tuan Luu"
                    },
                    {
                        "name": "Yu Rong"
                    }
                ],
                "author_detail": {
                    "name": "Yu Rong"
                },
                "author": "Yu Rong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20231v1",
                "updated": "2025-02-27T16:16:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    16,
                    37,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T16:16:37Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    16,
                    37,
                    3,
                    58,
                    0
                ],
                "title": "AI Will Always Love You: Studying Implicit Biases in Romantic AI\n  Companions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Will Always Love You: Studying Implicit Biases in Romantic AI\n  Companions"
                },
                "summary": "While existing studies have recognised explicit biases in generative models,\nincluding occupational gender biases, the nuances of gender stereotypes and\nexpectations of relationships between users and AI companions remain\nunderexplored. In the meantime, AI companions have become increasingly popular\nas friends or gendered romantic partners to their users. This study bridges the\ngap by devising three experiments tailored for romantic, gender-assigned AI\ncompanions and their users, effectively evaluating implicit biases across\nvarious-sized LLMs. Each experiment looks at a different dimension: implicit\nassociations, emotion responses, and sycophancy. This study aims to measure and\ncompare biases manifested in different companion systems by quantitatively\nanalysing persona-assigned model responses to a baseline through newly devised\nmetrics. The results are noteworthy: they show that assigning gendered,\nrelationship personas to Large Language Models significantly alters the\nresponses of these models, and in certain situations in a biased, stereotypical\nway.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While existing studies have recognised explicit biases in generative models,\nincluding occupational gender biases, the nuances of gender stereotypes and\nexpectations of relationships between users and AI companions remain\nunderexplored. In the meantime, AI companions have become increasingly popular\nas friends or gendered romantic partners to their users. This study bridges the\ngap by devising three experiments tailored for romantic, gender-assigned AI\ncompanions and their users, effectively evaluating implicit biases across\nvarious-sized LLMs. Each experiment looks at a different dimension: implicit\nassociations, emotion responses, and sycophancy. This study aims to measure and\ncompare biases manifested in different companion systems by quantitatively\nanalysing persona-assigned model responses to a baseline through newly devised\nmetrics. The results are noteworthy: they show that assigning gendered,\nrelationship personas to Large Language Models significantly alters the\nresponses of these models, and in certain situations in a biased, stereotypical\nway."
                },
                "authors": [
                    {
                        "name": "Clare Grogan"
                    },
                    {
                        "name": "Jackie Kay"
                    },
                    {
                        "name": "María Pérez-Ortiz"
                    }
                ],
                "author_detail": {
                    "name": "María Pérez-Ortiz"
                },
                "author": "María Pérez-Ortiz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04332v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04332v3",
                "updated": "2025-02-27T16:08:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    8,
                    51,
                    3,
                    58,
                    0
                ],
                "published": "2024-12-05T16:48:16Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    48,
                    16,
                    3,
                    340,
                    0
                ],
                "title": "Liquid: Language Models are Scalable and Unified Multi-modal Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Liquid: Language Models are Scalable and Unified Multi-modal Generators"
                },
                "summary": "We present Liquid, an auto-regressive generation paradigm that seamlessly\nintegrates visual comprehension and generation by tokenizing images into\ndiscrete codes and learning these code embeddings alongside text tokens within\na shared feature space for both vision and language. Unlike previous multimodal\nlarge language model (MLLM), Liquid achieves this integration using a single\nlarge language model (LLM), eliminating the need for external pretrained visual\nembeddings such as CLIP. For the first time, Liquid uncovers a scaling law that\nperformance drop unavoidably brought by the unified training of visual and\nlanguage tasks diminishes as the model size increases. Furthermore, the unified\ntoken space enables visual generation and comprehension tasks to mutually\nenhance each other, effectively removing the typical interference seen in\nearlier models. We show that existing LLMs can serve as strong foundations for\nLiquid, saving 100x in training costs while outperforming Chameleon in\nmultimodal capabilities and maintaining language performance comparable to\nmainstream LLMs like LLAMA2. Liquid also outperforms models like SD v2.1 and\nSD-XL (FID of 5.47 on MJHQ-30K), excelling in both vision-language and\ntext-only tasks. This work demonstrates that LLMs such as Qwen2.5 and GEMMA2\nare powerful multimodal generators, offering a scalable solution for enhancing\nboth vision-language understanding and generation. The code and models will be\nreleased at https://github.com/FoundationVision/Liquid.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Liquid, an auto-regressive generation paradigm that seamlessly\nintegrates visual comprehension and generation by tokenizing images into\ndiscrete codes and learning these code embeddings alongside text tokens within\na shared feature space for both vision and language. Unlike previous multimodal\nlarge language model (MLLM), Liquid achieves this integration using a single\nlarge language model (LLM), eliminating the need for external pretrained visual\nembeddings such as CLIP. For the first time, Liquid uncovers a scaling law that\nperformance drop unavoidably brought by the unified training of visual and\nlanguage tasks diminishes as the model size increases. Furthermore, the unified\ntoken space enables visual generation and comprehension tasks to mutually\nenhance each other, effectively removing the typical interference seen in\nearlier models. We show that existing LLMs can serve as strong foundations for\nLiquid, saving 100x in training costs while outperforming Chameleon in\nmultimodal capabilities and maintaining language performance comparable to\nmainstream LLMs like LLAMA2. Liquid also outperforms models like SD v2.1 and\nSD-XL (FID of 5.47 on MJHQ-30K), excelling in both vision-language and\ntext-only tasks. This work demonstrates that LLMs such as Qwen2.5 and GEMMA2\nare powerful multimodal generators, offering a scalable solution for enhancing\nboth vision-language understanding and generation. The code and models will be\nreleased at https://github.com/FoundationVision/Liquid."
                },
                "authors": [
                    {
                        "name": "Junfeng Wu"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Chuofan Ma"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Zehuan Yuan"
                    },
                    {
                        "name": "Song Bai"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "arxiv_comment": "Technical report. Project page:\n  https://foundationvision.github.io/Liquid/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04332v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04332v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17732v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17732v2",
                "updated": "2025-02-27T16:05:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    5,
                    40,
                    3,
                    58,
                    0
                ],
                "published": "2024-11-22T11:18:26Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    11,
                    18,
                    26,
                    4,
                    327,
                    0
                ],
                "title": "CheckMate: LLM-Powered Approximate Intermittent Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CheckMate: LLM-Powered Approximate Intermittent Computing"
                },
                "summary": "Batteryless IoT systems face energy constraints exacerbated by checkpointing\noverhead. Approximate computing offers solutions but demands manual expertise,\nlimiting scalability. This paper presents CheckMate, an automated framework\nleveraging LLMs for context-aware code approximations. CheckMate integrates\nvalidation of LLM-generated approximations to ensure correct execution and\nemploys Bayesian optimization to fine-tune approximation parameters\nautonomously, eliminating the need for developer input. Tested across six IoT\napplications, it reduces power cycles by up to 60% with an accuracy loss of\njust 8%, outperforming semi-automated tools like ACCEPT in speedup and\naccuracy. CheckMate's results establish it as a robust, user-friendly tool and\na foundational step toward automated approximation frameworks for intermittent\ncomputing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batteryless IoT systems face energy constraints exacerbated by checkpointing\noverhead. Approximate computing offers solutions but demands manual expertise,\nlimiting scalability. This paper presents CheckMate, an automated framework\nleveraging LLMs for context-aware code approximations. CheckMate integrates\nvalidation of LLM-generated approximations to ensure correct execution and\nemploys Bayesian optimization to fine-tune approximation parameters\nautonomously, eliminating the need for developer input. Tested across six IoT\napplications, it reduces power cycles by up to 60% with an accuracy loss of\njust 8%, outperforming semi-automated tools like ACCEPT in speedup and\naccuracy. CheckMate's results establish it as a robust, user-friendly tool and\na foundational step toward automated approximation frameworks for intermittent\ncomputing."
                },
                "authors": [
                    {
                        "name": "Abdur-Rahman Ibrahim Sayyid-Ali"
                    },
                    {
                        "name": "Abdul Rafay"
                    },
                    {
                        "name": "Muhammad Abdullah Soomro"
                    },
                    {
                        "name": "Muhammad Hamad Alizai"
                    },
                    {
                        "name": "Naveed Anwar Bhatti"
                    }
                ],
                "author_detail": {
                    "name": "Naveed Anwar Bhatti"
                },
                "author": "Naveed Anwar Bhatti",
                "arxiv_comment": "Accepted in SenSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17732v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17732v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20217v1",
                "updated": "2025-02-27T15:58:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    58,
                    42,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T15:58:42Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    58,
                    42,
                    3,
                    58,
                    0
                ],
                "title": "MARVEL: Multi-Agent Reinforcement Learning for constrained field-of-View\n  multi-robot Exploration in Large-scale environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARVEL: Multi-Agent Reinforcement Learning for constrained field-of-View\n  multi-robot Exploration in Large-scale environments"
                },
                "summary": "In multi-robot exploration, a team of mobile robot is tasked with efficiently\nmapping an unknown environments. While most exploration planners assume\nomnidirectional sensors like LiDAR, this is impractical for small robots such\nas drones, where lightweight, directional sensors like cameras may be the only\noption due to payload constraints. These sensors have a constrained\nfield-of-view (FoV), which adds complexity to the exploration problem,\nrequiring not only optimal robot positioning but also sensor orientation during\nmovement. In this work, we propose MARVEL, a neural framework that leverages\ngraph attention networks, together with novel frontiers and orientation\nfeatures fusion technique, to develop a collaborative, decentralized policy\nusing multi-agent reinforcement learning (MARL) for robots with constrained\nFoV. To handle the large action space of viewpoints planning, we further\nintroduce a novel information-driven action pruning strategy. MARVEL improves\nmulti-robot coordination and decision-making in challenging large-scale indoor\nenvironments, while adapting to various team sizes and sensor configurations\n(i.e., FoV and sensor range) without additional training. Our extensive\nevaluation shows that MARVEL's learned policies exhibit effective coordinated\nbehaviors, outperforming state-of-the-art exploration planners across multiple\nmetrics. We experimentally demonstrate MARVEL's generalizability in large-scale\nenvironments, of up to 90m by 90m, and validate its practical applicability\nthrough successful deployment on a team of real drone hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-robot exploration, a team of mobile robot is tasked with efficiently\nmapping an unknown environments. While most exploration planners assume\nomnidirectional sensors like LiDAR, this is impractical for small robots such\nas drones, where lightweight, directional sensors like cameras may be the only\noption due to payload constraints. These sensors have a constrained\nfield-of-view (FoV), which adds complexity to the exploration problem,\nrequiring not only optimal robot positioning but also sensor orientation during\nmovement. In this work, we propose MARVEL, a neural framework that leverages\ngraph attention networks, together with novel frontiers and orientation\nfeatures fusion technique, to develop a collaborative, decentralized policy\nusing multi-agent reinforcement learning (MARL) for robots with constrained\nFoV. To handle the large action space of viewpoints planning, we further\nintroduce a novel information-driven action pruning strategy. MARVEL improves\nmulti-robot coordination and decision-making in challenging large-scale indoor\nenvironments, while adapting to various team sizes and sensor configurations\n(i.e., FoV and sensor range) without additional training. Our extensive\nevaluation shows that MARVEL's learned policies exhibit effective coordinated\nbehaviors, outperforming state-of-the-art exploration planners across multiple\nmetrics. We experimentally demonstrate MARVEL's generalizability in large-scale\nenvironments, of up to 90m by 90m, and validate its practical applicability\nthrough successful deployment on a team of real drone hardware."
                },
                "authors": [
                    {
                        "name": "Jimmy Chiun"
                    },
                    {
                        "name": "Shizhe Zhang"
                    },
                    {
                        "name": "Yizhuo Wang"
                    },
                    {
                        "name": "Yuhong Cao"
                    },
                    {
                        "name": "Guillaume Sartoretti"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Sartoretti"
                },
                "author": "Guillaume Sartoretti",
                "arxiv_comment": "\\c{opyright} 20XX IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20196v1",
                "updated": "2025-02-27T15:36:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    36,
                    0,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T15:36:00Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    36,
                    0,
                    3,
                    58,
                    0
                ],
                "title": "ChineseEcomQA: A Scalable E-commerce Concept Evaluation Benchmark for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChineseEcomQA: A Scalable E-commerce Concept Evaluation Benchmark for\n  Large Language Models"
                },
                "summary": "With the increasing use of Large Language Models (LLMs) in fields such as\ne-commerce, domain-specific concept evaluation benchmarks are crucial for\nassessing their domain capabilities. Existing LLMs may generate factually\nincorrect information within the complex e-commerce applications. Therefore, it\nis necessary to build an e-commerce concept benchmark. Existing benchmarks\nencounter two primary challenges: (1) handle the heterogeneous and diverse\nnature of tasks, (2) distinguish between generality and specificity within the\ne-commerce field. To address these problems, we propose \\textbf{ChineseEcomQA},\na scalable question-answering benchmark focused on fundamental e-commerce\nconcepts. ChineseEcomQA is built on three core characteristics: \\textbf{Focus\non Fundamental Concept}, \\textbf{E-commerce Generality} and \\textbf{E-commerce\nExpertise}. Fundamental concepts are designed to be applicable across a diverse\narray of e-commerce tasks, thus addressing the challenge of heterogeneity and\ndiversity. Additionally, by carefully balancing generality and specificity,\nChineseEcomQA effectively differentiates between broad e-commerce concepts,\nallowing for precise validation of domain capabilities. We achieve this through\na scalable benchmark construction process that combines LLM validation,\nRetrieval-Augmented Generation (RAG) validation, and rigorous manual\nannotation. Based on ChineseEcomQA, we conduct extensive evaluations on\nmainstream LLMs and provide some valuable insights. We hope that ChineseEcomQA\ncould guide future domain-specific evaluations, and facilitate broader LLM\nadoption in e-commerce applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing use of Large Language Models (LLMs) in fields such as\ne-commerce, domain-specific concept evaluation benchmarks are crucial for\nassessing their domain capabilities. Existing LLMs may generate factually\nincorrect information within the complex e-commerce applications. Therefore, it\nis necessary to build an e-commerce concept benchmark. Existing benchmarks\nencounter two primary challenges: (1) handle the heterogeneous and diverse\nnature of tasks, (2) distinguish between generality and specificity within the\ne-commerce field. To address these problems, we propose \\textbf{ChineseEcomQA},\na scalable question-answering benchmark focused on fundamental e-commerce\nconcepts. ChineseEcomQA is built on three core characteristics: \\textbf{Focus\non Fundamental Concept}, \\textbf{E-commerce Generality} and \\textbf{E-commerce\nExpertise}. Fundamental concepts are designed to be applicable across a diverse\narray of e-commerce tasks, thus addressing the challenge of heterogeneity and\ndiversity. Additionally, by carefully balancing generality and specificity,\nChineseEcomQA effectively differentiates between broad e-commerce concepts,\nallowing for precise validation of domain capabilities. We achieve this through\na scalable benchmark construction process that combines LLM validation,\nRetrieval-Augmented Generation (RAG) validation, and rigorous manual\nannotation. Based on ChineseEcomQA, we conduct extensive evaluations on\nmainstream LLMs and provide some valuable insights. We hope that ChineseEcomQA\ncould guide future domain-specific evaluations, and facilitate broader LLM\nadoption in e-commerce applications."
                },
                "authors": [
                    {
                        "name": "Haibin Chen"
                    },
                    {
                        "name": "Kangtao Lv"
                    },
                    {
                        "name": "Chengwei Hu"
                    },
                    {
                        "name": "Yanshi Li"
                    },
                    {
                        "name": "Yujin Yuan"
                    },
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Xingyao Zhang"
                    },
                    {
                        "name": "Langming Liu"
                    },
                    {
                        "name": "Shilei Liu"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v2",
                "updated": "2025-02-27T15:29:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    29,
                    3,
                    3,
                    58,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20186v1",
                "updated": "2025-02-27T15:22:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    22,
                    14,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T15:22:14Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    22,
                    14,
                    3,
                    58,
                    0
                ],
                "title": "Layer-Aware Task Arithmetic: Disentangling Task-Specific and\n  Instruction-Following Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layer-Aware Task Arithmetic: Disentangling Task-Specific and\n  Instruction-Following Knowledge"
                },
                "summary": "Large language models (LLMs) demonstrate strong task-specific capabilities\nthrough fine-tuning, but merging multiple fine-tuned models often leads to\ndegraded performance due to overlapping instruction-following components. Task\nArithmetic (TA), which combines task vectors derived from fine-tuning, enables\nmulti-task learning and task forgetting but struggles to isolate task-specific\nknowledge from general instruction-following behavior. To address this, we\npropose Layer-Aware Task Arithmetic (LATA), a novel approach that assigns\nlayer-specific weights to task vectors based on their alignment with\ninstruction-following or task-specific components. By amplifying task-relevant\nlayers and attenuating instruction-following layers, LATA improves task\nlearning and forgetting performance while preserving overall model utility.\nExperiments on multiple benchmarks, including WikiText-2, GSM8K, and HumanEval,\ndemonstrate that LATA outperforms existing methods in both multi-task learning\nand selective task forgetting, achieving higher task accuracy and alignment\nwith minimal degradation in output quality. Our findings highlight the\nimportance of layer-wise analysis in disentangling task-specific and\ngeneral-purpose knowledge, offering a robust framework for efficient model\nmerging and editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate strong task-specific capabilities\nthrough fine-tuning, but merging multiple fine-tuned models often leads to\ndegraded performance due to overlapping instruction-following components. Task\nArithmetic (TA), which combines task vectors derived from fine-tuning, enables\nmulti-task learning and task forgetting but struggles to isolate task-specific\nknowledge from general instruction-following behavior. To address this, we\npropose Layer-Aware Task Arithmetic (LATA), a novel approach that assigns\nlayer-specific weights to task vectors based on their alignment with\ninstruction-following or task-specific components. By amplifying task-relevant\nlayers and attenuating instruction-following layers, LATA improves task\nlearning and forgetting performance while preserving overall model utility.\nExperiments on multiple benchmarks, including WikiText-2, GSM8K, and HumanEval,\ndemonstrate that LATA outperforms existing methods in both multi-task learning\nand selective task forgetting, achieving higher task accuracy and alignment\nwith minimal degradation in output quality. Our findings highlight the\nimportance of layer-wise analysis in disentangling task-specific and\ngeneral-purpose knowledge, offering a robust framework for efficient model\nmerging and editing."
                },
                "authors": [
                    {
                        "name": "Yan-Lun Chen"
                    },
                    {
                        "name": "Yi-Ru Wei"
                    },
                    {
                        "name": "Chia-Yi Hsu"
                    },
                    {
                        "name": "Chia-Mu Yu"
                    },
                    {
                        "name": "Chun-Ying Huang"
                    },
                    {
                        "name": "Ying-Dar Lin"
                    },
                    {
                        "name": "Yu-Sung Wu"
                    },
                    {
                        "name": "Wei-Bin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Wei-Bin Lee"
                },
                "author": "Wei-Bin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20175v1",
                "updated": "2025-02-27T15:13:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    13,
                    7,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T15:13:07Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    13,
                    7,
                    3,
                    58,
                    0
                ],
                "title": "An Extensive Evaluation of PDDL Capabilities in off-the-shelf LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Extensive Evaluation of PDDL Capabilities in off-the-shelf LLMs"
                },
                "summary": "In recent advancements, large language models (LLMs) have exhibited\nproficiency in code generation and chain-of-thought reasoning, laying the\ngroundwork for tackling automatic formal planning tasks. This study evaluates\nthe potential of LLMs to understand and generate Planning Domain Definition\nLanguage (PDDL), an essential representation in artificial intelligence\nplanning. We conduct an extensive analysis across 20 distinct models spanning 7\nmajor LLM families, both commercial and open-source. Our comprehensive\nevaluation sheds light on the zero-shot LLM capabilities of parsing,\ngenerating, and reasoning with PDDL. Our findings indicate that while some\nmodels demonstrate notable effectiveness in handling PDDL, others pose\nlimitations in more complex scenarios requiring nuanced planning knowledge.\nThese results highlight the promise and current limitations of LLMs in formal\nplanning tasks, offering insights into their application and guiding future\nefforts in AI-driven planning paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent advancements, large language models (LLMs) have exhibited\nproficiency in code generation and chain-of-thought reasoning, laying the\ngroundwork for tackling automatic formal planning tasks. This study evaluates\nthe potential of LLMs to understand and generate Planning Domain Definition\nLanguage (PDDL), an essential representation in artificial intelligence\nplanning. We conduct an extensive analysis across 20 distinct models spanning 7\nmajor LLM families, both commercial and open-source. Our comprehensive\nevaluation sheds light on the zero-shot LLM capabilities of parsing,\ngenerating, and reasoning with PDDL. Our findings indicate that while some\nmodels demonstrate notable effectiveness in handling PDDL, others pose\nlimitations in more complex scenarios requiring nuanced planning knowledge.\nThese results highlight the promise and current limitations of LLMs in formal\nplanning tasks, offering insights into their application and guiding future\nefforts in AI-driven planning paradigms."
                },
                "authors": [
                    {
                        "name": "Kaustubh Vyas"
                    },
                    {
                        "name": "Damien Graux"
                    },
                    {
                        "name": "Sébastien Montella"
                    },
                    {
                        "name": "Pavlos Vougiouklis"
                    },
                    {
                        "name": "Ruofei Lai"
                    },
                    {
                        "name": "Keshuang Li"
                    },
                    {
                        "name": "Yang Ren"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03471v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03471v4",
                "updated": "2025-02-27T15:11:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    11,
                    54,
                    3,
                    58,
                    0
                ],
                "published": "2024-04-04T14:24:06Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    14,
                    24,
                    6,
                    3,
                    95,
                    0
                ],
                "title": "The Impact of Unstated Norms in Bias Analysis of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Unstated Norms in Bias Analysis of Language Models"
                },
                "summary": "Bias in large language models (LLMs) has many forms, from overt\ndiscrimination to implicit stereotypes. Counterfactual bias evaluation is a\nwidely used approach to quantifying bias and often relies on template-based\nprobes that explicitly state group membership. It measures whether the outcome\nof a task performed by an LLM is invariant to a change in group membership. In\nthis work, we find that template-based probes can lead to unrealistic bias\nmeasurements. For example, LLMs appear to mistakenly cast text associated with\nWhite race as negative at higher rates than other groups. We hypothesize that\nthis arises artificially via a mismatch between commonly unstated norms, in the\nform of markedness, in the pretraining text of LLMs (e.g., Black president vs.\npresident) and templates used for bias measurement (e.g., Black president vs.\nWhite president). The findings highlight the potential misleading impact of\nvarying group membership through explicit mention in counterfactual bias\nquantification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias in large language models (LLMs) has many forms, from overt\ndiscrimination to implicit stereotypes. Counterfactual bias evaluation is a\nwidely used approach to quantifying bias and often relies on template-based\nprobes that explicitly state group membership. It measures whether the outcome\nof a task performed by an LLM is invariant to a change in group membership. In\nthis work, we find that template-based probes can lead to unrealistic bias\nmeasurements. For example, LLMs appear to mistakenly cast text associated with\nWhite race as negative at higher rates than other groups. We hypothesize that\nthis arises artificially via a mismatch between commonly unstated norms, in the\nform of markedness, in the pretraining text of LLMs (e.g., Black president vs.\npresident) and templates used for bias measurement (e.g., Black president vs.\nWhite president). The findings highlight the potential misleading impact of\nvarying group membership through explicit mention in counterfactual bias\nquantification."
                },
                "authors": [
                    {
                        "name": "Farnaz Kohankhaki"
                    },
                    {
                        "name": "D. B. Emerson"
                    },
                    {
                        "name": "Jacob-Junqi Tian"
                    },
                    {
                        "name": "Laleh Seyyed-Kalantari"
                    },
                    {
                        "name": "Faiza Khan Khattak"
                    }
                ],
                "author_detail": {
                    "name": "Faiza Khan Khattak"
                },
                "author": "Faiza Khan Khattak",
                "arxiv_comment": "15 Pages, 4 Figures, 4 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03471v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03471v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00468v2",
                "updated": "2025-02-27T15:10:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    10,
                    56,
                    3,
                    58,
                    0
                ],
                "published": "2024-06-29T15:28:45Z",
                "published_parsed": [
                    2024,
                    6,
                    29,
                    15,
                    28,
                    45,
                    5,
                    181,
                    0
                ],
                "title": "MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and\n  Efficient Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and\n  Efficient Evaluation"
                },
                "summary": "Large Multimodal Models (LMMs) exhibit impressive cross-modal understanding\nand reasoning abilities, often assessed through multiple-choice questions\n(MCQs) that include an image, a question, and several options. However, many\nbenchmarks used for such evaluations suffer from systematic biases. Remarkably,\nLarge Language Models (LLMs) without any visual perception capabilities achieve\nnon-trivial performance, undermining the credibility of these evaluations. To\naddress this issue while maintaining the efficiency of MCQ evaluations, we\npropose MMEvalPro, a benchmark designed to avoid Type-I errors through a\ntrilogy evaluation pipeline and more rigorous metrics. For each original\nquestion from existing benchmarks, human annotators augment it by creating one\nperception question and one knowledge anchor question through a meticulous\nannotation process. MMEvalPro comprises $2,138$ question triplets, totaling\n$6,414$ distinct questions. Two-thirds of these questions are manually labeled\nby human experts, while the rest are sourced from existing benchmarks (MMMU,\nScienceQA, and MathVista). Compared with the existing benchmarks, our\nexperiments with the latest LLMs and LMMs demonstrate that MMEvalPro is more\nchallenging (the best LMM lags behind human performance by $31.73\\%$, compared\nto an average gap of $8.03\\%$ in previous benchmarks) and more trustworthy (the\nbest LLM trails the best LMM by $23.09\\%$, whereas the gap for previous\nbenchmarks is just $14.64\\%$). Our in-depth analysis explains the reason for\nthe large performance gap and justifies the trustworthiness of evaluation,\nunderscoring its significant potential for advancing future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) exhibit impressive cross-modal understanding\nand reasoning abilities, often assessed through multiple-choice questions\n(MCQs) that include an image, a question, and several options. However, many\nbenchmarks used for such evaluations suffer from systematic biases. Remarkably,\nLarge Language Models (LLMs) without any visual perception capabilities achieve\nnon-trivial performance, undermining the credibility of these evaluations. To\naddress this issue while maintaining the efficiency of MCQ evaluations, we\npropose MMEvalPro, a benchmark designed to avoid Type-I errors through a\ntrilogy evaluation pipeline and more rigorous metrics. For each original\nquestion from existing benchmarks, human annotators augment it by creating one\nperception question and one knowledge anchor question through a meticulous\nannotation process. MMEvalPro comprises $2,138$ question triplets, totaling\n$6,414$ distinct questions. Two-thirds of these questions are manually labeled\nby human experts, while the rest are sourced from existing benchmarks (MMMU,\nScienceQA, and MathVista). Compared with the existing benchmarks, our\nexperiments with the latest LLMs and LMMs demonstrate that MMEvalPro is more\nchallenging (the best LMM lags behind human performance by $31.73\\%$, compared\nto an average gap of $8.03\\%$ in previous benchmarks) and more trustworthy (the\nbest LLM trails the best LMM by $23.09\\%$, whereas the gap for previous\nbenchmarks is just $14.64\\%$). Our in-depth analysis explains the reason for\nthe large performance gap and justifies the trustworthiness of evaluation,\nunderscoring its significant potential for advancing future research."
                },
                "authors": [
                    {
                        "name": "Jinsheng Huang"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Taian Guo"
                    },
                    {
                        "name": "Fu Zeng"
                    },
                    {
                        "name": "Yusheng Zhao"
                    },
                    {
                        "name": "Bohan Wu"
                    },
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Haozhe Zhao"
                    },
                    {
                        "name": "Zhihui Guo"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Jingyang Yuan"
                    },
                    {
                        "name": "Wei Ju"
                    },
                    {
                        "name": "Luchen Liu"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Baobao Chang"
                    },
                    {
                        "name": "Ming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhang"
                },
                "author": "Ming Zhang",
                "arxiv_comment": "18 pages, code released at https://github.com/chenllliang/MMEvalPro,\n  Homepage at https://mmevalpro.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20170v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20170v1",
                "updated": "2025-02-27T15:07:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    7,
                    47,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T15:07:47Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    7,
                    47,
                    3,
                    58,
                    0
                ],
                "title": "Re-evaluating Open-ended Evaluation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-evaluating Open-ended Evaluation of Large Language Models"
                },
                "summary": "Evaluation has traditionally focused on ranking candidates for a specific\nskill. Modern generalist models, such as Large Language Models (LLMs),\ndecidedly outpace this paradigm. Open-ended evaluation systems, where candidate\nmodels are compared on user-submitted prompts, have emerged as a popular\nsolution. Despite their many advantages, we show that the current Elo-based\nrating systems can be susceptible to and even reinforce biases in data,\nintentional or accidental, due to their sensitivity to redundancies. To address\nthis issue, we propose evaluation as a 3-player game, and introduce novel\ngame-theoretic solution concepts to ensure robustness to redundancy. We show\nthat our method leads to intuitive ratings and provide insights into the\ncompetitive landscape of LLM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation has traditionally focused on ranking candidates for a specific\nskill. Modern generalist models, such as Large Language Models (LLMs),\ndecidedly outpace this paradigm. Open-ended evaluation systems, where candidate\nmodels are compared on user-submitted prompts, have emerged as a popular\nsolution. Despite their many advantages, we show that the current Elo-based\nrating systems can be susceptible to and even reinforce biases in data,\nintentional or accidental, due to their sensitivity to redundancies. To address\nthis issue, we propose evaluation as a 3-player game, and introduce novel\ngame-theoretic solution concepts to ensure robustness to redundancy. We show\nthat our method leads to intuitive ratings and provide insights into the\ncompetitive landscape of LLM development."
                },
                "authors": [
                    {
                        "name": "Siqi Liu"
                    },
                    {
                        "name": "Ian Gemp"
                    },
                    {
                        "name": "Luke Marris"
                    },
                    {
                        "name": "Georgios Piliouras"
                    },
                    {
                        "name": "Nicolas Heess"
                    },
                    {
                        "name": "Marc Lanctot"
                    }
                ],
                "author_detail": {
                    "name": "Marc Lanctot"
                },
                "author": "Marc Lanctot",
                "arxiv_comment": "Published at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20170v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20170v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20167v1",
                "updated": "2025-02-27T15:05:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    5,
                    0,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T15:05:00Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    5,
                    0,
                    3,
                    58,
                    0
                ],
                "title": "Similarity-Distance-Magnitude Universal Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity-Distance-Magnitude Universal Verification"
                },
                "summary": "We solve the neural network robustness problem by adding Similarity (i.e.,\ncorrectly predicted depth-matches into training)-awareness and\nDistance-to-training-distribution-awareness to the existing output Magnitude\n(i.e., decision-boundary)-awareness of the softmax function. The resulting sdm\nactivation function provides strong signals of the relative epistemic\n(reducible) predictive uncertainty. We use this novel behavior to further\naddress the complementary HCI problem of mapping the output to\nhuman-interpretable summary statistics over relevant partitions of a held-out\ncalibration set. Estimates of prediction-conditional uncertainty are obtained\nvia a parsimonious learned transform over the class-conditional empirical CDFs\nof the output of a final-layer sdm activation function. For decision-making and\nas an intrinsic model check, estimates of class-conditional accuracy are\nobtained by further partitioning the high-probability regions of this\ncalibrated output into class-conditional, region-specific CDFs. The uncertainty\nestimates from sdm calibration are remarkably robust to test-time distribution\nshifts and out-of-distribution inputs; incorporate awareness of the effective\nsample size; provide estimates of uncertainty from the learning and data\nsplitting processes; and are well-suited for selective classification and\nconditional branching for additional test-time compute based on the predictive\nuncertainty, as for selective LLM generation, routing, and composition over\nmultiple models and retrieval. Finally, we construct sdm networks, LLMs with\nuncertainty-aware verification and interpretability-by-exemplar as intrinsic\nproperties. We provide open-source software implementing these results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We solve the neural network robustness problem by adding Similarity (i.e.,\ncorrectly predicted depth-matches into training)-awareness and\nDistance-to-training-distribution-awareness to the existing output Magnitude\n(i.e., decision-boundary)-awareness of the softmax function. The resulting sdm\nactivation function provides strong signals of the relative epistemic\n(reducible) predictive uncertainty. We use this novel behavior to further\naddress the complementary HCI problem of mapping the output to\nhuman-interpretable summary statistics over relevant partitions of a held-out\ncalibration set. Estimates of prediction-conditional uncertainty are obtained\nvia a parsimonious learned transform over the class-conditional empirical CDFs\nof the output of a final-layer sdm activation function. For decision-making and\nas an intrinsic model check, estimates of class-conditional accuracy are\nobtained by further partitioning the high-probability regions of this\ncalibrated output into class-conditional, region-specific CDFs. The uncertainty\nestimates from sdm calibration are remarkably robust to test-time distribution\nshifts and out-of-distribution inputs; incorporate awareness of the effective\nsample size; provide estimates of uncertainty from the learning and data\nsplitting processes; and are well-suited for selective classification and\nconditional branching for additional test-time compute based on the predictive\nuncertainty, as for selective LLM generation, routing, and composition over\nmultiple models and retrieval. Finally, we construct sdm networks, LLMs with\nuncertainty-aware verification and interpretability-by-exemplar as intrinsic\nproperties. We provide open-source software implementing these results."
                },
                "authors": [
                    {
                        "name": "Allen Schmaltz"
                    }
                ],
                "author_detail": {
                    "name": "Allen Schmaltz"
                },
                "author": "Allen Schmaltz",
                "arxiv_comment": "35 pages (8 Tables, 4 Algorithms, 5 Listings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16860v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16860v2",
                "updated": "2025-02-27T14:50:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    50,
                    10,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-24T05:51:53Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    5,
                    51,
                    53,
                    0,
                    55,
                    0
                ],
                "title": "LongAttn: Selecting Long-context Training Data via Token-level Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongAttn: Selecting Long-context Training Data via Token-level Attention"
                },
                "summary": "With the development of large language models (LLMs), there has been an\nincreasing need for significant advancements in handling long contexts. To\nenhance long-context capabilities, constructing high-quality training data with\nlong-range dependencies is crucial. Existing methods to select long-context\ndata often rely on sentence-level analysis, which can be greatly optimized in\nboth performance and efficiency. In this paper, we propose a novel token-level\nframework, LongAttn, which leverages the self-attention mechanism of LLMs to\nmeasure the long-range dependencies for the data. By calculating token-level\ndependency strength and distribution uniformity of token scores, LongAttn\neffectively quantifies long-range dependencies, enabling more accurate and\nefficient data selection. We filter LongABC-32K from open-source long-context\ndatasets (ArXiv, Book, and Code). Through our comprehensive experiments,\nLongAttn has demonstrated its excellent effectiveness, scalability, and\nefficiency. To facilitate future research in long-context data, we released our\ncode and the high-quality long-context training data LongABC-32K.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), there has been an\nincreasing need for significant advancements in handling long contexts. To\nenhance long-context capabilities, constructing high-quality training data with\nlong-range dependencies is crucial. Existing methods to select long-context\ndata often rely on sentence-level analysis, which can be greatly optimized in\nboth performance and efficiency. In this paper, we propose a novel token-level\nframework, LongAttn, which leverages the self-attention mechanism of LLMs to\nmeasure the long-range dependencies for the data. By calculating token-level\ndependency strength and distribution uniformity of token scores, LongAttn\neffectively quantifies long-range dependencies, enabling more accurate and\nefficient data selection. We filter LongABC-32K from open-source long-context\ndatasets (ArXiv, Book, and Code). Through our comprehensive experiments,\nLongAttn has demonstrated its excellent effectiveness, scalability, and\nefficiency. To facilitate future research in long-context data, we released our\ncode and the high-quality long-context training data LongABC-32K."
                },
                "authors": [
                    {
                        "name": "Longyun Wu"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Guangxiang Zhao"
                    },
                    {
                        "name": "Zhuocheng Yu"
                    },
                    {
                        "name": "Junfeng Ran"
                    },
                    {
                        "name": "Xiangyu Wong"
                    },
                    {
                        "name": "Lin Sun"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "17 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16860v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16860v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20147v1",
                "updated": "2025-02-27T14:38:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    38,
                    17,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T14:38:17Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    38,
                    17,
                    3,
                    58,
                    0
                ],
                "title": "Geometry and Mechanics of Non-Euclidean Curved-Crease Origami",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometry and Mechanics of Non-Euclidean Curved-Crease Origami"
                },
                "summary": "Recently there have been extensive theoretical, numerical and experimental\nworks on curved-fold origami. However, we notice that a unified and complete\ngeometric framework for describing the geometry and mechanics of curved-fold\norigami, especially those with nontrivial Gaussian curvature at the crease\n(non-Euclidean crease), is still absent. Herein we provide a unified geometric\nframework that describes the shape of a generic curved-fold origami composed of\ntwo general strips. The explicit description indicates that four configurations\nemerge, determined by its spatial crease and configuration branch. Within this\ngeometric framework, we derive the equilibrium equations and study the\nmechanical response of the curved-crease origami, focusing on Euler's buckling\nbehavior. Both linear stability analysis and finite element simulation indicate\nthat the overlaid configuration exhibits a lower buckling threshold. To further\ncapture the large deformation behavior efficiently, we develop a bistrip model\nbased on the anisotropic Kirchhoff rod theory, which predicts the main features\nsuccessfully. This work bridges the geometry and mechanics of curved-crease\norigami, offering insights for applications in robotics, actuators, and\ndeployable space structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently there have been extensive theoretical, numerical and experimental\nworks on curved-fold origami. However, we notice that a unified and complete\ngeometric framework for describing the geometry and mechanics of curved-fold\norigami, especially those with nontrivial Gaussian curvature at the crease\n(non-Euclidean crease), is still absent. Herein we provide a unified geometric\nframework that describes the shape of a generic curved-fold origami composed of\ntwo general strips. The explicit description indicates that four configurations\nemerge, determined by its spatial crease and configuration branch. Within this\ngeometric framework, we derive the equilibrium equations and study the\nmechanical response of the curved-crease origami, focusing on Euler's buckling\nbehavior. Both linear stability analysis and finite element simulation indicate\nthat the overlaid configuration exhibits a lower buckling threshold. To further\ncapture the large deformation behavior efficiently, we develop a bistrip model\nbased on the anisotropic Kirchhoff rod theory, which predicts the main features\nsuccessfully. This work bridges the geometry and mechanics of curved-crease\norigami, offering insights for applications in robotics, actuators, and\ndeployable space structures."
                },
                "authors": [
                    {
                        "name": "Zhixuan Wen"
                    },
                    {
                        "name": "Tian Yu"
                    },
                    {
                        "name": "Fan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fan Feng"
                },
                "author": "Fan Feng",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20144v1",
                "updated": "2025-02-27T14:35:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    35,
                    47,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T14:35:47Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    35,
                    47,
                    3,
                    58,
                    0
                ],
                "title": "Robust sensitivity control in digital pathology via tile score\n  distribution matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust sensitivity control in digital pathology via tile score\n  distribution matching"
                },
                "summary": "Deploying digital pathology models across medical centers is challenging due\nto distribution shifts. Recent advances in domain generalization improve model\ntransferability in terms of aggregated performance measured by the Area Under\nCurve (AUC). However, clinical regulations often require to control the\ntransferability of other metrics, such as prescribed sensitivity levels. We\nintroduce a novel approach to control the sensitivity of whole slide image\n(WSI) classification models, based on optimal transport and Multiple Instance\nLearning (MIL). Validated across multiple cohorts and tasks, our method enables\nrobust sensitivity control with only a handful of calibration samples,\nproviding a practical solution for reliable deployment of computational\npathology systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying digital pathology models across medical centers is challenging due\nto distribution shifts. Recent advances in domain generalization improve model\ntransferability in terms of aggregated performance measured by the Area Under\nCurve (AUC). However, clinical regulations often require to control the\ntransferability of other metrics, such as prescribed sensitivity levels. We\nintroduce a novel approach to control the sensitivity of whole slide image\n(WSI) classification models, based on optimal transport and Multiple Instance\nLearning (MIL). Validated across multiple cohorts and tasks, our method enables\nrobust sensitivity control with only a handful of calibration samples,\nproviding a practical solution for reliable deployment of computational\npathology systems."
                },
                "authors": [
                    {
                        "name": "Arthur Pignet"
                    },
                    {
                        "name": "John Klein"
                    },
                    {
                        "name": "Genevieve Robin"
                    },
                    {
                        "name": "Antoine Olivier"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Olivier"
                },
                "author": "Antoine Olivier",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20140v1",
                "updated": "2025-02-27T14:31:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    31,
                    42,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T14:31:42Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    31,
                    42,
                    3,
                    58,
                    0
                ],
                "title": "Telephone Surveys Meet Conversational AI: Evaluating a LLM-Based\n  Telephone Survey System at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Telephone Surveys Meet Conversational AI: Evaluating a LLM-Based\n  Telephone Survey System at Scale"
                },
                "summary": "Telephone surveys remain a valuable tool for gathering insights but typically\nrequire substantial resources in training and coordinating human interviewers.\nThis work presents an AI-driven telephone survey system integrating\ntext-to-speech (TTS), a large language model (LLM), and speech-to-text (STT)\nthat mimics the versatility of human-led interviews on scale.\n  We tested the system across two populations, a pilot study in the United\nStates (n = 75) and a large-scale deployment in Peru (n = 2,739), inviting\nparticipants via web-based links and contacting them via direct phone calls.\nThe AI agent successfully administered open-ended and closed-ended questions,\nhandled basic clarifications, and dynamically navigated branching logic,\nallowing fast large-scale survey deployment without interviewer recruitment or\ntraining.\n  Our findings demonstrate that while the AI system's probing for qualitative\ndepth was more limited than human interviewers, overall data quality approached\nhuman-led standards for structured items. This study represents one of the\nfirst successful large-scale deployments of an LLM-based telephone interviewer\nin a real-world survey context. The AI-powered telephone survey system has the\npotential for expanding scalable, consistent data collecting across market\nresearch, social science, and public opinion studies, thus improving\noperational efficiency while maintaining appropriate data quality for research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Telephone surveys remain a valuable tool for gathering insights but typically\nrequire substantial resources in training and coordinating human interviewers.\nThis work presents an AI-driven telephone survey system integrating\ntext-to-speech (TTS), a large language model (LLM), and speech-to-text (STT)\nthat mimics the versatility of human-led interviews on scale.\n  We tested the system across two populations, a pilot study in the United\nStates (n = 75) and a large-scale deployment in Peru (n = 2,739), inviting\nparticipants via web-based links and contacting them via direct phone calls.\nThe AI agent successfully administered open-ended and closed-ended questions,\nhandled basic clarifications, and dynamically navigated branching logic,\nallowing fast large-scale survey deployment without interviewer recruitment or\ntraining.\n  Our findings demonstrate that while the AI system's probing for qualitative\ndepth was more limited than human interviewers, overall data quality approached\nhuman-led standards for structured items. This study represents one of the\nfirst successful large-scale deployments of an LLM-based telephone interviewer\nin a real-world survey context. The AI-powered telephone survey system has the\npotential for expanding scalable, consistent data collecting across market\nresearch, social science, and public opinion studies, thus improving\noperational efficiency while maintaining appropriate data quality for research."
                },
                "authors": [
                    {
                        "name": "Max M. Lang"
                    },
                    {
                        "name": "Sol Eskenazi"
                    }
                ],
                "author_detail": {
                    "name": "Sol Eskenazi"
                },
                "author": "Sol Eskenazi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13474v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13474v2",
                "updated": "2025-02-27T14:29:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    29,
                    8,
                    3,
                    58,
                    0
                ],
                "published": "2024-06-19T11:53:21Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    11,
                    53,
                    21,
                    2,
                    171,
                    0
                ],
                "title": "BoA: Attention-aware Post-training Quantization without Backpropagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BoA: Attention-aware Post-training Quantization without Backpropagation"
                },
                "summary": "Post-training quantization (PTQ) is a promising solution for deploying large\nlanguage models (LLMs) on resource-constrained devices. Early methods developed\nfor smaller networks like ResNet rely on gradient-based optimization, which\nbecomes impractical for hyper-scale LLMs with billions of parameters. While\nrecently proposed backpropagation-free or transformation-based methods\nalleviate this issue, their performance remains limited by either a lack of\ninter-layer dependency consideration or the use of naive nearest-rounding-based\ninteger weight assignment to save the heavy computational cost of weight\noptimization. We thus introduce a novel backpropagation-free PTQ algorithm that\noptimizes integer weights by considering inter-layer dependencies. The key\ninnovation is the development of attention-aware Hessian matrices that capture\ninter-layer interactions within the attention module. Extensive experiments\ndemonstrate that our approach not only outperforms existing weight quantization\nmethods but also shows good synergy with conventional methods to suppress\nactivation outliers, leading to state-of-the-art weight-activation quantization\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) is a promising solution for deploying large\nlanguage models (LLMs) on resource-constrained devices. Early methods developed\nfor smaller networks like ResNet rely on gradient-based optimization, which\nbecomes impractical for hyper-scale LLMs with billions of parameters. While\nrecently proposed backpropagation-free or transformation-based methods\nalleviate this issue, their performance remains limited by either a lack of\ninter-layer dependency consideration or the use of naive nearest-rounding-based\ninteger weight assignment to save the heavy computational cost of weight\noptimization. We thus introduce a novel backpropagation-free PTQ algorithm that\noptimizes integer weights by considering inter-layer dependencies. The key\ninnovation is the development of attention-aware Hessian matrices that capture\ninter-layer interactions within the attention module. Extensive experiments\ndemonstrate that our approach not only outperforms existing weight quantization\nmethods but also shows good synergy with conventional methods to suppress\nactivation outliers, leading to state-of-the-art weight-activation quantization\nperformance."
                },
                "authors": [
                    {
                        "name": "Junhan Kim"
                    },
                    {
                        "name": "Ho-young Kim"
                    },
                    {
                        "name": "Eulrang Cho"
                    },
                    {
                        "name": "Chungman Lee"
                    },
                    {
                        "name": "Joonyoung Kim"
                    },
                    {
                        "name": "Yongkweon Jeon"
                    }
                ],
                "author_detail": {
                    "name": "Yongkweon Jeon"
                },
                "author": "Yongkweon Jeon",
                "arxiv_comment": "19 pages, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13474v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13474v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20129v1",
                "updated": "2025-02-27T14:24:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    24,
                    51,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T14:24:51Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    24,
                    51,
                    3,
                    58,
                    0
                ],
                "title": "Finite State Automata Inside Transformers with Chain-of-Thought: A\n  Mechanistic Study on State Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finite State Automata Inside Transformers with Chain-of-Thought: A\n  Mechanistic Study on State Tracking"
                },
                "summary": "Chain-of-Thought (CoT) significantly enhances the performance of large\nlanguage models (LLMs) across a wide range of tasks, and prior research shows\nthat CoT can theoretically increase expressiveness. However, there is limited\nmechanistic understanding of the algorithms that Transformer+CoT can learn. In\nthis work, we (1) evaluate the state tracking capabilities of Transformer+CoT\nand its variants, confirming the effectiveness of CoT. (2) Next, we identify\nthe circuit, a subset of model components, responsible for tracking the world\nstate, finding that late-layer MLP neurons play a key role. We propose two\nmetrics, compression and distinction, and show that the neuron sets for each\nstate achieve nearly 100% accuracy, providing evidence of an implicit finite\nstate automaton (FSA) embedded within the model. (3) Additionally, we explore\nthree realistic settings: skipping intermediate steps, introducing data noise,\nand testing length generalization. Our results demonstrate that Transformer+CoT\nlearns robust algorithms (FSA), highlighting its resilience in challenging\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) significantly enhances the performance of large\nlanguage models (LLMs) across a wide range of tasks, and prior research shows\nthat CoT can theoretically increase expressiveness. However, there is limited\nmechanistic understanding of the algorithms that Transformer+CoT can learn. In\nthis work, we (1) evaluate the state tracking capabilities of Transformer+CoT\nand its variants, confirming the effectiveness of CoT. (2) Next, we identify\nthe circuit, a subset of model components, responsible for tracking the world\nstate, finding that late-layer MLP neurons play a key role. We propose two\nmetrics, compression and distinction, and show that the neuron sets for each\nstate achieve nearly 100% accuracy, providing evidence of an implicit finite\nstate automaton (FSA) embedded within the model. (3) Additionally, we explore\nthree realistic settings: skipping intermediate steps, introducing data noise,\nand testing length generalization. Our results demonstrate that Transformer+CoT\nlearns robust algorithms (FSA), highlighting its resilience in challenging\nscenarios."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Wenyu Du"
                    },
                    {
                        "name": "Dongming Jin"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Zhi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Jin"
                },
                "author": "Zhi Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20127v1",
                "updated": "2025-02-27T14:19:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    19,
                    45,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T14:19:45Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    19,
                    45,
                    3,
                    58,
                    0
                ],
                "title": "SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning"
                },
                "summary": "Mainstream issue-resolving frameworks predominantly rely on commercial\nmodels, leading to high costs and privacy concerns. Existing training\napproaches for issue resolving struggle with poor generalization and fail to\nfully leverage open-source development resources. We propose Subtask-oriented\nReinforced Fine-Tuning (SoRFT), a novel training approach to enhance the issue\nresolving capability of LLMs. We decomposes issue resolving into structured\nsubtasks: file localization, function localization, line localization, and code\nedit generation. SoRFT consists of two training stages: (1) rejection-sampled\nsupervised fine-tuning, Chain of Thought (CoT) data is filtered using\nground-truth before fine-tuning the LLM, and (2) rule-based reinforcement\nlearning, which leverages PPO with ground-truth based rewards. We evaluate the\nSoRFT-trained model on SWE-Bench Verified and SWE-Bench Lite, achieving\nstate-of-the-art (SOTA) performance among open-source models (e.g., resolve\n21.4% issues on SWE-Bench Verified with SoRFT-Qwen-7B). The experimental\nresults demonstrate that SoRFT significantly enhances issue-resolving\nperformance, improves model generalization, and provides a cost-efficient\nalternative to commercial models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainstream issue-resolving frameworks predominantly rely on commercial\nmodels, leading to high costs and privacy concerns. Existing training\napproaches for issue resolving struggle with poor generalization and fail to\nfully leverage open-source development resources. We propose Subtask-oriented\nReinforced Fine-Tuning (SoRFT), a novel training approach to enhance the issue\nresolving capability of LLMs. We decomposes issue resolving into structured\nsubtasks: file localization, function localization, line localization, and code\nedit generation. SoRFT consists of two training stages: (1) rejection-sampled\nsupervised fine-tuning, Chain of Thought (CoT) data is filtered using\nground-truth before fine-tuning the LLM, and (2) rule-based reinforcement\nlearning, which leverages PPO with ground-truth based rewards. We evaluate the\nSoRFT-trained model on SWE-Bench Verified and SWE-Bench Lite, achieving\nstate-of-the-art (SOTA) performance among open-source models (e.g., resolve\n21.4% issues on SWE-Bench Verified with SoRFT-Qwen-7B). The experimental\nresults demonstrate that SoRFT significantly enhances issue-resolving\nperformance, improves model generalization, and provides a cost-efficient\nalternative to commercial models."
                },
                "authors": [
                    {
                        "name": "Zexiong Ma"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Pengfei Gao"
                    },
                    {
                        "name": "Xiangxin Meng"
                    },
                    {
                        "name": "Yanzhen Zou"
                    },
                    {
                        "name": "Bing Xie"
                    }
                ],
                "author_detail": {
                    "name": "Bing Xie"
                },
                "author": "Bing Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20125v1",
                "updated": "2025-02-27T14:16:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    16,
                    22,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T14:16:22Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    16,
                    22,
                    3,
                    58,
                    0
                ],
                "title": "Discovering Antagonists in Networks of Systems: Robot Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering Antagonists in Networks of Systems: Robot Deployment"
                },
                "summary": "A contextual anomaly detection method is proposed and applied to the physical\nmotions of a robot swarm executing a coverage task. Using simulations of a\nswarm's normal behavior, a normalizing flow is trained to predict the\nlikelihood of a robot motion within the current context of its environment.\nDuring application, the predicted likelihood of the observed motions is used by\na detection criterion that categorizes a robot agent as normal or antagonistic.\nThe proposed method is evaluated on five different strategies of antagonistic\nbehavior. Importantly, only readily available simulated data of normal robot\nbehavior is used for training such that the nature of the anomalies need not be\nknown beforehand. The best detection criterion correctly categorizes at least\n80% of each antagonistic type while maintaining a false positive rate of less\nthan 5% for normal robot agents. Additionally, the method is validated in\nhardware experiments, yielding results similar to the simulated scenarios.\nCompared to the state-of-the-art approach, both the predictive performance of\nthe normalizing flow and the robustness of the detection criterion are\nincreased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A contextual anomaly detection method is proposed and applied to the physical\nmotions of a robot swarm executing a coverage task. Using simulations of a\nswarm's normal behavior, a normalizing flow is trained to predict the\nlikelihood of a robot motion within the current context of its environment.\nDuring application, the predicted likelihood of the observed motions is used by\na detection criterion that categorizes a robot agent as normal or antagonistic.\nThe proposed method is evaluated on five different strategies of antagonistic\nbehavior. Importantly, only readily available simulated data of normal robot\nbehavior is used for training such that the nature of the anomalies need not be\nknown beforehand. The best detection criterion correctly categorizes at least\n80% of each antagonistic type while maintaining a false positive rate of less\nthan 5% for normal robot agents. Additionally, the method is validated in\nhardware experiments, yielding results similar to the simulated scenarios.\nCompared to the state-of-the-art approach, both the predictive performance of\nthe normalizing flow and the robustness of the detection criterion are\nincreased."
                },
                "authors": [
                    {
                        "name": "Ingeborg Wenger"
                    },
                    {
                        "name": "Peter Eberhard"
                    },
                    {
                        "name": "Henrik Ebel"
                    }
                ],
                "author_detail": {
                    "name": "Henrik Ebel"
                },
                "author": "Henrik Ebel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.9; I.2.11; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20122v1",
                "updated": "2025-02-27T14:14:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    14,
                    50,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T14:14:50Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    14,
                    50,
                    3,
                    58,
                    0
                ],
                "title": "Self-Training Elicits Concise Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Training Elicits Concise Reasoning in Large Language Models"
                },
                "summary": "Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to\nutilize additional computation through intermediate tokens to solve complex\ntasks. However, we posit that typical reasoning traces contain many redundant\ntokens, incurring extraneous inference costs. Upon examination of the output\ndistribution of current LLMs, we find evidence on their latent ability to\nreason more concisely, relative to their default behavior. To elicit this\ncapability, we propose simple fine-tuning methods which leverage self-generated\nconcise reasoning paths obtained by best-of-N sampling and few-shot\nconditioning, in task-specific settings. Our combined method achieves a 30%\nreduction in output tokens on average, across five model families on GSM8K and\nMATH, while maintaining average accuracy. By exploiting the fundamental\nstochasticity and in-context learning capabilities of LLMs, our self-training\napproach robustly elicits concise reasoning on a wide range of models,\nincluding those with extensive post-training. Code is available at\nhttps://github.com/TergelMunkhbat/concise-reasoning",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to\nutilize additional computation through intermediate tokens to solve complex\ntasks. However, we posit that typical reasoning traces contain many redundant\ntokens, incurring extraneous inference costs. Upon examination of the output\ndistribution of current LLMs, we find evidence on their latent ability to\nreason more concisely, relative to their default behavior. To elicit this\ncapability, we propose simple fine-tuning methods which leverage self-generated\nconcise reasoning paths obtained by best-of-N sampling and few-shot\nconditioning, in task-specific settings. Our combined method achieves a 30%\nreduction in output tokens on average, across five model families on GSM8K and\nMATH, while maintaining average accuracy. By exploiting the fundamental\nstochasticity and in-context learning capabilities of LLMs, our self-training\napproach robustly elicits concise reasoning on a wide range of models,\nincluding those with extensive post-training. Code is available at\nhttps://github.com/TergelMunkhbat/concise-reasoning"
                },
                "authors": [
                    {
                        "name": "Tergel Munkhbat"
                    },
                    {
                        "name": "Namgyu Ho"
                    },
                    {
                        "name": "Seohyun Kim"
                    },
                    {
                        "name": "Yongjin Yang"
                    },
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "23 pages, 10 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11807v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11807v6",
                "updated": "2025-02-27T13:57:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    13,
                    57,
                    52,
                    3,
                    58,
                    0
                ],
                "published": "2024-03-18T14:04:47Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    14,
                    4,
                    47,
                    0,
                    78,
                    0
                ],
                "title": "How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming\n  Ability in Multi-Agent Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming\n  Ability in Multi-Agent Environments"
                },
                "summary": "Decision-making is a complex process requiring diverse abilities, making it\nan excellent framework for evaluating Large Language Models (LLMs). Researchers\nhave examined LLMs' decision-making through the lens of Game Theory. However,\nexisting evaluation mainly focus on two-player scenarios where an LLM competes\nagainst another. Additionally, previous benchmarks suffer from test set leakage\ndue to their static design. We introduce GAMA($\\gamma$)-Bench, a new framework\nfor evaluating LLMs' Gaming Ability in Multi-Agent environments. It includes\neight classical game theory scenarios and a dynamic scoring scheme specially\ndesigned to quantitatively assess LLMs' performance. $\\gamma$-Bench allows\nflexible game settings and adapts the scoring system to different game\nparameters, enabling comprehensive evaluation of robustness, generalizability,\nand strategies for improvement. Our results indicate that GPT-3.5 demonstrates\nstrong robustness but limited generalizability, which can be enhanced using\nmethods like Chain-of-Thought. We also evaluate 13 LLMs from 6 model families,\nincluding GPT-3.5, GPT-4, Gemini, LLaMA-3.1, Mixtral, and Qwen-2.\nGemini-1.5-Pro outperforms others, scoring of $69.8$ out of $100$, followed by\nLLaMA-3.1-70B ($65.9$) and Mixtral-8x22B ($62.4$). Our code and experimental\nresults are publicly available at https://github.com/CUHK-ARISE/GAMABench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-making is a complex process requiring diverse abilities, making it\nan excellent framework for evaluating Large Language Models (LLMs). Researchers\nhave examined LLMs' decision-making through the lens of Game Theory. However,\nexisting evaluation mainly focus on two-player scenarios where an LLM competes\nagainst another. Additionally, previous benchmarks suffer from test set leakage\ndue to their static design. We introduce GAMA($\\gamma$)-Bench, a new framework\nfor evaluating LLMs' Gaming Ability in Multi-Agent environments. It includes\neight classical game theory scenarios and a dynamic scoring scheme specially\ndesigned to quantitatively assess LLMs' performance. $\\gamma$-Bench allows\nflexible game settings and adapts the scoring system to different game\nparameters, enabling comprehensive evaluation of robustness, generalizability,\nand strategies for improvement. Our results indicate that GPT-3.5 demonstrates\nstrong robustness but limited generalizability, which can be enhanced using\nmethods like Chain-of-Thought. We also evaluate 13 LLMs from 6 model families,\nincluding GPT-3.5, GPT-4, Gemini, LLaMA-3.1, Mixtral, and Qwen-2.\nGemini-1.5-Pro outperforms others, scoring of $69.8$ out of $100$, followed by\nLLaMA-3.1-70B ($65.9$) and Mixtral-8x22B ($62.4$). Our code and experimental\nresults are publicly available at https://github.com/CUHK-ARISE/GAMABench."
                },
                "authors": [
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Eric John Li"
                    },
                    {
                        "name": "Man Ho Lam"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Youliang Yuan"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "arxiv_comment": "Accepted to ICLR 2025; 11 pages of main text; 26 pages of appendices;\n  Included models: GPT-3.5-{0613, 1106, 0125}, GPT-4-0125, GPT-4o-0806,\n  Gemini-{1.0, 1.5)-Pro, LLaMA-3.1-{7, 70, 405}B, Mixtral-8x{7, 22}B,\n  Qwen-2-72B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.11807v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11807v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20082v1",
                "updated": "2025-02-27T13:41:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    13,
                    41,
                    7,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T13:41:07Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    13,
                    41,
                    7,
                    3,
                    58,
                    0
                ],
                "title": "LongRoPE2: Near-Lossless LLM Context Window Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongRoPE2: Near-Lossless LLM Context Window Scaling"
                },
                "summary": "LongRoPE2 is a novel approach that extends the effective context window of\npre-trained large language models (LLMs) to the target length, while preserving\nthe performance on the original shorter context window. This is achieved by\nthree contributions: (1) a hypothesis that insufficient training in higher RoPE\ndimensions contributes to the persistent out-of-distribution (OOD) issues\nobserved in existing methods; (2) an effective RoPE rescaling algorithm that\nadopts evolutionary search guided by \"needle-driven\" perplexity to address the\ninsufficient training problem; (3) a mixed context window training approach\nthat fine-tunes model weights to adopt rescaled RoPE for long-context sequences\nwhile preserving the short-context performance with the original RoPE.\nExtensive experiments on LLaMA3-8B and Phi3-mini-3.8B across various benchmarks\nvalidate the hypothesis and demonstrate the effectiveness of LongRoPE2.\nRemarkably, LongRoPE2 extends LLaMA3-8B to achieve a 128K effective context\nlength while retaining over 98.5% of short-context performance, using only 10B\ntokens -- 80x fewer than Meta's approach, which fails to reach the target\neffective context length. Code will be available at\nhttps://github.com/microsoft/LongRoPE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongRoPE2 is a novel approach that extends the effective context window of\npre-trained large language models (LLMs) to the target length, while preserving\nthe performance on the original shorter context window. This is achieved by\nthree contributions: (1) a hypothesis that insufficient training in higher RoPE\ndimensions contributes to the persistent out-of-distribution (OOD) issues\nobserved in existing methods; (2) an effective RoPE rescaling algorithm that\nadopts evolutionary search guided by \"needle-driven\" perplexity to address the\ninsufficient training problem; (3) a mixed context window training approach\nthat fine-tunes model weights to adopt rescaled RoPE for long-context sequences\nwhile preserving the short-context performance with the original RoPE.\nExtensive experiments on LLaMA3-8B and Phi3-mini-3.8B across various benchmarks\nvalidate the hypothesis and demonstrate the effectiveness of LongRoPE2.\nRemarkably, LongRoPE2 extends LLaMA3-8B to achieve a 128K effective context\nlength while retaining over 98.5% of short-context performance, using only 10B\ntokens -- 80x fewer than Meta's approach, which fails to reach the target\neffective context length. Code will be available at\nhttps://github.com/microsoft/LongRoPE."
                },
                "authors": [
                    {
                        "name": "Ning Shang"
                    },
                    {
                        "name": "Li Lyna Zhang"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Gaokai Zhang"
                    },
                    {
                        "name": "Gilsinia Lopez"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Weizhu Chen"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06153v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06153v3",
                "updated": "2025-02-27T13:33:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    13,
                    33,
                    55,
                    3,
                    58,
                    0
                ],
                "published": "2024-10-08T15:52:42Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    15,
                    52,
                    42,
                    1,
                    282,
                    0
                ],
                "title": "AgentSquare: Automatic LLM Agent Search in Modular Design Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentSquare: Automatic LLM Agent Search in Modular Design Space"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have led to a rapid\ngrowth of agentic systems capable of handling a wide range of complex tasks.\nHowever, current research largely relies on manual, task-specific design,\nlimiting their adaptability to novel tasks. In this paper, we introduce a new\nresearch problem: Modularized LLM Agent Search (MoLAS). We propose a modular\ndesign space that abstracts existing LLM agent designs into four fundamental\nmodules with uniform IO interface: Planning, Reasoning, Tool Use, and Memory.\nBuilding on this design space, we present a novel LLM agent search framework\ncalled AgentSquare, which introduces two core mechanisms, i.e., module\nevolution and recombination, to efficiently search for optimized LLM agents. To\nfurther accelerate the process, we design a performance predictor that uses\nin-context surrogate models to skip unpromising agent designs. Extensive\nexperiments across six benchmarks, covering the diverse scenarios of web,\nembodied, tool use and game applications, show that AgentSquare substantially\noutperforms hand-crafted agents, achieving an average performance gain of 17.2%\nagainst best-known human designs. Moreover, AgentSquare can generate\ninterpretable design insights, enabling a deeper understanding of agentic\narchitecture and its impact on task performance. We believe that the modular\ndesign space and AgentSquare search framework offer a platform for fully\nexploiting the potential of prior successful designs and consolidating the\ncollective efforts of research community. Code repo is available at\nhttps://github.com/tsinghua-fib-lab/AgentSquare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have led to a rapid\ngrowth of agentic systems capable of handling a wide range of complex tasks.\nHowever, current research largely relies on manual, task-specific design,\nlimiting their adaptability to novel tasks. In this paper, we introduce a new\nresearch problem: Modularized LLM Agent Search (MoLAS). We propose a modular\ndesign space that abstracts existing LLM agent designs into four fundamental\nmodules with uniform IO interface: Planning, Reasoning, Tool Use, and Memory.\nBuilding on this design space, we present a novel LLM agent search framework\ncalled AgentSquare, which introduces two core mechanisms, i.e., module\nevolution and recombination, to efficiently search for optimized LLM agents. To\nfurther accelerate the process, we design a performance predictor that uses\nin-context surrogate models to skip unpromising agent designs. Extensive\nexperiments across six benchmarks, covering the diverse scenarios of web,\nembodied, tool use and game applications, show that AgentSquare substantially\noutperforms hand-crafted agents, achieving an average performance gain of 17.2%\nagainst best-known human designs. Moreover, AgentSquare can generate\ninterpretable design insights, enabling a deeper understanding of agentic\narchitecture and its impact on task performance. We believe that the modular\ndesign space and AgentSquare search framework offer a platform for fully\nexploiting the potential of prior successful designs and consolidating the\ncollective efforts of research community. Code repo is available at\nhttps://github.com/tsinghua-fib-lab/AgentSquare."
                },
                "authors": [
                    {
                        "name": "Yu Shang"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Keyu Zhao"
                    },
                    {
                        "name": "Likai Ma"
                    },
                    {
                        "name": "Jiahe Liu"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06153v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06153v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20073v1",
                "updated": "2025-02-27T13:31:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    13,
                    31,
                    13,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T13:31:13Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    13,
                    31,
                    13,
                    3,
                    58,
                    0
                ],
                "title": "Collab-Overcooked: Benchmarking and Evaluating Large Language Models as\n  Collaborative Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collab-Overcooked: Benchmarking and Evaluating Large Language Models as\n  Collaborative Agents"
                },
                "summary": "Large language models (LLMs) based agent systems have made great strides in\nreal-world applications beyond traditional NLP tasks. This paper proposes a new\nLLM-powered Multi-Agent System (LLM-MAS) benchmark, Collab-Overcooked, built on\nthe popular Overcooked-AI game with more applicable and challenging tasks in\ninteractive environments. Collab-Overcooked extends existing benchmarks from\ntwo novel perspectives. First, it provides a multi-agent framework supporting\ndiverse tasks and objectives and encourages collaboration through natural\nlanguage communication. Second, it introduces a spectrum of process-oriented\nevaluation metrics to assess the fine-grained collaboration capabilities of\ndifferent LLM agents, a dimension often overlooked in prior work. We conduct\nextensive experiments over 10 popular LLMs and show that, while the LLMs\npresent a strong ability in goal interpretation, there is a significant\ndiscrepancy in active collaboration and continuous adaption that are critical\nfor efficiently fulfilling complicated tasks. Notably, we highlight the\nstrengths and weaknesses in LLM-MAS and provide insights for improving and\nevaluating LLM-MAS on a unified and open-sourced benchmark. Environments, 30\nopen-ended tasks, and an integrated evaluation package are now publicly\navailable at https://github.com/YusaeMeow/Collab-Overcooked.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) based agent systems have made great strides in\nreal-world applications beyond traditional NLP tasks. This paper proposes a new\nLLM-powered Multi-Agent System (LLM-MAS) benchmark, Collab-Overcooked, built on\nthe popular Overcooked-AI game with more applicable and challenging tasks in\ninteractive environments. Collab-Overcooked extends existing benchmarks from\ntwo novel perspectives. First, it provides a multi-agent framework supporting\ndiverse tasks and objectives and encourages collaboration through natural\nlanguage communication. Second, it introduces a spectrum of process-oriented\nevaluation metrics to assess the fine-grained collaboration capabilities of\ndifferent LLM agents, a dimension often overlooked in prior work. We conduct\nextensive experiments over 10 popular LLMs and show that, while the LLMs\npresent a strong ability in goal interpretation, there is a significant\ndiscrepancy in active collaboration and continuous adaption that are critical\nfor efficiently fulfilling complicated tasks. Notably, we highlight the\nstrengths and weaknesses in LLM-MAS and provide insights for improving and\nevaluating LLM-MAS on a unified and open-sourced benchmark. Environments, 30\nopen-ended tasks, and an integrated evaluation package are now publicly\navailable at https://github.com/YusaeMeow/Collab-Overcooked."
                },
                "authors": [
                    {
                        "name": "Haochen Sun"
                    },
                    {
                        "name": "Shuwen Zhang"
                    },
                    {
                        "name": "Lei Ren"
                    },
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Hao Fu"
                    },
                    {
                        "name": "Caixia Yuan"
                    },
                    {
                        "name": "Xiaojie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojie Wang"
                },
                "author": "Xiaojie Wang",
                "arxiv_comment": "25 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03326v2",
                "updated": "2025-02-27T13:21:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    13,
                    21,
                    25,
                    3,
                    58,
                    0
                ],
                "published": "2024-10-22T08:44:01Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    8,
                    44,
                    1,
                    1,
                    296,
                    0
                ],
                "title": "xApp-Level Conflict Mitigation in O-RAN, a Mobility Driven Energy Saving\n  Case",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xApp-Level Conflict Mitigation in O-RAN, a Mobility Driven Energy Saving\n  Case"
                },
                "summary": "This paper investigates the emerging challenges of conflict detection and\nmitigation in Open Radio Access Network (O-RAN). Conflicts between xApps can\narise that affect network performance and stability due to the disaggregated\nnature of O-RAN. This work provides a detailed theoretical framework of\nExtended Application (xApp)-level conflicts, i.e., direct, indirect, and\nimplicit conflicts. Leveraging conflict graphs, we further highlight how\nconflicts impact Key Performance Indicators (KPIs) and explore strategies for\nconflict detection using Service Level Agreements (SLAs) and Quality of Service\n(QoS) thresholds. We evaluate the effectiveness of several mitigation\nstrategies in a simulated environment with Mobility Robustness Optimization\n(MRO) and Energy Saving (ES) xApps and present experimental results showing\ncomparisons among these strategies. The findings of this research provide\nsignificant insights for enhancing O-RAN deployments with flexible and\nefficient conflict management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the emerging challenges of conflict detection and\nmitigation in Open Radio Access Network (O-RAN). Conflicts between xApps can\narise that affect network performance and stability due to the disaggregated\nnature of O-RAN. This work provides a detailed theoretical framework of\nExtended Application (xApp)-level conflicts, i.e., direct, indirect, and\nimplicit conflicts. Leveraging conflict graphs, we further highlight how\nconflicts impact Key Performance Indicators (KPIs) and explore strategies for\nconflict detection using Service Level Agreements (SLAs) and Quality of Service\n(QoS) thresholds. We evaluate the effectiveness of several mitigation\nstrategies in a simulated environment with Mobility Robustness Optimization\n(MRO) and Energy Saving (ES) xApps and present experimental results showing\ncomparisons among these strategies. The findings of this research provide\nsignificant insights for enhancing O-RAN deployments with flexible and\nefficient conflict management."
                },
                "authors": [
                    {
                        "name": "Abdul Wadud"
                    },
                    {
                        "name": "Fatemeh Golpayegani"
                    },
                    {
                        "name": "Nima Afraz"
                    }
                ],
                "author_detail": {
                    "name": "Nima Afraz"
                },
                "author": "Nima Afraz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18934v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18934v2",
                "updated": "2025-02-27T13:20:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    13,
                    20,
                    53,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-26T08:36:20Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    8,
                    36,
                    20,
                    2,
                    57,
                    0
                ],
                "title": "Kanana: Compute-efficient Bilingual Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kanana: Compute-efficient Bilingual Language Models"
                },
                "summary": "We introduce Kanana, a series of bilingual language models that demonstrate\nexceeding performance in Korean and competitive performance in English. The\ncomputational cost of Kanana is significantly lower than that of\nstate-of-the-art models of similar size. The report details the techniques\nemployed during pre-training to achieve compute-efficient yet competitive\nmodels, including high quality data filtering, staged pre-training, depth\nup-scaling, and pruning and distillation. Furthermore, the report outlines the\nmethodologies utilized during the post-training of the Kanana models,\nencompassing supervised fine-tuning and preference optimization, aimed at\nenhancing their capability for seamless interaction with users. Lastly, the\nreport elaborates on plausible approaches used for language model adaptation to\nspecific scenarios, such as embedding, retrieval augmented generation, and\nfunction calling. The Kanana model series spans from 2.1B to 32.5B parameters\nwith 2.1B models (base, instruct, embedding) publicly released to promote\nresearch on Korean language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Kanana, a series of bilingual language models that demonstrate\nexceeding performance in Korean and competitive performance in English. The\ncomputational cost of Kanana is significantly lower than that of\nstate-of-the-art models of similar size. The report details the techniques\nemployed during pre-training to achieve compute-efficient yet competitive\nmodels, including high quality data filtering, staged pre-training, depth\nup-scaling, and pruning and distillation. Furthermore, the report outlines the\nmethodologies utilized during the post-training of the Kanana models,\nencompassing supervised fine-tuning and preference optimization, aimed at\nenhancing their capability for seamless interaction with users. Lastly, the\nreport elaborates on plausible approaches used for language model adaptation to\nspecific scenarios, such as embedding, retrieval augmented generation, and\nfunction calling. The Kanana model series spans from 2.1B to 32.5B parameters\nwith 2.1B models (base, instruct, embedding) publicly released to promote\nresearch on Korean language models."
                },
                "authors": [
                    {
                        "name": "Kanana LLM Team"
                    },
                    {
                        "name": "Yunju Bak"
                    },
                    {
                        "name": "Hojin Lee"
                    },
                    {
                        "name": "Minho Ryu"
                    },
                    {
                        "name": "Jiyeon Ham"
                    },
                    {
                        "name": "Seungjae Jung"
                    },
                    {
                        "name": "Daniel Wontae Nam"
                    },
                    {
                        "name": "Taegyeong Eo"
                    },
                    {
                        "name": "Donghun Lee"
                    },
                    {
                        "name": "Doohae Jung"
                    },
                    {
                        "name": "Boseop Kim"
                    },
                    {
                        "name": "Nayeon Kim"
                    },
                    {
                        "name": "Jaesun Park"
                    },
                    {
                        "name": "Hyunho Kim"
                    },
                    {
                        "name": "Hyunwoong Ko"
                    },
                    {
                        "name": "Changmin Lee"
                    },
                    {
                        "name": "Kyoung-Woon On"
                    },
                    {
                        "name": "Seulye Baeg"
                    },
                    {
                        "name": "Junrae Cho"
                    },
                    {
                        "name": "Sunghee Jung"
                    },
                    {
                        "name": "Jieun Kang"
                    },
                    {
                        "name": "EungGyun Kim"
                    },
                    {
                        "name": "Eunhwa Kim"
                    },
                    {
                        "name": "Byeongil Ko"
                    },
                    {
                        "name": "Daniel Lee"
                    },
                    {
                        "name": "Minchul Lee"
                    },
                    {
                        "name": "Miok Lee"
                    },
                    {
                        "name": "Shinbok Lee"
                    },
                    {
                        "name": "Gaeun Seo"
                    }
                ],
                "author_detail": {
                    "name": "Gaeun Seo"
                },
                "author": "Gaeun Seo",
                "arxiv_comment": "40 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18934v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18934v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06899v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06899v2",
                "updated": "2025-02-27T13:08:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    13,
                    8,
                    46,
                    3,
                    58,
                    0
                ],
                "published": "2024-11-11T11:57:37Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    11,
                    57,
                    37,
                    0,
                    316,
                    0
                ],
                "title": "LongSafety: Enhance Safety for Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongSafety: Enhance Safety for Long-Context LLMs"
                },
                "summary": "Recent advancements in model architectures and length extrapolation\ntechniques have significantly extended the context length of large language\nmodels (LLMs), paving the way for their application in increasingly complex\ntasks. However, despite the growing capabilities of long-context LLMs, the\nsafety issues in long-context scenarios remain underexplored. While safety\nalignment in short context has been widely studied, the safety concerns of\nlong-context LLMs have not been adequately addressed. In this work, we\nintroduce \\textbf{LongSafety}, a comprehensive safety alignment dataset for\nlong-context LLMs, containing 10 tasks and 17k samples, with an average length\nof 40.9k tokens. Our experiments demonstrate that training with LongSafety can\nenhance long-context safety performance while enhancing short-context safety\nand preserving general capabilities. Furthermore, we demonstrate that\nlong-context safety does not equal long-context alignment with short-context\nsafety data and LongSafety has generalizing capabilities in context length and\nlong-context safety scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in model architectures and length extrapolation\ntechniques have significantly extended the context length of large language\nmodels (LLMs), paving the way for their application in increasingly complex\ntasks. However, despite the growing capabilities of long-context LLMs, the\nsafety issues in long-context scenarios remain underexplored. While safety\nalignment in short context has been widely studied, the safety concerns of\nlong-context LLMs have not been adequately addressed. In this work, we\nintroduce \\textbf{LongSafety}, a comprehensive safety alignment dataset for\nlong-context LLMs, containing 10 tasks and 17k samples, with an average length\nof 40.9k tokens. Our experiments demonstrate that training with LongSafety can\nenhance long-context safety performance while enhancing short-context safety\nand preserving general capabilities. Furthermore, we demonstrate that\nlong-context safety does not equal long-context alignment with short-context\nsafety data and LongSafety has generalizing capabilities in context length and\nlong-context safety scenarios."
                },
                "authors": [
                    {
                        "name": "Mianqiu Huang"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Shaojun Zhou"
                    },
                    {
                        "name": "Mozhi Zhang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Linyang Li"
                    },
                    {
                        "name": "Chenkun Tan"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Pengyu Wang"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Yaqian Zhou"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06899v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06899v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20060v1",
                "updated": "2025-02-27T13:02:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    13,
                    2,
                    26,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T13:02:26Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    13,
                    2,
                    26,
                    3,
                    58,
                    0
                ],
                "title": "Continuous Phase-Shifting Holography Utilizing a Moving Diffraction\n  Grating",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Phase-Shifting Holography Utilizing a Moving Diffraction\n  Grating"
                },
                "summary": "Fabrication of optical coherence tomography (OCT) fiber probes in cardiology\ninvolves sequentially splicing and cleaving multiple fibers to form a lens.\nDuring this process, the splice location-normally invisible under standard\nillumination-must be identified with micron-level accuracy. This paper presents\nan approach for splice detection using digital lensless microscopy, which is\nwell-suited for deployment within the restricted space constraints typically\nfound in fiber-fabrication setups. The optical setup incorporates a movable\nRonchi grating that simultaneously acts as a beam splitter and a phase\nmodulator. This design enables spatial separation of the reference and object\nbeams while preserving the inherent stability of common-path interferometry.\nThe acquired hologram set is processed using the continuous phase-shifting\ntechnique, which is known for its robustness against phase-shifting errors. The\ntheoretical foundations for recording such holograms have been developed, and\nthe conditions under which background noise and the conjugate order are\nsuppressed have been identified. Experiments demonstrated the acquisition of\nhigh-contrast quantitative phase images of fiber splices with a spatial\nresolution down to 1 $\\mu m$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fabrication of optical coherence tomography (OCT) fiber probes in cardiology\ninvolves sequentially splicing and cleaving multiple fibers to form a lens.\nDuring this process, the splice location-normally invisible under standard\nillumination-must be identified with micron-level accuracy. This paper presents\nan approach for splice detection using digital lensless microscopy, which is\nwell-suited for deployment within the restricted space constraints typically\nfound in fiber-fabrication setups. The optical setup incorporates a movable\nRonchi grating that simultaneously acts as a beam splitter and a phase\nmodulator. This design enables spatial separation of the reference and object\nbeams while preserving the inherent stability of common-path interferometry.\nThe acquired hologram set is processed using the continuous phase-shifting\ntechnique, which is known for its robustness against phase-shifting errors. The\ntheoretical foundations for recording such holograms have been developed, and\nthe conditions under which background noise and the conjugate order are\nsuppressed have been identified. Experiments demonstrated the acquisition of\nhigh-contrast quantitative phase images of fiber splices with a spatial\nresolution down to 1 $\\mu m$."
                },
                "authors": [
                    {
                        "name": "G. S. Kalenkov"
                    },
                    {
                        "name": "S. G. Kalenkov"
                    },
                    {
                        "name": "A. E. Shtanko"
                    },
                    {
                        "name": "B. C. Quirk"
                    },
                    {
                        "name": "R. A. Mclaughlin"
                    }
                ],
                "author_detail": {
                    "name": "R. A. Mclaughlin"
                },
                "author": "R. A. Mclaughlin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17839v2",
                "updated": "2025-02-27T12:55:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    55,
                    8,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-25T04:38:38Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    4,
                    38,
                    38,
                    1,
                    56,
                    0
                ],
                "title": "Say Less, Mean More: Leveraging Pragmatics in Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Say Less, Mean More: Leveraging Pragmatics in Retrieval-Augmented\n  Generation"
                },
                "summary": "We propose a simple, unsupervised method that injects pragmatic principles in\nretrieval-augmented generation (RAG) frameworks such as Dense Passage Retrieval\nto enhance the utility of retrieved contexts. Our approach first identifies\nwhich sentences in a pool of documents retrieved by RAG are most relevant to\nthe question at hand, cover all the topics addressed in the input question and\nno more, and then highlights these sentences within their context, before they\nare provided to the LLM, without truncating or altering the context in any\nother way. We show that this simple idea brings consistent improvements in\nexperiments on three question answering tasks (ARC-Challenge, PubHealth and\nPopQA) using five different LLMs. It notably enhances relative accuracy by up\nto 19.7% on PubHealth and 10% on ARC-Challenge compared to a conventional RAG\nsystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple, unsupervised method that injects pragmatic principles in\nretrieval-augmented generation (RAG) frameworks such as Dense Passage Retrieval\nto enhance the utility of retrieved contexts. Our approach first identifies\nwhich sentences in a pool of documents retrieved by RAG are most relevant to\nthe question at hand, cover all the topics addressed in the input question and\nno more, and then highlights these sentences within their context, before they\nare provided to the LLM, without truncating or altering the context in any\nother way. We show that this simple idea brings consistent improvements in\nexperiments on three question answering tasks (ARC-Challenge, PubHealth and\nPopQA) using five different LLMs. It notably enhances relative accuracy by up\nto 19.7% on PubHealth and 10% on ARC-Challenge compared to a conventional RAG\nsystem."
                },
                "authors": [
                    {
                        "name": "Haris Riaz"
                    },
                    {
                        "name": "Ellen Riloff"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    }
                ],
                "author_detail": {
                    "name": "Mihai Surdeanu"
                },
                "author": "Mihai Surdeanu",
                "arxiv_comment": "16 pages, 2 figures, 8 tables. Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.00693v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.00693v3",
                "updated": "2025-02-27T12:49:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    49,
                    5,
                    3,
                    58,
                    0
                ],
                "published": "2023-06-01T14:02:45Z",
                "published_parsed": [
                    2023,
                    6,
                    1,
                    14,
                    2,
                    45,
                    3,
                    152,
                    0
                ],
                "title": "GPT4Image: Large Pre-trained Models Help Vision Models Learn Better on\n  Perception Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT4Image: Large Pre-trained Models Help Vision Models Learn Better on\n  Perception Task"
                },
                "summary": "The upsurge in pre-trained large models started by ChatGPT has swept across\nthe entire deep learning community. Such powerful models demonstrate advanced\ngenerative ability and multimodal understanding capability, which quickly set\nnew state of the arts on a variety of benchmarks. The pre-trained LLM usually\nplays the role as a universal AI model that can conduct various tasks like\narticle analysis and image comprehension. However, due to the prohibitively\nhigh memory and computational cost of implementing such a large model, the\nconventional models (such as CNN and ViT) are still essential for many visual\nperception tasks. In this paper, we propose to enhance the representation\nability of ordinary vision models on perception tasks (e.g. image\nclassification) by taking advantage of the off-the-shelf large pre-trained\nmodels. We present a new learning framework, dubbed GPT4Image, where the\nknowledge of the large pre-trained models are extracted to help CNNs and ViTs\nlearn better representations and achieve higher performance. Firstly, we curate\na high quality description set by prompting a multimodal LLM to generate\ndescriptions for training images. Then, these detailed descriptions are fed\ninto a pre-trained encoder to extract text embeddings that encodes the rich\nsemantics of images. During training, text embeddings will serve as extra\nsupervising signal and be aligned with image representations learned by vision\nmodels. The alignment process helps vision models achieve better performance\nwith the aid of pre-trained LLMs. We conduct extensive experiments to verify\nthe effectiveness of the proposed algorithm on various visual perception tasks\nfor heterogeneous model architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The upsurge in pre-trained large models started by ChatGPT has swept across\nthe entire deep learning community. Such powerful models demonstrate advanced\ngenerative ability and multimodal understanding capability, which quickly set\nnew state of the arts on a variety of benchmarks. The pre-trained LLM usually\nplays the role as a universal AI model that can conduct various tasks like\narticle analysis and image comprehension. However, due to the prohibitively\nhigh memory and computational cost of implementing such a large model, the\nconventional models (such as CNN and ViT) are still essential for many visual\nperception tasks. In this paper, we propose to enhance the representation\nability of ordinary vision models on perception tasks (e.g. image\nclassification) by taking advantage of the off-the-shelf large pre-trained\nmodels. We present a new learning framework, dubbed GPT4Image, where the\nknowledge of the large pre-trained models are extracted to help CNNs and ViTs\nlearn better representations and achieve higher performance. Firstly, we curate\na high quality description set by prompting a multimodal LLM to generate\ndescriptions for training images. Then, these detailed descriptions are fed\ninto a pre-trained encoder to extract text embeddings that encodes the rich\nsemantics of images. During training, text embeddings will serve as extra\nsupervising signal and be aligned with image representations learned by vision\nmodels. The alignment process helps vision models achieve better performance\nwith the aid of pre-trained LLMs. We conduct extensive experiments to verify\nthe effectiveness of the proposed algorithm on various visual perception tasks\nfor heterogeneous model architectures."
                },
                "authors": [
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Zhongqian Fu"
                    },
                    {
                        "name": "Chao Xu"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "arxiv_comment": "GitHub:\n  https://github.com/huawei-noah/Efficient-Computing/tree/master/GPT4Image/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.00693v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.00693v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v3",
                "updated": "2025-02-27T12:30:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    30,
                    43,
                    3,
                    58,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "ICLR 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20041v1",
                "updated": "2025-02-27T12:29:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    29,
                    44,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T12:29:44Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    29,
                    44,
                    3,
                    58,
                    0
                ],
                "title": "3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary\n  Affordance Detection in 3D Worlds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary\n  Affordance Detection in 3D Worlds"
                },
                "summary": "3D Affordance detection is a challenging problem with broad applications on\nvarious robotic tasks. Existing methods typically formulate the detection\nparadigm as a label-based semantic segmentation task. This paradigm relies on\npredefined labels and lacks the ability to comprehend complex natural language,\nresulting in limited generalization in open-world scene. To address these\nlimitations, we reformulate the traditional affordance detection paradigm into\n\\textit{Instruction Reasoning Affordance Segmentation} (IRAS) task. This task\nis designed to output a affordance mask region given a query reasoning text,\nwhich avoids fixed categories of input labels. We accordingly propose the\n\\textit{3D-AffordanceLLM} (3D-ADLLM), a framework designed for reasoning\naffordance detection in 3D open-scene. Specifically, 3D-ADLLM introduces large\nlanguage models (LLMs) to 3D affordance perception with a custom-designed\ndecoder for generating affordance masks, thus achieving open-world reasoning\naffordance detection. In addition, given the scarcity of 3D affordance datasets\nfor training large models, we seek to extract knowledge from general\nsegmentation data and transfer it to affordance detection. Thus, we propose a\nmulti-stage training strategy that begins with a novel pre-training task, i.e.,\n\\textit{Referring Object Part Segmentation}~(ROPS). This stage is designed to\nequip the model with general recognition and segmentation capabilities at the\nobject-part level. Then followed by fine-tuning with the IRAS task, 3D-ADLLM\nobtains the reasoning ability for affordance detection. In summary, 3D-ADLLM\nleverages the rich world knowledge and human-object interaction reasoning\nability of LLMs, achieving approximately an 8\\% improvement in mIoU on\nopen-vocabulary affordance detection tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Affordance detection is a challenging problem with broad applications on\nvarious robotic tasks. Existing methods typically formulate the detection\nparadigm as a label-based semantic segmentation task. This paradigm relies on\npredefined labels and lacks the ability to comprehend complex natural language,\nresulting in limited generalization in open-world scene. To address these\nlimitations, we reformulate the traditional affordance detection paradigm into\n\\textit{Instruction Reasoning Affordance Segmentation} (IRAS) task. This task\nis designed to output a affordance mask region given a query reasoning text,\nwhich avoids fixed categories of input labels. We accordingly propose the\n\\textit{3D-AffordanceLLM} (3D-ADLLM), a framework designed for reasoning\naffordance detection in 3D open-scene. Specifically, 3D-ADLLM introduces large\nlanguage models (LLMs) to 3D affordance perception with a custom-designed\ndecoder for generating affordance masks, thus achieving open-world reasoning\naffordance detection. In addition, given the scarcity of 3D affordance datasets\nfor training large models, we seek to extract knowledge from general\nsegmentation data and transfer it to affordance detection. Thus, we propose a\nmulti-stage training strategy that begins with a novel pre-training task, i.e.,\n\\textit{Referring Object Part Segmentation}~(ROPS). This stage is designed to\nequip the model with general recognition and segmentation capabilities at the\nobject-part level. Then followed by fine-tuning with the IRAS task, 3D-ADLLM\nobtains the reasoning ability for affordance detection. In summary, 3D-ADLLM\nleverages the rich world knowledge and human-object interaction reasoning\nability of LLMs, achieving approximately an 8\\% improvement in mIoU on\nopen-vocabulary affordance detection tasks."
                },
                "authors": [
                    {
                        "name": "Hengshuo Chu"
                    },
                    {
                        "name": "Xiang Deng"
                    },
                    {
                        "name": "Xiaoyang Chen"
                    },
                    {
                        "name": "Yinchuan Li"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "ICLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14117v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14117v2",
                "updated": "2025-02-27T12:29:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    29,
                    11,
                    3,
                    58,
                    0
                ],
                "published": "2024-05-23T02:44:12Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    2,
                    44,
                    12,
                    3,
                    144,
                    0
                ],
                "title": "Knowledge Localization: Mission Not Accomplished? Enter Query\n  Localization!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Localization: Mission Not Accomplished? Enter Query\n  Localization!"
                },
                "summary": "Large language models (LLMs) store extensive factual knowledge, but the\nmechanisms behind how they store and express this knowledge remain unclear. The\nKnowledge Neuron (KN) thesis is a prominent theory for explaining these\nmechanisms. This theory is based on the Knowledge Localization (KL) assumption,\nwhich suggests that a fact can be localized to a few knowledge storage units,\nnamely knowledge neurons.\n  However, this assumption has two limitations: first, it may be too rigid\nregarding knowledge storage, and second, it neglects the role of the attention\nmodule in knowledge expression.\n  In this paper, we first re-examine the KL assumption and demonstrate that its\nlimitations do indeed exist. To address these, we then present two new\nfindings, each targeting one of the limitations: one focusing on knowledge\nstorage and the other on knowledge expression. We summarize these findings as\n\\textbf{Query Localization} (QL) assumption and argue that the KL assumption\ncan be viewed as a simplification of the QL assumption. Based on QL assumption,\nwe further propose the Consistency-Aware KN modification method, which improves\nthe performance of knowledge modification, further validating our new\nassumption. We conduct 39 sets of experiments, along with additional\nvisualization experiments, to rigorously confirm our conclusions. Code is\navailable at https://github.com/heng840/KnowledgeLocalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) store extensive factual knowledge, but the\nmechanisms behind how they store and express this knowledge remain unclear. The\nKnowledge Neuron (KN) thesis is a prominent theory for explaining these\nmechanisms. This theory is based on the Knowledge Localization (KL) assumption,\nwhich suggests that a fact can be localized to a few knowledge storage units,\nnamely knowledge neurons.\n  However, this assumption has two limitations: first, it may be too rigid\nregarding knowledge storage, and second, it neglects the role of the attention\nmodule in knowledge expression.\n  In this paper, we first re-examine the KL assumption and demonstrate that its\nlimitations do indeed exist. To address these, we then present two new\nfindings, each targeting one of the limitations: one focusing on knowledge\nstorage and the other on knowledge expression. We summarize these findings as\n\\textbf{Query Localization} (QL) assumption and argue that the KL assumption\ncan be viewed as a simplification of the QL assumption. Based on QL assumption,\nwe further propose the Consistency-Aware KN modification method, which improves\nthe performance of knowledge modification, further validating our new\nassumption. We conduct 39 sets of experiments, along with additional\nvisualization experiments, to rigorously confirm our conclusions. Code is\navailable at https://github.com/heng840/KnowledgeLocalization."
                },
                "authors": [
                    {
                        "name": "Yuheng Chen"
                    },
                    {
                        "name": "Pengfei Cao"
                    },
                    {
                        "name": "Yubo Chen"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "ICLR 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14117v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14117v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20012v1",
                "updated": "2025-02-27T11:49:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    11,
                    49,
                    14,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T11:49:14Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    11,
                    49,
                    14,
                    3,
                    58,
                    0
                ],
                "title": "Learning Classifiers That Induce Markets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Classifiers That Induce Markets"
                },
                "summary": "When learning is used to inform decisions about humans, such as for loans,\nhiring, or admissions, this can incentivize users to strategically modify their\nfeatures to obtain positive predictions. A key assumption is that modifications\nare costly, and are governed by a cost function that is exogenous and\npredetermined. We challenge this assumption, and assert that the deployment of\na classifier is what creates costs. Our idea is simple: when users seek\npositive predictions, this creates demand for important features; and if\nfeatures are available for purchase, then a market will form, and competition\nwill give rise to prices. We extend the strategic classification framework to\nsupport this notion, and study learning in a setting where a classifier can\ninduce a market for features. We present an analysis of the learning task,\ndevise an algorithm for computing market prices, propose a differentiable\nlearning framework, and conduct experiments to explore our novel setting and\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When learning is used to inform decisions about humans, such as for loans,\nhiring, or admissions, this can incentivize users to strategically modify their\nfeatures to obtain positive predictions. A key assumption is that modifications\nare costly, and are governed by a cost function that is exogenous and\npredetermined. We challenge this assumption, and assert that the deployment of\na classifier is what creates costs. Our idea is simple: when users seek\npositive predictions, this creates demand for important features; and if\nfeatures are available for purchase, then a market will form, and competition\nwill give rise to prices. We extend the strategic classification framework to\nsupport this notion, and study learning in a setting where a classifier can\ninduce a market for features. We present an analysis of the learning task,\ndevise an algorithm for computing market prices, propose a differentiable\nlearning framework, and conduct experiments to explore our novel setting and\napproach."
                },
                "authors": [
                    {
                        "name": "Yonatan Sommer"
                    },
                    {
                        "name": "Ivri Hikri"
                    },
                    {
                        "name": "Lotan Amit"
                    },
                    {
                        "name": "Nir Rosenfeld"
                    }
                ],
                "author_detail": {
                    "name": "Nir Rosenfeld"
                },
                "author": "Nir Rosenfeld",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04671v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04671v2",
                "updated": "2025-02-27T11:26:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    11,
                    26,
                    20,
                    3,
                    58,
                    0
                ],
                "published": "2024-11-07T12:55:17Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    12,
                    55,
                    17,
                    3,
                    312,
                    0
                ],
                "title": "CUIfy the XR: An Open-Source Package to Embed LLM-powered Conversational\n  Agents in XR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CUIfy the XR: An Open-Source Package to Embed LLM-powered Conversational\n  Agents in XR"
                },
                "summary": "Recent developments in computer graphics, machine learning, and sensor\ntechnologies enable numerous opportunities for extended reality (XR) setups for\neveryday life, from skills training to entertainment. With large corporations\noffering affordable consumer-grade head-mounted displays (HMDs), XR will likely\nbecome pervasive, and HMDs will develop as personal devices like smartphones\nand tablets. However, having intelligent spaces and naturalistic interactions\nin XR is as important as technological advances so that users grow their\nengagement in virtual and augmented spaces. To this end, large language model\n(LLM)--powered non-player characters (NPCs) with speech-to-text (STT) and\ntext-to-speech (TTS) models bring significant advantages over conventional or\npre-scripted NPCs for facilitating more natural conversational user interfaces\n(CUIs) in XR. This paper provides the community with an open-source,\ncustomizable, extendable, and privacy-aware Unity package, CUIfy, that\nfacilitates speech-based NPC-user interaction with widely used LLMs, STT, and\nTTS models. Our package also supports multiple LLM-powered NPCs per environment\nand minimizes latency between different computational models through streaming\nto achieve usable interactions between users and NPCs. We publish our source\ncode in the following repository: https://gitlab.lrz.de/hctl/cuify",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in computer graphics, machine learning, and sensor\ntechnologies enable numerous opportunities for extended reality (XR) setups for\neveryday life, from skills training to entertainment. With large corporations\noffering affordable consumer-grade head-mounted displays (HMDs), XR will likely\nbecome pervasive, and HMDs will develop as personal devices like smartphones\nand tablets. However, having intelligent spaces and naturalistic interactions\nin XR is as important as technological advances so that users grow their\nengagement in virtual and augmented spaces. To this end, large language model\n(LLM)--powered non-player characters (NPCs) with speech-to-text (STT) and\ntext-to-speech (TTS) models bring significant advantages over conventional or\npre-scripted NPCs for facilitating more natural conversational user interfaces\n(CUIs) in XR. This paper provides the community with an open-source,\ncustomizable, extendable, and privacy-aware Unity package, CUIfy, that\nfacilitates speech-based NPC-user interaction with widely used LLMs, STT, and\nTTS models. Our package also supports multiple LLM-powered NPCs per environment\nand minimizes latency between different computational models through streaming\nto achieve usable interactions between users and NPCs. We publish our source\ncode in the following repository: https://gitlab.lrz.de/hctl/cuify"
                },
                "authors": [
                    {
                        "name": "Kadir Burak Buldu"
                    },
                    {
                        "name": "Süleyman Özdel"
                    },
                    {
                        "name": "Ka Hei Carrie Lau"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Daniel Saad"
                    },
                    {
                        "name": "Sofie Schönborn"
                    },
                    {
                        "name": "Auxane Boch"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    },
                    {
                        "name": "Efe Bozkir"
                    }
                ],
                "author_detail": {
                    "name": "Efe Bozkir"
                },
                "author": "Efe Bozkir",
                "arxiv_comment": "7th IEEE International Conference on Artificial Intelligence &\n  eXtended and Virtual Reality (IEEE AIxVR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04671v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04671v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01928v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01928v2",
                "updated": "2025-02-27T11:25:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    11,
                    25,
                    2,
                    3,
                    58,
                    0
                ],
                "published": "2024-12-02T19:30:36Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    19,
                    30,
                    36,
                    0,
                    337,
                    0
                ],
                "title": "MALT: Improving Reasoning with Multi-Agent LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MALT: Improving Reasoning with Multi-Agent LLM Training"
                },
                "summary": "Large Language Models (LLMs) often produce answers with a single\nchain-of-thought, which restricts their ability to explore reasoning paths or\nself-correct flawed outputs in complex tasks. In this paper, we introduce MALT\n(Multi-Agent LLM Training), a novel post-training strategy that divides the\nreasoning process into generation, verification, and refinement steps using a\nsequential pipeline of heterogeneous agents. During data generation, each agent\nis repeatedly sampled to form a multi-agent search tree, where final outputs\nare graded against ground-truth data. We then apply value iteration to\npropagate reward signals back to each role-conditioned model, automatically\nproducing multi-agent post-training data without human or teacher-model\nsupervision. Our off-policy approach allows each agent to specialize by\nlearning from correct and incorrect trajectories, ultimately improving the\nend-to-end reasoning chain. On MATH, GSM8K, and CSQA, MALT surpasses the same\nbaseline LLM with a relative improvement of 15.66%, 7.42%, and 9.40%\nrespectively, making it an important advance towards multi-agent cooperative\ntraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often produce answers with a single\nchain-of-thought, which restricts their ability to explore reasoning paths or\nself-correct flawed outputs in complex tasks. In this paper, we introduce MALT\n(Multi-Agent LLM Training), a novel post-training strategy that divides the\nreasoning process into generation, verification, and refinement steps using a\nsequential pipeline of heterogeneous agents. During data generation, each agent\nis repeatedly sampled to form a multi-agent search tree, where final outputs\nare graded against ground-truth data. We then apply value iteration to\npropagate reward signals back to each role-conditioned model, automatically\nproducing multi-agent post-training data without human or teacher-model\nsupervision. Our off-policy approach allows each agent to specialize by\nlearning from correct and incorrect trajectories, ultimately improving the\nend-to-end reasoning chain. On MATH, GSM8K, and CSQA, MALT surpasses the same\nbaseline LLM with a relative improvement of 15.66%, 7.42%, and 9.40%\nrespectively, making it an important advance towards multi-agent cooperative\ntraining."
                },
                "authors": [
                    {
                        "name": "Sumeet Ramesh Motwani"
                    },
                    {
                        "name": "Chandler Smith"
                    },
                    {
                        "name": "Rocktim Jyoti Das"
                    },
                    {
                        "name": "Rafael Rafailov"
                    },
                    {
                        "name": "Ivan Laptev"
                    },
                    {
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "name": "Fabio Pizzati"
                    },
                    {
                        "name": "Ronald Clark"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Schroeder de Witt"
                },
                "author": "Christian Schroeder de Witt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01928v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01928v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19996v1",
                "updated": "2025-02-27T11:22:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    11,
                    22,
                    25,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T11:22:25Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    11,
                    22,
                    25,
                    3,
                    58,
                    0
                ],
                "title": "Modern DDoS Threats and Countermeasures: Insights into Emerging Attacks\n  and Detection Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern DDoS Threats and Countermeasures: Insights into Emerging Attacks\n  and Detection Strategies"
                },
                "summary": "Distributed Denial of Service (DDoS) attacks persist as significant threats\nto online services and infrastructure, evolving rapidly in sophistication and\neluding traditional detection mechanisms. This evolution demands a\ncomprehensive examination of current trends in DDoS attacks and the efficacy of\nmodern detection strategies. This paper offers an comprehensive survey of\nemerging DDoS attacks and detection strategies over the past decade. We delve\ninto the diversification of attack targets, extending beyond conventional web\nservices to include newer network protocols and systems, and the adoption of\nadvanced adversarial tactics. Additionally, we review current detection\ntechniques, highlighting essential features that modern systems must integrate\nto effectively neutralize these evolving threats. Given the technological\ndemands of contemporary network systems, such as high-volume and in-line packet\nprocessing capabilities, we also explore how innovative hardware technologies\nlike programmable switches can significantly enhance the development and\ndeployment of robust DDoS detection systems. We conclude by identifying open\nproblems and proposing future directions for DDoS research. In particular, our\nsurvey sheds light on the investigation of DDoS attack surfaces for emerging\nsystems, protocols, and adversarial strategies. Moreover, we outlines critical\nopen questions in the development of effective detection systems, e.g., the\ncreation of defense mechanisms independent of control planes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Denial of Service (DDoS) attacks persist as significant threats\nto online services and infrastructure, evolving rapidly in sophistication and\neluding traditional detection mechanisms. This evolution demands a\ncomprehensive examination of current trends in DDoS attacks and the efficacy of\nmodern detection strategies. This paper offers an comprehensive survey of\nemerging DDoS attacks and detection strategies over the past decade. We delve\ninto the diversification of attack targets, extending beyond conventional web\nservices to include newer network protocols and systems, and the adoption of\nadvanced adversarial tactics. Additionally, we review current detection\ntechniques, highlighting essential features that modern systems must integrate\nto effectively neutralize these evolving threats. Given the technological\ndemands of contemporary network systems, such as high-volume and in-line packet\nprocessing capabilities, we also explore how innovative hardware technologies\nlike programmable switches can significantly enhance the development and\ndeployment of robust DDoS detection systems. We conclude by identifying open\nproblems and proposing future directions for DDoS research. In particular, our\nsurvey sheds light on the investigation of DDoS attack surfaces for emerging\nsystems, protocols, and adversarial strategies. Moreover, we outlines critical\nopen questions in the development of effective detection systems, e.g., the\ncreation of defense mechanisms independent of control planes."
                },
                "authors": [
                    {
                        "name": "Jincheng Wang"
                    },
                    {
                        "name": "Le Yu"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Xiapu Luo"
                    }
                ],
                "author_detail": {
                    "name": "Xiapu Luo"
                },
                "author": "Xiapu Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13461v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13461v2",
                "updated": "2025-02-27T11:13:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    11,
                    13,
                    19,
                    3,
                    58,
                    0
                ],
                "published": "2024-10-17T11:46:33Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    11,
                    46,
                    33,
                    3,
                    291,
                    0
                ],
                "title": "Progressive Mixed-Precision Decoding for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Mixed-Precision Decoding for Efficient LLM Inference"
                },
                "summary": "In spite of the great potential of large language models (LLMs) across\nvarious tasks, their deployment on resource-constrained devices remains\nchallenging due to their excessive computational and memory demands.\nQuantization has emerged as an effective solution by storing weights in reduced\nprecision. However, utilizing low precisions (i.e.~2/3-bit) to substantially\nalleviate the memory-boundedness of LLM decoding, still suffers from\nprohibitive performance drop. In this work, we argue that existing approaches\nfail to explore the diversity in computational patterns, redundancy, and\nsensitivity to approximations of the different phases of LLM inference,\nresorting to a uniform quantization policy throughout. Instead, we propose a\nnovel phase-aware method that selectively allocates precision during different\nphases of LLM inference, achieving both strong context extraction during\nprefill and efficient memory bandwidth utilization during decoding. To further\naddress the memory-boundedness of the decoding phase, we introduce Progressive\nMixed-Precision Decoding (PMPD), a technique that enables the gradual lowering\nof precision deeper in the generated sequence, together with a spectrum of\nprecision-switching schedulers that dynamically drive the precision-lowering\ndecisions in either task-adaptive or prompt-adaptive manner. Extensive\nevaluation across diverse language tasks shows that when targeting Nvidia GPUs,\nPMPD achieves 1.4$-$12.2$\\times$ speedup in matrix-vector multiplications over\nfp16 models, while when targeting an LLM-optimized NPU, our approach delivers a\nthroughput gain of 3.8$-$8.0$\\times$ over fp16 models and up to 1.54$\\times$\nover uniform quantization approaches while preserving the output quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In spite of the great potential of large language models (LLMs) across\nvarious tasks, their deployment on resource-constrained devices remains\nchallenging due to their excessive computational and memory demands.\nQuantization has emerged as an effective solution by storing weights in reduced\nprecision. However, utilizing low precisions (i.e.~2/3-bit) to substantially\nalleviate the memory-boundedness of LLM decoding, still suffers from\nprohibitive performance drop. In this work, we argue that existing approaches\nfail to explore the diversity in computational patterns, redundancy, and\nsensitivity to approximations of the different phases of LLM inference,\nresorting to a uniform quantization policy throughout. Instead, we propose a\nnovel phase-aware method that selectively allocates precision during different\nphases of LLM inference, achieving both strong context extraction during\nprefill and efficient memory bandwidth utilization during decoding. To further\naddress the memory-boundedness of the decoding phase, we introduce Progressive\nMixed-Precision Decoding (PMPD), a technique that enables the gradual lowering\nof precision deeper in the generated sequence, together with a spectrum of\nprecision-switching schedulers that dynamically drive the precision-lowering\ndecisions in either task-adaptive or prompt-adaptive manner. Extensive\nevaluation across diverse language tasks shows that when targeting Nvidia GPUs,\nPMPD achieves 1.4$-$12.2$\\times$ speedup in matrix-vector multiplications over\nfp16 models, while when targeting an LLM-optimized NPU, our approach delivers a\nthroughput gain of 3.8$-$8.0$\\times$ over fp16 models and up to 1.54$\\times$\nover uniform quantization approaches while preserving the output quality."
                },
                "authors": [
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Fuwen Tan"
                    },
                    {
                        "name": "Alexandros Kouris"
                    },
                    {
                        "name": "Royson Lee"
                    },
                    {
                        "name": "Hongxiang Fan"
                    },
                    {
                        "name": "Stylianos I. Venieris"
                    }
                ],
                "author_detail": {
                    "name": "Stylianos I. Venieris"
                },
                "author": "Stylianos I. Venieris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13461v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13461v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05087v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05087v2",
                "updated": "2025-02-27T11:04:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    11,
                    4,
                    13,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-07T17:04:39Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    4,
                    39,
                    4,
                    38,
                    0
                ],
                "title": "Mitigating Unintended Memorization with LoRA in Federated Learning for\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Unintended Memorization with LoRA in Federated Learning for\n  LLMs"
                },
                "summary": "Federated learning (FL) is a popular paradigm for collaborative training\nwhich avoids direct data exposure between clients. However, data privacy issues\nstill remain: FL-trained large language models are capable of memorizing and\ncompleting phrases and sentences contained in training data when given with\ntheir prefixes. Thus, it is possible for adversarial and honest-but-curious\nclients to recover training data of other participants simply through targeted\nprompting. In this work, we demonstrate that a popular and simple fine-tuning\nstrategy, low-rank adaptation (LoRA), reduces memorization during FL up to a\nfactor of 10. We study this effect by performing a medical question-answering\nfine-tuning task and injecting multiple replicas of out-of-distribution\nsensitive sequences drawn from an external clinical dataset. We observe a\nreduction in memorization for a wide variety of Llama 2 and 3 models, and find\nthat LoRA can reduce memorization in centralized learning as well. Furthermore,\nwe show that LoRA can be combined with other privacy-preserving techniques such\nas gradient clipping and Gaussian noising, secure aggregation, and Goldfish\nloss to further improve record-level privacy while maintaining performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) is a popular paradigm for collaborative training\nwhich avoids direct data exposure between clients. However, data privacy issues\nstill remain: FL-trained large language models are capable of memorizing and\ncompleting phrases and sentences contained in training data when given with\ntheir prefixes. Thus, it is possible for adversarial and honest-but-curious\nclients to recover training data of other participants simply through targeted\nprompting. In this work, we demonstrate that a popular and simple fine-tuning\nstrategy, low-rank adaptation (LoRA), reduces memorization during FL up to a\nfactor of 10. We study this effect by performing a medical question-answering\nfine-tuning task and injecting multiple replicas of out-of-distribution\nsensitive sequences drawn from an external clinical dataset. We observe a\nreduction in memorization for a wide variety of Llama 2 and 3 models, and find\nthat LoRA can reduce memorization in centralized learning as well. Furthermore,\nwe show that LoRA can be combined with other privacy-preserving techniques such\nas gradient clipping and Gaussian noising, secure aggregation, and Goldfish\nloss to further improve record-level privacy while maintaining performance."
                },
                "authors": [
                    {
                        "name": "Thierry Bossy"
                    },
                    {
                        "name": "Julien Vignoud"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Juan R. Troncoso Pastoriza"
                    },
                    {
                        "name": "Martin Jaggi"
                    }
                ],
                "author_detail": {
                    "name": "Martin Jaggi"
                },
                "author": "Martin Jaggi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05087v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05087v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19982v1",
                "updated": "2025-02-27T11:03:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    11,
                    3,
                    33,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T11:03:33Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    11,
                    3,
                    33,
                    3,
                    58,
                    0
                ],
                "title": "Erasing Without Remembering: Safeguarding Knowledge Forgetting in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Erasing Without Remembering: Safeguarding Knowledge Forgetting in Large\n  Language Models"
                },
                "summary": "In this paper, we explore machine unlearning from a novel dimension, by\nstudying how to safeguard model unlearning in large language models (LLMs). Our\ngoal is to prevent unlearned models from recalling any related memory of the\ntargeted knowledge.We begin by uncovering a surprisingly simple yet overlooked\nfact: existing methods typically erase only the exact expressions of the\ntargeted knowledge, leaving paraphrased or related information intact. To\nrigorously measure such oversights, we introduce UGBench, the first benchmark\ntailored for evaluating the generalisation performance across 13\nstate-of-the-art methods.UGBench reveals that unlearned models can still recall\nparaphrased answers and retain target facts in intermediate layers. To address\nthis, we propose PERMU, a perturbation-based method that significantly enhances\nthe generalisation capabilities for safeguarding LLM unlearning.Experiments\ndemonstrate that PERMU delivers up to a 50.13% improvement in unlearning while\nmaintaining a 43.53% boost in robust generalisation. Our code can be found in\nhttps://github.com/MaybeLizzy/UGBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore machine unlearning from a novel dimension, by\nstudying how to safeguard model unlearning in large language models (LLMs). Our\ngoal is to prevent unlearned models from recalling any related memory of the\ntargeted knowledge.We begin by uncovering a surprisingly simple yet overlooked\nfact: existing methods typically erase only the exact expressions of the\ntargeted knowledge, leaving paraphrased or related information intact. To\nrigorously measure such oversights, we introduce UGBench, the first benchmark\ntailored for evaluating the generalisation performance across 13\nstate-of-the-art methods.UGBench reveals that unlearned models can still recall\nparaphrased answers and retain target facts in intermediate layers. To address\nthis, we propose PERMU, a perturbation-based method that significantly enhances\nthe generalisation capabilities for safeguarding LLM unlearning.Experiments\ndemonstrate that PERMU delivers up to a 50.13% improvement in unlearning while\nmaintaining a 43.53% boost in robust generalisation. Our code can be found in\nhttps://github.com/MaybeLizzy/UGBench."
                },
                "authors": [
                    {
                        "name": "Huazheng Wang"
                    },
                    {
                        "name": "Yongcheng Jing"
                    },
                    {
                        "name": "Haifeng Sun"
                    },
                    {
                        "name": "Yingjie Wang"
                    },
                    {
                        "name": "Jingyu Wang"
                    },
                    {
                        "name": "Jianxin Liao"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19981v1",
                "updated": "2025-02-27T11:03:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    11,
                    3,
                    27,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T11:03:27Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    11,
                    3,
                    27,
                    3,
                    58,
                    0
                ],
                "title": "The Lookahead Limitation: Why Multi-Operand Addition is Hard for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Lookahead Limitation: Why Multi-Operand Addition is Hard for LLMs"
                },
                "summary": "Autoregressive large language models (LLMs) exhibit impressive performance\nacross various tasks but struggle with simple arithmetic, such as addition of\ntwo or more operands. We show that this struggle arises from LLMs' use of a\nsimple one-digit lookahead heuristic, which works fairly well (but not perfect)\nfor two-operand addition but fails in multi-operand cases, where the carry-over\nlogic is more complex. Our probing experiments and digit-wise accuracy\nevaluation show that LLMs fail precisely where a one-digit lookahead is\ninsufficient to account for cascading carries. We analyze the impact of\ntokenization strategies on arithmetic performance and show that all\ninvestigated models, regardless of tokenization, are inherently limited in the\naddition of multiple operands due to their reliance on a one-digit lookahead\nheuristic. Our findings reveal fundamental limitations that prevent LLMs from\ngeneralizing to more complex numerical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive large language models (LLMs) exhibit impressive performance\nacross various tasks but struggle with simple arithmetic, such as addition of\ntwo or more operands. We show that this struggle arises from LLMs' use of a\nsimple one-digit lookahead heuristic, which works fairly well (but not perfect)\nfor two-operand addition but fails in multi-operand cases, where the carry-over\nlogic is more complex. Our probing experiments and digit-wise accuracy\nevaluation show that LLMs fail precisely where a one-digit lookahead is\ninsufficient to account for cascading carries. We analyze the impact of\ntokenization strategies on arithmetic performance and show that all\ninvestigated models, regardless of tokenization, are inherently limited in the\naddition of multiple operands due to their reliance on a one-digit lookahead\nheuristic. Our findings reveal fundamental limitations that prevent LLMs from\ngeneralizing to more complex numerical reasoning."
                },
                "authors": [
                    {
                        "name": "Tanja Baeumel"
                    },
                    {
                        "name": "Josef van Genabith"
                    },
                    {
                        "name": "Simon Ostermann"
                    }
                ],
                "author_detail": {
                    "name": "Simon Ostermann"
                },
                "author": "Simon Ostermann",
                "arxiv_comment": "Pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19980v1",
                "updated": "2025-02-27T11:03:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    11,
                    3,
                    24,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T11:03:24Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    11,
                    3,
                    24,
                    3,
                    58,
                    0
                ],
                "title": "Can Textual Gradient Work in Federated Learning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Textual Gradient Work in Federated Learning?"
                },
                "summary": "Recent studies highlight the promise of LLM-based prompt optimization,\nespecially with TextGrad, which automates differentiation'' via texts and\nbackpropagates textual feedback. This approach facilitates training in various\nreal-world applications that do not support numerical gradient propagation or\nloss calculation. In this paper, we systematically explore the potential and\nchallenges of incorporating textual gradient into Federated Learning (FL). Our\ncontributions are fourfold. Firstly, we introduce a novel FL paradigm,\nFederated Textual Gradient (FedTextGrad), that allows clients to upload locally\noptimized prompts derived from textual gradients, while the server aggregates\nthe received prompts. Unlike traditional FL frameworks, which are designed for\nnumerical aggregation, FedTextGrad is specifically tailored for handling\ntextual data, expanding the applicability of FL to a broader range of problems\nthat lack well-defined numerical loss functions. Secondly, building on this\ndesign, we conduct extensive experiments to explore the feasibility of\nFedTextGrad. Our findings highlight the importance of properly tuning key\nfactors (e.g., local steps) in FL training. Thirdly, we highlight a major\nchallenge in FedTextGrad aggregation: retaining essential information from\ndistributed prompt updates. Last but not least, in response to this issue, we\nimprove the vanilla variant of FedTextGrad by providing actionable guidance to\nthe LLM when summarizing client prompts by leveraging the Uniform Information\nDensity principle. Through this principled study, we enable the adoption of\ntextual gradients in FL for optimizing LLMs, identify important issues, and\npinpoint future directions, thereby opening up a new research area that\nwarrants further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies highlight the promise of LLM-based prompt optimization,\nespecially with TextGrad, which automates differentiation'' via texts and\nbackpropagates textual feedback. This approach facilitates training in various\nreal-world applications that do not support numerical gradient propagation or\nloss calculation. In this paper, we systematically explore the potential and\nchallenges of incorporating textual gradient into Federated Learning (FL). Our\ncontributions are fourfold. Firstly, we introduce a novel FL paradigm,\nFederated Textual Gradient (FedTextGrad), that allows clients to upload locally\noptimized prompts derived from textual gradients, while the server aggregates\nthe received prompts. Unlike traditional FL frameworks, which are designed for\nnumerical aggregation, FedTextGrad is specifically tailored for handling\ntextual data, expanding the applicability of FL to a broader range of problems\nthat lack well-defined numerical loss functions. Secondly, building on this\ndesign, we conduct extensive experiments to explore the feasibility of\nFedTextGrad. Our findings highlight the importance of properly tuning key\nfactors (e.g., local steps) in FL training. Thirdly, we highlight a major\nchallenge in FedTextGrad aggregation: retaining essential information from\ndistributed prompt updates. Last but not least, in response to this issue, we\nimprove the vanilla variant of FedTextGrad by providing actionable guidance to\nthe LLM when summarizing client prompts by leveraging the Uniform Information\nDensity principle. Through this principled study, we enable the adoption of\ntextual gradients in FL for optimizing LLMs, identify important issues, and\npinpoint future directions, thereby opening up a new research area that\nwarrants further investigation."
                },
                "authors": [
                    {
                        "name": "Minghui Chen"
                    },
                    {
                        "name": "Ruinan Jin"
                    },
                    {
                        "name": "Wenlong Deng"
                    },
                    {
                        "name": "Yuanyuan Chen"
                    },
                    {
                        "name": "Zhi Huang"
                    },
                    {
                        "name": "Han Yu"
                    },
                    {
                        "name": "Xiaoxiao Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxiao Li"
                },
                "author": "Xiaoxiao Li",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01492v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01492v2",
                "updated": "2025-02-27T10:54:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    10,
                    54,
                    51,
                    3,
                    58,
                    0
                ],
                "published": "2024-11-03T09:17:56Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    9,
                    17,
                    56,
                    6,
                    308,
                    0
                ],
                "title": "EEE-Bench: A Comprehensive Multimodal Electrical And Electronics\n  Engineering Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEE-Bench: A Comprehensive Multimodal Electrical And Electronics\n  Engineering Benchmark"
                },
                "summary": "Recent studies on large language models (LLMs) and large multimodal models\n(LMMs) have demonstrated promising skills in various domains including science\nand mathematics. However, their capability in more challenging and real-world\nrelated scenarios like engineering has not been systematically studied. To\nbridge this gap, we propose EEE-Bench, a multimodal benchmark aimed at\nassessing LMMs' capabilities in solving practical engineering tasks, using\nelectrical and electronics engineering (EEE) as the testbed. Our benchmark\nconsists of 2860 carefully curated problems spanning 10 essential subdomains\nsuch as analog circuits, control systems, etc. Compared to benchmarks in other\ndomains, engineering problems are intrinsically 1) more visually complex and\nversatile and 2) less deterministic in solutions. Successful solutions to these\nproblems often demand more-than-usual rigorous integration of visual and\ntextual information as models need to understand intricate images like abstract\ncircuits and system diagrams while taking professional instructions, making\nthem excellent candidates for LMM evaluations. Alongside EEE-Bench, we provide\nextensive quantitative evaluations and fine-grained analysis of 17 widely-used\nopen and closed-sourced LLMs and LMMs. Our results demonstrate notable\ndeficiencies of current foundation models in EEE, with an average performance\nranging from 19.48% to 46.78%. Finally, we reveal and explore a critical\nshortcoming in LMMs which we term laziness: the tendency to take shortcuts by\nrelying on the text while overlooking the visual context when reasoning for\ntechnical image problems. In summary, we believe EEE-Bench not only reveals\nsome noteworthy limitations of LMMs but also provides a valuable resource for\nadvancing research on their application in practical engineering tasks, driving\nfuture improvements in their capability to handle complex, real-world\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies on large language models (LLMs) and large multimodal models\n(LMMs) have demonstrated promising skills in various domains including science\nand mathematics. However, their capability in more challenging and real-world\nrelated scenarios like engineering has not been systematically studied. To\nbridge this gap, we propose EEE-Bench, a multimodal benchmark aimed at\nassessing LMMs' capabilities in solving practical engineering tasks, using\nelectrical and electronics engineering (EEE) as the testbed. Our benchmark\nconsists of 2860 carefully curated problems spanning 10 essential subdomains\nsuch as analog circuits, control systems, etc. Compared to benchmarks in other\ndomains, engineering problems are intrinsically 1) more visually complex and\nversatile and 2) less deterministic in solutions. Successful solutions to these\nproblems often demand more-than-usual rigorous integration of visual and\ntextual information as models need to understand intricate images like abstract\ncircuits and system diagrams while taking professional instructions, making\nthem excellent candidates for LMM evaluations. Alongside EEE-Bench, we provide\nextensive quantitative evaluations and fine-grained analysis of 17 widely-used\nopen and closed-sourced LLMs and LMMs. Our results demonstrate notable\ndeficiencies of current foundation models in EEE, with an average performance\nranging from 19.48% to 46.78%. Finally, we reveal and explore a critical\nshortcoming in LMMs which we term laziness: the tendency to take shortcuts by\nrelying on the text while overlooking the visual context when reasoning for\ntechnical image problems. In summary, we believe EEE-Bench not only reveals\nsome noteworthy limitations of LMMs but also provides a valuable resource for\nadvancing research on their application in practical engineering tasks, driving\nfuture improvements in their capability to handle complex, real-world\nscenarios."
                },
                "authors": [
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Jike Zhong"
                    },
                    {
                        "name": "Tianle Chen"
                    },
                    {
                        "name": "Yuxiang Lai"
                    },
                    {
                        "name": "Konstantinos Psounis"
                    }
                ],
                "author_detail": {
                    "name": "Konstantinos Psounis"
                },
                "author": "Konstantinos Psounis",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01492v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01492v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19965v1",
                "updated": "2025-02-27T10:45:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    10,
                    45,
                    27,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T10:45:27Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    10,
                    45,
                    27,
                    3,
                    58,
                    0
                ],
                "title": "Deterministic or probabilistic? The psychology of LLMs as random number\n  generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deterministic or probabilistic? The psychology of LLMs as random number\n  generators"
                },
                "summary": "Large Language Models (LLMs) have transformed text generation through\ninherently probabilistic context-aware mechanisms, mimicking human natural\nlanguage. In this paper, we systematically investigate the performance of\nvarious LLMs when generating random numbers, considering diverse configurations\nsuch as different model architectures, numerical ranges, temperature, and\nprompt languages. Our results reveal that, despite their stochastic\ntransformers-based architecture, these models often exhibit deterministic\nresponses when prompted for random numerical outputs. In particular, we find\nsignificant differences when changing the model, as well as the prompt\nlanguage, attributing this phenomenon to biases deeply embedded within the\ntraining data. Models such as DeepSeek-R1 can shed some light on the internal\nreasoning process of LLMs, despite arriving to similar results. These biases\ninduce predictable patterns that undermine genuine randomness, as LLMs are\nnothing but reproducing our own human cognitive biases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed text generation through\ninherently probabilistic context-aware mechanisms, mimicking human natural\nlanguage. In this paper, we systematically investigate the performance of\nvarious LLMs when generating random numbers, considering diverse configurations\nsuch as different model architectures, numerical ranges, temperature, and\nprompt languages. Our results reveal that, despite their stochastic\ntransformers-based architecture, these models often exhibit deterministic\nresponses when prompted for random numerical outputs. In particular, we find\nsignificant differences when changing the model, as well as the prompt\nlanguage, attributing this phenomenon to biases deeply embedded within the\ntraining data. Models such as DeepSeek-R1 can shed some light on the internal\nreasoning process of LLMs, despite arriving to similar results. These biases\ninduce predictable patterns that undermine genuine randomness, as LLMs are\nnothing but reproducing our own human cognitive biases."
                },
                "authors": [
                    {
                        "name": "Javier Coronado-Blázquez"
                    }
                ],
                "author_detail": {
                    "name": "Javier Coronado-Blázquez"
                },
                "author": "Javier Coronado-Blázquez",
                "arxiv_comment": "31 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13305v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13305v3",
                "updated": "2025-02-27T10:38:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    10,
                    38,
                    51,
                    3,
                    58,
                    0
                ],
                "published": "2024-10-17T08:05:02Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    5,
                    2,
                    3,
                    291,
                    0
                ],
                "title": "Reference-Based Post-OCR Processing with LLM for Precise Diacritic Text\n  in Historical Document Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reference-Based Post-OCR Processing with LLM for Precise Diacritic Text\n  in Historical Document Recognition"
                },
                "summary": "Extracting fine-grained OCR text from aged documents in diacritic languages\nremains challenging due to unexpected artifacts, time-induced degradation, and\nlack of datasets. While standalone spell correction approaches have been\nproposed, they show limited performance for historical documents due to\nnumerous possible OCR error combinations and differences between modern and\nclassical corpus distributions. We propose a method utilizing available\ncontent-focused ebooks as a reference base to correct imperfect OCR-generated\ntext, supported by large language models. This technique generates\nhigh-precision pseudo-page-to-page labels for diacritic languages, where small\nstrokes pose significant challenges in historical conditions. The pipeline\neliminates various types of noise from aged documents and addresses issues such\nas missing characters, words, and disordered sequences. Our post-processing\nmethod, which generated a large OCR dataset of classical Vietnamese books,\nachieved a mean grading score of 8.72 on a 10-point scale. This outperformed\nthe state-of-the-art transformer-based Vietnamese spell correction model, which\nscored 7.03 when evaluated on a sampled subset of the dataset. We also trained\na baseline OCR model to assess and compare it with well-known engines.\nExperimental results demonstrate the strength of our baseline model compared to\nwidely used open-source solutions. The resulting dataset will be released\npublicly to support future studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting fine-grained OCR text from aged documents in diacritic languages\nremains challenging due to unexpected artifacts, time-induced degradation, and\nlack of datasets. While standalone spell correction approaches have been\nproposed, they show limited performance for historical documents due to\nnumerous possible OCR error combinations and differences between modern and\nclassical corpus distributions. We propose a method utilizing available\ncontent-focused ebooks as a reference base to correct imperfect OCR-generated\ntext, supported by large language models. This technique generates\nhigh-precision pseudo-page-to-page labels for diacritic languages, where small\nstrokes pose significant challenges in historical conditions. The pipeline\neliminates various types of noise from aged documents and addresses issues such\nas missing characters, words, and disordered sequences. Our post-processing\nmethod, which generated a large OCR dataset of classical Vietnamese books,\nachieved a mean grading score of 8.72 on a 10-point scale. This outperformed\nthe state-of-the-art transformer-based Vietnamese spell correction model, which\nscored 7.03 when evaluated on a sampled subset of the dataset. We also trained\na baseline OCR model to assess and compare it with well-known engines.\nExperimental results demonstrate the strength of our baseline model compared to\nwidely used open-source solutions. The resulting dataset will be released\npublicly to support future studies."
                },
                "authors": [
                    {
                        "name": "Thao Do"
                    },
                    {
                        "name": "Dinh Phu Tran"
                    },
                    {
                        "name": "An Vo"
                    },
                    {
                        "name": "Daeyoung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Daeyoung Kim"
                },
                "author": "Daeyoung Kim",
                "arxiv_comment": "Accepted in the AAAI 2025 (39th) AISI track. Dataset and repo are in\n  the paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13305v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13305v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19954v1",
                "updated": "2025-02-27T10:30:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    10,
                    30,
                    50,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T10:30:50Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    10,
                    30,
                    50,
                    3,
                    58,
                    0
                ],
                "title": "Collaborative Stance Detection via Small-Large Language Model\n  Consistency Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Stance Detection via Small-Large Language Model\n  Consistency Verification"
                },
                "summary": "Stance detection on social media aims to identify attitudes expressed in\ntweets towards specific targets. Current studies prioritize Large Language\nModels (LLMs) over Small Language Models (SLMs) due to the overwhelming\nperformance improving provided by LLMs. However, heavily relying on LLMs for\nstance detection, regardless of the cost, is impractical for real-world social\nmedia monitoring systems that require vast data analysis. To this end, we\npropose \\textbf{\\underline{Co}}llaborative Stance Detection via Small-Large\nLanguage Model Consistency \\textbf{\\underline{Ver}}ification (\\textbf{CoVer})\nframework, which enhances LLM utilization via context-shared batch reasoning\nand logical verification between LLM and SLM. Specifically, instead of\nprocessing each text individually, CoVer processes texts batch-by-batch,\nobtaining stance predictions and corresponding explanations via LLM reasoning\nin a shared context. Then, to exclude the bias caused by context noises, CoVer\nintroduces the SLM for logical consistency verification. Finally, texts that\nrepeatedly exhibit low logical consistency are classified using\nconsistency-weighted aggregation of prior LLM stance predictions. Our\nexperiments show that CoVer outperforms state-of-the-art methods across\nmultiple benchmarks in the zero-shot setting, achieving 0.54 LLM queries per\ntweet while significantly enhancing performance. Our CoVer offers a more\npractical solution for LLM deploying for social media stance detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stance detection on social media aims to identify attitudes expressed in\ntweets towards specific targets. Current studies prioritize Large Language\nModels (LLMs) over Small Language Models (SLMs) due to the overwhelming\nperformance improving provided by LLMs. However, heavily relying on LLMs for\nstance detection, regardless of the cost, is impractical for real-world social\nmedia monitoring systems that require vast data analysis. To this end, we\npropose \\textbf{\\underline{Co}}llaborative Stance Detection via Small-Large\nLanguage Model Consistency \\textbf{\\underline{Ver}}ification (\\textbf{CoVer})\nframework, which enhances LLM utilization via context-shared batch reasoning\nand logical verification between LLM and SLM. Specifically, instead of\nprocessing each text individually, CoVer processes texts batch-by-batch,\nobtaining stance predictions and corresponding explanations via LLM reasoning\nin a shared context. Then, to exclude the bias caused by context noises, CoVer\nintroduces the SLM for logical consistency verification. Finally, texts that\nrepeatedly exhibit low logical consistency are classified using\nconsistency-weighted aggregation of prior LLM stance predictions. Our\nexperiments show that CoVer outperforms state-of-the-art methods across\nmultiple benchmarks in the zero-shot setting, achieving 0.54 LLM queries per\ntweet while significantly enhancing performance. Our CoVer offers a more\npractical solution for LLM deploying for social media stance detection."
                },
                "authors": [
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Zixiang Tang"
                    },
                    {
                        "name": "Teli Liu"
                    },
                    {
                        "name": "Min Liu"
                    }
                ],
                "author_detail": {
                    "name": "Min Liu"
                },
                "author": "Min Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19953v1",
                "updated": "2025-02-27T10:27:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    10,
                    27,
                    48,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T10:27:48Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    10,
                    27,
                    48,
                    3,
                    58,
                    0
                ],
                "title": "GeoEdit: Geometric Knowledge Editing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeoEdit: Geometric Knowledge Editing for Large Language Models"
                },
                "summary": "Regular updates are essential for maintaining up-to-date knowledge in large\nlanguage models (LLMs). Consequently, various model editing methods have been\ndeveloped to update specific knowledge within LLMs. However, training-based\napproaches often struggle to effectively incorporate new knowledge while\npreserving unrelated general knowledge. To address this challenge, we propose a\nnovel framework called Geometric Knowledge Editing (GeoEdit). GeoEdit utilizes\nthe geometric relationships of parameter updates from fine-tuning to\ndifferentiate between neurons associated with new knowledge updates and those\nrelated to general knowledge perturbations. By employing a direction-aware\nknowledge identification method, we avoid updating neurons with directions\napproximately orthogonal to existing knowledge, thus preserving the model's\ngeneralization ability. For the remaining neurons, we integrate both old and\nnew knowledge for aligned directions and apply a \"forget-then-learn\" editing\nstrategy for opposite directions. Additionally, we introduce an\nimportance-guided task vector fusion technique that filters out redundant\ninformation and provides adaptive neuron-level weighting, further enhancing\nmodel editing performance. Extensive experiments on two publicly available\ndatasets demonstrate the superiority of GeoEdit over existing state-of-the-art\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regular updates are essential for maintaining up-to-date knowledge in large\nlanguage models (LLMs). Consequently, various model editing methods have been\ndeveloped to update specific knowledge within LLMs. However, training-based\napproaches often struggle to effectively incorporate new knowledge while\npreserving unrelated general knowledge. To address this challenge, we propose a\nnovel framework called Geometric Knowledge Editing (GeoEdit). GeoEdit utilizes\nthe geometric relationships of parameter updates from fine-tuning to\ndifferentiate between neurons associated with new knowledge updates and those\nrelated to general knowledge perturbations. By employing a direction-aware\nknowledge identification method, we avoid updating neurons with directions\napproximately orthogonal to existing knowledge, thus preserving the model's\ngeneralization ability. For the remaining neurons, we integrate both old and\nnew knowledge for aligned directions and apply a \"forget-then-learn\" editing\nstrategy for opposite directions. Additionally, we introduce an\nimportance-guided task vector fusion technique that filters out redundant\ninformation and provides adaptive neuron-level weighting, further enhancing\nmodel editing performance. Extensive experiments on two publicly available\ndatasets demonstrate the superiority of GeoEdit over existing state-of-the-art\nmethods."
                },
                "authors": [
                    {
                        "name": "Yujie Feng"
                    },
                    {
                        "name": "Liming Zhan"
                    },
                    {
                        "name": "Zexin Lu"
                    },
                    {
                        "name": "Yongxin Xu"
                    },
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Yasha Wang"
                    },
                    {
                        "name": "Jiannong Cao"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11401v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11401v2",
                "updated": "2025-02-27T10:26:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    10,
                    26,
                    26,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-17T03:36:25Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    3,
                    36,
                    25,
                    0,
                    48,
                    0
                ],
                "title": "Following the Autoregressive Nature of LLM Embeddings via Compression\n  and Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Following the Autoregressive Nature of LLM Embeddings via Compression\n  and Alignment"
                },
                "summary": "A new trend uses LLMs as dense text encoders via contrastive learning.\nHowever, since LLM embeddings predict the probability distribution of the next\ntoken, they are inherently generative and distributive, conflicting with\ncontrastive learning, which requires embeddings to capture full-text semantics\nand align via cosine similarity. This discrepancy hinders the full utilization\nof LLMs' pre-training capabilities, resulting in inefficient learning. In\nresponse to this issue, we propose AutoRegEmbed, a new contrastive learning\nmethod built on embedding conditional probability distributions, which\nintegrates two core tasks: information compression and conditional distribution\nalignment. The information compression task encodes text into the embedding\nspace, ensuring that the embedding vectors capture global semantics. The\nconditional distribution alignment task focuses on aligning text embeddings\nwith positive samples embeddings by leveraging the conditional distribution of\nembeddings while simultaneously reducing the likelihood of generating negative\nsamples from text embeddings, thereby achieving embedding alignment and\nuniformity. Experimental results demonstrate that our method significantly\noutperforms traditional contrastive learning approaches and achieves\nperformance comparable to state-of-the-art models when using the same amount of\ndata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new trend uses LLMs as dense text encoders via contrastive learning.\nHowever, since LLM embeddings predict the probability distribution of the next\ntoken, they are inherently generative and distributive, conflicting with\ncontrastive learning, which requires embeddings to capture full-text semantics\nand align via cosine similarity. This discrepancy hinders the full utilization\nof LLMs' pre-training capabilities, resulting in inefficient learning. In\nresponse to this issue, we propose AutoRegEmbed, a new contrastive learning\nmethod built on embedding conditional probability distributions, which\nintegrates two core tasks: information compression and conditional distribution\nalignment. The information compression task encodes text into the embedding\nspace, ensuring that the embedding vectors capture global semantics. The\nconditional distribution alignment task focuses on aligning text embeddings\nwith positive samples embeddings by leveraging the conditional distribution of\nembeddings while simultaneously reducing the likelihood of generating negative\nsamples from text embeddings, thereby achieving embedding alignment and\nuniformity. Experimental results demonstrate that our method significantly\noutperforms traditional contrastive learning approaches and achieves\nperformance comparable to state-of-the-art models when using the same amount of\ndata."
                },
                "authors": [
                    {
                        "name": "Jingcheng Deng"
                    },
                    {
                        "name": "Zhongtao Jiang"
                    },
                    {
                        "name": "Liang Pang"
                    },
                    {
                        "name": "Liwei Chen"
                    },
                    {
                        "name": "Kun Xu"
                    },
                    {
                        "name": "Zihao Wei"
                    },
                    {
                        "name": "Huawei Shen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11401v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11401v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11843v2",
                "updated": "2025-02-27T10:11:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    10,
                    11,
                    6,
                    3,
                    58,
                    0
                ],
                "published": "2024-08-07T17:14:58Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    17,
                    14,
                    58,
                    2,
                    220,
                    0
                ],
                "title": "Identifying and Mitigating Social Bias Knowledge in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying and Mitigating Social Bias Knowledge in Language Models"
                },
                "summary": "Generating fair and accurate predictions plays a pivotal role in deploying\nlarge language models (LLMs) in the real world. However, existing debiasing\nmethods inevitably generate unfair or incorrect predictions as they are\ndesigned and evaluated to achieve parity across different social groups but\nleave aside individual commonsense facts, resulting in modified knowledge that\nelicits unreasonable or undesired predictions. In this paper, we first\nestablish a new bias mitigation benchmark, BiaScope, which systematically\nassesses performance by leveraging newly constructed datasets and metrics on\nknowledge retention and generalization. Then, we propose a novel debiasing\napproach, Fairness Stamp (FAST), which enables fine-grained calibration of\nindividual social biases. FAST identifies the decisive layer responsible for\nstoring social biases and then calibrates its outputs by integrating a small\nmodular network, considering both bias mitigation and knowledge-preserving\ndemands. Comprehensive experiments demonstrate that FAST surpasses\nstate-of-the-art baselines with superior debiasing performance while not\ncompromising the overall model capability for knowledge retention and\ndownstream predictions. This highlights the potential of fine-grained debiasing\nstrategies to achieve fairness in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating fair and accurate predictions plays a pivotal role in deploying\nlarge language models (LLMs) in the real world. However, existing debiasing\nmethods inevitably generate unfair or incorrect predictions as they are\ndesigned and evaluated to achieve parity across different social groups but\nleave aside individual commonsense facts, resulting in modified knowledge that\nelicits unreasonable or undesired predictions. In this paper, we first\nestablish a new bias mitigation benchmark, BiaScope, which systematically\nassesses performance by leveraging newly constructed datasets and metrics on\nknowledge retention and generalization. Then, we propose a novel debiasing\napproach, Fairness Stamp (FAST), which enables fine-grained calibration of\nindividual social biases. FAST identifies the decisive layer responsible for\nstoring social biases and then calibrates its outputs by integrating a small\nmodular network, considering both bias mitigation and knowledge-preserving\ndemands. Comprehensive experiments demonstrate that FAST surpasses\nstate-of-the-art baselines with superior debiasing performance while not\ncompromising the overall model capability for knowledge retention and\ndownstream predictions. This highlights the potential of fine-grained debiasing\nstrategies to achieve fairness in LLMs."
                },
                "authors": [
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Jianfei Yang"
                    },
                    {
                        "name": "Joey Tianyi Zhou"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "arxiv_comment": "NAACL 2025 Findings. arXiv admin note: substantial text overlap with\n  arXiv:2405.09341",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19928v1",
                "updated": "2025-02-27T09:56:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    9,
                    56,
                    55,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T09:56:55Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    9,
                    56,
                    55,
                    3,
                    58,
                    0
                ],
                "title": "RIS-Aided Positioning Under Adverse Conditions: Interference from\n  Unauthorized RIS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIS-Aided Positioning Under Adverse Conditions: Interference from\n  Unauthorized RIS"
                },
                "summary": "Positioning technology, which aims to determine the geometric information of\na device in a global coordinate, is a key component in integrated sensing and\ncommunication systems. In addition to traditional active anchor-based\npositioning systems, reconfigurable intelligent surfaces (RIS) have shown great\npotential for enhancing system performance. However, their ability to\nmanipulate electromagnetic waves and ease of deployment pose potential risks,\nas unauthorized RIS may be intentionally introduced to jeopardize the\npositioning service. Such an unauthorized RIS can cause unexpected interference\nin the original localization system, distorting the transmitted signals, and\nleading to degraded positioning accuracy. In this work, we investigate the\nscenario of RIS-aided positioning in the presence of interference from an\nunauthorized RIS. Theoretical lower bounds are employed to analyze the impact\nof unauthorized RIS on channel parameter estimation and positioning accuracy.\nSeveral codebook design strategies for unauthorized RIS are evaluated, and\nvarious system arrangements are discussed. The simulation results show that an\nunauthorized RIS path with a high channel gain or a delay similar to that of\nlegitimate RIS paths leads to poor positioning performance. Furthermore,\nunauthorized RIS generates more effective interference when using directional\nbeamforming codebooks compared to random codebooks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positioning technology, which aims to determine the geometric information of\na device in a global coordinate, is a key component in integrated sensing and\ncommunication systems. In addition to traditional active anchor-based\npositioning systems, reconfigurable intelligent surfaces (RIS) have shown great\npotential for enhancing system performance. However, their ability to\nmanipulate electromagnetic waves and ease of deployment pose potential risks,\nas unauthorized RIS may be intentionally introduced to jeopardize the\npositioning service. Such an unauthorized RIS can cause unexpected interference\nin the original localization system, distorting the transmitted signals, and\nleading to degraded positioning accuracy. In this work, we investigate the\nscenario of RIS-aided positioning in the presence of interference from an\nunauthorized RIS. Theoretical lower bounds are employed to analyze the impact\nof unauthorized RIS on channel parameter estimation and positioning accuracy.\nSeveral codebook design strategies for unauthorized RIS are evaluated, and\nvarious system arrangements are discussed. The simulation results show that an\nunauthorized RIS path with a high channel gain or a delay similar to that of\nlegitimate RIS paths leads to poor positioning performance. Furthermore,\nunauthorized RIS generates more effective interference when using directional\nbeamforming codebooks compared to random codebooks."
                },
                "authors": [
                    {
                        "name": "Mengting Li"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Alireza Pourafzal"
                    },
                    {
                        "name": "Henk Wymeersch"
                    }
                ],
                "author_detail": {
                    "name": "Henk Wymeersch"
                },
                "author": "Henk Wymeersch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19918v1",
                "updated": "2025-02-27T09:40:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    9,
                    40,
                    13,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T09:40:13Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    9,
                    40,
                    13,
                    3,
                    58,
                    0
                ],
                "title": "Meta-Reasoner: Dynamic Guidance for Optimized Inference-time Reasoning\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-Reasoner: Dynamic Guidance for Optimized Inference-time Reasoning\n  in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) increasingly rely on prolonged reasoning chains\nto solve complex tasks. However, this trial-and-error approach often leads to\nhigh computational overhead and error propagation, where early mistakes can\nderail subsequent steps. To address these issues, we introduce Meta-Reasoner, a\nframework that dynamically optimizes inference-time reasoning by enabling LLMs\nto \"think about how to think.\" Drawing inspiration from human meta-cognition\nand dual-process theory, Meta-Reasoner operates as a strategic advisor,\ndecoupling high-level guidance from step-by-step generation. It employs\n\"contextual multi-armed bandits\" to iteratively evaluate reasoning progress,\nand select optimal strategies (e.g., backtrack, clarify ambiguity, restart from\nscratch, or propose alternative approaches), and reallocates computational\nresources toward the most promising paths. Our evaluations on mathematical\nreasoning and puzzles highlight the potential of dynamic reasoning chains to\novercome inherent challenges in the LLM reasoning process and also show promise\nin broader applications, offering a scalable and adaptable solution for\nreasoning-intensive tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly rely on prolonged reasoning chains\nto solve complex tasks. However, this trial-and-error approach often leads to\nhigh computational overhead and error propagation, where early mistakes can\nderail subsequent steps. To address these issues, we introduce Meta-Reasoner, a\nframework that dynamically optimizes inference-time reasoning by enabling LLMs\nto \"think about how to think.\" Drawing inspiration from human meta-cognition\nand dual-process theory, Meta-Reasoner operates as a strategic advisor,\ndecoupling high-level guidance from step-by-step generation. It employs\n\"contextual multi-armed bandits\" to iteratively evaluate reasoning progress,\nand select optimal strategies (e.g., backtrack, clarify ambiguity, restart from\nscratch, or propose alternative approaches), and reallocates computational\nresources toward the most promising paths. Our evaluations on mathematical\nreasoning and puzzles highlight the potential of dynamic reasoning chains to\novercome inherent challenges in the LLM reasoning process and also show promise\nin broader applications, offering a scalable and adaptable solution for\nreasoning-intensive tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Sui"
                    },
                    {
                        "name": "Yufei He"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Simeng Han"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19917v1",
                "updated": "2025-02-27T09:37:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    9,
                    37,
                    30,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T09:37:30Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    9,
                    37,
                    30,
                    3,
                    58,
                    0
                ],
                "title": "Picking the Cream of the Crop: Visual-Centric Data Selection with\n  Collaborative Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Picking the Cream of the Crop: Visual-Centric Data Selection with\n  Collaborative Agents"
                },
                "summary": "To improve Multimodal Large Language Models' (MLLMs) ability to process\nimages and complex instructions, researchers predominantly curate large-scale\nvisual instruction tuning datasets, which are either sourced from existing\nvision tasks or synthetically generated using LLMs and image descriptions.\nHowever, they often suffer from critical flaws, including misaligned\ninstruction-image pairs and low-quality images. Such issues hinder training\nefficiency and limit performance improvements, as models waste resources on\nnoisy or irrelevant data with minimal benefit to overall capability. To address\nthis issue, we propose a \\textbf{Vi}sual-Centric \\textbf{S}election approach\nvia \\textbf{A}gents Collaboration (ViSA), which centers on image quality\nassessment and image-instruction relevance evaluation. Specifically, our\napproach consists of 1) an image information quantification method via visual\nagents collaboration to select images with rich visual information, and 2) a\nvisual-centric instruction quality assessment method to select high-quality\ninstruction data related to high-quality images. Finally, we reorganize 80K\ninstruction data from large open-source datasets. Extensive experiments\ndemonstrate that ViSA outperforms or is comparable to current state-of-the-art\nmodels on seven benchmarks, using only 2.5\\% of the original data, highlighting\nthe efficiency of our data selection approach. Moreover, we conduct ablation\nstudies to validate the effectiveness of each component of our method. The code\nis available at https://github.com/HITsz-TMG/ViSA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To improve Multimodal Large Language Models' (MLLMs) ability to process\nimages and complex instructions, researchers predominantly curate large-scale\nvisual instruction tuning datasets, which are either sourced from existing\nvision tasks or synthetically generated using LLMs and image descriptions.\nHowever, they often suffer from critical flaws, including misaligned\ninstruction-image pairs and low-quality images. Such issues hinder training\nefficiency and limit performance improvements, as models waste resources on\nnoisy or irrelevant data with minimal benefit to overall capability. To address\nthis issue, we propose a \\textbf{Vi}sual-Centric \\textbf{S}election approach\nvia \\textbf{A}gents Collaboration (ViSA), which centers on image quality\nassessment and image-instruction relevance evaluation. Specifically, our\napproach consists of 1) an image information quantification method via visual\nagents collaboration to select images with rich visual information, and 2) a\nvisual-centric instruction quality assessment method to select high-quality\ninstruction data related to high-quality images. Finally, we reorganize 80K\ninstruction data from large open-source datasets. Extensive experiments\ndemonstrate that ViSA outperforms or is comparable to current state-of-the-art\nmodels on seven benchmarks, using only 2.5\\% of the original data, highlighting\nthe efficiency of our data selection approach. Moreover, we conduct ablation\nstudies to validate the effectiveness of each component of our method. The code\nis available at https://github.com/HITsz-TMG/ViSA."
                },
                "authors": [
                    {
                        "name": "Zhenyu Liu"
                    },
                    {
                        "name": "Yunxin Li"
                    },
                    {
                        "name": "Baotian Hu"
                    },
                    {
                        "name": "Wenhan Luo"
                    },
                    {
                        "name": "Yaowei Wang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19915v1",
                "updated": "2025-02-27T09:36:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    9,
                    36,
                    27,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T09:36:27Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    9,
                    36,
                    27,
                    3,
                    58,
                    0
                ],
                "title": "LLM-driven Effective Knowledge Tracing by Integrating Dual-channel\n  Difficulty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-driven Effective Knowledge Tracing by Integrating Dual-channel\n  Difficulty"
                },
                "summary": "Knowledge Tracing (KT) is a fundamental technology in intelligent tutoring\nsystems used to simulate changes in students' knowledge state during learning,\ntrack personalized knowledge mastery, and predict performance. However, current\nKT models face three major challenges: (1) When encountering new questions,\nmodels face cold-start problems due to sparse interaction records, making\nprecise modeling difficult; (2) Traditional models only use historical\ninteraction records for student personalization modeling, unable to accurately\ntrack individual mastery levels, resulting in unclear personalized modeling;\n(3) The decision-making process is opaque to educators, making it challenging\nfor them to understand model judgments. To address these challenges, we propose\na novel Dual-channel Difficulty-aware Knowledge Tracing (DDKT) framework that\nutilizes Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG)\nfor subjective difficulty assessment, while integrating difficulty bias-aware\nalgorithms and student mastery algorithms for precise difficulty measurement.\nOur framework introduces three key innovations: (1) Difficulty Balance\nPerception Sequence (DBPS) - students' subjective perceptions combined with\nobjective difficulty, measuring gaps between LLM-assessed difficulty,\nmathematical-statistical difficulty, and students' subjective perceived\ndifficulty through attention mechanisms; (2) Difficulty Mastery Ratio (DMR) -\nprecise modeling of student mastery levels through different difficulty zones;\n(3) Knowledge State Update Mechanism - implementing personalized knowledge\nacquisition through gated networks and updating student knowledge state.\nExperimental results on two real datasets show our method consistently\noutperforms nine baseline models, improving AUC metrics by 2% to 10% while\neffectively addressing cold-start problems and enhancing model\ninterpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Tracing (KT) is a fundamental technology in intelligent tutoring\nsystems used to simulate changes in students' knowledge state during learning,\ntrack personalized knowledge mastery, and predict performance. However, current\nKT models face three major challenges: (1) When encountering new questions,\nmodels face cold-start problems due to sparse interaction records, making\nprecise modeling difficult; (2) Traditional models only use historical\ninteraction records for student personalization modeling, unable to accurately\ntrack individual mastery levels, resulting in unclear personalized modeling;\n(3) The decision-making process is opaque to educators, making it challenging\nfor them to understand model judgments. To address these challenges, we propose\na novel Dual-channel Difficulty-aware Knowledge Tracing (DDKT) framework that\nutilizes Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG)\nfor subjective difficulty assessment, while integrating difficulty bias-aware\nalgorithms and student mastery algorithms for precise difficulty measurement.\nOur framework introduces three key innovations: (1) Difficulty Balance\nPerception Sequence (DBPS) - students' subjective perceptions combined with\nobjective difficulty, measuring gaps between LLM-assessed difficulty,\nmathematical-statistical difficulty, and students' subjective perceived\ndifficulty through attention mechanisms; (2) Difficulty Mastery Ratio (DMR) -\nprecise modeling of student mastery levels through different difficulty zones;\n(3) Knowledge State Update Mechanism - implementing personalized knowledge\nacquisition through gated networks and updating student knowledge state.\nExperimental results on two real datasets show our method consistently\noutperforms nine baseline models, improving AUC metrics by 2% to 10% while\neffectively addressing cold-start problems and enhancing model\ninterpretability."
                },
                "authors": [
                    {
                        "name": "Jiahui Cen"
                    },
                    {
                        "name": "Jianghao Lin"
                    },
                    {
                        "name": "Weizhong Xuan"
                    },
                    {
                        "name": "Dong Zhou"
                    },
                    {
                        "name": "Jin Chen"
                    },
                    {
                        "name": "Aimin Yang"
                    },
                    {
                        "name": "Yongmei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yongmei Zhou"
                },
                "author": "Yongmei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19913v1",
                "updated": "2025-02-27T09:34:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    9,
                    34,
                    23,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T09:34:23Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    9,
                    34,
                    23,
                    3,
                    58,
                    0
                ],
                "title": "SkipPipe: Partial and Reordered Pipelining Framework for Training LLMs\n  in Heterogeneous Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkipPipe: Partial and Reordered Pipelining Framework for Training LLMs\n  in Heterogeneous Networks"
                },
                "summary": "Data and pipeline parallelism are ubiquitous for training of Large Language\nModels (LLM) on distributed nodes. Driven by the need for cost-effective\ntraining, recent work explores efficient communication arrangement for end to\nend training. Motivated by LLM's resistance to layer skipping and layer\nreordering, in this paper, we explore stage (several consecutive layers)\nskipping in pipeline training, and challenge the conventional practice of\nsequential pipeline execution. We derive convergence and throughput constraints\n(guidelines) for pipelining with skipping and swapping pipeline stages. Based\non these constraints, we propose SkipPipe, the first partial pipeline framework\nto reduce the end-to-end training time for LLMs while preserving the\nconvergence. The core of SkipPipe is a path scheduling algorithm that optimizes\nthe paths for individual microbatches and reduces idle time (due to microbatch\ncollisions) on the distributed nodes, complying with the given stage skipping\nratio. We extensively evaluate SkipPipe on LLaMa models from 500M to 8B\nparameters on up to 20 nodes. Our results show that SkipPipe reduces training\niteration time by up to $55\\%$ compared to full pipeline. Our partial pipeline\ntraining also improves resistance to layer omission during inference,\nexperiencing a drop in perplexity of only $7\\%$ when running only half the\nmodel. Our code is available at https://github.com/gensyn-ai/skippipe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data and pipeline parallelism are ubiquitous for training of Large Language\nModels (LLM) on distributed nodes. Driven by the need for cost-effective\ntraining, recent work explores efficient communication arrangement for end to\nend training. Motivated by LLM's resistance to layer skipping and layer\nreordering, in this paper, we explore stage (several consecutive layers)\nskipping in pipeline training, and challenge the conventional practice of\nsequential pipeline execution. We derive convergence and throughput constraints\n(guidelines) for pipelining with skipping and swapping pipeline stages. Based\non these constraints, we propose SkipPipe, the first partial pipeline framework\nto reduce the end-to-end training time for LLMs while preserving the\nconvergence. The core of SkipPipe is a path scheduling algorithm that optimizes\nthe paths for individual microbatches and reduces idle time (due to microbatch\ncollisions) on the distributed nodes, complying with the given stage skipping\nratio. We extensively evaluate SkipPipe on LLaMa models from 500M to 8B\nparameters on up to 20 nodes. Our results show that SkipPipe reduces training\niteration time by up to $55\\%$ compared to full pipeline. Our partial pipeline\ntraining also improves resistance to layer omission during inference,\nexperiencing a drop in perplexity of only $7\\%$ when running only half the\nmodel. Our code is available at https://github.com/gensyn-ai/skippipe."
                },
                "authors": [
                    {
                        "name": "Nikolay Blagoev"
                    },
                    {
                        "name": "Lydia Yiyu Chen"
                    },
                    {
                        "name": "Oğuzhan Ersoy"
                    }
                ],
                "author_detail": {
                    "name": "Oğuzhan Ersoy"
                },
                "author": "Oğuzhan Ersoy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13758v2",
                "updated": "2025-02-27T09:26:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    9,
                    26,
                    10,
                    3,
                    58,
                    0
                ],
                "published": "2024-02-21T12:35:19Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    12,
                    35,
                    19,
                    2,
                    52,
                    0
                ],
                "title": "Factual consistency evaluation of summarization in the Era of large\n  language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Factual consistency evaluation of summarization in the Era of large\n  language models"
                },
                "summary": "Factual inconsistency with source documents in automatically generated\nsummaries can lead to misinformation or pose risks. Existing factual\nconsistency (FC) metrics are constrained by their performance, efficiency, and\nexplainability. Recent advances in Large language models (LLMs) have\ndemonstrated remarkable potential in text evaluation but their effectiveness in\nassessing FC in summarization remains underexplored. Prior research has mostly\nfocused on proprietary LLMs, leaving essential factors that affect their\nassessment capabilities unexplored. Additionally, current FC evaluation\nbenchmarks are restricted to news articles, casting doubt on the generality of\nthe FC methods tested on them. In this paper, we first address the gap by\nintroducing TreatFact a dataset of LLM-generated summaries of clinical texts,\nannotated for FC by domain experts. Moreover, we benchmark 11 LLMs for FC\nevaluation across news and clinical domains and analyse the impact of model\nsize, prompts, pre-training and fine-tuning data. Our findings reveal that\ndespite proprietary models prevailing on the task, open-source LLMs lag behind.\nNevertheless, there is potential for enhancing the performance of open-source\nLLMs through increasing model size, expanding pre-training data, and developing\nwell-curated fine-tuning data. Experiments on TreatFact suggest that both\nprevious methods and LLM-based evaluators are unable to capture factual\ninconsistencies in clinical summaries, posing a new challenge for FC\nevaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Factual inconsistency with source documents in automatically generated\nsummaries can lead to misinformation or pose risks. Existing factual\nconsistency (FC) metrics are constrained by their performance, efficiency, and\nexplainability. Recent advances in Large language models (LLMs) have\ndemonstrated remarkable potential in text evaluation but their effectiveness in\nassessing FC in summarization remains underexplored. Prior research has mostly\nfocused on proprietary LLMs, leaving essential factors that affect their\nassessment capabilities unexplored. Additionally, current FC evaluation\nbenchmarks are restricted to news articles, casting doubt on the generality of\nthe FC methods tested on them. In this paper, we first address the gap by\nintroducing TreatFact a dataset of LLM-generated summaries of clinical texts,\nannotated for FC by domain experts. Moreover, we benchmark 11 LLMs for FC\nevaluation across news and clinical domains and analyse the impact of model\nsize, prompts, pre-training and fine-tuning data. Our findings reveal that\ndespite proprietary models prevailing on the task, open-source LLMs lag behind.\nNevertheless, there is potential for enhancing the performance of open-source\nLLMs through increasing model size, expanding pre-training data, and developing\nwell-curated fine-tuning data. Experiments on TreatFact suggest that both\nprevious methods and LLM-based evaluators are unable to capture factual\ninconsistencies in clinical summaries, posing a new challenge for FC\nevaluation."
                },
                "authors": [
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Qianqian Xie"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    }
                ],
                "author_detail": {
                    "name": "Sophia Ananiadou"
                },
                "author": "Sophia Ananiadou",
                "arxiv_comment": "published on ESWA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19907v1",
                "updated": "2025-02-27T09:25:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    9,
                    25,
                    50,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T09:25:50Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    9,
                    25,
                    50,
                    3,
                    58,
                    0
                ],
                "title": "Order Doesn't Matter, But Reasoning Does: Training LLMs with\n  Order-Centric Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Order Doesn't Matter, But Reasoning Does: Training LLMs with\n  Order-Centric Augmentation"
                },
                "summary": "Logical reasoning is essential for large language models (LLMs) to ensure\naccurate and coherent inference. However, LLMs struggle with reasoning order\nvariations and fail to generalize across logically equivalent transformations.\nLLMs often rely on fixed sequential patterns rather than true logical\nunderstanding. To address this issue, we introduce an order-centric data\naugmentation framework based on commutativity in logical reasoning. We first\nrandomly shuffle independent premises to introduce condition order\naugmentation. For reasoning steps, we construct a directed acyclic graph (DAG)\nto model dependencies between steps, which allows us to identify valid\nreorderings of steps while preserving logical correctness. By leveraging\norder-centric augmentations, models can develop a more flexible and generalized\nreasoning process. Finally, we conduct extensive experiments across multiple\nlogical reasoning benchmarks, demonstrating that our method significantly\nenhances LLMs' reasoning performance and adaptability to diverse logical\nstructures. We release our codes and augmented data in\nhttps://anonymous.4open.science/r/Order-Centric-Data-Augmentation-822C/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logical reasoning is essential for large language models (LLMs) to ensure\naccurate and coherent inference. However, LLMs struggle with reasoning order\nvariations and fail to generalize across logically equivalent transformations.\nLLMs often rely on fixed sequential patterns rather than true logical\nunderstanding. To address this issue, we introduce an order-centric data\naugmentation framework based on commutativity in logical reasoning. We first\nrandomly shuffle independent premises to introduce condition order\naugmentation. For reasoning steps, we construct a directed acyclic graph (DAG)\nto model dependencies between steps, which allows us to identify valid\nreorderings of steps while preserving logical correctness. By leveraging\norder-centric augmentations, models can develop a more flexible and generalized\nreasoning process. Finally, we conduct extensive experiments across multiple\nlogical reasoning benchmarks, demonstrating that our method significantly\nenhances LLMs' reasoning performance and adaptability to diverse logical\nstructures. We release our codes and augmented data in\nhttps://anonymous.4open.science/r/Order-Centric-Data-Augmentation-822C/."
                },
                "authors": [
                    {
                        "name": "Qianxi He"
                    },
                    {
                        "name": "Qianyu He"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    },
                    {
                        "name": "Weikang Zhou"
                    },
                    {
                        "name": "Zeye Sun"
                    },
                    {
                        "name": "Fei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Yu"
                },
                "author": "Fei Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19883v1",
                "updated": "2025-02-27T08:44:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    8,
                    44,
                    4,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T08:44:04Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    8,
                    44,
                    4,
                    3,
                    58,
                    0
                ],
                "title": "Beyond the Tip of Efficiency: Uncovering the Submerged Threats of\n  Jailbreak Attacks in Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Tip of Efficiency: Uncovering the Submerged Threats of\n  Jailbreak Attacks in Small Language Models"
                },
                "summary": "Small language models (SLMs) have become increasingly prominent in the\ndeployment on edge devices due to their high efficiency and low computational\ncost. While researchers continue to advance the capabilities of SLMs through\ninnovative training strategies and model compression techniques, the security\nrisks of SLMs have received considerably less attention compared to large\nlanguage models (LLMs).To fill this gap, we provide a comprehensive empirical\nstudy to evaluate the security performance of 13 state-of-the-art SLMs under\nvarious jailbreak attacks. Our experiments demonstrate that most SLMs are quite\nsusceptible to existing jailbreak attacks, while some of them are even\nvulnerable to direct harmful prompts.To address the safety concerns, we\nevaluate several representative defense methods and demonstrate their\neffectiveness in enhancing the security of SLMs. We further analyze the\npotential security degradation caused by different SLM techniques including\narchitecture compression, quantization, knowledge distillation, and so on. We\nexpect that our research can highlight the security challenges of SLMs and\nprovide valuable insights to future work in developing more robust and secure\nSLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small language models (SLMs) have become increasingly prominent in the\ndeployment on edge devices due to their high efficiency and low computational\ncost. While researchers continue to advance the capabilities of SLMs through\ninnovative training strategies and model compression techniques, the security\nrisks of SLMs have received considerably less attention compared to large\nlanguage models (LLMs).To fill this gap, we provide a comprehensive empirical\nstudy to evaluate the security performance of 13 state-of-the-art SLMs under\nvarious jailbreak attacks. Our experiments demonstrate that most SLMs are quite\nsusceptible to existing jailbreak attacks, while some of them are even\nvulnerable to direct harmful prompts.To address the safety concerns, we\nevaluate several representative defense methods and demonstrate their\neffectiveness in enhancing the security of SLMs. We further analyze the\npotential security degradation caused by different SLM techniques including\narchitecture compression, quantization, knowledge distillation, and so on. We\nexpect that our research can highlight the security challenges of SLMs and\nprovide valuable insights to future work in developing more robust and secure\nSLMs."
                },
                "authors": [
                    {
                        "name": "Sibo Yi"
                    },
                    {
                        "name": "Tianshuo Cong"
                    },
                    {
                        "name": "Xinlei He"
                    },
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Jiaxing Song"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxing Song"
                },
                "author": "Jiaxing Song",
                "arxiv_comment": "12 pages. 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19859v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19859v4",
                "updated": "2025-02-27T08:36:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    8,
                    36,
                    29,
                    3,
                    58,
                    0
                ],
                "published": "2024-06-28T11:58:26Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    11,
                    58,
                    26,
                    4,
                    180,
                    0
                ],
                "title": "MetaDesigner: Advancing Artistic Typography Through AI-Driven,\n  User-Centric, and Multilingual WordArt Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaDesigner: Advancing Artistic Typography Through AI-Driven,\n  User-Centric, and Multilingual WordArt Synthesis"
                },
                "summary": "MetaDesigner introduces a transformative framework for artistic typography\nsynthesis, powered by Large Language Models (LLMs) and grounded in a\nuser-centric design paradigm. Its foundation is a multi-agent system comprising\nthe Pipeline, Glyph, and Texture agents, which collectively orchestrate the\ncreation of customizable WordArt, ranging from semantic enhancements to\nintricate textural elements. A central feedback mechanism leverages insights\nfrom both multimodal models and user evaluations, enabling iterative refinement\nof design parameters. Through this iterative process, MetaDesigner dynamically\nadjusts hyperparameters to align with user-defined stylistic and thematic\npreferences, consistently delivering WordArt that excels in visual quality and\ncontextual resonance. Empirical evaluations underscore the system's versatility\nand effectiveness across diverse WordArt applications, yielding outputs that\nare both aesthetically compelling and context-sensitive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaDesigner introduces a transformative framework for artistic typography\nsynthesis, powered by Large Language Models (LLMs) and grounded in a\nuser-centric design paradigm. Its foundation is a multi-agent system comprising\nthe Pipeline, Glyph, and Texture agents, which collectively orchestrate the\ncreation of customizable WordArt, ranging from semantic enhancements to\nintricate textural elements. A central feedback mechanism leverages insights\nfrom both multimodal models and user evaluations, enabling iterative refinement\nof design parameters. Through this iterative process, MetaDesigner dynamically\nadjusts hyperparameters to align with user-defined stylistic and thematic\npreferences, consistently delivering WordArt that excels in visual quality and\ncontextual resonance. Empirical evaluations underscore the system's versatility\nand effectiveness across diverse WordArt applications, yielding outputs that\nare both aesthetically compelling and context-sensitive."
                },
                "authors": [
                    {
                        "name": "Jun-Yan He"
                    },
                    {
                        "name": "Zhi-Qi Cheng"
                    },
                    {
                        "name": "Chenyang Li"
                    },
                    {
                        "name": "Jingdong Sun"
                    },
                    {
                        "name": "Qi He"
                    },
                    {
                        "name": "Wangmeng Xiang"
                    },
                    {
                        "name": "Hanyuan Chen"
                    },
                    {
                        "name": "Jin-Peng Lan"
                    },
                    {
                        "name": "Xianhui Lin"
                    },
                    {
                        "name": "Kang Zhu"
                    },
                    {
                        "name": "Bin Luo"
                    },
                    {
                        "name": "Yifeng Geng"
                    },
                    {
                        "name": "Xuansong Xie"
                    },
                    {
                        "name": "Alexander G. Hauptmann"
                    }
                ],
                "author_detail": {
                    "name": "Alexander G. Hauptmann"
                },
                "author": "Alexander G. Hauptmann",
                "arxiv_comment": "Accepted by ICLR 2025, Project:\n  https://modelscope.cn/studios/WordArt/WordArt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19859v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19859v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19870v1",
                "updated": "2025-02-27T08:21:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    8,
                    21,
                    28,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T08:21:28Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    8,
                    21,
                    28,
                    3,
                    58,
                    0
                ],
                "title": "MMKE-Bench: A Multimodal Editing Benchmark for Diverse Visual Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMKE-Bench: A Multimodal Editing Benchmark for Diverse Visual Knowledge"
                },
                "summary": "Knowledge editing techniques have emerged as essential tools for updating the\nfactual knowledge of large language models (LLMs) and multimodal models (LMMs),\nallowing them to correct outdated or inaccurate information without retraining\nfrom scratch. However, existing benchmarks for multimodal knowledge editing\nprimarily focus on entity-level knowledge represented as simple triplets, which\nfail to capture the complexity of real-world multimodal information. To address\nthis issue, we introduce MMKE-Bench, a comprehensive MultiModal Knowledge\nEditing Benchmark, designed to evaluate the ability of LMMs to edit diverse\nvisual knowledge in real-world scenarios. MMKE-Bench addresses these\nlimitations by incorporating three types of editing tasks: visual entity\nediting, visual semantic editing, and user-specific editing. Besides,\nMMKE-Bench uses free-form natural language to represent and edit knowledge,\noffering a more flexible and effective format. The benchmark consists of 2,940\npieces of knowledge and 8,363 images across 33 broad categories, with\nevaluation questions automatically generated and human-verified. We assess five\nstate-of-the-art knowledge editing methods on three prominent LMMs, revealing\nthat no method excels across all criteria, and that visual and user-specific\nedits are particularly challenging. MMKE-Bench sets a new standard for\nevaluating the robustness of multimodal knowledge editing techniques, driving\nprogress in this rapidly evolving field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge editing techniques have emerged as essential tools for updating the\nfactual knowledge of large language models (LLMs) and multimodal models (LMMs),\nallowing them to correct outdated or inaccurate information without retraining\nfrom scratch. However, existing benchmarks for multimodal knowledge editing\nprimarily focus on entity-level knowledge represented as simple triplets, which\nfail to capture the complexity of real-world multimodal information. To address\nthis issue, we introduce MMKE-Bench, a comprehensive MultiModal Knowledge\nEditing Benchmark, designed to evaluate the ability of LMMs to edit diverse\nvisual knowledge in real-world scenarios. MMKE-Bench addresses these\nlimitations by incorporating three types of editing tasks: visual entity\nediting, visual semantic editing, and user-specific editing. Besides,\nMMKE-Bench uses free-form natural language to represent and edit knowledge,\noffering a more flexible and effective format. The benchmark consists of 2,940\npieces of knowledge and 8,363 images across 33 broad categories, with\nevaluation questions automatically generated and human-verified. We assess five\nstate-of-the-art knowledge editing methods on three prominent LMMs, revealing\nthat no method excels across all criteria, and that visual and user-specific\nedits are particularly challenging. MMKE-Bench sets a new standard for\nevaluating the robustness of multimodal knowledge editing techniques, driving\nprogress in this rapidly evolving field."
                },
                "authors": [
                    {
                        "name": "Yuntao Du"
                    },
                    {
                        "name": "Kailin Jiang"
                    },
                    {
                        "name": "Zhi Gao"
                    },
                    {
                        "name": "Chenrui Shi"
                    },
                    {
                        "name": "Zilong Zheng"
                    },
                    {
                        "name": "Siyuan Qi"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16848v2",
                "updated": "2025-02-27T08:15:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    8,
                    15,
                    49,
                    3,
                    58,
                    0
                ],
                "published": "2024-10-22T09:35:42Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    9,
                    35,
                    42,
                    1,
                    296,
                    0
                ],
                "title": "ETHIC: Evaluating Large Language Models on Long-Context Tasks with High\n  Information Coverage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETHIC: Evaluating Large Language Models on Long-Context Tasks with High\n  Information Coverage"
                },
                "summary": "Recent advancements in large language models (LLM) capable of processing\nextremely long texts highlight the need for a dedicated evaluation benchmark to\nassess their long-context capabilities. However, existing methods, like the\nneedle-in-a-haystack test, do not effectively assess whether these models fully\nutilize contextual information, raising concerns about the reliability of\ncurrent evaluation techniques. To thoroughly examine the effectiveness of\nexisting benchmarks, we introduce a new metric called information coverage\n(IC), which quantifies the proportion of the input context necessary for\nanswering queries. Our findings indicate that current benchmarks exhibit low\nIC; although the input context may be extensive, the actual usable context is\noften limited. To address this, we present ETHIC, a novel benchmark designed to\nassess LLMs' ability to leverage the entire context. Our benchmark comprises\n1,986 test instances spanning four long-context tasks with high IC scores in\nthe domains of books, debates, medicine, and law. Our evaluations reveal\nsignificant performance drops in contemporary LLMs, highlighting a critical\nchallenge in managing long contexts. Our benchmark is available at\nhttps://github.com/dmis-lab/ETHIC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLM) capable of processing\nextremely long texts highlight the need for a dedicated evaluation benchmark to\nassess their long-context capabilities. However, existing methods, like the\nneedle-in-a-haystack test, do not effectively assess whether these models fully\nutilize contextual information, raising concerns about the reliability of\ncurrent evaluation techniques. To thoroughly examine the effectiveness of\nexisting benchmarks, we introduce a new metric called information coverage\n(IC), which quantifies the proportion of the input context necessary for\nanswering queries. Our findings indicate that current benchmarks exhibit low\nIC; although the input context may be extensive, the actual usable context is\noften limited. To address this, we present ETHIC, a novel benchmark designed to\nassess LLMs' ability to leverage the entire context. Our benchmark comprises\n1,986 test instances spanning four long-context tasks with high IC scores in\nthe domains of books, debates, medicine, and law. Our evaluations reveal\nsignificant performance drops in contemporary LLMs, highlighting a critical\nchallenge in managing long contexts. Our benchmark is available at\nhttps://github.com/dmis-lab/ETHIC."
                },
                "authors": [
                    {
                        "name": "Taewhoo Lee"
                    },
                    {
                        "name": "Chanwoong Yoon"
                    },
                    {
                        "name": "Kyochul Jang"
                    },
                    {
                        "name": "Donghyeon Lee"
                    },
                    {
                        "name": "Minju Song"
                    },
                    {
                        "name": "Hyunjae Kim"
                    },
                    {
                        "name": "Jaewoo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoo Kang"
                },
                "author": "Jaewoo Kang",
                "arxiv_comment": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18798v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18798v2",
                "updated": "2025-02-27T08:11:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    8,
                    11,
                    40,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-26T04:10:18Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    4,
                    10,
                    18,
                    2,
                    57,
                    0
                ],
                "title": "ANPMI: Assessing the True Comprehension Capabilities of LLMs for\n  Multiple Choice Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ANPMI: Assessing the True Comprehension Capabilities of LLMs for\n  Multiple Choice Questions"
                },
                "summary": "Multiple-choice benchmarks, consisting of various prompts and choices, are\namong the most widely used methods to assess a language model's natural\nlanguage understanding capability. Given a specific prompt, we typically\ncompute $P(Choice|Prompt)$ to evaluate how likely a language model is to\ngenerate the correct choice compared to incorrect ones. However, we observe\nthat performance measured using this approach reflects not only the model's\ncomprehension of the prompt but also its inherent biases for certain choices\nregardless of the prompt. This issue makes it challenging to accurately measure\na model's natural language understanding, as models may select the answer\nwithout fully understanding the prompt. To address this limitation, we propose\na novel metric called ANPMI, which normalizes Pointwise Mutual Information\n(PMI) by $-\\log P(Choice)$. ANPMI provides a more accurate assessment of the\nmodel's natural language understanding by ensuring that it is challenging to\nanswer a question without properly understanding the prompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple-choice benchmarks, consisting of various prompts and choices, are\namong the most widely used methods to assess a language model's natural\nlanguage understanding capability. Given a specific prompt, we typically\ncompute $P(Choice|Prompt)$ to evaluate how likely a language model is to\ngenerate the correct choice compared to incorrect ones. However, we observe\nthat performance measured using this approach reflects not only the model's\ncomprehension of the prompt but also its inherent biases for certain choices\nregardless of the prompt. This issue makes it challenging to accurately measure\na model's natural language understanding, as models may select the answer\nwithout fully understanding the prompt. To address this limitation, we propose\na novel metric called ANPMI, which normalizes Pointwise Mutual Information\n(PMI) by $-\\log P(Choice)$. ANPMI provides a more accurate assessment of the\nmodel's natural language understanding by ensuring that it is challenging to\nanswer a question without properly understanding the prompt."
                },
                "authors": [
                    {
                        "name": "Gyeongje Cho"
                    },
                    {
                        "name": "Yeonkyoung So"
                    },
                    {
                        "name": "Jaejin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jaejin Lee"
                },
                "author": "Jaejin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18798v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18798v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05930v2",
                "updated": "2025-02-27T08:09:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    8,
                    9,
                    23,
                    3,
                    58,
                    0
                ],
                "published": "2024-05-09T17:16:20Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    17,
                    16,
                    20,
                    3,
                    130,
                    0
                ],
                "title": "Trustworthy AI-Generative Content for Intelligent Network Service:\n  Robustness, Security, and Fairness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthy AI-Generative Content for Intelligent Network Service:\n  Robustness, Security, and Fairness"
                },
                "summary": "AI-generated content (AIGC) models, represented by large language models\n(LLM), have revolutionized content creation. High-speed next-generation\ncommunication technology is an ideal platform for providing powerful AIGC\nnetwork services. At the same time, advanced AIGC techniques can also make\nfuture network services more intelligent, especially various online content\ngeneration services. However, the significant untrustworthiness concerns of\ncurrent AIGC models, such as robustness, security, and fairness, greatly affect\nthe credibility of intelligent network services, especially in ensuring secure\nAIGC services. This paper proposes TrustGAIN, a trustworthy AIGC framework that\nincorporates robust, secure, and fair network services. We first discuss the\nrobustness to adversarial attacks faced by AIGC models in network systems and\nthe corresponding protection issues. Subsequently, we emphasize the importance\nof avoiding unsafe and illegal services and ensuring the fairness of the AIGC\nnetwork services. Then as a case study, we propose a novel sentiment\nanalysis-based detection method to guide the robust detection of unsafe content\nin network services. We conduct our experiments on fake news, malicious code,\nand unsafe review datasets to represent LLM application scenarios. Our results\nindicate that TrustGAIN is an exploration of future networks that can support\ntrustworthy AIGC network services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-generated content (AIGC) models, represented by large language models\n(LLM), have revolutionized content creation. High-speed next-generation\ncommunication technology is an ideal platform for providing powerful AIGC\nnetwork services. At the same time, advanced AIGC techniques can also make\nfuture network services more intelligent, especially various online content\ngeneration services. However, the significant untrustworthiness concerns of\ncurrent AIGC models, such as robustness, security, and fairness, greatly affect\nthe credibility of intelligent network services, especially in ensuring secure\nAIGC services. This paper proposes TrustGAIN, a trustworthy AIGC framework that\nincorporates robust, secure, and fair network services. We first discuss the\nrobustness to adversarial attacks faced by AIGC models in network systems and\nthe corresponding protection issues. Subsequently, we emphasize the importance\nof avoiding unsafe and illegal services and ensuring the fairness of the AIGC\nnetwork services. Then as a case study, we propose a novel sentiment\nanalysis-based detection method to guide the robust detection of unsafe content\nin network services. We conduct our experiments on fake news, malicious code,\nand unsafe review datasets to represent LLM application scenarios. Our results\nindicate that TrustGAIN is an exploration of future networks that can support\ntrustworthy AIGC network services."
                },
                "authors": [
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Yaju Liu"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Jianhua Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianhua Li"
                },
                "author": "Jianhua Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19860v1",
                "updated": "2025-02-27T08:04:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    8,
                    4,
                    27,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T08:04:27Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    8,
                    4,
                    27,
                    3,
                    58,
                    0
                ],
                "title": "MIND: Towards Immersive Psychological Healing with Multi-agent Inner\n  Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIND: Towards Immersive Psychological Healing with Multi-agent Inner\n  Dialogue"
                },
                "summary": "Mental health issues are worsening in today's competitive society, such as\ndepression and anxiety. Traditional healings like counseling and chatbots fail\nto engage effectively, they often provide generic responses lacking emotional\ndepth. Although large language models (LLMs) have the potential to create more\nhuman-like interactions, they still struggle to capture subtle emotions. This\nrequires LLMs to be equipped with human-like adaptability and warmth. To fill\nthis gap, we propose the MIND (Multi-agent INner Dialogue), a novel paradigm\nthat provides more immersive psychological healing environments. Considering\nthe strong generative and role-playing ability of LLM agents, we predefine an\ninteractive healing framework and assign LLM agents different roles within the\nframework to engage in interactive inner dialogues with users, thereby\nproviding an immersive healing experience. We conduct extensive human\nexperiments in various real-world healing dimensions, and find that MIND\nprovides a more user-friendly experience than traditional paradigms. This\ndemonstrates that MIND effectively leverages the significant potential of LLMs\nin psychological healing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mental health issues are worsening in today's competitive society, such as\ndepression and anxiety. Traditional healings like counseling and chatbots fail\nto engage effectively, they often provide generic responses lacking emotional\ndepth. Although large language models (LLMs) have the potential to create more\nhuman-like interactions, they still struggle to capture subtle emotions. This\nrequires LLMs to be equipped with human-like adaptability and warmth. To fill\nthis gap, we propose the MIND (Multi-agent INner Dialogue), a novel paradigm\nthat provides more immersive psychological healing environments. Considering\nthe strong generative and role-playing ability of LLM agents, we predefine an\ninteractive healing framework and assign LLM agents different roles within the\nframework to engage in interactive inner dialogues with users, thereby\nproviding an immersive healing experience. We conduct extensive human\nexperiments in various real-world healing dimensions, and find that MIND\nprovides a more user-friendly experience than traditional paradigms. This\ndemonstrates that MIND effectively leverages the significant potential of LLMs\nin psychological healing."
                },
                "authors": [
                    {
                        "name": "Yujia Chen"
                    },
                    {
                        "name": "Changsong Li"
                    },
                    {
                        "name": "Yiming Wang"
                    },
                    {
                        "name": "Qingqing Xiao"
                    },
                    {
                        "name": "Nan Zhang"
                    },
                    {
                        "name": "Zifan Kong"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Binyu Yan"
                    }
                ],
                "author_detail": {
                    "name": "Binyu Yan"
                },
                "author": "Binyu Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12659v3",
                "updated": "2025-02-27T08:03:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    8,
                    3,
                    52,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-18T09:06:07Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    6,
                    7,
                    1,
                    49,
                    0
                ],
                "title": "The Hidden Risks of Large Reasoning Models: A Safety Assessment of R1",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hidden Risks of Large Reasoning Models: A Safety Assessment of R1"
                },
                "summary": "The rapid development of large reasoning models, such as OpenAI-o3 and\nDeepSeek-R1, has led to significant improvements in complex reasoning over\nnon-reasoning large language models~(LLMs). However, their enhanced\ncapabilities, combined with the open-source access of models like DeepSeek-R1,\nraise serious safety concerns, particularly regarding their potential for\nmisuse. In this work, we present a comprehensive safety assessment of these\nreasoning models, leveraging established safety benchmarks to evaluate their\ncompliance with safety regulations. Furthermore, we investigate their\nsusceptibility to adversarial attacks, such as jailbreaking and prompt\ninjection, to assess their robustness in real-world applications. Through our\nmulti-faceted analysis, we uncover four key findings: (1) There is a\nsignificant safety gap between the open-source R1 models and the o3-mini model,\non both safety benchmark and attack, suggesting more safety effort on R1 is\nneeded. (2) The distilled reasoning model shows poorer safety performance\ncompared to its safety-aligned base models. (3) The stronger the model's\nreasoning ability, the greater the potential harm it may cause when answering\nunsafe questions. (4) The thinking process in R1 models pose greater safety\nconcerns than their final answers. Our study provides insights into the\nsecurity implications of reasoning models and highlights the need for further\nadvancements in R1 models' safety to close the gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large reasoning models, such as OpenAI-o3 and\nDeepSeek-R1, has led to significant improvements in complex reasoning over\nnon-reasoning large language models~(LLMs). However, their enhanced\ncapabilities, combined with the open-source access of models like DeepSeek-R1,\nraise serious safety concerns, particularly regarding their potential for\nmisuse. In this work, we present a comprehensive safety assessment of these\nreasoning models, leveraging established safety benchmarks to evaluate their\ncompliance with safety regulations. Furthermore, we investigate their\nsusceptibility to adversarial attacks, such as jailbreaking and prompt\ninjection, to assess their robustness in real-world applications. Through our\nmulti-faceted analysis, we uncover four key findings: (1) There is a\nsignificant safety gap between the open-source R1 models and the o3-mini model,\non both safety benchmark and attack, suggesting more safety effort on R1 is\nneeded. (2) The distilled reasoning model shows poorer safety performance\ncompared to its safety-aligned base models. (3) The stronger the model's\nreasoning ability, the greater the potential harm it may cause when answering\nunsafe questions. (4) The thinking process in R1 models pose greater safety\nconcerns than their final answers. Our study provides insights into the\nsecurity implications of reasoning models and highlights the need for further\nadvancements in R1 models' safety to close the gap."
                },
                "authors": [
                    {
                        "name": "Kaiwen Zhou"
                    },
                    {
                        "name": "Chengzhi Liu"
                    },
                    {
                        "name": "Xuandong Zhao"
                    },
                    {
                        "name": "Shreedhar Jangam"
                    },
                    {
                        "name": "Jayanth Srinivasa"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Xin Eric Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Eric Wang"
                },
                "author": "Xin Eric Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19852v1",
                "updated": "2025-02-27T07:54:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    7,
                    54,
                    32,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T07:54:32Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    7,
                    54,
                    32,
                    3,
                    58,
                    0
                ],
                "title": "ConvCodeWorld: Benchmarking Conversational Code Generation in\n  Reproducible Feedback Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConvCodeWorld: Benchmarking Conversational Code Generation in\n  Reproducible Feedback Environments"
                },
                "summary": "Large language models (LLMs) have proven invaluable for code generation,\nparticularly in interactive settings. However, existing code generation\nbenchmarks fail to capture the diverse feedback encountered in multi-turn\ninteractions, limiting our ability to evaluate LLMs in these contexts. To\naddress this gap, we present a set of novel benchmarks that explicitly model\nthe quality of feedback provided to code generation LLMs. Our contributions are\nthreefold: First, we introduce CONVCODEWORLD, a novel and reproducible\nenvironment for benchmarking interactive code generation. CONVCODEWORLD\nsimulates 9 distinct interactive code generation scenarios while systematically\ncombining three types of feedback: (a) compilation feedback; (b) execution\nfeedback with varying test coverage; (c) verbal feedback generated by GPT-4o\nwith different levels of expertise. Second, we introduce CONVCODEBENCH, a fast,\nstatic version of benchmark that uses pre-generated feedback logs, eliminating\nthe need for costly dynamic verbal feedback generation while maintaining strong\nSpearman's rank correlations (0.82 to 0.99) with CONVCODEWORLD. Third,\nextensive evaluations of both closed-source and open-source LLMs including\nR1-Distill on CONVCODEWORLD reveal key insights: (a) LLM performance varies\nsignificantly based on the feedback provided; (b) Weaker LLMs, with sufficient\nfeedback, can outperform single-turn results of state-of-the-art LLMs without\nfeedback; (c) Training on a specific feedback combination can limit an LLM's\nability to utilize unseen combinations; (d) LLMs solve problems in fewer turns\n(high MRR) may not solve as many problems overall (high Recall), and vice\nversa. All implementations and benchmarks will be made publicly available at\nhttps://huggingface.co/spaces/ConvCodeWorld/ConvCodeWorld",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have proven invaluable for code generation,\nparticularly in interactive settings. However, existing code generation\nbenchmarks fail to capture the diverse feedback encountered in multi-turn\ninteractions, limiting our ability to evaluate LLMs in these contexts. To\naddress this gap, we present a set of novel benchmarks that explicitly model\nthe quality of feedback provided to code generation LLMs. Our contributions are\nthreefold: First, we introduce CONVCODEWORLD, a novel and reproducible\nenvironment for benchmarking interactive code generation. CONVCODEWORLD\nsimulates 9 distinct interactive code generation scenarios while systematically\ncombining three types of feedback: (a) compilation feedback; (b) execution\nfeedback with varying test coverage; (c) verbal feedback generated by GPT-4o\nwith different levels of expertise. Second, we introduce CONVCODEBENCH, a fast,\nstatic version of benchmark that uses pre-generated feedback logs, eliminating\nthe need for costly dynamic verbal feedback generation while maintaining strong\nSpearman's rank correlations (0.82 to 0.99) with CONVCODEWORLD. Third,\nextensive evaluations of both closed-source and open-source LLMs including\nR1-Distill on CONVCODEWORLD reveal key insights: (a) LLM performance varies\nsignificantly based on the feedback provided; (b) Weaker LLMs, with sufficient\nfeedback, can outperform single-turn results of state-of-the-art LLMs without\nfeedback; (c) Training on a specific feedback combination can limit an LLM's\nability to utilize unseen combinations; (d) LLMs solve problems in fewer turns\n(high MRR) may not solve as many problems overall (high Recall), and vice\nversa. All implementations and benchmarks will be made publicly available at\nhttps://huggingface.co/spaces/ConvCodeWorld/ConvCodeWorld"
                },
                "authors": [
                    {
                        "name": "Hojae Han"
                    },
                    {
                        "name": "Seung-won Hwang"
                    },
                    {
                        "name": "Rajhans Samdani"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]