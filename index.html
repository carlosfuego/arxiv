
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>ArXiv Results</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                margin: 0;
                padding: 0;
                display: flex;
                justify-content: center;
                align-items: center;
                min-height: 100vh;
                background-color: #f9f9f9;
            }
            .container {
                width: 17cm;
                padding: 2cm;
                background-color: #fff;
                box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            }
            h1, h2 {
                color: #333;
            }
            .paper {
                border: 1px solid #ccc;
                padding: 1em;
                margin-bottom: 1em;
                border-radius: 8px;
                box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            }
            .authors, .summary, .updated, .tags, .links {
                margin: 0.5em 0;
            }
            .tags span {
                background-color: #f0f0f0;
                border-radius: 4px;
                padding: 0.2em 0.5em;
                margin-right: 0.5em;
            }
            .links a {
                margin-right: 1em;
                text-decoration: none;
                color: #1a73e8;
            }
            .links a:hover {
                text-decoration: underline;
            }
        </style>
    </head>
    <body>
        <div class="container">
    <h1>Keyword: kv cache</h1>

            <div class="paper">
                <h2>#1 Apple Intelligence Foundation Language Models: Tech Report 2025</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ethan Li, Anders Boesen Lindbo Larsen, Chen Zhang, Xiyou Zhou, Jun Qin, Dian Ang Yap, Narendran Raghavan, Xuankai Chang, Margit Bowler, Eray Yildiz, John Peebles, Hannah Gillis Coleman, Matteo Ronchi, Peter Gray, Keen You, Anthony Spalvieri-Kruse, Ruoming Pang, Reed Li, Yuli Yang, Emad Soroush, Zhiyun Lu, Crystal Xiao, Rong Situ, Jordan Huffaker, David Griffiths, Zaid Ahmed, Peng Zhang, Daniel Parilla, Asaf Liberman, Jennifer Mallalieu, Parsa Mazaheri, Qibin Chen, Manjot Bilkhu, Aonan Zhang, Eric Wang, Dave Nelson, Michael FitzMaurice, Thomas Voice, Jeremy Liu, Josh Shaffer, Shiwen Zhao, Prasanth Yadla, Farzin Rasteh, Pengsheng Guo, Arsalan Farooq, Jeremy Snow, Stephen Murphy, Tao Lei, Minsik Cho, George Horrell, Sam Dodge, Lindsay Hislop, Sumeet Singh, Alex Dombrowski, Aiswarya Raghavan, Sasha Sirovica, Mandana Saebi, Faye Lao, Max Lam, TJ Lu, Zhaoyang Xu, Karanjeet Singh, Marc Kirchner, David Mizrahi, Rajat Arora, Haotian Zhang, Henry Mason, Lawrence Zhou, Yi Hua, Ankur Jain, Felix Bai, Joseph Astrauskas, Floris Weers, Josh Gardner, Mira Chiang, Yi Zhang, Pulkit Agrawal, Tony Sun, Quentin Keunebroek, Matthew Hopkins, Bugu Wu, Tao Jia, Chen Chen, Xingyu Zhou, Nanzhu Wang, Peng Liu, Ruixuan Hou, Rene Rauch, Yuan Gao, Afshin Dehghan, Jonathan Janke, Zirui Wang, Cha Chen, Xiaoyi Ren, Feng Nan, Josh Elman, Dong Yin, Yusuf Goren, Jeff Lai, Yiran Fei, Syd Evans, Muyang Yu, Guoli Yin, Yi Qin, Erin Feldman, Isha Garg, Aparna Rajamani, Karla Vega, Walker Cheng, TJ Collins, Hans Han, Raul Rea Menacho, Simon Yeung, Sophy Lee, Phani Mutyala, Ying-Chang Cheng, Zhe Gan, Sprite Chu, Justin Lazarow, Alessandro Pappalardo, Federico Scozzafava, Jing Lu, Erik Daxberger, Laurent Duchesne, Jen Liu, David Güera, Stefano Ligas, Mary Beth Kery, Brent Ramerth, Ciro Sannino, Marcin Eichner, Haoshuo Huang, Rui Qian, Moritz Schwarzer-Becker, David Riazati, Mingfei Gao, Bailin Wang, Jack Cackler, Yang Lu, Ransen Niu, John Dennison, Guillaume Klein, Jeffrey Bigham, Deepak Gopinath, Navid Shiee, Darren Botten, Guillaume Tartavel, Alex Guillen Garcia, Sam Xu, Victoria MönchJuan Haladjian, Zi-Yi Dou, Matthias Paulik, Adolfo Lopez Mendez, Zhen Li, Hong-You Chen, Chao Jia, Dhaval Doshi, Zhengdong Zhang, Raunak Manjani, Aaron Franklin, Zhile Ren, David Chen, Artsiom Peshko, Nandhitha Raghuram, Hans Hao, Jiulong Shan, Kavya Nerella, Ramsey Tantawi, Vivek Kumar, Saiwen Wang, Brycen Wershing, Bhuwan Dhingra, Dhruti Shah, Ob Adaranijo, Xin Zheng, Tait Madsen, Hadas Kotek, Chang Liu, Yin Xia, Hanli Li, Suma Jayaram, Yanchao Sun, Ahmed Fakhry, Vasileios Saveris, Dustin Withers, Yanghao Li, Alp Aygar, Andres Romero Mier Y Teran, Kaiwei Huang, Mark Lee, Xiujun Li, Yuhong Li, Tyler Johnson, Jay Tang, Joseph Yitan Cheng, Futang Peng, Andrew Walkingshaw, Lucas Guibert, Abhishek Sharma, Cheng Shen, Piotr Maj, Yasutaka Tanaka, You-Cyuan Jhang, Vivian Ma, Tommi Vehvilainen, Kelvin Zou, Jeff Nichols, Matthew Lei, David Qiu, Yihao Qian, Gokul Santhanam, Wentao Wu, Yena Han, Dominik Moritz, Haijing Fu, Mingze Xu, Vivek Rathod, Jian Liu, Louis D'hauwe, Qin Ba, Haitian Sun, Haoran Yan, Philipp Dufter, Anh Nguyen, Yihao Feng, Emma Wang, Keyu He, Rahul Nair, Sanskruti Shah, Jiarui Lu, Patrick Sonnenberg, Jeremy Warner, Yuanzhi Li, Bowen Pan, Ziyi Zhong, Joe Zhou, Sam Davarnia, Olli Saarikivi, Irina Belousova, Rachel Burger, Shang-Chen Wu, Di Feng, Bas Straathof, James Chou, Yuanyang Zhang, Marco Zuliani, Eduardo Jimenez, Abhishek Sundararajan, Xianzhi Du, Chang Lan, Nilesh Shahdadpuri, Peter Grasch, Sergiu Sima, Josh Newnham, Varsha Paidi, Jianyu Wang, Kaelen Haag, Alex Braunstein, Daniele Molinari, Richard Wei, Brenda Yang, Nicholas Lusskin, Joanna Arreaza-Taylor, Meng Cao, Nicholas Seidl, Simon Wang, Jiaming Hu, Yiping Ma, Mengyu Li, Kieran Liu, Hang Su, Sachin Ravi, Chong Wang, Xin Wang, Kevin Smith, Haoxuan You, Binazir Karimzadeh, Rui Li, Jinhao Lei, Wei Fang, Alec Doane, Sam Wiseman, Ismael Fernandez, Jane Li, Andrew Hansen, Javier Movellan, Christopher Neubauer, Hanzhi Zhou, Chris Chaney, Nazir Kamaldin, Valentin Wolf, Fernando Bermúdez-Medina, Joris Pelemans, Peter Fu, Howard Xing, Xiang Kong, Wayne Shan, Gabriel Jacoby-Cooper, Dongcai Shen, Tom Gunter, Guillaume Seguin, Fangping Shi, Shiyu Li, Yang Xu, Areeba Kamal, Dan Masi, Saptarshi Guha, Qi Zhu, Jenna Thibodeau, Changyuan Zhang, Rebecca Callahan, Charles Maalouf, Wilson Tsao, Boyue Li, Qingqing Cao, Naomy Sabo, Cheng Leong, Yi Wang, Anupama Mann Anupama, Colorado Reed, Kenneth Jung, Zhifeng Chen, Mohana Prasad Sathya Moorthy, Yifei He, Erik Hornberger, Devi Krishna, Senyu Tong, Michael, Lee, David Haldimann, Yang Zhao, Bowen Zhang, Chang Gao, Chris Bartels, Sushma Rao, Nathalie Tran, Simon Lehnerer, Co Giang, Patrick Dong, Junting Pan, Biyao Wang, Dongxu Li, Mehrdad Farajtabar, Dongseong Hwang, Grace Duanmu, Eshan Verma, Sujeeth Reddy, Qi Shan, Hongbin Gao, Nan Du, Pragnya Sridhar, Forrest Huang, Yingbo Wang, Nikhil Bhendawade, Diane Zhu, Sai Aitharaju, Fred Hohman, Lauren Gardiner, Chung-Cheng Chiu, Yinfei Yang, Alper Kokmen, Frank Chu, Ke Ye, Kaan Elgin, Oron Levy, John Park, Donald Zhang, Eldon Schoop, Nina Wenzel, Michael Booker, Hyunjik Kim, Chinguun Erdenebileg, Nan Dun, Eric Liang Yang, Priyal Chhatrapati, Vishaal Mahtani, Haiming Gang, Kohen Chia, Deepa Seshadri, Donghan Yu, Yan Meng, Kelsey Peterson, Zhen Yang, Yongqiang Wang, Carina Peng, Doug Kang, Anuva Agarwal, Albert Antony, Juan Lao Tebar, Albin Madappally Jose, Regan Poston, Andy De Wang, Gerard Casamayor, Elmira Amirloo, Violet Yao, Wojciech Kryscinski, Kun Duan, Lezhi L
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce two multilingual, multimodal foundation language models that power Apple Intelligence features across Apple devices and services: i a 3B-parameter on-device model optimized for Apple silicon through architectural innovations such as KV-cache sharing and 2-bit quantization-aware training; and ii a scalable server model built on a novel Parallel-Track Mixture-of-Experts PT-MoE transformer that combines track parallelism, mixture-of-experts sparse computation, and interleaved global-local attention to deliver high quality with competitive cost on Apple's Private Cloud Compute platform. Both models are trained on large-scale multilingual and multimodal datasets sourced via responsible web crawling, licensed corpora, and high-quality synthetic data, then further refined with supervised fine-tuning and reinforcement learning on a new asynchronous platform. The resulting models support several additional languages while understanding images and executing tool calls. In public benchmarks and human evaluations, both the server model and the on-device model match or surpass comparably sized open baselines.   A new Swift-centric Foundation Models framework exposes guided generation, constrained tool calling, and LoRA adapter fine-tuning, allowing developers to integrate these capabilities with a few lines of code. The latest advancements in Apple Intelligence models are grounded in our Responsible AI approach with safeguards like content filtering and locale-specific evaluation, as well as our commitment to protecting our users' privacy with innovations like Private Cloud Compute.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T16:34:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13575v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13575v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 Re-thinking Memory-Bound Limitations in CGRAs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiangfeng Liu, Zhe Jiang, Anzhen Zhu, Xiaomeng Han, Mingsong Lyu, Qingxu Deng, Nan Guan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators commonly employed to boost performance in workloads with iterative structures. Existing research typically focuses on compiler or architecture optimizations aimed at improving CGRA performance, energy efficiency, flexibility, and area utilization, under the idealistic assumption that kernels can access all data from Scratchpad Memory (SPM). However, certain complex workloads-particularly in fields like graph analytics, irregular database operations, and specialized forms of high-performance computing (e.g., unstructured mesh simulations)-exhibit irregular memory access patterns that hinder CGRA utilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To address this challenge, we conduct a thorough analysis of the underlying causes of performance degradation, then propose a redesigned memory subsystem and refine the memory model. With both microarchitectural and theoretical optimization, our solution can effectively manage irregular memory accesses through CGRA-specific runahead execution mechanism and cache reconfiguration techniques. Our results demonstrate that we can achieve performance comparable to the original SPM-only system while requiring only 1.27% of the storage size. The runahead execution mechanism achieves an average 3.04x speedup (up to 6.91x), with cache reconfiguration technique providing an additional 6.02% improvement, significantly enhancing CGRA performance for irregular memory access patterns.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T12:13:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>B.3.0; B.6.0</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3760386' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.09570v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.09570v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 Spotlight Attention: Towards Efficient LLM Generation via Non-linear
  Hashing-based KV Cache Retrieval</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wenhao Li, Yuxin Zhang, Gen Luo, Haiyuan Wan, Ziyang Gong, Fei Chao, Rongrong Ji
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reducing the key-value (KV) cache burden in Large Language Models (LLMs) significantly accelerates inference. Dynamically selecting critical KV caches during decoding helps maintain performance. Existing methods use random linear hashing to identify important tokens, but this approach is inefficient due to the orthogonal distribution of queries and keys within two narrow cones in LLMs. We introduce Spotlight Attention, a novel method that employs non-linear hashing functions to optimize the embedding distribution of queries and keys, enhancing coding efficiency and robustness. We also developed a lightweight, stable training framework using a Bradley-Terry ranking-based loss, enabling optimization of the non-linear hashing module on GPUs with 16GB memory in 8 hours. Experimental results show that Spotlight Attention drastically improves retrieval precision while shortening the length of the hash code at least 5$\times$ compared to traditional linear hashing. Finally, we exploit the computational advantages of bitwise operations by implementing specialized CUDA kernels, achieving hashing retrieval for 512K tokens in under 100$\mu$s on a single A100 GPU, with end-to-end throughput up to 3$\times$ higher than vanilla decoding.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T10:11:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19740v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19740v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed
  Criticality Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Diogo Costa, Jose Martins, Sandro Pinto
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate heterogeneous computing platforms, combining general-purpose processors with specialized accelerators such as AI engines, GPUs, and high-speed networking interfaces. This heterogeneity introduces challenges, as these accelerators and DMA-capable devices act as independent bus masters, directly accessing memory. Consequently, ensuring both security and timing predictability in such environments becomes critical. To address these concerns, the Input-Output Memory Management Unit (IOMMU) plays a key role in mediating and regulating memory access, preventing unauthorized transactions while enforcing isolation and access control policies. While prior work has explored IOMMU-related side-channel vulnerabilities from a security standpoint, its role in performance interference remains largely unexplored. Moreover, many of the same architectural properties that enable side-channel leakage, such as shared TLBs, caching effects, and translation overheads, can also introduce timing unpredictability. In this work, we analyze the contention effects within IOMMU structures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how their shared nature introduce unpredictable delays. Our findings reveal that IOMMU-induced interference primarily affects small memory transactions, where translation overheads significantly impact execution time. Additionally, we hypothesize that contention effects arising from IOTLBs exhibit similar behavior across architectures due to shared caching principles, such as prefetching and hierarchical TLB structures. Notably, our experiments show that IOMMU interference can delay DMA transactions by up to 1.79x for lower-size transfers on the Arm SMMUv2 implementation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T08:30:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19670v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19670v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 FiRST: Finetuning Router-Selective Transformers for Input-Adaptive
  Latency Reduction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Akriti Jain, Saransh Sharma, Koyel Mukherjee, Soumyabrata Pal
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Auto-regressive Large Language Models (LLMs) demonstrate remarkable performance across different domains such as vision and language processing. However, due to sequential processing through a stack of transformer layers, autoregressive decoding faces significant computation/latency challenges, particularly in resource-constrained environments like mobile and edge devices. Existing approaches in literature that aim to improve latency via skipping layers have two distinct flavors - 1) Early exit, and 2) Input-agnostic heuristics where tokens exit at pre-determined layers irrespective of input sequence. Both the above strategies have limitations - the former cannot be applied to handle KV Caching necessary for speed-ups in modern framework and the latter does not capture the variation in layer importance across tasks or more generally, across input sequences. To address both limitations, we propose FiRST, an algorithm that reduces inference latency by using layer-specific routers to select a subset of transformer layers adaptively for each input sequence - the prompt (during the prefill stage) decides which layers will be skipped during decoding. FiRST preserves compatibility with KV caching enabling faster inference while being quality-aware. FiRST is model-agnostic and can be easily enabled on any pre-trained LLM. Our approach reveals that input adaptivity is critical - indeed, different task-specific middle layers play a crucial role in evolving hidden representations depending on tasks. Extensive experiments show that FiRST significantly reduces latency while outperforming other layer selection strategies in quality metics. It retains competitive performance to base model (without layer skipping) and in some cases, even improves upon it. FiRST is thus a promising and efficient solution for LLM deployment in low-resource environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T04:58:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.12513v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.12513v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D
  Space</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lin Li, Zehuan Huang, Haoran Feng, Gengxiong Zhuang, Rui Chen, Chunchao Guo, Lu Sheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> 3D local editing of specified regions is crucial for game industry and robot interaction. Recent methods typically edit rendered multi-view images and then reconstruct 3D models, but they face challenges in precisely preserving unedited regions and overall coherence. Inspired by structured 3D generative models, we propose VoxHammer, a novel training-free approach that performs precise and coherent editing in 3D latent space. Given a 3D model, VoxHammer first predicts its inversion trajectory and obtains its inverted latents and key-value tokens at each timestep. Subsequently, in the denoising and editing phase, we replace the denoising features of preserved regions with the corresponding inverted latents and cached key-value tokens. By retaining these contextual features, this approach ensures consistent reconstruction of preserved areas and coherent integration of edited parts. To evaluate the consistency of preserved regions, we constructed Edit3D-Bench, a human-annotated dataset comprising hundreds of samples, each with carefully labeled 3D editing regions. Experiments demonstrate that VoxHammer significantly outperforms existing methods in terms of both 3D consistency of preserved regions and overall quality. Our method holds promise for synthesizing high-quality edited paired data, thereby laying the data foundation for in-context 3D generation. See our project page at https://huanngzh.github.io/VoxHammer-Page/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-26T17:59:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19247v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19247v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 Enabling MoE on the Edge via Importance-Driven Expert Scheduling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guoying Zhu, Meng Li, Haipeng Dai, Xuechen Liu, Weijun Wang, Keran Li, Jun xiao, Ligeng Chen, Wei Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Mixture of Experts (MoE) architecture has emerged as a key technique for scaling Large Language Models by activating only a subset of experts per query. Deploying MoE on consumer-grade edge hardware, however, is constrained by limited device memory, making dynamic expert offloading essential. Unlike prior work that treats offloading purely as a scheduling problem, we leverage expert importance to guide decisions, substituting low-importance activated experts with functionally similar ones already cached in GPU memory, thereby preserving accuracy. As a result, this design reduces memory usage and data transfer, while largely eliminating PCIe overhead. In addition, we introduce a scheduling policy that maximizes the reuse ratio of GPU-cached experts, further boosting efficiency. Extensive evaluations show that our approach delivers 48% lower decoding latency with over 60% expert cache hit rate, while maintaining nearly lossless accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-26T12:32:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18983v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18983v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 Rethinking Caching for LLM Serving Systems: Beyond Traditional
  Heuristics</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jungwoo Kim, Minsang Kim, Jaeheon Lee, Chanwoo Moon, Heejin Kim, Taeho Hwang, Woosuk Chung, Yeseong Kim, Sungjin Lee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Serving Large Language Models (LLMs) at scale requires meeting strict Service Level Objectives (SLOs) under severe computational and memory constraints. Nevertheless, traditional caching strategies fall short: exact-matching and prefix caches neglect query semantics, while state-of-the-art semantic caches remain confined to traditional intuitions, offering little conceptual departure. Building on this, we present SISO, a semantic caching system that redefines efficiency for LLM serving. SISO introduces centroid-based caching to maximize coverage with minimal memory, locality-aware replacement to preserve high-value entries, and dynamic thresholding to balance accuracy and latency under varying workloads. Across diverse datasets, SISO delivers up to 1.71$\times$ higher hit ratios and consistently stronger SLO attainment compared to state-of-the-art systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-26T07:09:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18736v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18736v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 Physical Autoregressive Model for Robotic Manipulation without Action
  Pretraining</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zijian Song, Sihan Qin, Tianshui Chen, Liang Lin, Guangrun Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The scarcity of manipulation data has motivated the use of pretrained large models from other modalities in robotics. In this work, we build upon autoregressive video generation models to propose a Physical Autoregressive Model (PAR), where physical tokens combine frames and actions to represent the joint evolution of the robot and its environment. PAR leverages the world knowledge embedded in video pretraining to understand physical dynamics without requiring action pretraining, enabling accurate video prediction and consistent action trajectories. It also adopts a DiT-based de-tokenizer to model frames and actions as continuous tokens, mitigating quantization errors and facilitating mutual enhancement. Furthermore, we incorporate a causal mask with inverse kinematics, parallel training, and the KV-cache mechanism to further improve performance and efficiency. Experiments on the ManiSkill benchmark show that PAR achieves a 100\% success rate on the PushCube task, matches the performance of action-pretrained baselines on other tasks, and accurately predicts future videos with tightly aligned action trajectories. These findings underscore a promising direction for robotic manipulation by transferring world knowledge from autoregressive video pretraining. The project page is here: https://hcplab-sysu.github.io/PhysicalAutoregressiveModel/
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-26T03:23:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.09822v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.09822v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 Krul: Efficient State Restoration for Multi-turn Conversations with
  Dynamic Cross-layer KV Sharing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junyi Wen, Junyuan Liang, Zicong Hong, Wuhui Chen, Ting Cai, Zibin Zheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Efficient state restoration in multi-turn conversations with large language models (LLMs) remains a critical challenge, primarily due to the overhead of recomputing or loading full key-value (KV) caches for all historical tokens. To address this, existing approaches compress KV caches across adjacent layers with highly similar attention patterns. However, these methods often apply a fixed compression scheme across all conversations, selecting the same layer pairs for compression without considering conversation-specific attention dynamics. This static strategy overlooks variability in attention pattern similarity across different conversations, which can lead to noticeable accuracy degradation.   We present Krul, a multi-turn LLM inference system that enables accurate and efficient KV cache restoration. Krul dynamically selects compression strategies based on attention similarity across layer pairs and uses a recomputation-loading pipeline to restore the KV cache. It introduces three key innovations: 1) a preemptive compression strategy selector to preserve critical context for future conversation turns and selects a customized strategy for the conversation; 2) a token-wise heterogeneous attention similarity estimator to mitigate the attention similarity computation and storage overhead during model generation; 3) a bubble-free restoration scheduler to reduce potential bubbles brought by the imbalance of recomputing and loading stream due to compressed KV caches. Empirical evaluations on real-world tasks demonstrate that Krul achieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x reduction in KV cache storage compared to state-of-the-art methods without compromising generation quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-26T01:55:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.08045v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.08045v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhuoping Yang, Jinming Zhuang, Xingzhen Chen, Alex K. Jones, Peipei Zhou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> GPUs are critical for compute-intensive applications, yet emerging workloads such as recommender systems, graph analytics, and data analytics often exceed GPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as external memory, and the GPU-centric approach enables GPU threads to directly issue NVMe requests, further avoiding CPU intervention. However, current GPU-centric approaches adopt synchronous I/O, forcing threads to stall during long communication delays.   We propose AGILE, a lightweight asynchronous GPU-centric I/O library that eliminates deadlock risks and integrates a flexible HBM-based software cache. AGILE overlaps computation and I/O, improving performance by up to 1.88$\times$ across workloads with diverse computation-to-communication ratios. Compared to BaM on DLRM, AGILE achieves up to 1.75$\times$ speedup through efficient design and overlapping; on graph applications, AGILE reduces software cache overhead by up to 3.12$\times$ and NVMe I/O overhead by up to 2.85$\times$; AGILE also lowers per-thread register usage by up to 1.32$\times$.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-26T01:45:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.19365v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.19365v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 Strata: Hierarchical Context Caching for Long Context Language Model
  Serving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhiqiang Xie, Ziyi Xu, Mark Zhao, Yuwei An, Vikram Sharma Mailthody, Scott Mahlke, Michael Garland, Christos Kozyrakis
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) with expanding context windows face significant performance hurdles. While caching key-value (KV) states is critical for avoiding redundant computation, the storage footprint of long-context caches quickly exceeds GPU memory capacity, forcing production systems to adopt hierarchical caching across memory hierarchies. However, transferring large cached contexts back to the GPU introduces severe performance bottlenecks: fragmented I/O from paged layouts prevents full bandwidth utilization, and existing schedulers fail to account for cache-loading delays, leaving systems loading-bound rather than compute-bound. We present Strata, a hierarchical context caching framework designed for efficient long context LLM serving. Strata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling GPU and CPU memory layouts and employs cache-aware request scheduling to balance compute with I/O latency and overlapping unavoidable stalls with complementary tasks. Built on SGLang and deployed in production, Strata achieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache and 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without degrading short-context performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-26T00:09:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18572v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18572v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 Real-time 3D Visualization of Radiance Fields on Light Field Displays</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jonghyun Kim, Cheng Sun, Michael Stengel, Matthew Chan, Andrew Russell, Jaehyun Jung, Wil Braithwaite, Shalini De Mello, David Luebke
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Radiance fields have revolutionized photo-realistic 3D scene visualization by enabling high-fidelity reconstruction of complex environments, making them an ideal match for light field displays. However, integrating these technologies presents significant computational challenges, as light field displays require multiple high-resolution renderings from slightly shifted viewpoints, while radiance fields rely on computationally intensive volume rendering. In this paper, we propose a unified and efficient framework for real-time radiance field rendering on light field displays. Our method supports a wide range of radiance field representations, including NeRFs, 3D Gaussian Splatting, and Sparse Voxels, within a shared architecture based on a single-pass plane sweeping strategy and caching of shared, non-directional components. The framework generalizes across different scene formats without retraining, and avoids redundant computation across views. We further demonstrate a real-time interactive application on a Looking Glass display, achieving 200+ FPS at 512p across 45 views, enabling seamless, immersive 3D interaction. On standard benchmarks, our method achieves up to 22x speedup compared to independently rendering each view, while preserving image quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T22:21:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.GR</span><span>eess.IV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18540v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18540v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 DiskJoin: Large-scale Vector Similarity Join with SSD</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yanqi Chen, Xiao Yan, Alexandra Meliou, Eric Lo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Similarity join--a widely used operation in data science--finds all pairs of items that have distance smaller than a threshold. Prior work has explored distributed computation methods to scale similarity join to large data volumes but these methods require a cluster deployment, and efficiency suffers from expensive inter-machine communication. On the other hand, disk-based solutions are more cost-effective by using a single machine and storing the large dataset on high-performance external storage, such as NVMe SSDs, but in these methods the disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin, the first disk-based similarity join algorithm that can process billion-scale vector datasets efficiently on a single machine. DiskJoin improves disk I/O by tailoring the data access patterns to avoid repetitive accesses and read amplification. It also uses main memory as a dynamic cache and carefully manages cache eviction to improve cache hit rate and reduce disk retrieval time. For further acceleration, we adopt a probabilistic pruning technique that can effectively prune a large number of vector pairs from computation. Our evaluation on real-world, large-scale datasets shows that DiskJoin significantly outperforms alternatives, achieving speedups from 50x to 1000x.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T21:07:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18494v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18494v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yang Xiang, Fernando García-Redondo, Arvind Sharma, Van Dai Nguyen, Andrea Fantini, Philippe Matagne, Siddharth Rao, Subhali Subhechha, Lynn Verschueren, Mohammed Aftab Baig, Marie Garcia Bardon, Geert Hellings
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This work explores the cross-node scaling potential of SOT-MRAM for last-level caches (LLCs) under heterogeneous system scaling paradigm. We perform extensive Design-Technology Co-Optimization (DTCO) exercises to evaluate the bitcell footprint for different cell configurations at a representative 7 nm technology and to assess their implications on read and write power-performance. We crucially identify the MTJ routing struggle in conventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary bitcell area scaling challenge and propose to use BEOL read selectors (BEOL RSs) that enable (10 -- 40) % bitcell area reduction and eventually match sub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet the required SOT switching current, provided the magnetic free layer properties be engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This is particularly to attribute to their (i) more available Si fins for write transistor and (ii) lower bitline resistance at reduced cell width. We nevertheless underscore the read tradeoff associated with BEOL RSs, with the low-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the imperfectly rectifying diode selectors suffering (2.5 -- 5)$\times$ energy cost relative to 2T1R. This article thus highlights the realistic prospects and hurdles of BEOL RSs towards holistic power-performance-area scaling of SOT-MRAM.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T08:49:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.ET</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18250v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18250v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 MARM: Unlocking the Future of Recommendation Systems through Memory
  Augmentation and Scalable Complexity</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiao Lv, Jiangxia Cao, Shijie Guan, Xiaoyou Zhou, Zhiguang Qi, Yaqiang Zang, Ming Li, Ben Wang, Kun Gai, Guorui Zhou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Scaling-law has guided the language model designing for past years, however, it is worth noting that the scaling laws of NLP cannot be directly applied to RecSys due to the following reasons: (1) The amount of training samples and model parameters is typically not the bottleneck for the model. Our recommendation system can generate over 50 billion user samples daily, and such a massive amount of training data can easily allow our model parameters to exceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the stability and robustness of the recommendation system, it is essential to control computational complexity FLOPs carefully. Considering the above differences with LLM, we can draw a conclusion that: for a RecSys model, compared to model parameters, the computational complexity FLOPs is a more expensive factor that requires careful control. In this paper, we propose our milestone work, MARM (Memory Augmented Recommendation Model), which explores a new cache scaling-laws successfully.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T15:48:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span><span>N/A</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.09425v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.09425v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 ILRe: Intermediate Layer Retrieval for Context Compression in Causal
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Manlai Liang, Mandi Liu, Jiangzhou Ji, Huaijun Li, Haobo Yang, Yaohan He, Jinlong Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have demonstrated success across many benchmarks. However, they still exhibit limitations in long-context scenarios, primarily due to their short effective context length, quadratic computational complexity, and high memory overhead when processing lengthy inputs. To mitigate these issues, we introduce a novel context compression pipeline, called Intermediate Layer Retrieval (ILRe), which determines one intermediate decoder layer offline, encodes context by streaming chunked prefill only up to that layer, and recalls tokens by the attention scores between the input query and full key cache in that specified layer. In particular, we propose a multi-pooling kernels allocating strategy in the token recalling process to maintain the completeness of semantics. Our approach not only reduces the prefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance comparable to or better than the full context in the long context scenarios. Without additional post training or operator development, ILRe can process a single $1M$ tokens request in less than half a minute (speedup $\approx 180\times$) and scores RULER-$1M$ benchmark of $\approx 79.8$ with model Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T10:59:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17892v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17892v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 SuperGen: An Efficient Ultra-high-resolution Video Generation System
  with Sketching and Tiling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fanjiang Ye, Zepeng Zhao, Yi Mu, Jucheng Shen, Renjie Li, Kaijian Wang, Desen Sun, Saurabh Agarwal, Myungjin Lee, Triston Cao, Aditya Akella, Arvind Krishnamurthy, T. S. Eugene Ng, Zhengzhong Tu, Yuke Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion models have recently achieved remarkable success in generative tasks (e.g., image and video generation), and the demand for high-quality content (e.g., 2K/4K videos) is rapidly increasing across various domains. However, generating ultra-high-resolution videos on existing standard-resolution (e.g., 720p) platforms remains challenging due to the excessive re-training requirements and prohibitively high computational and memory costs. To this end, we introduce SuperGen, an efficient tile-based framework for ultra-high-resolution video generation. SuperGen features a novel training-free algorithmic innovation with tiling to successfully support a wide range of resolutions without additional training efforts while significantly reducing both memory footprint and computational complexity. Moreover, SuperGen incorporates a tile-tailored, adaptive, region-aware caching strategy that accelerates video generation by exploiting redundancy across denoising steps and spatial regions. SuperGen also integrates cache-guided, communication-minimized tile parallelism for enhanced throughput and minimized latency. Evaluations demonstrate that SuperGen harvests the maximum performance gains while achieving high output quality across various benchmarks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T07:49:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17756v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17756v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 OmniCache: A Trajectory-Oriented Global Perspective on Training-Free
  Cache Reuse for Diffusion Transformer Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Huanpeng Chu, Wei Wu, Guanyu Fen, Yutao Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion models have emerged as a powerful paradigm for generative tasks such as image synthesis and video generation, with Transformer architectures further enhancing performance. However, the high computational cost of diffusion Transformers-stemming from a large number of sampling steps and complex per-step computations-presents significant challenges for real-time deployment. In this paper, we introduce OmniCache, a training-free acceleration method that exploits the global redundancy inherent in the denoising process. Unlike existing methods that determine caching strategies based on inter-step similarities and tend to prioritize reusing later sampling steps, our approach originates from the sampling perspective of DIT models. We systematically analyze the model's sampling trajectories and strategically distribute cache reuse across the entire sampling process. This global perspective enables more effective utilization of cached computations throughout the diffusion trajectory, rather than concentrating reuse within limited segments of the sampling procedure. In addition, during cache reuse, we dynamically estimate the corresponding noise and filter it out to reduce its impact on the sampling direction. Extensive experiments demonstrate that our approach accelerates the sampling process while maintaining competitive generative quality, offering a promising and practical solution for efficient deployment of diffusion-based generative models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T03:07:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.16212v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.16212v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters
  at Scale</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ge Shi, Hanieh Sadri, Qian Wang, Yu Zhang, Ying Xiong, Yong Zhang, Zhenan Fan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large language models to enhance their task-specific performance by selectively tuning the top-activated experts for the task. Serving these fine-tuned models at scale is challenging: deploying merged models in isolation is prohibitively resource-hungry, while existing multi-adapter serving systems with LoRA-style additive updates are incompatible with ESFT's expert-oriented paradigm. We present ExpertWeave, a system that serves multiple ESFT adapters concurrently over a single shared MoE base model, drastically reducing the memory footprint and improving resource utilization. To seamlessly integrate into existing inference pipelines for MoE models with non-intrusive modifications and minimal latency overhead, ExpertWeave introduces a virtual-memory-assisted expert weight manager that co-locates base-model and adapter experts without incurring memory overhead from fragmentation, and a fused kernel for batched rerouting to enable lightweight redirection of tokens to the appropriate experts at runtime. Our evaluations show that ExpertWeave can simultaneously serve multiple adapters of a 16B MoE model on a single accelerator where the baseline runs out of memory, or provides up to 94x more KV cache capacity and achieves up to 18% higher throughput while using comparable resources, all without compromising model accuracy. ExpertWeave maintains low overhead even when scaling to 20 adapters, with a 4-11% latency increase compared with serving the base model alone. Source code will be released soon.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T03:05:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17624v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17624v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated
  Prefill and Decode Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaojuan Tang, Fanxu Meng, Pingzhi Tang, Yuxuan Wang, Di Yin, Xing Sun, Muhan Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses key-value states into a low-rank latent vector, caching only this vector to reduce memory. In tensor parallelism (TP), however, attention heads are computed across multiple devices, and each device must load the full cache, eroding the advantage of MLA over Grouped Query Attention (GQA). We propose Tensor-Parallel Latent Attention (TPLA): a scheme that partitions both the latent representation and each head's input dimension across devices, performs attention independently per shard, and then combines results with an all-reduce. TPLA preserves the benefits of a compressed KV cache while unlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in TPLA still leverages the full latent representation, maintaining stronger representational capacity. TPLA is drop-in compatible with models pre-trained using MLA: it supports MLA-style prefilling and enables efficient tensor-parallel decoding without retraining. Applying simple orthogonal transforms -- e.g., the Hadamard transform or PCA -- before TP slicing further mitigates cross-shard interference, yielding minimal accuracy degradation. By reducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x and 1.93x speedups, respectively, at a 32K-token context length while maintaining performance on commonsense and LongBench benchmarks. TPLA can be implemented with FlashAttention-3, enabling practical end-to-end acceleration.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T02:24:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15881v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15881v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD
  NPUs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aadesh Deshmukh, Venkata Yaswanth Raparti, Samuel Hsu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformer-based deep learning models are increasingly deployed on energy, and DRAM bandwidth constrained devices such as laptops and gaming consoles, which presents significant challenges in meeting the latency requirements of the models. The industry is turning to neural processing units (NPUs) for superior performance-per-watt (perf/watt); however, efficiently mapping dynamic attention layers to the NPUs remains a challenging task. For optimizing perf/watt, AMD XDNA NPUs employ software managed caches and share system memory with host. This requires substantial engineering effort to unlock efficient tiling, buffer allocation, and data movement to extract the maximum efficiency from the device. This paper introduces Zen-Attention, a framework that optimizes DRAM bandwidth utilization in the attention layer of models by systematically exploring the complex design space of layer folding, tiling, and data-movement on the interconnect, and the tensor layouts to come up with an optimal solution. Our evaluation includes comparative analysis of end-to-end model latency and specific attention latency in each model. We demonstrate how the framework enhances mapping capabilities by varying input dimensions, which require padding and masking in the attention block. For representative transformer models, the Zen-Attention Framework achieves up to 4x improvement in the latency of the attention block and up to 32% improvement in end-to-end network latency compared to the baseline Unfolded- approaches.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T01:33:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17593v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17593v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 RT-Cache: Training-Free Retrieval for Real-Time Manipulation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Owen Kwon, Abraham George, Alison Bartsch, Amir Barati Farimani
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Real robots are expected to repeat the same behavior in new environments with very little new data, yet modern controllers either incur heavy per-step inference or require deployment-time fine-tuning. We propose RT-Cache, a training-free retrieval-as-control pipeline that caches diverse image action trajectories in a unified vector memory and, at test time, embeds the current frame to retrieve and replay multi-step snippets, replacing per-step model calls. A hierarchical search keeps lookups sub-second at million scale, shifting cost from compute to storage and enabling real-time control on modest GPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher success and lower completion time than strong retrieval baselines (approximately x2 higher success and ~30% faster in our settings), and a single-episode anchoring study shows immediate adaptation to a more complex, contact-rich task without fine-tuning. RT-Cache turns experience into an append-only memory, offering a simple, scalable path to few-shot deployment today and a foundation for multimodal keys and optional integration with high-level policies. Project page: https://rt-cache.github.io/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T00:15:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.AI</span><span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.09040v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.09040v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 PRISM: Efficient Long-Range Reasoning With Short-Context LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dulhan Jayalath, James Bradley Wendt, Nicholas Monath, Sandeep Tata, Beliz Gunel
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Long-range tasks demand reasoning over long inputs. However, existing solutions are limited, e.g., long-context models require large compute budgets, parameter-efficient fine-tuning (PEFT) needs training data, and retrieval-augmented generation (RAG) entails complex task-specific designs. Though in-context approaches overcome many of these issues, methods with short-context LLMs are inefficient, trading context for processing more tokens. We introduce PRISM, a highly token-efficient in-context method based on structured schemas that outperforms baselines on diverse tasks with 4x shorter contexts. This approach produces concise outputs and efficiently leverages key-value (KV) caches to reduce costs by up to 54%. PRISM scales down to tiny contexts without increasing costs or sacrificing quality, and generalizes to new tasks with minimal effort by generating schemas from task descriptions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-24T22:09:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.18914v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.18914v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 Evaluating Compiler Optimization Impacts on zkVM Performance</h2>
                <div class="authors">
                    <strong>Authors:</strong> Thomas Gassmann, Stefanos Chaliasos, Thodoris Sotiropoulos, Zhendong Su
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Zero-knowledge proofs (ZKPs) are the cornerstone of programmable cryptography. They enable (1) privacy-preserving and verifiable computation across blockchains, and (2) an expanding range of off-chain applications such as credential schemes. Zero-knowledge virtual machines (zkVMs) lower the barrier by turning ZKPs into a drop-in backend for standard compilation pipelines. This lets developers write proof-generating programs in conventional languages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits. However, these VMs inherit compiler infrastructures tuned for traditional architectures rather than for proof systems. In particular, standard compiler optimizations assume features that are absent in zkVMs, including cache locality, branch prediction, or instruction-level parallelism. Therefore, their impact on proof generation is questionable.   We present the first systematic study of the impact of compiler optimizations on zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an unoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero and SP1). While standard LLVM optimization levels do improve zkVM performance (over 40\%), their impact is far smaller than on traditional CPUs, since their decisions rely on hardware features rather than proof constraints. Guided by a fine-grained pass-level analysis, we~\emph{slightly} refine a small set of LLVM passes to be zkVM-aware, improving zkVM execution time by up to 45\% (average +4.6\% on RISC Zero, +1\% on SP1) and achieving consistent proving-time gains. Our work highlights the potential of compiler-level optimizations for zkVM performance and opens new direction for zkVM-specific passes, backends, and superoptimizers.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-24T20:51:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17518v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17518v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 Practical Insertion-Only Convex Hull</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ivor van der Hoog, Henrik Reinstädtler, Eva Rotenberg
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Convex hull data structures are fundamental in computational geometry. We study insertion-only data structures, supporting various containment and intersection queries. When $P$ is sorted by $x$- or $y$-coordinate, convex hulls can be constructed in linear time using classical algorithms such as Graham scan. We investigate a variety of methods tailored to the insertion-only setting. We explore a broad selection of trade-offs involving robustness, memory access patterns, and space usage, providing an extensive evaluation of both existing and novel techniques. Logarithmic-time methods rely on pointer-based tree structures, which suffer in practice due to poor memory locality. Motivated by this, we develop a vector-based solution inspired by Overmars' logarithmic method. Our structure has worse asymptotic bounds, supporting queries in $O(\log^2 n)$ time, but stores data in $O(\log n)$ contiguous vectors, greatly improving cache performance.   Through empirical evaluation on real-world and synthetic data sets, we uncover surprising trends. Let $h$ denote the size of the convex hull. We show that a na\"ive $O(h)$ insertion-only algorithm based on Graham scan consistently outperforms both theoretical and practical state-of-the-art methods under realistic workloads, even on data sets with rather large convex hulls. While tree-based methods with $O(\log h)$ update times offer solid theoretical guarantees, they are never optimal in practice. In contrast, our vector-based logarithmic method, despite its theoretically inferior bounds, is highly competitive across all tested scenarios. It is optimal whenever the convex hull becomes large.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-24T19:28:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17496v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17496v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 TreePO: Bridging the Gap of Policy Optimization and Efficacy and
  Inference Efficiency with Heuristic Tree-based Modeling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yizhi Li, Qingshui Gu, Zhoufutu Wen, Ziniu Li, Tianshun Xing, Shuyue Guo, Tianyu Zheng, Xin Zhou, Xingwei Qu, Wangchunshu Zhou, Zheng Zhang, Wei Shen, Qian Liu, Chenghua Lin, Jian Yang, Ge Zhang, Wenhao Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in aligning large language models via reinforcement learning have achieved remarkable gains in solving complex reasoning problems, but at the cost of expensive on-policy rollouts and limited exploration of diverse reasoning paths. In this work, we introduce TreePO, involving a self-guided rollout algorithm that views sequence generation as a tree-structured searching process. Composed of dynamic tree sampling policy and fixed-length segment decoding, TreePO leverages local uncertainty to warrant additional branches. By amortizing computation across common prefixes and pruning low-value paths early, TreePO essentially reduces the per-update compute burden while preserving or enhancing exploration diversity. Key contributions include: (1) a segment-wise sampling algorithm that alleviates the KV cache burden through contiguous segments and spawns new branches along with an early-stop mechanism; (2) a tree-based segment-level advantage estimation that considers both global and local proximal policy optimization. and (3) analysis on the effectiveness of probability and quality-driven dynamic divergence and fallback strategy. We empirically validate the performance gain of TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours from 22\% up to 43\% of the sampling design for the trained models, meanwhile showing up to 40\% reduction at trajectory-level and 35\% at token-level sampling compute for the existing models. While offering a free lunch of inference efficiency, TreePO reveals a practical path toward scaling RL-based post-training with fewer samples and less compute. Home page locates at https://m-a-p.ai/TreePO.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-24T16:52:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17445v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17445v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 TinySR: Pruning Diffusion for Real-World Image Super-Resolution</h2>
                <div class="authors">
                    <strong>Authors:</strong> Linwei Dong, Qingnan Fan, Yuhang Yu, Qi Zhang, Jinwei Chen, Yawei Luo, Changqing Zou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Real-world image super-resolution (Real-ISR) focuses on recovering high-quality images from low-resolution inputs that suffer from complex degradations like noise, blur, and compression. Recently, diffusion models (DMs) have shown great potential in this area by leveraging strong generative priors to restore fine details. However, their iterative denoising process incurs high computational overhead, posing challenges for real-time applications. Although one-step distillation methods, such as OSEDiff and TSD-SR, offer faster inference, they remain fundamentally constrained by their large, over-parameterized model architectures. In this work, we present TinySR, a compact yet effective diffusion model specifically designed for Real-ISR that achieves real-time performance while maintaining perceptual quality. We introduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy to facilitate more effective decision-making in depth pruning. We achieve VAE compression through channel pruning, attention removal and lightweight SepConv. We eliminate time- and prompt-related modules and perform pre-caching techniques to further speed up the model. TinySR significantly reduces computational cost and model size, achieving up to 5.68x speedup and 83% parameter reduction compared to its teacher TSD-SR, while still providing high quality results.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-24T16:17:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17434v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17434v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 DiCache: Let Diffusion Model Determine Its Own Cache</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiazi Bu, Pengyang Ling, Yujie Zhou, Yibin Wang, Yuhang Zang, Tong Wu, Dahua Lin, Jiaqi Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent years have witnessed the rapid development of acceleration techniques for diffusion models, especially caching-based acceleration methods. These studies seek to answer two fundamental questions: "When to cache" and "How to use cache", typically relying on predefined empirical laws or dataset-level priors to determine the timing of caching and utilizing handcrafted rules for leveraging multi-step caches. However, given the highly dynamic nature of the diffusion process, they often exhibit limited generalizability and fail on outlier samples. In this paper, a strong correlation is revealed between the variation patterns of the shallow-layer feature differences in the diffusion model and those of final model outputs. Moreover, we have observed that the features from different model layers form similar trajectories. Based on these observations, we present DiCache, a novel training-free adaptive caching strategy for accelerating diffusion models at runtime, answering both when and how to cache within a unified framework. Specifically, DiCache is composed of two principal components: (1) Online Probe Profiling Scheme leverages a shallow-layer online probe to obtain a stable prior for the caching error in real time, enabling the model to autonomously determine caching schedules. (2) Dynamic Cache Trajectory Alignment combines multi-step caches based on shallow-layer probe feature trajectory to better approximate the current feature, facilitating higher visual quality. Extensive experiments validate DiCache's capability in achieving higher efficiency and improved visual fidelity over state-of-the-art methods on various leading diffusion models including WAN 2.1, HunyuanVideo for video generation, and Flux for image generation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-24T13:30:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17356v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17356v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained
  Elastic Long-Context LLM Serving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bingyang Wu, Zili Zhang, Yinmin Zhong, Guanzhe Huang, Yibo Zhu, Xuanzhe Liu, Xin Jin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Prefix caching is crucial to accelerate multi-turn interactions and requests with shared prefixes. At the cluster level, existing prefix caching systems are tightly coupled with request scheduling to optimize cache efficiency and computation performance together, leading to load imbalance, data redundancy, and memory fragmentation of caching systems across instances. To address these issues, memory pooling is promising to shield the scheduler from the underlying cache management so that it can focus on the computation optimization. However, because existing prefix caching systems only transfer increasingly longer prefix caches between instances, they cannot achieve low-latency memory pooling.   To address these problems, we propose a unified segment-level prefix cache pool, TokenLake. It uses a declarative cache interface to expose requests' query tensors, prefix caches, and cache-aware operations to TokenLake for efficient pooling. Powered by this abstraction, TokenLake can manage prefix cache at the segment level with a heavy-hitter-aware load balancing algorithm to achieve better cache load balance, deduplication, and defragmentation. TokenLake also transparently minimizes the communication volume of query tensors and new caches. Based on TokenLake, the scheduler can schedule requests elastically by using existing techniques without considering prefix cache management. Evaluations on real-world workloads show that TokenLake can improve throughput by up to 2.6$\times$ and 2.0$\times$ and boost hit rate by 2.0$\times$ and 2.1$\times$, compared to state-of-the-art cache-aware routing and cache-centric PD-disaggregation solutions, respectively.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-24T05:45:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17219v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17219v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 DPad: Efficient Diffusion Language Models with Suffix Dropout</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xinhua Chen, Sitao Huang, Cong Guo, Chiyue Wei, Yintao He, Jianyi Zhang, Hai "Helen" Li, Yiran Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion-based Large Language Models (dLLMs) parallelize text generation by framing decoding as a denoising process, but suffer from high computational overhead since they predict all future suffix tokens at each step while retaining only a small fraction. We propose Diffusion Scratchpad (DPad), a training-free method that restricts attention to a small set of nearby suffix tokens, preserving fidelity while eliminating redundancy. DPad integrates two strategies: (i) a sliding window, which maintains a fixed-length suffix window, and (ii) distance-decay dropout, which deterministically removes distant suffix tokens before attention computation. This simple design is compatible with existing optimizations such as prefix caching and can be implemented with only a few lines of code. Comprehensive evaluations across multiple benchmarks on LLaDA-1.5 and Dream models demonstrate that DPad delivers up to $\mathbf{61.4\times}$ speedup over vanilla dLLMs while maintaining comparable accuracy, highlighting its potential for efficient and scalable long-sequence inference. Our code is available at https://github.com/Crys-Chen/DPad.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-23T20:28:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.14148v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.14148v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nishant Gavhane, Arush Mehrotra, Rohit Chawla, Peter Proenca
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The deployment of large-scale Mixture-of-Experts (MoE) models on edge devices presents significant challenges due to memory constraints. While MoE architectures enable efficient utilization of computational resources by activating only a subset of experts per inference, they require careful memory management to operate efficiently in resource-constrained environments. Traditional heuristic-based expert caching strategies such as MoE-Infinity struggle to maintain high cache hit rates as models parameters scale. In this work, we introduce MoE-Beyond, a learning-based expert activation predictor trained to predict expert activations during autoregressive decoding. By framing the task as a multi-label sequence prediction problem, we train a lightweight transformer model on 66 million expert activation traces extracted from LDJnr-Puffin dataset [5] using DeepSeek-V2-Chat-Lite MoE. Our predictor generalizes effectively across unseen prompts from WebGLM-QA dataset [6], achieving 97.5% accuracy and an 86.6% F1-score. Simulation results show that MoE-Beyond improves GPU cache hit rate from 17% to 72% when only 10% of experts fit in GPU cache, outperforming heuristic baselines.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-23T20:28:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17137v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17137v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 VQL: An End-to-End Context-Aware Vector Quantization Attention for
  Ultra-Long User Behavior Modeling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kaiyuan Li, Yongxiang Tang, Yanhua Cheng, Yong Bai, Yanxiang Zeng, Chao Wang, Xialong Liu, Peng Jiang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In large-scale recommender systems, ultra-long user behavior sequences encode rich signals of evolving interests. Extending sequence length generally improves accuracy, but directly modeling such sequences in production is infeasible due to latency and memory constraints. Existing solutions fall into two categories: (1) top-k retrieval, which truncates the sequence and may discard most attention mass when L >> k; and (2) encoder-based compression, which preserves coverage but often over-compresses and fails to incorporate key context such as temporal gaps or target-aware signals. Neither class achieves a good balance of low-loss compression, context awareness, and efficiency.   We propose VQL, a context-aware Vector Quantization Attention framework for ultra-long behavior modeling, with three innovations. (1) Key-only quantization: only attention keys are quantized, while values remain intact; we prove that softmax normalization yields an error bound independent of sequence length, and a codebook loss directly supervises quantization quality. This also enables L-free inference via offline caches. (2) Multi-scale quantization: attention heads are partitioned into groups, each with its own small codebook, which reduces quantization error while keeping cache size fixed. (3) Efficient context injection: static features (e.g., item category, modality) are directly integrated, and relative position is modeled via a separable temporal kernel. All context is injected without enlarging the codebook, so cached representations remain query-independent.   Experiments on three large-scale datasets (KuaiRand-1K, KuaiRec, TMALL) show that VQL consistently outperforms strong baselines, achieving higher accuracy while reducing inference latency, establishing a new state of the art in balancing accuracy and efficiency for ultra-long sequence recommendation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-23T19:58:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17125v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17125v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 Learned Structure in CARTRIDGES: Keys as Shareable Routers in
  Self-Studied Representations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Maurizio Diaz
                </div>
                <div class="summary">
                    <strong>Summary:</strong> A bottleneck for long-context LLM inference is the linearly growing KV cache. Recent work has proposed CARTRIDGES, an approach which leverages offline compute to train a much smaller KV cache than is typically required for a full document (up to 40x less memory usage at inference time). In this paper, we present the first mechanistic exploration of the learned CARTRIDGE key-value cache structure. In particular, we propose that (1) CARTRIDGE keys act as stable, shareable retrieval routers for the compressed corpora and (2) most of the learned compression occurs within the CARTRIDGE value vectors. We present empirical evidence of our routing theory across tasks, model families, and model sizes; for example, we can ablate the learned CARTRIDGE key vectors between tasks with little performance loss. Finally, we propose a slight improvement in initialization called Sampled Chunk Initialization (SCI). We suggest that SCI can lead to faster CARTRIDGE convergence than previously demonstrated in the literature. Our findings lay the groundwork for broader empirical study of CARTRIDGE training optimization which may be crucial for further scaling.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-23T14:20:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17032v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17032v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 HiCache: Training-free Acceleration of Diffusion Models via Hermite
  Polynomial-based Feature Caching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Liang Feng, Shikang Zheng, Jiacheng Liu, Yuqi Lin, Qinming Zhou, Peiliang Cai, Xinyu Wang, Junjie Chen, Chang Zou, Yue Ma, Linfeng Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion models have achieved remarkable success in content generation but suffer from prohibitive computational costs due to iterative sampling. While recent feature caching methods tend to accelerate inference through temporal extrapolation, these methods still suffer from server quality loss due to the failure in modeling the complex dynamics of feature evolution. To solve this problem, this paper presents HiCache, a training-free acceleration framework that fundamentally improves feature prediction by aligning mathematical tools with empirical properties. Our key insight is that feature derivative approximations in Diffusion Transformers exhibit multivariate Gaussian characteristics, motivating the use of Hermite polynomials-the potentially theoretically optimal basis for Gaussian-correlated processes. Besides, We further introduce a dual-scaling mechanism that ensures numerical stability while preserving predictive accuracy. Extensive experiments demonstrate HiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding baseline quality, maintaining strong performance across text-to-image, video generation, and super-resolution tasks. Core implementation is provided in the appendix, with complete code to be released upon acceptance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-23T10:35:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.16984v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.16984v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 Enhancing Memory Efficiency in Large Language Model Training Through
  Chronos-aware Pipeline Parallelism</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xinyuan Lin, Chenlu Li, Zongle Huang, Chunyu Wang, Bo Xiao, Huazhong Yang, Shishi Duan, Yongpan Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Larger model sizes and longer sequence lengths have empowered the Large Language Model (LLM) to achieve outstanding performance across various domains. However, this progress brings significant storage capacity challenges for LLM pretraining. High Bandwidth Memory (HBM) is expensive and requires more advanced packaging technologies for capacity expansion, creating an urgent need for memory-efficient scheduling strategies. Yet, prior pipeline parallelism schedules have primarily focused on reducing bubble overhead, often neglecting memory efficiency and lacking compatibility with other memory-efficient strategies. Consequently, these methods struggle to meet the storage demands of storage capacity for next-generation LLM. This work presents ChronosPipe, a Chronos-aware pipeline parallelism for memory-efficient LLM pretraining. The core insight of ChronosPipe is to treat HBM as a fast but small 'cache,' optimizing and exploiting temporal locality within LLM pretraining to enhance HBM utilization. ChronosPipe introduces a pipeline scheduling strategy, Chronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal locality of activations. Additionally, it leverages Chronos-Recomp and Chronos-Offload to efficiently harness the intrinsic temporal locality of activations and weights in Deep Neural Networks. Experiment results show that ChronosPipe can expand the trainable model size by 2.4x while maintaining comparable throughput, achieving 1.5x better than the 1F1B strategy combined with recomputation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-23T08:40:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.03182v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.03182v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long
  Sequences</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jungyoub Cha, Hyunjong Kim, Sungzoon Cho
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Speculative decoding is a widely adopted technique for accelerating inference in large language models (LLMs), but its performance degrades on long inputs due to increased attention cost and reduced draft accuracy. We introduce SpecExtend, a drop-in enhancement that improves the performance of speculative decoding on long sequences without any additional training. First, SpecExtend integrates efficient attention mechanisms such as FlashAttention and Hybrid Tree Attention into both the draft and target models. To improve draft accuracy and speed on long inputs without retraining, we propose Cross-model Retrieval, a novel KV cache eviction strategy that uses the target model's attention scores to dynamically select relevant context for the draft model. Extensive evaluations on three long-context understanding datasets show that SpecExtend accelerates standard tree-based speculative decoding by up to 2.22x for inputs up to 16K tokens, providing an effective solution for speculative decoding of long sequences. Our code is available at https://github.com/jycha98/SpecExtend .
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-22T08:45:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span><span>I.2.7; C.4</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.20776v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.20776v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion
  Transformers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shikang Zheng, Liang Feng, Xinyu Wang, Qinming Zhou, Peiliang Cai, Chang Zou, Jiacheng Liu, Yuqi Lin, Junjie Chen, Yue Ma, Linfeng Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion Transformers (DiTs) have demonstrated exceptional performance in high-fidelity image and video generation. To reduce their substantial computational costs, feature caching techniques have been proposed to accelerate inference by reusing hidden representations from previous timesteps. However, current methods often struggle to maintain generation quality at high acceleration ratios, where prediction errors increase sharply due to the inherent instability of long-step forecasting. In this work, we adopt an ordinary differential equation (ODE) perspective on the hidden-feature sequence, modeling layer representations along the trajectory as a feature-ODE. We attribute the degradation of existing caching strategies to their inability to robustly integrate historical features under large skipping intervals. To address this, we propose FoCa (Forecast-then-Calibrate), which treats feature caching as a feature-ODE solving problem. Extensive experiments on image synthesis, video generation, and super-resolution tasks demonstrate the effectiveness of FoCa, especially under aggressive acceleration. Without additional training, FoCa achieves near-lossless speedups of 5.50 times on FLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high quality with a 4.53 times speedup on DiT.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-22T08:34:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.16211v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.16211v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 Joint Cache Placement and Routing in Satellite-Terrestrial Edge
  Computing Network: A GNN-Enabled DRL Approach</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuhao Zheng, Ting You, Kejia Peng, Chang Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this letter, we investigate the problem of joint content caching and routing in satellite-terrestrial edge computing networks (STECNs) to improve caching service for geographically distributed users. To handle the challenges arising from dynamic low Earth orbit (LEO) satellite topologies and heterogeneous content demands, we propose a learning-based framework that integrates graph neural networks (GNNs) with deep reinforcement learning (DRL). The satellite network is represented as a dynamic graph, where GNNs are embedded within the DRL agent to capture spatial and topological dependencies and support routing-aware decision-making. The caching strategy is optimized by formulating the problem as a Markov decision process (MDP) and applying soft actor-critic (SAC) algorithm. Simulation results demonstrate that our approach significantly improves the delivery success rate and reduces communication traffic cost.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-22T07:57:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.16184v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.16184v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yixuan Wang, Haoyu Qiao, Lujun Li, Qingfu Zhu, Wanxiang Che
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) confront significant memory challenges due to the escalating KV cache with increasing sequence length. As a crucial technique, existing cross-layer KV cache sharing methods either necessitate modified model architectures with subsequent pre-training or incur significant performance degradation at high compression rates. To mitigate these challenges, we propose CommonKV, a training-free method for cross-layer KV cache compression through adjacent parameters sharing. Inspired by the high similarity observed in cross-layer hidden states, we utilize Singular Value Decomposition (SVD) to achieve weight sharing across adjacent parameters, resulting in a more easily mergeable latent KV cache. Furthermore, we also introduce an adaptive budget allocation strategy. It dynamically assigns compression budgets based on cosine similarity, ensuring that dissimilar caches are not over-compressed. Experiments across multiple backbone models and benchmarks including LongBench and Ruler demonstrate that the proposed method consistently outperforms existing low-rank and cross-layer approaches at various compression ratios. Moreover, we find that the benefits of CommonKV are orthogonal to other quantization and eviction methods. By integrating these approaches, we can ultimately achieve a 98\% compression ratio without significant performance loss.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-22T06:55:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.16134v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.16134v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 Lightweight and Fast Real-time Image Enhancement via Decomposition of
  the Spatial-aware Lookup Tables</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wontae Kim, Keuntek Lee, Nam Ik Cho
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The image enhancement methods based on 3D lookup tables (3D LUTs) efficiently reduce both model size and runtime by interpolating pre-calculated values at the vertices. However, the 3D LUT methods have a limitation due to their lack of spatial information, as they convert color values on a point-by-point basis. Although spatial-aware 3D LUT methods address this limitation, they introduce additional modules that require a substantial number of parameters, leading to increased runtime as image resolution increases. To address this issue, we propose a method for generating image-adaptive LUTs by focusing on the redundant parts of the tables. Our efficient framework decomposes a 3D LUT into a linear sum of low-dimensional LUTs and employs singular value decomposition (SVD). Furthermore, we enhance the modules for spatial feature fusion to be more cache-efficient. Extensive experimental results demonstrate that our model effectively decreases both the number of parameters and runtime while maintaining spatial awareness and performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-22T06:28:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.IV</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.16121v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.16121v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 Dynamic Optimization of Storage Systems Using Reinforcement Learning
  Techniques</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chiyu Cheng, Chang Zhou, Yang Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The exponential growth of data-intensive applications has placed unprecedented demands on modern storage systems, necessitating dynamic and efficient optimization strategies. Traditional heuristics employed for storage performance optimization often fail to adapt to the variability and complexity of contemporary workloads, leading to significant performance bottlenecks and resource inefficiencies. To address these challenges, this paper introduces RL-Storage, a novel reinforcement learning (RL)-based framework designed to dynamically optimize storage system configurations. RL-Storage leverages deep Q-learning algorithms to continuously learn from real-time I/O patterns and predict optimal storage parameters, such as cache size, queue depths, and readahead settings[1].This work underscores the transformative potential of reinforcement learning techniques in addressing the dynamic nature of modern storage systems. By autonomously adapting to workload variations in real time, RL-Storage provides a robust and scalable solution for optimizing storage performance, paving the way for next-generation intelligent storage infrastructures.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-22T03:36:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.OS</span><span>cs.DC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.00068v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.00068v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based
  Side-Channel Attacks on Fully Associative Randomized Caches</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chris Cao, Gururaj Saileshwar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent work presented at USENIX Security 2025 (SEC'25) claims that occupancy-based attacks can recover AES keys from the MIRAGE randomized cache. In this paper, we examine these claims and find that they arise from a modeling flaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of MIRAGE uses a constant seed to initialize the random number generator used for global evictions in MIRAGE, causing every AES encryption they trace to evict the same deterministic sequence of cache lines. This artificially creates a highly repeatable timing pattern that is not representative of a realistic implementation of MIRAGE, where eviction sequences vary randomly between encryptions. When we instead randomize the eviction seed for each run, reflecting realistic operation, the correlation between AES T-table accesses and attacker runtimes disappears, and the attack fails. These findings show that the reported leakage is an artifact of incorrect modeling, and not an actual vulnerability in MIRAGE.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T22:45:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10431v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10431v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of
  Vision-Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xinyu Chen, Haotian Zhai, Can Zhang, Xiupeng Shi, Ruirui Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In zero-shot setting, test-time adaptation adjusts pre-trained models using unlabeled data from the test phase to enhance performance on unknown test distributions. Existing cache-enhanced TTA methods rely on a low-entropy criterion to select samples for prototype construction, assuming intra-class compactness. However, low-entropy samples may be unreliable under distribution shifts, and the resulting prototypes may not ensure compact intra-class distributions. This study identifies a positive correlation between cache-enhanced performance and intra-class compactness. Based on this observation, we propose a Multi-Cache enhanced Prototype-based Test-Time Adaptation (MCP) featuring three caches: an entropy cache for initializing prototype representations with low-entropy samples, an align cache for integrating visual and textual information to achieve compact intra-class distributions, and a negative cache for prediction calibration using high-entropy samples. We further developed MCP++, a framework incorporating cross-modal prototype alignment and residual learning, introducing prototype residual fine-tuning. Comparative and ablation experiments across 15 downstream tasks demonstrate that the proposed method and framework achieve state-of-the-art generalization performance. Project Page available at: https://zhaihaotian.github.io/MCP-ICCV25/
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T20:13:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.01225v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.01225v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO
  Serving and Fast Scaling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zahra Yousefijamarani, Xinglu Wang, Qian Wang, Morgan Lindsay Heisler, Taha Shabani, Niloofar Gholipour, Parham Yassini, Hong Chang, Kan Chen, Qiantao Zhang, Xiaolong Bai, Jiannan Wang, Ying Xiong, Yong Zhang, Zhenan Fan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modern large language model (LLM) serving systems face challenges from highly variable requests with diverse lengths, priorities, and stage-specific service-level objectives (SLOs). Meeting these requires real-time scheduling, rapid and cost-effective scaling, and support for both collocated and disaggregated Prefill/Decode (P/D) architectures.   We present \textbf{HyperFlexis}, a unified LLM serving system that integrates algorithmic and system-level innovations to jointly optimize scheduling and scaling under multiple SLOs. It features a multi-SLO-aware scheduler that leverages budget estimation and request prioritization to ensure proactive SLO compliance for both new and ongoing requests. The system supports prefill- and decode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV cache transfers. It also enables cost-effective scaling decisions, prefill-decode instance linking during scaling, and rapid P/D role transitions. To accelerate scaling and reduce cold-start latency, a device-to-device (D2D) weight transfer mechanism is proposed that lowers weight loading overhead by up to \textbf{19.39$\times$}. These optimizations allow the system to achieve up to \textbf{4.44$\times$} higher SLO attainment, \textbf{65.82\%} lower request latency, and cost parity with state-of-the-art baselines. The code will be released soon.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T18:40:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15919v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15919v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 StreamMem: Query-Agnostic KV Cache Memory for Streaming Video
  Understanding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yanlai Yang, Zhuokai Zhao, Satya Narayan Shukla, Aashu Singh, Shlok Kumar Mishra, Lizhu Zhang, Mengye Ren
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multimodal large language models (MLLMs) have made significant progress in visual-language reasoning, but their ability to efficiently handle long videos remains limited. Despite recent advances in long-context MLLMs, storing and attending to the key-value (KV) cache for long visual contexts incurs substantial memory and computational overhead. Existing visual compression methods require either encoding the entire visual context before compression or having access to the questions in advance, which is impractical for long video understanding and multi-turn conversational settings. In this work, we propose StreamMem, a query-agnostic KV cache memory mechanism for streaming video understanding. Specifically, StreamMem encodes new video frames in a streaming manner, compressing the KV cache using attention scores between visual tokens and generic query tokens, while maintaining a fixed-size KV memory to enable efficient question answering (QA) in memory-constrained, long-video scenarios. Evaluation on three long video understanding and two streaming video question answering benchmarks shows that StreamMem achieves state-of-the-art performance in query-agnostic KV cache compression and is competitive with query-aware compression approaches.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T16:56:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15717v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15717v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 GoVector: An I/O-Efficient Caching Strategy for High-Dimensional Vector
  Nearest Neighbor Search</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yijie Zhou, Shengyuan Lin, Shufeng Gong, Song Yu, Shuhao Fan, Yanfeng Zhang, Ge Yu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Graph-based high-dimensional vector indices have become a mainstream solution for large-scale approximate nearest neighbor search (ANNS). However, their substantial memory footprint often requires storage on secondary devices, where frequent on-demand loading of graph and vector data leads to I/O becoming the dominant bottleneck, accounting for over 90\% of query latency. Existing static caching strategies mitigate this issue only in the initial navigation phase by preloading entry points and multi-hop neighbors, but they fail in the second phase where query-dependent nodes must be dynamically accessed to achieve high recall. We propose GoVector, an I/O-efficient caching strategy tailored for disk-based graph indices. GoVector combines (1) a static cache that stores entry points and frequently accessed neighbors, and (2) a dynamic cache that adaptively captures nodes with high spatial locality during the second search phase. To further align storage layout with similarity-driven search patterns, GoVector reorders nodes on disk so that similar vectors are colocated on the same or adjacent pages, thereby improving locality and reducing I/O overhead. Extensive experiments on multiple public datasets show that GoVector achieves substantial performance improvements. At 90% recall, it reduces I/O operations by 46% on average, increases query throughput by 1.73x, and lowers query latency by 42% compared to state-of-the-art disk-based graph indexing systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T16:21:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15694v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15694v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 CausalMesh: A Formally Verified Causal Cache for Stateful Serverless
  Computing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haoran Zhang, Zihao Zhang, Shuai Mu, Sebastian Angel, Vincent Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Stateful serverless workflows consist of multiple serverless functions that access state on a remote database. Developers sometimes add a cache layer between the serverless runtime and the database to improve I/O latency. However, in a serverless environment, functions in the same workflow may be scheduled to different nodes with different caches, which can cause non-intuitive anomalies. This paper presents CausalMesh, a novel approach to causally consistent caching in environments where a computation may migrate from one machine to another, such as in serverless computing. CausalMesh is the first cache system that supports coordination-free and abort-free read/write operations and read transactions when clients roam among multiple servers. CausalMesh also supports read-write transactional causal consistency in the presence of client roaming, but at the cost of abort-freedom.   We have formally verified CausalMesh's protocol in Dafny, and our experimental evaluation shows that CausalMesh has lower latency and higher throughput than existing proposals
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T15:25:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.14778/3704965.3704969' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.15647v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15647v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 GATEBLEED: Exploiting On-Core Accelerator Power Gating for High
  Performance & Stealthy Attacks on AI</h2>
                <div class="authors">
                    <strong>Authors:</strong> Joshua Kalyanapu, Farshad Dizani, Darsh Asher, Azam Ghanbari, Rosario Cammarota, Aydin Aysu, Samira Mirbagher Ajorpaz
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As power consumption from AI training and inference continues to increase, AI accelerators are being integrated directly into the CPU. Intel's Advanced Matrix Extensions (AMX) is one such example, debuting on the 4th generation Intel Xeon Scalable CPU. We discover a timing side and covert channel, GATEBLEED, caused by the aggressive power gating utilized to keep the CPU within operating limits. We show that the GATEBLEED side channel is a threat to AI privacy as many ML models such as transformers and CNNs make critical computationally-heavy decisions based on private values like confidence thresholds and routing logits. Timing delays from selective powering down of AMX components mean that each matrix multiplication is a potential leakage point when executed on the AMX accelerator. Our research identifies over a dozen potential gadgets across popular ML libraries (HuggingFace, PyTorch, TensorFlow, etc.), revealing that they can leak sensitive and private information. GATEBLEED poses a risk for local and remote timing inference, even under previous protective measures. GATEBLEED can be used as a high performance, stealthy remote covert channel and a generic magnifier for timing transmission channels, capable of bypassing traditional cache defenses to leak arbitrary memory addresses and evading state of the art microarchitectural attack detectors under realistic network conditions and system configurations in which previous attacks fail. We implement an end-to-end microarchitectural inference attack on a transformer model optimized with Intel AMX, achieving a membership inference accuracy of 81% and a precision of 0.89. In a CNN-based or transformer-based mixture-of-experts model optimized with Intel AMX, we leak expert choice with 100% accuracy. To our knowledge, this is the first side-channel attack on AI privacy that exploits hardware optimizations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T14:58:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.17033v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.17033v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 Efficient Mixed-Precision Large Language Model Inference with TurboMind</h2>
                <div class="authors">
                    <strong>Authors:</strong> Li Zhang, Youhe Jiang, Guoliang He, Xin Chen, Han Lv, Qian Yao, Fangcheng Fu, Kai Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Mixed-precision inference techniques reduce the memory and computational demands of Large Language Models (LLMs) by applying hybrid precision formats to model weights, activations, and KV caches. This work introduces mixed-precision LLM inference techniques that encompass (i) systematic memory and compute optimization across hierarchical storage and tensor core architectures, and (ii) comprehensive end-to-end mixed-precision optimization across diverse precision formats and hardware configurations. Our approach features two novel mixed-precision pipelines designed for optimal hardware utilization: a General Matrix Multiply (GEMM) pipeline that optimizes matrix operations through offline weight packing and online acceleration, and an attention pipeline that enables efficient attention computation with arbitrary Query, Key, and Value precision combinations. The key implementation of the pipelines includes (i) hardware-aware weight packing for automatic format optimization, (ii) adaptive head alignment for efficient attention computation, (iii) instruction-level parallelism for memory hierarchy exploitation, and (iv) KV memory loading pipeline for enhanced inference efficiency. We conduct comprehensive evaluations across 16 popular LLMs and 4 representative GPU architectures. Results demonstrate that our approach achieves up to 61% lower serving latency (30% on average) and up to 156% higher throughput (58% on average) in mixed-precision workloads compared to existing mixed-precision frameworks, establishing consistent performance improvements across all tested configurations and hardware types. This work is integrated into TurboMind, a high-performance inference engine of the LMDeploy project, which is open-sourced and publicly available at https://github.com/InternLM/lmdeploy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T14:24:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15601v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15601v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 Time-Optimal Directed q-Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Felix Windisch, Florian Unger
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Directed q-analysis is a recent extension of q-analysis, an established method for extracting structure from networks, to directed graphs. Until recently, a lack of efficient algorithms heavily restricted the application of this technique: Previous approaches scale with the square of the input size, which is also the maximal size of the output, rendering such approaches worst-case optimal. In practice, output sizes of relevant networks are usually far from the worst case, a fact that could be exploited by an (efficient) output-sensitive algorithm. We develop such an algorithm and formally describe it in detail. The key insight, obtained by carefully studying various approaches to directed q-analysis and how they relate to each other, is that inverting the order of computation leads to significant complexity gains. Targeted precomputation and caching tactics further reduce the introduced overhead, enough to achieve (under mild assumptions) a time complexity that is linear in output size. The resulting algorithm for performing directed q-analysis is shown to be time-optimal.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T13:57:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15583v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15583v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 QVecOpt: An Efficient Storage and Computing Opti-mization Framework for
  Large-scale Quantum State Simulation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mingyang Yu, Haorui Yang, Donglin Wang, Desheng Kong, Ji Du, Yulong Fu, Jing Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In response to the challenges in large-scale quantum state simulation on classical computing platforms, including memory limits, frequent disk I/O, and high computational complexity, this study builds upon a previously proposed hierarchical storage-based quantum simulation system and introduces an optimization framework, the Quantum Vector Optimization Framework (QVecOpt). QVecOpt integrates four strategies: amplitude pairing, cache optimization, block storage optimization, and parallel optimization. These collectively enhance state vector storage and computational scheduling. The amplitude pairing mechanism locates relevant amplitude pairs via bitwise XOR, reducing traversal complexity of single-qubit gates from $O(2^n)$ to $O(1)$. Cache optimization pre-allocates buffers and loads only required data, cutting disk I/O. Block storage optimization partitions the state vector for on-demand loading and local updates, reducing redundant access. Parallel optimization distributes the state vector across nodes for collaborative computation, achieving near-linear speedup. Complexity analysis shows that, compared with hierarchical storage simulation, the method reduces state vector traversals for single-qubit gates from $2^n$ to 1, removing the main bottleneck. It also lowers computational and I/O complexity from $O(2^n)$ to $O(2^n/C)$ and $O(2^n/B)$. In simulations of 16-29 qubits, efficiency improves nearly tenfold, breaking the memory bottleneck of existing tools and enabling high-bit quantum circuit simulations beyond traditional methods. This work provides an efficient, scalable solution for classical simulation of large-scale quantum computation with significant academic and practical value.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T13:24:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.ET</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15545v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15545v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lukas Höllein, Aljaž Božič, Michael Zollhöfer, Matthias Nießner
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present 3DGS-LM, a new method that accelerates the reconstruction of 3D Gaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored Levenberg-Marquardt (LM). Existing methods reduce the optimization time by decreasing the number of Gaussians or by improving the implementation of the differentiable rasterizer. However, they still rely on the ADAM optimizer to fit Gaussian parameters of a scene in thousands of iterations, which can take up to an hour. To this end, we change the optimizer to LM that runs in conjunction with the 3DGS differentiable rasterizer. For efficient GPU parallization, we propose a caching data structure for intermediate gradients that allows us to efficiently calculate Jacobian-vector products in custom CUDA kernels. In every LM iteration, we calculate update directions from multiple image subsets using these kernels and combine them in a weighted mean. Overall, our method is 20% faster than the original 3DGS while obtaining the same reconstruction quality. Our optimization is also agnostic to other methods that acclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T12:52:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.12892v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.12892v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 TrimCaching: Parameter-sharing Edge Caching for AI Model Downloading</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guanqiao Qu, Zheng Lin, Qian Chen, Jian Li, Fangming Liu, Xianhao Chen, Kaibin Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Next-generation mobile networks are expected to facilitate fast AI model downloading to end users. By caching models on edge servers, mobile networks can deliver models to end users with low latency, resulting in a paradigm of edge model caching. In this paper, we develop a novel model placement framework, called parameter-sharing model caching (TrimCaching). TrimCaching exploits the key observation that a wide range of AI models, such as convolutional neural networks or large language models, can share a significant proportion of parameter blocks containing reusable knowledge, thereby improving storage efficiency. To this end, we formulate a parameter-sharing model placement problem to maximize the cache hit ratio in multi-edge wireless networks by balancing the fundamental tradeoff between storage efficiency and service latency. We show that the formulated problem is a submodular maximization problem with submodular constraints, for which no polynomial-time approximation algorithm exists. To tackle this challenge, we study an important special case, where a small fixed number of parameter blocks are shared across models, which often holds in practice. In such a case, a polynomial-time algorithm with a $\left(1-\epsilon\right)/2$-approximation guarantee is developed. Subsequently, we address the original problem for the general case by developing a greedy algorithm. Simulation results demonstrate that the proposed TrimCaching framework significantly improves the cache hit ratio compared with state-of-the-art content caching without exploiting shared parameters in AI models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T11:43:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.14204v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.14204v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 Gorgeous: Revisiting the Data Layout for Disk-Resident High-Dimensional
  Vector Search</h2>
                <div class="authors">
                    <strong>Authors:</strong> Peiqi Yin, Xiao Yan, Qihui Zhou, Hui Li, Xiaolu Li, Lin Zhang, Meiling Wang, Xin Yao, James Cheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Similarity-based vector search underpins many important applications, but a key challenge is processing massive vector datasets (e.g., in TBs). To reduce costs, some systems utilize SSDs as the primary data storage. They employ a proximity graph, which connects similar vectors to form a graph and is the state-of-the-art index for vector search. However, these systems are hindered by sub-optimal data layouts that fail to effectively utilize valuable memory space to reduce disk access and suffer from poor locality for accessing disk-resident data. Through extensive profiling and analysis, we found that the structure of the proximity graph index is accessed more frequently than the vectors themselves, yet existing systems do not distinguish between the two. To address this problem, we design the Gorgeous system with the principle of prioritizing graph structure over vectors. Specifically, Gorgeous features a memory cache that keeps the adjacency lists of graph nodes to improve cache hits and a disk block format that explicitly stores neighbors' adjacency lists along with a vector to enhance data locality. Experimental results show that Gorgeous consistently outperforms two state-of-the-art disk-based systems for vector search, boosting average query throughput by over 60% and reducing query latency by over 35%.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T06:26:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15290v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15290v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache
  Channel Pruning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Huanxuan Liao, Yixing Xu, Shizhu He, Guanchen Li, Xuanwu Yin, Dong Li, Emad Barsoum, Jun Zhao, Kang Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Long-context inference in large language models (LLMs) is increasingly constrained by the KV cache bottleneck: memory usage grows linearly with sequence length, while attention computation scales quadratically. Existing approaches address this issue by compressing the KV cache along the temporal axis through strategies such as token eviction or merging to reduce memory and computational overhead. However, these methods often neglect fine-grained importance variations across feature dimensions (i.e., the channel axis), thereby limiting their ability to effectively balance efficiency and model accuracy. In reality, we observe that channel saliency varies dramatically across both queries and positions: certain feature channels carry near-zero information for a given query, while others spike in relevance. To address this oversight, we propose SPARK, a training-free plug-and-play method that applies unstructured sparsity by pruning KV at the channel level, while dynamically restoring the pruned entries during attention score computation. Notably, our approach is orthogonal to existing KV compression and quantization techniques, making it compatible for integration with them to achieve further acceleration. By reducing channel-level redundancy, SPARK enables processing of longer sequences within the same memory budget. For sequences of equal length, SPARK not only preserves or improves model accuracy but also reduces KV cache storage by over 30% compared to eviction-based methods. Furthermore, even with an aggressive pruning ratio of 80%, SPARK maintains performance with less degradation than 5% compared to the baseline eviction method, demonstrating its robustness and effectiveness. Our code will be available at https://github.com/Xnhyacinth/SparK.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T03:48:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15212v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15212v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in
  Mixture-of-Experts LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruyi Ding, Tianhong Xu, Xinyi Shen, Aidong Adam Ding, Yunsi Fei
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The transformer architecture has become a cornerstone of modern AI, fueling remarkable progress across applications in natural language processing, computer vision, and multimodal learning. As these models continue to scale explosively for performance, implementation efficiency remains a critical challenge. Mixture of Experts (MoE) architectures, selectively activating specialized subnetworks (experts), offer a unique balance between model accuracy and computational cost. However, the adaptive routing in MoE architectures, where input tokens are dynamically directed to specialized experts based on their semantic meaning inadvertently opens up a new attack surface for privacy breaches. These input-dependent activation patterns leave distinctive temporal and spatial traces in hardware execution, which adversaries could exploit to deduce sensitive user data. In this work, we propose MoEcho, discovering a side channel analysis based attack surface that compromises user privacy on MoE based systems. Specifically, in MoEcho, we introduce four novel architectural side channels on different computing platforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and Performance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting these vulnerabilities, we propose four attacks that effectively breach user privacy in large language models (LLMs) and vision language models (VLMs) based on MoE architectures: Prompt Inference Attack, Response Reconstruction Attack, Visual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first runtime architecture level security analysis of the popular MoE structure common in modern transformers, highlighting a serious security and privacy threat and calling for effective and timely safeguards when harnessing MoE based models for developing efficient large scale AI services.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-20T20:02:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15036v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15036v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 Rethinking the Potential of Layer Freezing for Efficient DNN Training</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chence Yang, Ci Zhang, Lei Lu, Qitao Tan, Sheng Li, Ao Li, Xulong Tang, Shaoyi Huang, Jinzhen Wang, Guoming Li, Jundong Li, Xiaoming Zhai, Jin Lu, Geng Yuan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the growing size of deep neural networks and datasets, the computational costs of training have significantly increased. The layer-freezing technique has recently attracted great attention as a promising method to effectively reduce the cost of network training. However, in traditional layer-freezing methods, frozen layers are still required for forward propagation to generate feature maps for unfrozen layers, limiting the reduction of computation costs. To overcome this, prior works proposed a hypothetical solution, which caches feature maps from frozen layers as a new dataset, allowing later layers to train directly on stored feature maps. While this approach appears to be straightforward, it presents several major challenges that are severely overlooked by prior literature, such as how to effectively apply augmentations to feature maps and the substantial storage overhead introduced. If these overlooked challenges are not addressed, the performance of the caching method will be severely impacted and even make it infeasible. This paper is the first to comprehensively explore these challenges and provides a systematic solution. To improve training accuracy, we propose \textit{similarity-aware channel augmentation}, which caches channels with high augmentation sensitivity with a minimum additional storage cost. To mitigate storage overhead, we incorporate lossy data compression into layer freezing and design a \textit{progressive compression} strategy, which increases compression rates as more layers are frozen, effectively reducing storage costs. Finally, our solution achieves significant reductions in training cost while maintaining model accuracy, with a minor time overhead. Additionally, we conduct a comprehensive evaluation of freezing and compression strategies, providing insights into optimizing their application for efficient DNN training.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-20T19:54:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15033v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15033v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 Lossless Compression of Neural Network Components: Weights, Checkpoints,
  and K/V Caches in Low-Precision Formats</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anat Heilper, Doron Singer
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As deep learning models grow and deployment becomes more widespread, reducing the storage and transmission costs of neural network weights has become increasingly important. While prior work such as ZipNN has shown that lossless compression methods - particularly those based on Huffman encoding floating-point exponents can significantly reduce model sizes, these techniques have primarily been applied to higher-precision formats such as FP32 and BF16. In this work, we extend the ZipNN approach to lower-precision floating-point formats, specifically FP8 and FP4, which are gaining popularity for efficient inference. We design a compression method that separates and compresses the exponent and mantissa components independently using entropy coding. Our evaluation shows compression ratios up to 62% for BF16 and 83% for FP8. We also investigate the compressibility of key-value (K/V) cache tensors used in large language models (LLMs), finding that they, too, exhibit compressible patterns, enabling memory savings during deployment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-20T12:46:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.NE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19263v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19263v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 Diverse Negative Sampling for Implicit Collaborative Filtering</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yueqing Xuan, Kacper Sokol, Mark Sanderson, Jeffrey Chan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Implicit collaborative filtering recommenders are usually trained to learn user positive preferences. Negative sampling, which selects informative negative items to form negative training data, plays a crucial role in this process. Since items are often clustered in the latent space, existing negative sampling strategies normally oversample negative items from the dense regions. This leads to homogeneous negative data and limited model expressiveness. In this paper, we propose Diverse Negative Sampling (DivNS), a novel approach that explicitly accounts for diversity in negative training data during the negative sampling process. DivNS first finds hard negative items with large preference scores and constructs user-specific caches that store unused but highly informative negative samples. Then, its diversity-augmented sampler selects a diverse subset of negative items from the cache while ensuring dissimilarity from the user's hard negatives. Finally, a synthetic negatives generator combines the selected diverse negatives with hard negatives to form more effective training data. The resulting synthetic negatives are both informative and diverse, enabling recommenders to learn a broader item space and improve their generalisability. Extensive experiments on four public datasets demonstrate the effectiveness of DivNS in improving recommendation quality while maintaining computational efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-20T06:48:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.14468v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.14468v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 You Only Evaluate Once: A Tree-based Rerank Method at Meituan</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shuli Wang, Yinqiu Huang, Changhao Li, Yuan Zhou, Yonggang Liu, Yongqiang Zhang, Yinhua Zhu, Haitao Wang, Xingxing Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reranking plays a crucial role in modern recommender systems by capturing the mutual influences within the list. Due to the inherent challenges of combinatorial search spaces, most methods adopt a two-stage search paradigm: a simple General Search Unit (GSU) efficiently reduces the candidate space, and an Exact Search Unit (ESU) effectively selects the optimal sequence. These methods essentially involve making trade-offs between effectiveness and efficiency, while suffering from a severe \textbf{inconsistency problem}, that is, the GSU often misses high-value lists from ESU. To address this problem, we propose YOLOR, a one-stage reranking method that removes the GSU while retaining only the ESU. Specifically, YOLOR includes: (1) a Tree-based Context Extraction Module (TCEM) that hierarchically aggregates multi-scale contextual features to achieve "list-level effectiveness", and (2) a Context Cache Module (CCM) that enables efficient feature reuse across candidate permutations to achieve "permutation-level efficiency". Extensive experiments across public and industry datasets validate YOLOR's performance, and we have successfully deployed YOLOR on the Meituan food delivery platform.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-20T04:36:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3746252.3761539' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.14420v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.14420v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 H2EAL: Hybrid-Bonding Architecture with Hybrid Sparse Attention for
  Efficient Long-Context LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zizhuo Fu, Xiaotian Guo, Wenxuan Zeng, Shuzhang Zhong, Yadong Zhang, Peiyu Chen, Runsheng Wang, Le Ye, Meng Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have demonstrated remarkable proficiency in a wide range of natural language processing applications. However, the high energy and latency overhead induced by the KV cache limits the edge deployment, especially for long contexts. Emerging hybrid bonding (HB) technology has been proposed as a promising alternative to conventional near-memory processing (NMP) architectures, offering improved bandwidth efficiency and lower power consumption while exhibiting characteristics of distributed memory. In this paper, we propose H2EAL, a hybrid bonding-based accelerator with sparse attention algorithm-hardware co-design for efficient LLM inference at the edge. At the algorithm level, we propose a hybrid sparse attention scheme with static and dynamic sparsity for different heads to fully leverage the sparsity with high accuracy. At the hardware level, we co-design the hardware to support hybrid sparse attention and propose memory-compute co-placement to address the distributed memory bottleneck. Since different attention heads exhibit different sparse patterns and the attention structure often mismatches the HB architecture, we further develop a load-balancing scheduler with parallel tiled attention to address workload imbalance and optimize the mapping strategy. Extensive experiments demonstrate H2EAL achieves 5.20~48.21x speedup and 6.22~73.48x energy efficiency improvement over baseline HB implementation, with a negligible average accuracy drop of 0.87% on multiple benchmarks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-20T03:42:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.16653v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.16653v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 Scavenger+: Revisiting Space-Time Tradeoffs in Key-Value Separated
  LSM-trees</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jianshun Zhang, Fang Wang, Jiaxin Ou, Yi Wang, Ming Zhao, Sheng Qiu, Junxun Huang, Baoquan Li, Peng Fang, Dan Feng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Key-Value Stores (KVS) based on log-structured merge-trees (LSM-trees) are widely used in storage systems but face significant challenges, such as high write amplification caused by compaction. KV-separated LSM-trees address write amplification but introduce significant space amplification, a critical concern in cost-sensitive scenarios. Garbage collection (GC) can reduce space amplification, but existing strategies are often inefficient and fail to account for workload characteristics. Moreover, current key-value (KV) separated LSM-trees overlook the space amplification caused by the index LSM-tree. In this paper, we systematically analyze the sources of space amplification in KV-separated LSM-trees and propose Scavenger+, which achieves a better performance-space trade-off. Scavenger+ introduces (1) an I/O-efficient garbage collection scheme to reduce I/O overhead, (2) a space-aware compaction strategy based on compensated size to mitigate index-induced space amplification, and (3) a dynamic GC scheduler that adapts to system load to make better use of CPU and storage resources. Extensive experiments demonstrate that Scavenger+ significantly improves write performance and reduces space amplification compared to state-of-the-art KV-separated LSM-trees, including BlobDB, Titan, and TerarkDB.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-19T15:26:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/TC.2025.3587513' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.13935v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.13935v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 Scavenger: Better Space-Time Trade-Offs for Key-Value Separated
  LSM-trees</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jianshun Zhang, Fang Wang, Sheng Qiu, Yi Wang, Jiaxin Ou, Junxun Huang, Baoquan Li, Peng Fang, Dan Feng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Key-Value Stores (KVS) implemented with log-structured merge-tree (LSM-tree) have gained widespread acceptance in storage systems. Nonetheless, a significant challenge arises in the form of high write amplification due to the compaction process. While KV-separated LSM-trees successfully tackle this issue, they also bring about substantial space amplification problems, a concern that cannot be overlooked in cost-sensitive scenarios. Garbage collection (GC) holds significant promise for space amplification reduction, yet existing GC strategies often fall short in optimization performance, lacking thorough consideration of workload characteristics. Additionally, current KV-separated LSM-trees also ignore the adverse effect of the space amplification in the index LSM-tree. In this paper, we systematically analyze the sources of space amplification of KV-separated LSM-trees and introduce Scavenger, which achieves a better trade-off between performance and space amplification. Scavenger initially proposes an I/O-efficient garbage collection scheme to reduce I/O overhead and incorporates a space-aware compaction strategy based on compensated size to minimize the space amplification of index LSM-trees. Extensive experiments show that Scavenger significantly improves write performance and achieves lower space amplification than other KV-separated LSM-trees (including BlobDB, Titan, and TerarkDB).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-19T15:08:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/ICDE60146.2024.00312' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.13909v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.13909v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 Tight Inter-Core Cache Contention Analysis for WCET Estimation on
  Multicore Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shuai Zhao, Jieyu Jiang, Shenlin Cai, Yaowei Liang, Chen Jie, Yinjie Fang, Wei Zhang, Guoquan Zhang, Yaoyao Gu, Xiang Xiao, Wei Qin, Xiangzhen Ouyang, Wanli Chang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> WCET (Worst-Case Execution Time) estimation on multicore architecture is particularly challenging mainly due to the complex accesses over cache shared by multiple cores. Existing analysis identifies possible contentions between parallel tasks by leveraging the partial order of the tasks or their program regions. Unfortunately, they overestimate the number of cache misses caused by a remote block access without considering the actual cache state and the number of accesses. This paper reports a new analysis for inter-core cache contention. Based on the order of program regions in a task, we first identify memory references that could be affected if a remote access occurs in a region. Afterwards, a fine-grained contention analysis is constructed that computes the number of cache misses based on the access quantity of local and remote blocks. We demonstrate that the overall inter-core cache interference of a task can be obtained via dynamic programming. Experiments show that compared to existing methods, the proposed analysis reduces inter-core cache interference and WCET estimations by 52.31% and 8.94% on average, without significantly increasing computation overhead.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-19T14:30:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.13863v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.13863v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 Zobrist Hash-based Duplicate Detection in Symbolic Regression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bogdan Burlacu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Symbolic regression encompasses a family of search algorithms that aim to discover the best fitting function for a set of data without requiring an a priori specification of the model structure. The most successful and commonly used technique for symbolic regression is Genetic Programming (GP), an evolutionary search method that evolves a population of mathematical expressions through the mechanism of natural selection. In this work we analyze the efficiency of the evolutionary search in GP and show that many points in the search space are re-visited and re-evaluated multiple times by the algorithm, leading to wasted computational effort. We address this issue by introducing a caching mechanism based on the Zobrist hash, a type of hashing frequently used in abstract board games for the efficient construction and subsequent update of transposition tables. We implement our caching approach using the open-source framework Operon and demonstrate its performance on a selection of real-world regression problems, where we observe up to 34\% speedups without any detrimental effects on search quality. The hashing approach represents a straightforward way to improve runtime performance while also offering some interesting possibilities for adjusting search strategy based on cached information.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-19T14:18:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.13859v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.13859v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 INDS: Incremental Named Data Streaming for Real-Time Point Cloud Video</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruonan Chai, Yixiang Zhu, Xinjiao Li, Jiawei Li, Zili Meng, Dirk Kutscher
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Real-time streaming of point cloud video, characterized by massive data volumes and high sensitivity to packet loss, remains a key challenge for immersive applications under dynamic network conditions. While connection-oriented protocols such as TCP and more modern alternatives like QUIC alleviate some transport-layer inefficiencies, including head-of-line blocking, they still retain a coarse-grained, segment-based delivery model and a centralized control loop that limit fine-grained adaptation and effective caching. We introduce INDS (Incremental Named Data Streaming), an adaptive streaming framework based on Information-Centric Networking (ICN) that rethinks delivery for hierarchical, layered media. INDS leverages the Octree structure of point cloud video and expressive content naming to support progressive, partial retrieval of enhancement layers based on consumer bandwidth and decoding capability. By combining time-windows with Group-of-Frames (GoF), INDS's naming scheme supports fine-grained in-network caching and facilitates efficient multi-user data reuse. INDS can be deployed as an overlay, remaining compatible with QUIC-based transport infrastructure as well as future Media-over-QUIC (MoQ) architectures, without requiring changes to underlying IP networks. Our prototype implementation shows up to 80% lower delay, 15-50% higher throughput, and 20-30% increased cache hit rates compared to state-of-the-art DASH-style systems. Together, these results establish INDS as a scalable, cache-friendly solution for real-time point cloud streaming under variable and lossy conditions, while its compatibility with MoQ overlays further positions it as a practical, forward-compatible architecture for emerging immersive media systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-19T11:54:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MM</span><span>C.2.1; C.2.4; H.5.1</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.13756v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.13756v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint
  Caching and Resource-Aware Graph Partitioning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xianfeng Song, Yi Zou, Zheng Shi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Graph Neural Networks (GNNs) have shown remarkable capabilities in processing graph-structured data prevalent in various real-world applications. However, the scalability of full-batch GNN training becomes severely limited by high communication overhead and load imbalance in distributed environments. In this paper, we present CaPGNN, a novel framework for efficient parallel full-batch GNN training on single-server with multi-GPU, designed specifically to reduce redundant inter-GPU communication and balance computational workloads. We propose a joint adaptive caching algorithm that leverages both CPU and GPU memory to significantly reduce the repetitive transmission of vertex features across partitions. Additionally, we introduce a resource-aware graph partitioning algorithm that adjusts subgraph sizes dynamically according to the heterogeneous computational and communication capacities of GPUs. Extensive experiments on large-scale benchmark datasets demonstrate that CaPGNN effectively reduces communication costs by up to 96% and accelerates GNN training by up to 12.7 times compared to state-of-the-art approaches. Our results highlight the potential of adaptive caching and resource-aware partitioning to facilitate scalable, efficient, and practical deployment of full-batch GNN training in distributed computing environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-19T10:21:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.13716v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.13716v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units
  with Precision Recovery</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weicheng Xue, Baisong Xu, Kai Yang, Yongxiang Liu, Dengdeng Fan, Pengxiang Xu, Yonghong Tian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Low-precision matrix engines, such as FP16 cube, offer high throughput but lack support for full-precision computation. In this work, we propose SGEMM-cube, a high-performance algorithm for emulating FP32 general matrix-matrix multiplication (GEMM) using only FP16 computation units on a representative AI accelerator. The method decomposes each FP32 operand into two FP16 values and compensates for numerical errors through a tunable scaling strategy. A detailed analysis of numerical errors, including underflow conditions and precision loss, guides the selection of scaling parameters to preserve up to 22 bits of mantissa accuracy. We further investigate the effect of computation order on accuracy and demonstrate that a term-wise accumulation scheme improves numerical stability over conventional FP32 GEMM in low-exponent regimes. Finally, a cache-aware blocking strategy and double-buffered pipeline are introduced to overlap memory transfers with computation, enabling SGEMM-cube to achieve up to 77\% of the theoretical FP32-equivalent peak performance on Ascend 910A NPU lacking native FP32 support. Extensive numerical experiments confirm that our method not only recovers the accuracy of native FP32 GEMM but also exhibits superior numerical stability under certain conditions, due to its structured and error-aware computation order.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-19T09:13:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.23387v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.23387v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale
  Architectures</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anders Johansson, Evan Weinberg, Christian R. Trott, Megan J. McCarthy, Stan G. Moore
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Since its inception in 1995, LAMMPS has grown to be a world-class molecular dynamics code, with thousands of users, over one million lines of code, and multi-scale simulation capabilities. We discuss how LAMMPS has adapted to the modern heterogeneous computing landscape by integrating the Kokkos performance portability library into the existing C++ code. We investigate performance portability of simple pairwise, many-body reactive, and machine-learned force-field interatomic potentials. We present results on GPUs across different vendors and generations, and analyze performance trends, probing FLOPS throughput, memory bandwidths, cache capabilities, and thread-atomic operation performance. Finally, we demonstrate strong scaling on all current US exascale machines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the three potentials.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-19T05:27:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.PF</span><span>physics.comp-ph</span><span>C.1.4; C.2.4; C.4; D.1.3; D.3.4; E.1; I.6; I.6.8; J.2</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.13523v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.13523v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated
  Diffusion Transformers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wongi Jeong, Kyungryeol Lee, Hoigi Seo, Se Young Chun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion transformers have emerged as an alternative to U-net-based diffusion models for high-fidelity image and video generation, offering superior scalability. However, their heavy computation remains a major obstacle to real-world deployment. Existing acceleration methods primarily exploit the temporal dimension such as reusing cached features across diffusion timesteps. Here, we propose Region-Adaptive Latent Upsampling (RALU), a training-free framework that accelerates inference along spatial dimension. RALU performs mixed-resolution sampling across three stages: 1) low-resolution denoising latent diffusion to efficiently capture global semantic structure, 2) region-adaptive upsampling on specific regions prone to artifacts at full-resolution, and 3) all latent upsampling at full-resolution for detail refinement. To stabilize generations across resolution transitions, we leverage noise-timestep rescheduling to adapt the noise level across varying resolutions. Our method significantly reduces computation while preserving image quality by achieving up to 7.0$\times$ speed-up on FLUX and 3.0$\times$ on Stable Diffusion 3 with minimal degradation. Furthermore, RALU is complementary to existing temporal accelerations such as caching methods, thus can be seamlessly integrated to further reduce inference latency without compromising generation quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-19T03:13:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>eess.IV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.08422v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.08422v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A
  first-principles DFT+$U$+$V$ study</h2>
                <div class="authors">
                    <strong>Authors:</strong> Indukuru Ramesh Reddy, Sayandeep Ghosh, Bongjae Kim, Chang-Jong Kang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Nonlocal Coulomb interactions play a crucial role in stabilizing distinct electronic phases in kagome materials. In this work, we systematically investigate the effects of on-site ($U$) and inter-site ($V$) Coulomb interactions on the electronic structure and stability of charge-density-wave (CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory (DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and stability of CDW phases, whereas $U$ suppresses these phases, highlighting a fundamental competition between local and nonlocal Coulomb interactions. By directly comparing our theoretical results with angle-resolved photoemission spectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that accurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings establish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable insights into the correlated electronic states in kagome metals and serving as a foundation for future explorations of correlation-driven phenomena in related materials.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-19T01:38:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.str-el</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.17995v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.17995v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data
  Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ayoub Ben Chaliah, Hela Dellagi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present Datarus-R1-14B, a 14 B-parameter open-weights language model fine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and graduate-level problem solver. Datarus is trained not on isolated question-answer pairs but on full analytical trajectories including reasoning steps, code execution, error traces, self-corrections, and final conclusions, all captured in a ReAct-style notebook format spanning finance, medicine, numerical analysis, and other quantitative domains. Our training pipeline combines (i) a trajectory-centric synthetic data generator that yielded 144 000 tagged notebook episodes, (ii) a dual-reward framework blending a lightweight tag-based structural signal with a Hierarchical Reward Model (HRM) that scores both single-step soundness and end-to-end coherence, and (iii) a memory-optimized implementation of Group Relative Policy Optimization (GRPO) featuring KV-cache reuse, sequential generation, and reference-model sharding. A cosine curriculum smoothly shifts emphasis from structural fidelity to semantic depth, reducing the format collapse and verbosity that often plague RL-aligned LLMs. A central design choice in Datarus is it dual reasoning interface. In agentic mode the model produces ReAct-tagged steps that invoke Python tools to execute real code; in reflection mode it outputs compact Chain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On demanding postgraduate-level problems, Datarus exhibits an "AHA-moment" pattern: it sketches hypotheses, revises them once or twice, and converges avoiding the circular, token-inflating loops common to contemporary systems. Across standard public benchmarks Datarus surpasses similar size models and even reaches the level of larger reasoning models such as QwQ-32B achieving up to 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting 18-49% fewer tokens per solution.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-18T21:58:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.13382v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.13382v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical
  Manufacturing Scale-Up</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sakhinana Sagar Srinivas, Shivam Gupta, Venkataramana Runkana
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in generative AI have accelerated the discovery of novel chemicals and materials. However, scaling these discoveries to industrial production remains a major bottleneck due to the synthesis gap -- the need to develop entirely new manufacturing processes. This challenge requires detailed engineering blueprints: PFDs for equipment layouts and material/energy flows, and PIDs for process plant operations. Current AI systems cannot yet reliably generate these critical engineering schematics, creating a fundamental obstacle to manufacturing scale-up of novel discoveries. We present a closed-loop, physics-aware framework for automated generation of industrially viable PFDs and PIDs. The framework integrates three key components: (1) domain-specialized small language models (SLMs) trained for auto-generation of PFDs and PIDs, (2) a hierarchical knowledge graph containing process flow and instrumentation descriptions for 1,020+ chemicals for Graph Retrieval-Augmented Generation (GRAG), and (3) an open-source chemical process simulator for modeling, simulation, optimization, and analysis of novel chemical processes. The SLMs are trained through a multi-stage pipeline on synthetic datasets, with process simulator-in-the-loop validation ensuring feasibility. To enhance computational efficiency, the framework implements structural pruning (width and depth) guided by importance heuristics to reduce language model size while preserving accuracy, followed by advanced inference optimizations including FlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization, and Test-Time Inference Scaling. Experimental results demonstrate that our framework generates simulator-validated process descriptions with high fidelity.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-18T16:52:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.24584v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.24584v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning
  Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruikang Liu, Yuxuan Sun, Manyi Zhang, Haoli Bai, Xianzhi Yu, Tiezheng Yu, Chun Yuan, Lu Hou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in reasoning language models have demonstrated remarkable performance in complex tasks, but their extended chain-of-thought reasoning process increases inference overhead. While quantization has been widely adopted to reduce the inference cost of large language models, its impact on reasoning models remains understudied. In this paper, we conduct the first systematic study on quantized reasoning models, evaluating the open-sourced DeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B parameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache, and activation quantization using state-of-the-art algorithms at varying bit-widths, with extensive evaluation across mathematical (AIME, MATH-500), scientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our findings reveal that while lossless quantization can be achieved with W8A8 or W4A16 quantization, lower bit-widths introduce significant accuracy risks. We further identify model size, model origin, and task difficulty as critical determinants of performance. Contrary to expectations, quantized models do not exhibit increased output lengths. In addition, strategically scaling the model sizes or reasoning steps can effectively enhance the performance. All quantized models and codes are open-sourced in https://github.com/ruikangliu/Quantized-Reasoning-Models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-18T16:06:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.04823v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.04823v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 Some optimization possibilities in data plane programming</h2>
                <div class="authors">
                    <strong>Authors:</strong> Altangerel Gereltsetseg, Tejfel Máté
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Software-defined networking (SDN) technology aims to create a highly flexible network by decoupling control plane and the data plane and programming them independently. There has been a lot of research on improving and optimizing the control plane, and data plane programming is a relatively new concept, so study on it is one of the hot topics for researchers. At the 2019 Dagstuhl Seminar, well-known scientists on computer networking discussed challenges and problems in the field of data plane programming that need to be addressed over the next 10 years. Based on this seminar issues and papers review, we suggested some possible solutions which are for optimizing data plane to improve packet processing performance and link utilization. The suggestions include (i) enriching data plane language with asynchronous external function, (ii) compression based on payload size, (iii) in-network caching for fast packet processing, and (iv) offloading external functions to an additional thread, virtual machine (VM) or server, etc. In addition, we implemented some of these in the P4 data plane language to illustrate the practicality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-18T09:41:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.12767v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.12767v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 Dissecting CPU-GPU Unified Physical Memory on AMD MI300A APUs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jacob Wahlgren, Gabin Schieffer, Ruimin Shi, Edgar A. León, Roger Pearce, Maya Gokhale, Ivy Peng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Discrete GPUs are a cornerstone of HPC and data center systems, requiring management of separate CPU and GPU memory spaces. Unified Virtual Memory (UVM) has been proposed to ease the burden of memory management; however, at a high cost in performance. The recent introduction of AMD's MI300A Accelerated Processing Units (APUs)--as deployed in the El Capitan supercomputer--enables HPC systems featuring integrated CPU and GPU with Unified Physical Memory (UPM) for the first time. This work presents the first comprehensive characterization of the UPM architecture on MI300A. We first analyze the UPM system properties, including memory latency, bandwidth, and coherence overhead. We then assess the efficiency of the system software in memory allocation, page fault handling, TLB management, and Infinity Cache utilization. We propose a set of porting strategies for transforming applications for the UPM architecture and evaluate six applications on the MI300A APU. Our results show that applications on UPM using the unified memory model can match or outperform those in the explicitly managed model--while reducing memory costs by up to 44%.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-18T09:06:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.12743v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.12743v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuanxin Wei, Lansong Diao, Bujiao Chen, Shenggan Cheng, Zhengping Qian, Wenyuan Yu, Nong Xiao, Wei Lin, Jiangsu Du
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Leveraging the Transformer architecture and the diffusion process, video DiT models have emerged as a dominant approach for high-quality video generation. However, their multi-step iterative denoising process incurs high computational cost and inference latency. Caching, a widely adopted optimization method in DiT models, leverages the redundancy in the diffusion process to skip computations in different granularities (e.g., step, cfg, block). Nevertheless, existing caching methods are limited to single-granularity strategies, struggling to balance generation quality and inference speed in a flexible manner. In this work, we propose MixCache, a training-free caching-based framework for efficient video DiT inference. It first distinguishes the interference and boundary between different caching strategies, and then introduces a context-aware cache triggering strategy to determine when caching should be enabled, along with an adaptive hybrid cache decision strategy for dynamically selecting the optimal caching granularity. Extensive experiments on diverse models demonstrate that, MixCache can significantly accelerate video generation (e.g., 1.94$\times$ speedup on Wan 14B, 1.97$\times$ speedup on HunyuanVideo) while delivering both superior generation quality and inference efficiency compared to baseline methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-18T07:49:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.GR</span><span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.12691v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.12691v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for
  NGINX</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aayush Gupta, Arpit Bhayani
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Web proxies such as NGINX commonly rely on least-recently-used (LRU) eviction, which is size agnostic and can thrash under periodic bursts and mixed object sizes. We introduce Cold-RL, a learned eviction policy for NGINX that replaces LRU's forced-expire path with a dueling Deep Q-Network served by an ONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL samples the K least-recently-used objects, extracts six lightweight features (age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT), and requests a bitmask of victims; a hard timeout of 500 microseconds triggers immediate fallback to native LRU. Policies are trained offline by replaying NGINX access logs through a cache simulator with a simple reward: a retained object earns one point if it is hit again before TTL expiry. We compare against LRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial workloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538, a 146 percent improvement over the best classical baseline; at 100 MB, from 0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods (about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th percentile eviction latency within budget. To our knowledge, this is the first reinforcement learning eviction policy integrated into NGINX with strict SLOs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-17T20:01:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.DB</span><span>cs.NI</span><span>C.2.4; C.4; D.4.2; I.2.6</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.12485v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.12485v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 Accelerating LLM Inference via Dynamic KV Cache Placement in
  Heterogeneous Memory System</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yunhua Fang, Rui Xie, Asad Ul Haq, Linsen Ma, Kaoutar El Maghraoui, Naigang Wang, Meng Wang, Liu Liu, Tong Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Model (LLM) inference is increasingly constrained by memory bandwidth, with frequent access to the key-value (KV) cache dominating data movement. While attention sparsity reduces some memory traffic, the relevance of past tokens varies over time, requiring the full KV cache to remain accessible and sustaining pressure on both bandwidth and capacity. With advances in interconnects such as NVLink and LPDDR5X, modern AI hardware now integrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making heterogeneous memory systems a practical solution. This work investigates dynamic KV cache placement across such systems to maximize aggregated bandwidth utilization under capacity constraints. Rather than proposing a specific scheduling policy, we formulate the placement problem mathematically and derive a theoretical upper bound, revealing substantial headroom for runtime optimization. To our knowledge, this is the first formal treatment of dynamic KV cache scheduling in heterogeneous memory systems for LLM inference.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-17T19:07:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>cs.AI</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.13231v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.13231v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 ZigzagAttention: Efficient Long-Context Inference with Exclusive
  Retrieval and Streaming Heads</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhuorui Liu, Chen Zhang, Dawei Song
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the rapid development of large language models (LLMs), handling long context has become one of the vital abilities in LLMs. Such long-context ability is accompanied by difficulties in deployment, especially due to the increased consumption of KV cache. There is certain work aiming to optimize the memory footprint of KV cache, inspired by the observation that attention heads can be categorized into retrieval heads that are of great significance and streaming heads that are of less significance. Typically, identifying the streaming heads and and waiving the KV cache in the streaming heads would largely reduce the overhead without hurting the performance that much. However, since employing both retrieval and streaming heads in one layer decomposes one large round of attention computation into two small ones, it may unexpectedly bring extra latency on accessing and indexing tensors. Based on this intuition, we impose an important improvement to the identification process of retrieval and streaming heads, in which we design a criterion that enforces exclusively retrieval or streaming heads gathered in one unique layer. In this way, we further eliminate the extra latency and only incur negligible performance degradation. Our method named \textsc{ZigzagAttention} is competitive among considered baselines owing to reduced latency and comparable performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-17T15:48:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.12407v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.12407v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 Enhancement of the energy storage and electrocaloric effect performances
  in 0.4 BCZT 0.6 BSTSn medium entropy ceramic prepared by sol gel method</h2>
                <div class="authors">
                    <strong>Authors:</strong> S. Khardazi, Z. Gargar, A. Lyubchyk, O. Zakir, D. Mezzane, M. Amjoud, A. Alimoussa, Z. Kutnjak
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Based on the traditional polycrystalline ferroelectric Ba0.85Ca0.15Zr0.10Ti0.90O3, the 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6 Ba0.9Sr0.1Ti0.9Sn0.1O3 medium entropy material with good energy storage and electrocaloric effect performances is designed and synthesized by the solgel method. The structural, dielectric, energy storage and electrocaloric effect properties of the prepared sample were studied. The findings demonstrate that the 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6 Ba0.9Sr0.1Ti0.9Sn0.1O3 ceramic simultaneously has a significant recoverable energy storage density of 255.4 mJ/cm3, an efficiency of 67.9%, a large ECE temperature change of 1.36 K, and a high ECE responsivity of 0.453 K.mm/kV under a low electric field of 30 kV/cm. Moreover, excellent temperature stability of Wrec (less than 10%) was achieved in the investigated sample 0.4BCZT 0.6BSTSn.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-17T13:05:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.mtrl-sci</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1016/j.jssc.2025.125547' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.12357v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.12357v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based
  Token Eviction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Raghavv Goel, Junyoung Park, Mukul Gagrani, Dalton Jones, Matthew Morse, Harper Langston, Mingu Lee, Chris Lott
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While long context support of large language models has extended their abilities, it also incurs challenges in memory and compute which becomes crucial bottlenecks in resource-restricted devices. Token eviction, a widely adopted post-training methodology designed to alleviate the bottlenecks by evicting less important tokens from the cache, typically uses attention scores as proxy metrics for token importance. However, one major limitation of attention score as a token-wise importance metrics is that it lacks the information about contribution of tokens to the attention output. In this paper, we propose a simple eviction criterion based on the contribution of cached tokens to attention outputs. Our method, CAOTE, optimizes for eviction error due to token eviction, by seamlessly integrating attention scores and value vectors. This is the first method which uses value tokens on top of attention-based eviction scores in closed-form. Additionally, CAOTE can act as a meta-heuristic method with flexible usage with any token eviction method. We show that CAOTE, when combined with the state-of-the-art attention score-based methods, always improves accuracies on the downstream task, indicating the importance of leveraging information from values during token eviction process.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-16T23:41:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.14051v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.14051v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 Memory-Augmented Transformers: A Systematic Review from Neuroscience
  Principles to Enhanced Model Architectures</h2>
                <div class="authors">
                    <strong>Authors:</strong> Parsa Omidi, Xingshuai Huang, Axel Laborieux, Bahareh Nikpour, Tianyu Shi, Armaghan Eshaghi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Memory is fundamental to intelligence, enabling learning, reasoning, and adaptability across biological and artificial systems. While Transformer architectures excel at sequence modeling, they face critical limitations in long-range context retention, continual learning, and knowledge integration. This review presents a unified framework bridging neuroscience principles, including dynamic multi-timescale memory, selective attention, and consolidation, with engineering advances in Memory-Augmented Transformers. We organize recent progress through three taxonomic dimensions: functional objectives (context extension, reasoning, knowledge integration, adaptation), memory representations (parameter-encoded, state-based, explicit, hybrid), and integration mechanisms (attention fusion, gated control, associative retrieval). Our analysis of core memory operations (reading, writing, forgetting, and capacity management) reveals a shift from static caches toward adaptive, test-time learning systems. We identify persistent challenges in scalability and interference, alongside emerging solutions including hierarchical buffering and surprise-gated updates. This synthesis provides a roadmap toward cognitively-inspired, lifelong-learning Transformer architectures.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-16T03:17:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10824v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10824v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 KV-Auditor: Auditing Local Differential Privacy for Correlated Key-Value
  Estimation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jingnan Xu, Leixia Wang, Xiaofeng Meng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> To protect privacy for data-collection-based services, local differential privacy (LDP) is widely adopted due to its rigorous theoretical bound on privacy loss. However, mistakes in complex theoretical analysis or subtle implementation errors may undermine its practical guarantee. To address this, auditing is crucial to confirm that LDP protocols truly protect user data. However, existing auditing methods, though, mainly target machine learning and federated learning tasks based on centralized differentially privacy (DP), with limited attention to LDP. Moreover, the few studies on LDP auditing focus solely on simple frequency estimation task for discrete data, leaving correlated key-value data - which requires both discrete frequency estimation for keys and continuous mean estimation for values - unexplored.   To bridge this gap, we propose KV-Auditor, a framework for auditing LDP-based key-value estimation mechanisms by estimating their empirical privacy lower bounds. Rather than traditional LDP auditing methods that relies on binary output predictions, KV-Auditor estimates this lower bound by analyzing unbounded output distributions, supporting continuous data. Specifically, we classify state-of-the-art LDP key-value mechanisms into interactive and non-interactive types. For non-interactive mechanisms, we propose horizontal KV-Auditor for small domains with sufficient samples and vertical KV-Auditor for large domains with limited samples. For interactive mechanisms, we design a segmentation strategy to capture incremental privacy leakage across iterations. Finally, we perform extensive experiments to validate the effectiveness of our approach, offering insights for optimizing LDP-based key-value estimators.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-15T14:17:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.11495v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.11495v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for
  Vision-Language-Action Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chenghao Liu, Jiachen Zhang, Chengxuan Li, Zhimu Zhou, Shixin Wu, Songfang Huang, Huiling Duan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vision-Language-Action (VLA) models process visual inputs independently at each timestep, discarding valuable temporal information inherent in robotic manipulation tasks. This frame-by-frame processing makes models vulnerable to visual noise while ignoring the substantial coherence between consecutive frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a training-free approach that intelligently integrates historical and current visual representations to enhance VLA inference quality. Our method employs dual-dimension detection combining efficient grayscale pixel difference analysis with attention-based semantic relevance assessment, enabling selective temporal token fusion through hard fusion strategies and keyframe anchoring to prevent error accumulation. Comprehensive experiments across LIBERO, SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0 percentage points average on LIBERO (72.4\% vs 68.4\% baseline), cross-environment validation on SimplerEnv (4.8\% relative improvement), and 8.7\% relative improvement on real robot tasks. Our approach proves model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably, TTF reveals that selective Query matrix reuse in attention mechanisms enhances rather than compromises performance, suggesting promising directions for direct KQV matrix reuse strategies that achieve computational acceleration while improving task success rates.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-15T12:03:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.LG</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19257v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19257v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless
  Edge-Device Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rui Bao, Nan Xue, Yaping Sun, Zhiyong Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The integration of wireless communications and Large Language Models (LLMs) is poised to unlock ubiquitous intelligent services, yet deploying them in wireless edge-device collaborative environments presents a critical trade-off between inference quality and end-to-end latency. A fundamental mismatch exists between task complexity and resource allocation: offloading simple queries invites prohibitive latency, while on-device models lack the capacity for demanding computations. To address this challenge, we propose a dynamic, quality-latency aware routing framework that orchestrates inference between a lightweight model on the mobile device and a powerful model on the edge server. Our framework employs two distinct cost models: for single-turn queries, it fuses a BERT-predicted semantic score with communication and computation overheads; for multi-turn dialogues, it further quantifies context-aware costs arising from model switching and KV-cache management. While maintaining full inference quality, extensive experiments demonstrate that our framework cuts average response latency by 5-15% and reduces large model invocations by 10-20% against competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-15T07:55:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>cs.AI</span><span>cs.LG</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.11291v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.11291v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mukund Choudhary, KV Aditya Srivatsa, Gaurja Aeron, Antara Raaghavi Bhattacharya, Dang Khoa Dang Dinh, Ikhlasul Akmal Hanif, Daria Kotova, Ekaterina Kochmar, Monojit Choudhury
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have demonstrated potential in reasoning tasks, but their performance on linguistics puzzles remains consistently poor. These puzzles, often derived from Linguistics Olympiad (LO) contests, provide a minimal contamination environment to assess LLMs' linguistic reasoning abilities across low-resource languages. This work analyses LLMs' performance on 629 problems across 41 low-resource languages by labelling each with linguistically informed features to unveil weaknesses. Our analyses show that LLMs struggle with puzzles involving higher morphological complexity and perform better on puzzles involving linguistic features that are also found in English. We also show that splitting words into morphemes as a pre-processing step improves solvability, indicating a need for more informed and language-specific tokenisers. These findings thus offer insights into some challenges in linguistic reasoning and modelling of low-resource languages.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-15T06:53:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.11260v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.11260v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal
  Parallelism</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zedong Liu, Shenggan Cheng, Guangming Tan, Yang You, Dingwen Tao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multimodal large language models (MLLMs) extend LLMs to handle images, videos, and audio by incorporating feature extractors and projection modules. However, these additional components -- combined with complex inference pipelines and heterogeneous workloads -- introduce significant inference overhead. Therefore, efficiently serving MLLMs remains a major challenge. Current tightly coupled serving architectures struggle to distinguish between mixed request types or adapt parallelism strategies to different inference stages, leading to increased time-to-first-token (TTFT) latency and poor resource utilization. To address this, we introduce Elastic Multimodal Parallelism (EMP), a new serving paradigm that elastically adapts to resource heterogeneity across request types and inference stages. Building upon EMP, we develop ElasticMM, an MLLM serving system that (1) separates requests into independent modality groups with dynamic resource allocation via a modality-aware load balancer; (2) decouples inference stages and enables parallelism adjustment and adaptive scaling via elastic partition scheduling; and (3) improves inference efficiency through unified multimodal prefix caching and non-blocking encoding. Experiments on diverse real-world datasets show that ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by up to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level objectives (SLOs).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-15T04:27:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.10069v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.10069v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 A Survey on Diffusion Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tianyi Li, Mingda Chen, Bowei Guo, Zhiqiang Shen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:47:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10875v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10875v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guangda Liu, Chengwei Li, Zhenyu Ning, Minyi Guo, Jieru Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have been widely deployed with rapidly expanding context windows to support increasingly demanding applications. However, long contexts pose significant deployment challenges, primarily due to the KV cache whose size grows proportionally with context length. While KV cache compression methods are proposed to address this issue, KV dropping methods incur considerable accuracy loss, and KV retrieval methods suffer from significant efficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization framework to enhance KV retrieval efficiency while preserving accuracy. On the algorithm side, FreeKV introduces speculative retrieval to shift the KV selection and recall processes out of the critical path, combined with fine-grained correction to ensure accuracy. On the system side, FreeKV employs hybrid KV layouts across CPU and GPU memory to eliminate fragmented data transfers, and leverages double-buffered streamed recall to further improve efficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy across various scenarios and models, delivering up to 13$\times$ speedup compared to SOTA KV retrieval methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T16:12:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.13109v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.13109v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit
  KV Cache</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dayou Du, Shijie Cao, Jianyi Cheng, Luo Mai, Ting Cao, Mao Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rise of long-context Large Language Models (LLMs) amplifies memory and bandwidth demands during autoregressive decoding, as the Key-Value (KV) cache grows with each generated token. Low-bit KV-cache quantization (e.g., 4-bit or 2-bit) can reduce memory footprint while preserving accuracy, but existing systems suffer from slow decoding due to their exclusive reliance on CUDA cores, neglecting Tensor Cores (the primary source of compute on modern GPUs). We present BitDecoding, a new long-context LLM inference system with a low-bit KV cache. BitDecoding enables efficient low-bit KV-cache decoding by cooperatively leveraging CUDA cores and Tensor Cores. It introduces methods for automatically inducing optimized layouts to exploit Tensor Cores, along with warp-level parallelization strategies for dequantization. For unified system support, BitDecoding includes a query transformation module supporting diverse attention variants, a quantization kernel that supports both tensor-wise and channel-wise scaling used in various quantization algorithms with high performance, and a dequantization kernel with a software-defined pipeline to coordinate CUDA and Tensor Cores execution for mixed-precision operations. Evaluated on RTX 4090, A100, and H100, BitDecoding accelerates decoding by up to 7.5x, 4.8x, and 8.9x, respectively, over FP16 FlashDecoding-v2, and surpasses the state-of-the-art low-bit system QServe by up to 4.3x. On LLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding latency by 3x, showing substantial improvements for long-context generation. The code is available at https://github.com/DD-DuDa/BitDecoding.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T15:37:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>cs.AI</span><span>cs.CL</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.18773v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.18773v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 EVCtrl: Efficient Control Adapter for Visual Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zixiang Yang, Yue Ma, Yinhan Zhang, Shanhui Mo, Dongrui Liu, Linfeng Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Visual generation includes both image and video generation, training probabilistic models to create coherent, diverse, and semantically faithful content from scratch. While early research focused on unconditional sampling, practitioners now demand controllable generation that allows precise specification of layout, pose, motion, or style. While ControlNet grants precise spatial-temporal control, its auxiliary branch markedly increases latency and introduces redundant computation in both uncontrolled regions and denoising steps, especially for video. To address this problem, we introduce EVCtrl, a lightweight, plug-and-play control adapter that slashes overhead without retraining the model. Specifically, we propose a spatio-temporal dual caching strategy for sparse control information. For spatial redundancy, we first profile how each layer of DiT-ControlNet responds to fine-grained control, then partition the network into global and local functional zones. A locality-aware cache focuses computation on the local zones that truly need the control signal, skipping the bulk of redundant computation in global regions. For temporal redundancy, we selectively omit unnecessary denoising steps to improve efficiency. Extensive experiments on CogVideo-Controlnet, Wan2.1-Controlnet, and Flux demonstrate that our method is effective in image and video control generation without the need for training. For example, it achieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and Wan2.1-Controlnet, respectively, with almost no degradation in generation quality.Codes are available in the supplementary materials.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:11:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10963v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10963v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 SurfaceLogicKV: Surface and Logic Attention Behaviors are All You Need
  for Robust KV Cache Compression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mengjie Li, William J. Song
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The increasing input sequence length in Large Language Models (LLMs) puts significant pressure on key-value (KV) cache storage, making efficient inference challenging. Explicitly distinguishing attention behavior into our self-defined surface memorization and logic construction reveals essential roles in long-context reasoning. We observe that an individual attention head can display various behaviors, with nearly 98.5% effectively ignoring completely irrelevant information. The remaining 1.5% behaves as logic construction, and 0.5% behaves as surface memorization. Based on layer- and head-wise integration, we propose a novel two-stage SurfaceLogicKV method to utilize these attention behaviors for KV Cache compression. As a result, it achieves improved compressing robustness while maintaining competitive performance across various tasks and long sequences compared to baselines or even FullKV in some specific situations
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:08:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15806v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15806v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 Routing and Wavelength Assignment with Minimal Attack Radius for QKD
  Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mengyao Li, Qiaolun Zhang, Zongshuai Yang, Stefano Bregni, Alberto Gatto, Raouf Boutaba, Massimo Tornatore
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Quantum Key Distribution (QKD) can distribute keys with guaranteed security but remains susceptible to key exchange interruption due to physical-layer threats, such as high-power jamming attacks. To address this challenge, we first introduce a novel metric, namely Maximum Number of Affected Requests (maxNAR), to quantify the worst-case impact of a single physical-layer attack, and then we investigate a new problem of Routing and Wavelength Assignment with Minimal Attack Radius (RWA-MAR). We formulate the problem using an Integer Linear Programming (ILP) model and propose a scalable heuristic to efficiently minimize maxNAR. Our approach incorporates key caching through Quantum Key Pools (QKPs) to enhance resilience and optimize resource utilization. Moreover, we model the impact of different QKD network architectures, employing Optical Bypass (OB) for optical switching of quantum channels and Trusted Relay (TR) for secure key forwarding. Moreover, a tunable parameter is designed in the heuristic to guide the preference for OB or TR, offering enhanced adaptability and dynamic control in diverse network scenarios. Simulation results confirm that our method significantly outperforms the baseline in terms of security and scalability.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T13:10:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>quant-ph</span><span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10613v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10613v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 Yan: Foundational Interactive Video Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Deheng Ye, Fangyun Zhou, Jiacheng Lv, Jianqi Ma, Jun Zhang, Junyan Lv, Junyou Li, Minwen Deng, Mingyu Yang, Qiang Fu, Wei Yang, Wenkai Lv, Yangbin Yu, Yewen Wang, Yonghang Guan, Zhihao Hu, Zhongbin Fang, Zhongqian Sun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present Yan, a foundational framework for interactive video generation, covering the entire pipeline from simulation and generation to editing. Specifically, Yan comprises three core modules. AAA-level Simulation: We design a highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based shift-window denoising inference process, achieving real-time 1080P/60FPS interactive simulation. Multi-Modal Generation: We introduce a hierarchical autoregressive caption method that injects game-specific knowledge into open-domain multi-modal video diffusion models (VDMs), then transforming the VDM into a frame-wise, action-controllable, real-time infinite interactive video generator. Notably, when the textual and visual prompts are sourced from different domains, the model demonstrates strong generalization, allowing it to blend and compose the style and mechanics across domains flexibly according to user prompts. Multi-Granularity Editing: We propose a hybrid model that explicitly disentangles interactive mechanics simulation from visual rendering, enabling multi-granularity video content editing during interaction through text. Collectively, Yan offers an integration of these modules, pushing interactive video generation beyond isolated capabilities toward a comprehensive AI-driven interactive creation paradigm, paving the way for the next generation of creative tools, media, and entertainment. The project page is: https://greatx3.github.io/Yan/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T10:26:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.08601v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.08601v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic
  Parallelism in LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Keyu Chen, Zhifeng Shen, Daohai Yu, Haoqian Wu, Wei Wen, Jianfeng He, Ruizhi Qiao, Xing Sun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The increasing scale and complexity of large language models (LLMs) pose significant inference latency challenges, primarily due to their autoregressive decoding paradigm characterized by the sequential nature of next-token prediction. By re-examining the outputs of autoregressive models, we observed that some segments exhibit parallelizable structures, which we term intrinsic parallelism. Decoding each parallelizable branch simultaneously (i.e. parallel decoding) can significantly improve the overall inference speed of LLMs. In this paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which addresses two core challenges: automated construction of parallelizable data and efficient parallel decoding mechanism. More specifically, we introduce a non-invasive pipeline that automatically extracts and validates parallelizable structures from the responses of autoregressive models. To empower efficient adaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which enables seamless transitions between serial and parallel decoding modes while maintaining a reusable KV cache, maximizing computational efficiency. Extensive evaluations across General Tasks, Retrieval-Augmented Generation, Mathematical Reasoning, demonstrate that ASPD achieves unprecedented performance in both effectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up to 3.19x speedup (1.85x on average) while maintaining response quality within 1% difference compared to autoregressive models, realizing significant acceleration without compromising generation quality. Our framework sets a groundbreaking benchmark for efficient LLM parallel inference, paving the way for its deployment in latency-sensitive applications such as AI-powered customer service bots and answer retrieval engines.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T09:04:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.08895v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.08895v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 NanoControl: A Lightweight Framework for Precise and Efficient Control
  in Diffusion Transformer</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shanyuan Liu, Jian Zhu, Junda Lu, Yue Gong, Liuzhuozheng Li, Bo Cheng, Yuhang Ma, Liebucha Wu, Xiaoyu Wu, Dawei Leng, Yuhui Yin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in text-to-image synthesis. However, in the domain of controllable text-to-image generation using DiTs, most existing methods still rely on the ControlNet paradigm originally designed for UNet-based diffusion models. This paradigm introduces significant parameter overhead and increased computational costs. To address these challenges, we propose the Nano Control Diffusion Transformer (NanoControl), which employs Flux as the backbone network. Our model achieves state-of-the-art controllable text-to-image generation performance while incurring only a 0.024\% increase in parameter count and a 0.029\% increase in GFLOPs, thus enabling highly efficient controllable generation. Specifically, rather than duplicating the DiT backbone for control, we design a LoRA-style (low-rank adaptation) control module that directly learns control signals from raw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation mechanism that integrates condition-specific key-value information into the backbone in a simple yet highly effective manner, facilitating deep fusion of conditional features. Extensive benchmark experiments demonstrate that NanoControl significantly reduces computational overhead compared to conventional control approaches, while maintaining superior generation quality and achieving improved controllability.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T07:54:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10424v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10424v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 XQuant: Breaking the Memory Wall for LLM Inference with KV Cache
  Rematerialization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aditya Tomar, Coleman Hooper, Minjae Lee, Haocheng Xi, Rishabh Tiwari, Wonjun Kang, Luca Manolache, Michael W. Mahoney, Kurt Keutzer, Amir Gholami
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Although LLM inference has emerged as a critical workload for many downstream applications, efficiently inferring LLMs is challenging due to the substantial memory footprint and bandwidth requirements. In parallel, compute capabilities have steadily outpaced both memory capacity and bandwidth over the last few decades, a trend that remains evident in modern GPU hardware and exacerbates the challenge of LLM inference. As such, new algorithms are emerging that trade increased computation for reduced memory operations. To that end, we present XQuant, which takes advantage of this trend, enabling an order-of-magnitude reduction in memory consumption through low-bit quantization with substantial accuracy benefits relative to state-of-the-art KV cache quantization methods. We accomplish this by quantizing and caching the layer input activations X, instead of using standard KV caching, and then rematerializing the Keys and Values on-the-fly during inference. This results in an immediate 2$\times$ memory savings compared to KV caching. By applying XQuant, we achieve up to $\sim 7.7\times$ memory savings with $<0.1$ perplexity degradation compared to the FP16 baseline. Furthermore, our approach leverages the fact that X values are similar across layers. Building on this observation, we introduce XQuant-CL, which exploits the cross-layer similarity in the X embeddings for extreme compression. Across different models, XQuant-CL attains up to 10$\times$ memory savings relative to the FP16 baseline with only 0.01 perplexity degradation, and 12.5$\times$ memory savings with only $0.1$ perplexity degradation. XQuant exploits the rapidly increasing compute capabilities of hardware platforms to eliminate the memory bottleneck, while surpassing state-of-the-art KV cache quantization methods and achieving near-FP16 accuracy across a wide range of models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T06:52:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10395v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10395v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache
  Compression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Payman Behnam, Yaosheng Fu, Ritchie Zhao, Po-An Tsai, Zhiding Yu, Alexey Tumanov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformer-based Large Language Models rely critically on the KV cache to efficiently handle extended contexts during the decode phase. Yet, the size of the KV cache grows proportionally with the input length, burdening both memory bandwidth and capacity as decoding progresses. To address this challenge, we present RocketKV, a training-free KV cache compression strategy containing two consecutive stages. In the first stage, it performs coarse-grain permanent KV cache eviction on the input sequence tokens. In the second stage, it adopts a hybrid sparse attention method to conduct fine-grain top-k sparse attention, approximating the attention scores by leveraging both head and sequence dimensionality reductions. We show that RocketKV provides a compression ratio of up to 400$\times$, end-to-end speedup of up to 3.7$\times$ as well as peak memory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU compared to the full KV cache baseline, while achieving negligible accuracy loss on a variety of long-context tasks. We also propose a variant of RocketKV for multi-turn scenarios, which consistently outperforms other existing methods and achieves accuracy nearly on par with an oracle top-k attention scheme. The source code is available here: https://github.com/NVlabs/RocketKV.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-13T17:55:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.14051v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.14051v3' target='_blank'>pdf</a>
                </div>
            </div>
            <h1>Keyword: LLM Inference</h1>

            <div class="paper">
                <h2>#1 (Un)solvable Matrix Models for BPS Correlators</h2>
                <div class="authors">
                    <strong>Authors:</strong> Prokopii Anempodistov, Adolfo Holguin, Vladimir Kazakov, Harish Murali
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We propose and study a family of complex matrix models computing the protected two- and three-point correlation functions in $\mathcal{N}=4$ SYM. Our description allows us to directly relate the eigenvalue density of the matrix model for ``Huge" operators with $ \Delta \sim N^2$ to the shape of droplets in the dual Lin-Lunin-Maldacena (LLM) geometry. We demonstrate how to determine the eigenvalue distribution for various choices of operators such as those of exponential, character, or coherent state type, which then allows us to efficiently compute one-point functions of light chiral primaries in generic LLM backgrounds. In particular, we successfully match the results for light probes with the supergravity calculations of Skenderis and Taylor. We provide a large $N$ formalism for one-point functions of ``Giant" probes, such as operators dual to giant graviton branes in LLM backgrounds, and explicitly apply it for particular backgrounds. We also explicitly compute the correlator of three huge half-BPS operators of exponential type and stacks of determinant operators by reducing them to the known matrix model problems such as the Potts or $O(n)$ model on random planar graphs. Finally, we point out a curious relation between the correlators of $\frac{1}{4}$-BPS and $\frac{1}{8}$-BPS coherent state operators and the Eguchi-Kawai reduction of the Principal Chiral Model in $2D$ and $3D$ correspondingly.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T17:58:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>hep-th</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20094v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20094v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical
  Reinforcement Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Utsav Singh, Pramit Bhattacharyya, Vinay P. Namboodiri
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have shown remarkable abilities in logical reasoning, in-context learning, and code generation. However, translating natural language instructions into effective robotic control policies remains a significant challenge, especially for tasks requiring long-horizon planning and operating under sparse reward conditions. Hierarchical Reinforcement Learning (HRL) provides a natural framework to address this challenge in robotics; however, it typically suffers from non-stationarity caused by the changing behavior of the lower-level policy during training, destabilizing higher-level policy learning. We introduce LGR2, a novel HRL framework that leverages LLMs to generate language-guided reward functions for the higher-level policy. By decoupling high-level reward generation from low-level policy changes, LGR2 fundamentally mitigates the non-stationarity problem in off-policy HRL, enabling stable and efficient learning. To further enhance sample efficiency in sparse environments, we integrate goal-conditioned hindsight experience relabeling. Extensive experiments across simulated and real-world robotic navigation and manipulation tasks demonstrate LGR2 outperforms both hierarchical and non-hierarchical baselines, achieving over 55% success rates on challenging tasks and robust transfer to real robots, without additional fine-tuning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T17:57:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.05881v6' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.05881v6' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 AudioStory: Generating Long-Form Narrative Audio with Large Language
  Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuxin Guo, Teng Wang, Yuying Ge, Shijie Ma, Yixiao Ge, Wei Zou, Ying Shan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in text-to-audio (TTA) generation excel at synthesizing short audio clips but struggle with long-form narrative audio, which requires temporal coherence and compositional reasoning. To address this gap, we propose AudioStory, a unified framework that integrates large language models (LLMs) with TTA systems to generate structured, long-form audio narratives. AudioStory possesses strong instruction-following reasoning generation capabilities. It employs LLMs to decompose complex narrative queries into temporally ordered sub-tasks with contextual cues, enabling coherent scene transitions and emotional tone consistency. AudioStory has two appealing features: (1) Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser collaboration into two specialized components, i.e., a bridging query for intra-event semantic alignment and a residual query for cross-event coherence preservation. (2) End-to-end training: By unifying instruction comprehension and audio generation within a single end-to-end framework, AudioStory eliminates the need for modular training pipelines while enhancing synergy between components. Furthermore, we establish a benchmark AudioStory-10K, encompassing diverse domains such as animated soundscapes and natural sound narratives. Extensive experiments show the superiority of AudioStory on both single-audio generation and narrative audio generation, surpassing prior TTA baselines in both instruction-following ability and audio fidelity. Our code is available at https://github.com/TencentARC/AudioStory
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T17:55:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.MM</span><span>cs.SD</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20088v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20088v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy
  Retriever Poisoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yanbo Dai, Zhenlan Ji, Zongjie Li, Kuan Li, Shuai Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Retrieval-Augmented Generation (RAG) has become a standard approach for improving the reliability of large language models (LLMs). Prior work demonstrates the vulnerability of RAG systems by misleading them into generating attacker-chosen outputs through poisoning the knowledge base. However, this paper uncovers that such attacks could be mitigated by the strong \textit{self-correction ability (SCA)} of modern LLMs, which can reject false context once properly configured. This SCA poses a significant challenge for attackers aiming to manipulate RAG systems.   In contrast to previous poisoning methods, which primarily target the knowledge base, we introduce \textsc{DisarmRAG}, a new poisoning paradigm that compromises the retriever itself to suppress the SCA and enforce attacker-chosen outputs. This compromisation enables the attacker to straightforwardly embed anti-SCA instructions into the context provided to the generator, thereby bypassing the SCA. To this end, we present a contrastive-learning-based model editing technique that performs localized and stealthy edits, ensuring the retriever returns a malicious instruction only for specific victim queries while preserving benign retrieval behavior. To further strengthen the attack, we design an iterative co-optimization framework that automatically discovers robust instructions capable of bypassing prompt-based defenses. We extensively evaluate DisarmRAG across six LLMs and three QA benchmarks. Our results show near-perfect retrieval of malicious instructions, which successfully suppress SCA and achieve attack success rates exceeding 90\% under diverse defensive prompts. Also, the edited retriever remains stealthy under several detection methods, highlighting the urgent need for retriever-centric defenses.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T17:49:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20083v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20083v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 Approximate Lifted Model Construction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Malte Luttermann, Jan Speller, Marcel Gehrke, Tanya Braun, Ralf Möller, Mattis Hartwig
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Probabilistic relational models such as parametric factor graphs enable efficient (lifted) inference by exploiting the indistinguishability of objects. In lifted inference, a representative of indistinguishable objects is used for computations. To obtain a relational (i.e., lifted) representation, the Advanced Colour Passing (ACP) algorithm is the state of the art. The ACP algorithm, however, requires underlying distributions, encoded as potential-based factorisations, to exactly match to identify and exploit indistinguishabilities. Hence, ACP is unsuitable for practical applications where potentials learned from data inevitably deviate even if associated objects are indistinguishable. To mitigate this problem, we introduce the $\varepsilon$-Advanced Colour Passing ($\varepsilon$-ACP) algorithm, which allows for a deviation of potentials depending on a hyperparameter $\varepsilon$. $\varepsilon$-ACP efficiently uncovers and exploits indistinguishabilities that are not exact. We prove that the approximation error induced by $\varepsilon$-ACP is strictly bounded and our experiments show that the approximation error is close to zero in practice.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T17:48:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.DS</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.20784v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.20784v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 Refining Czech GEC: Insights from a Multi-Experiment Approach</h2>
                <div class="authors">
                    <strong>Authors:</strong> Petr Pechman, Milan Straka, Jana Straková, Jakub Náplava
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present a grammar error correction (GEC) system that achieves state of the art for the Czech language. Our system is based on a neural network translation approach with the Transformer architecture, and its key feature is its real-time synthetic generation pipeline, which dynamically augments sentences with artificial errors by introducing both language-agnostic and Czech-specific errors. We conduct a comprehensive series of experiments, investigating the Czech GEC corpora as bases for synthetic error introduction, several error generation strategies, domain balancing, tokenization granularity, model size, and data scaling during fine-tuning. Additionally, we evaluate the performance of large language models (LLMs) on Czech GEC in both end-user and expert fine-tuning scenarios. Our best-performing model is superior both in performance and computational efficiency. The source code and the trained model links are available on https://github.com/ufal/tsd2025-gec.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T17:43:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1007/978-3-032-02551-7_7' target='_blank'>doi</a><a href='http://arxiv.org/abs/2506.22402v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.22402v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 X-ray view of a massive node of the Cosmic Web at z=3 II. Discovery of
  extended X-ray emission around a hyperluminous QSO</h2>
                <div class="authors">
                    <strong>Authors:</strong> Andrea Travascio, Sebastiano Cantalupo, Gabriele Pezzulli, Paolo Tozzi, Luca Di Mascolo, Michela Esposito, Titouan Lazeyras, Marika Lepore, Stefano Borgani, Martin Elvis, Giuseppina Fabbiano, Marta Galbiati, Nicholas Ledos, Riccardo Middei, Antonio Pensabene, Enrico Piconcelli, Giada Quadri, Fabio Vito, Weichen Wang, Luca Zappacosta
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While the warm, ionized gas in the CGM at z>3 is now routinely observed around bright QSOs in Lya emission, little is known about the CGM hot phase due to its expected faintness in the X-ray band, often referred to as the ICM. Here, we report the analysis of 634 ks of Chandra X-ray observations in the MQN01 Cosmic Node, a region containing one of the brightest Lya nebulae and the largest galaxy overdensity discovered so far at z>3. We detect 66 net counts of X-ray emission in the 0.5-2 keV band extending to at least 30 kpc from the brightest QSO in MQN01. The morphology and spectrum are consistent with thermal emission from hot plasma in CIE. Photoionization is negligible, and IC is disfavored. A joint spatial and spectral MCMC analysis provides consistency with a beta-model with a steep density profile and a gas temperature kT~1.8 keV and virial halo mass Mvir~3e13 Mo. The inferred hot gas mass is Mhot(<Rvir)~2.6e12 Mo, which is ~8.3% of Mvir, or ~56% of the theoretical cosmological baryon budget of the halo. The hot gas also emits an exceptionally high Lx, with a measured L2-10~2.3e45 erg/s within the central 30 kpc. This system is a clear outlier in the Lx-Tx plane, indicating a thermodynamic state distinct from that of evolved lower-redshift hot halos. The cooling time in the inner 15-30 kpc is comparable to the local dynamical time, suggesting that the gas could become locally unstable in the absence of heating or feedback. Moreover, the thermal pressure associated with the detected CGM hot phase is large enough to confine the cold and dense clumps, which are required to reproduce the high Lya emission associated with the inner regions of the MQN01 structure. Although limited to a single system, our results provide unique information on the multi-phase properties of the CGM and a view of the nascent thermal hot gas phase observed in local galaxy clusters.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T17:40:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.GA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20074v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20074v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 Convergence Rates for Realizations of Gaussian Random Variables</h2>
                <div class="authors">
                    <strong>Authors:</strong> Daniel Winkle, Ingo Steinwart, Bernard Haasdonk
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper investigates the approximation of Gaussian random variables in Banach spaces, focusing on the high-probability bounds for the approximation of Gaussian random variables using finitely many observations. We derive non-asymptotic error bounds for the approximation of a Gaussian process $ X $ by its conditional expectation, given finitely many linear functionals. Specifically, we quantify the difference between the covariance of $ X $ and its finite-dimensional approximation, establishing a direct relationship between the quality of the covariance approximation and the convergence of the process in the Banach space norm. Our approach avoids the reliance on spectral methods or eigenfunction expansions commonly used in Hilbert space settings, and instead uses finite, linear observations. This makes our result particularly suitable for practical applications in nonparametric statistics, machine learning, and Bayesian inference.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T17:33:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.ST</span><span>stat.TH</span><span>Primary 60G15, Secondary 62F15</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.13940v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.13940v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with
  Cognitive-Inspired Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chengzu Li, Wenshan Wu, Huanyu Zhang, Qingtao Li, Zeyu Gao, Yan Xia, José Hernández-Orallo, Ivan Vulić, Furu Wei
                </div>
                <div class="summary">
                    <strong>Summary:</strong> For human cognitive process, spatial reasoning and perception are closely entangled, yet the nature of this interplay remains underexplored in the evaluation of multimodal large language models (MLLMs). While recent MLLM advancements show impressive performance on reasoning, their capacity for human-like spatial cognition remains an open question. In this work, we introduce a systematic evaluation framework to assess the spatial reasoning abilities of state-of-the-art MLLMs relative to human performance. Central to our work is 11Plus-Bench, a high-quality benchmark derived from realistic standardized spatial aptitude tests. 11Plus-Bench also features fine-grained expert annotations of both perceptual complexity and reasoning process, enabling detailed instance-level analysis of model behavior. Through extensive experiments across 14 MLLMs and human evaluation, we find that current MLLMs exhibit early signs of spatial cognition. Despite a large performance gap compared to humans, MLLMs' cognitive profiles resemble those of humans in that cognitive effort correlates strongly with reasoning-related complexity. However, instance-level performance in MLLMs remains largely random, whereas human correctness is highly predictable and shaped by abstract pattern complexity. These findings highlight both emerging capabilities and limitations in current MLLMs' spatial reasoning capabilities and provide actionable insights for advancing model design.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T17:22:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20068v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20068v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 Joint Analysis of HI Absorption Zeeman Measurements and the Morphology
  of Filamentary HI Emission</h2>
                <div class="authors">
                    <strong>Authors:</strong> Marta Nowotka, Susan E. Clark, Blakesley Burkhart, Laura Fissel, Tao-Chung Ching, Timothy Robishaw, Carl Heiles
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present a joint analysis of HI absorption Zeeman measurements and the morphology of filamentary HI emission to investigate the three-dimensional structure of the magnetic field in the diffuse neutral interstellar medium (ISM). Our analysis is based on the Arecibo Millennium Survey and new data from the Five-hundred-meter Aperture Spherical radio Telescope (FAST) toward radio sources 3C 75, 3C 207, and 3C 409. Toward 3C 409, we make a 4$\sigma$ Zeeman detection and infer $B_{LOS}$ = 9.1 +/- 1.9$\mu$G, in agreement with Arecibo results. We quantify the dispersion of HI filaments at the locations and velocities of Zeeman components using GALFA-HI narrow-channel emission maps. Focusing on a subsample of 42 spectrally distinct components, we find a weak but statistically significant positive correlation (Spearman r = 0.3, $p = 0.01$) between $|B_{LOS}|$ and the circular variance of HI filament orientation angles. To examine its origin, we characterize the environments probed by HI absorption using dust emission, 3D dust maps, OH absorption, and CO emission. We find evidence that existing HI absorption Zeeman measurements trace magnetic fields that are coherent on parsec scales, probe primarily local gas ($100$-$500$ pc, often at distances consistent with the Local Bubble wall), and exhibit systematic differences in the magnitude of $B_{LOS}$. We attribute the correlation between Zeeman measurements and filamentary HI morphology to large-scale variations in magnetic field strength and/or inclination angle across different Galactic environments, which could arise due to the Local Bubble geometry or enhanced total field strength in denser regions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T17:18:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.GA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20065v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20065v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 StepWiser: Stepwise Generative Judges for Wiser Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wei Xiong, Wenting Zhao, Weizhe Yuan, Olga Golovneva, Tong Zhang, Jason Weston, Sainbayar Sukhbaatar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As models increasingly leverage multi-step reasoning strategies to solve complex problems, supervising the logical validity of these intermediate steps has become a critical research challenge. Process reward models address this by providing step-by-step feedback, but current approaches have two major drawbacks: they typically function as classifiers without providing explanations, and their reliance on supervised fine-tuning with static datasets limits generalization. Inspired by recent advances, we reframe stepwise reward modeling from a classification task to a reasoning task itself. We thus propose a generative judge that reasons about the policy model's reasoning steps (i.e., meta-reasons), outputting thinking tokens before delivering a final verdict. Our model, StepWiser, is trained by reinforcement learning using relative outcomes of rollouts. We show it provides (i) better judgment accuracy on intermediate steps than existing methods; (ii) can be used to improve the policy model at training time; and (iii) improves inference-time search.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T17:17:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19229v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19229v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without
  Human Annotations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Peng-Hao Hsu, Ke Zhang, Fu-En Wang, Tao Tu, Ming-Feng Li, Yu-Lun Liu, Albert Y. C. Chen, Min Sun, Cheng-Hao Kuo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Open-vocabulary (OV) 3D object detection is an emerging field, yet its exploration through image-based methods remains limited compared to 3D point cloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view indoor 3D object detector trained without human annotations. In particular, OpenM3D is a single-stage detector adapting the 2D-induced voxel features from the ImGeoNet model. To support OV, it is jointly trained with a class-agnostic 3D localization loss requiring high-quality 3D pseudo boxes and a voxel-semantic alignment loss requiring diverse pre-trained CLIP features. We follow the training setting of OV-3DET where posed RGB-D images are given but no human annotations of 3D boxes or classes are available. We propose a 3D Pseudo Box Generation method using a graph embedding technique that combines 2D segments into coherent 3D structures. Our pseudo-boxes achieve higher precision and recall than other methods, including the method proposed in OV-3DET. We further sample diverse CLIP features from 2D segments associated with each coherent 3D structure to align with the corresponding voxel feature. The key to training a highly accurate single-stage detector requires both losses to be learned toward high-quality targets. At inference, OpenM3D, a highly efficient detector, requires only multi-view images for input and demonstrates superior accuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor benchmarks compared to existing methods. We outperform a strong two-stage method that leverages our class-agnostic detector with a ViT CLIP-based OV classifier and a baseline incorporating multi-view depth estimator on both accuracy and speed.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T17:17:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20063v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20063v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 Scaling Decentralized Learning with FLock</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zehua Cheng, Rui Sun, Jiahao Sun, Yike Guo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Fine-tuning the large language models (LLMs) are prevented by the deficiency of centralized control and the massive computing and communication overhead on the decentralized schemes. While the typical standard federated learning (FL) supports data privacy, the central server requirement creates a single point of attack and vulnerability to poisoning attacks. Generalizing the result in this direction to 70B-parameter models in the heterogeneous, trustless environments has turned out to be a huge, yet unbroken bottleneck. This paper introduces FLock, a decentralized framework for secure and efficient collaborative LLM fine-tuning. Integrating a blockchain-based trust layer with economic incentives, FLock replaces the central aggregator with a secure, auditable protocol for cooperation among untrusted parties. We present the first empirical validation of fine-tuning a 70B LLM in a secure, multi-domain, decentralized setting. Our experiments show the FLock framework defends against backdoor poisoning attacks that compromise standard FL optimizers and fosters synergistic knowledge transfer. The resulting models show a >68% reduction in adversarial attack success rates. The global model also demonstrates superior cross-domain generalization, outperforming models trained in isolation on their own specialized data.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T16:50:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.15349v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.15349v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to
  Enhance LLM Safety Guardrail to Potential Attacks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sheng Liu, Qiang Sheng, Danding Wang, Yang Li, Guang Yang, Juan Cao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Despite advances in improving large language model (LLM) to refuse to answer malicious instructions, widely used LLMs remain vulnerable to jailbreak attacks where attackers generate instructions with distributions differing from safety alignment corpora. New attacks expose LLMs' inability to recognize unseen malicious instructions, highlighting a critical distributional mismatch between training data and real-world attacks that forces developers into reactive patching cycles. To tackle this challenge, we propose IMAGINE, a synthesis framework that leverages embedding space distribution analysis to generate jailbreak-like instructions. This approach effectively fills the distributional gap between authentic jailbreak patterns and safety alignment corpora. IMAGINE follows an iterative optimization process that dynamically evolves text generation distributions across iterations, thereby augmenting the coverage of safety alignment data distributions through synthesized data examples. Based on the safety-aligned corpus enhanced through IMAGINE, our framework demonstrates significant decreases in attack success rate on Qwen2.5, Llama3.1, and Llama3.2 without compromising their utility.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T03:02:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20038v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20038v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 Pruning Strategies for Backdoor Defense in LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Santosh Chapagain, Shah Muhammad Hamdi, Soukaina Filali Boubrahimi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Backdoor attacks are a significant threat to the performance and integrity of pre-trained language models. Although such models are routinely fine-tuned for downstream NLP tasks, recent work shows they remain vulnerable to backdoor attacks that survive vanilla fine-tuning. These attacks are difficult to defend because end users typically lack knowledge of the attack triggers. Such attacks consist of stealthy malicious triggers introduced through subtle syntactic or stylistic manipulations, which can bypass traditional detection and remain in the model, making post-hoc purification essential. In this study, we explore whether attention-head pruning can mitigate these threats without any knowledge of the trigger or access to a clean reference model. To this end, we design and implement six pruning-based strategies: (i) gradient-based pruning, (ii) layer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2 sparsification, (iv) randomized ensemble pruning, (v) reinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning. Each method iteratively removes the least informative heads while monitoring validation accuracy to avoid over-pruning. Experimental evaluation shows that gradient-based pruning performs best while defending the syntactic triggers, whereas reinforcement learning and Bayesian pruning better withstand stylistic attacks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T16:34:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3746252.3760946' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.20032v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20032v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 Large Language Models (LLMs) for Electronic Design Automation (EDA)</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kangwei Xu, Denis Schwachhofer, Jason Blocklove, Ilia Polian, Peter Domanski, Dirk Pflüger, Siddharth Garg, Ramesh Karri, Ozgur Sinanoglu, Johann Knechtel, Zhuorui Zhao, Ulf Schlichtmann, Bing Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the growing complexity of modern integrated circuits, hardware engineers are required to devote more effort to the full design-to-manufacturing workflow. This workflow involves numerous iterations, making it both labor-intensive and error-prone. Therefore, there is an urgent demand for more efficient Electronic Design Automation (EDA) solutions to accelerate hardware development. Recently, large language models (LLMs) have shown remarkable advancements in contextual comprehension, logical reasoning, and generative capabilities. Since hardware designs and intermediate scripts can be represented as text, integrating LLM for EDA offers a promising opportunity to simplify and even automate the entire workflow. Accordingly, this paper provides a comprehensive overview of incorporating LLMs into EDA, with emphasis on their capabilities, limitations, and future opportunities. Three case studies, along with their outlook, are introduced to demonstrate the capabilities of LLMs in hardware design, testing, and optimization. Finally, future directions and challenges are highlighted to further explore the potential of LLMs in shaping the next-generation EDA, providing valuable insights for researchers interested in leveraging advanced AI technologies for EDA.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T16:33:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SY</span><span>cs.AI</span><span>cs.AR</span><span>cs.LG</span><span>cs.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20030v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20030v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 SELF: Surrogate-light Feature Selection with Large Language Models in
  Deep Recommender Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pengyue Jia, Zhaocheng Du, Yichao Wang, Xiangyu Zhao, Xiaopeng Li, Yuhao Wang, Qidong Liu, Huifeng Guo, Ruiming Tang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Feature selection is crucial in recommender systems for improving model efficiency and predictive performance. Conventional approaches typically employ surrogate models-such as decision trees or neural networks-to estimate feature importance. However, their effectiveness is inherently constrained, as these models may struggle under suboptimal training conditions, including feature collinearity, high-dimensional sparsity, and insufficient data. In this paper, we propose SELF, an SurrogatE-Light Feature selection method for deep recommender systems. SELF integrates semantic reasoning from Large Language Models (LLMs) with task-specific learning from surrogate models. Specifically, LLMs first produce a semantically informed ranking of feature importance, which is subsequently refined by a surrogate model, effectively integrating general world knowledge with task-specific learning. Comprehensive experiments on three public datasets from real-world recommender platforms validate the effectiveness of SELF.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T16:33:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.08516v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.08516v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 Microscopic Origin of Domain Wall Reconfiguration Dynamics in a Quantum
  Material via Quantum Simulation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jaka Vodeb
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Understanding how quantum materials relax from metastable states poses a fundamental challenge in condensed matter physics. In the layered dichalcogenide 1T-TaS$_2$, domain-wall-rich polaronic textures evolve toward a uniform ground state through reconfiguration events that exhibit a crossover from thermally activated to temperature-independent behavior-indicative of quantum tunneling. Here, we employ quantum simulation of a two-dimensional transverse-field Ising model (TFIM) with longitudinal bias to uncover the microscopic processes underlying this relaxation. Using a Schrieffer-Wolff transformation, we map the TFIM to a hardcore boson model, revealing that single-polaron tunneling events, rather than collective multi-particle transitions, dominate domain wall motion. A scaling analysis of reconfiguration rates across varying transverse fields $h_x$ shows collapse when temperature is rescaled as $T \to h_x^n T$ with $n \approx 1.2$, confirming the dominance of first- and second-order single-particle processes. This enables us to reconstruct a microscopic relaxation pathway consisting of cyclical polaron leakage followed by cascades of tunneling events. Our results establish quantum simulation as a powerful tool for inferring real-space mechanisms in strongly correlated systems and demonstrate a concrete strategy for bridging effective spin models with the non-equilibrium dynamics of quantum materials.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T16:33:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>quant-ph</span><span>cond-mat.str-el</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20028v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20028v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 Using item recommendations and LLMs in marketing email titles</h2>
                <div class="authors">
                    <strong>Authors:</strong> Deddy Jobson, Muktti Shukla, Phuong Dinh, Julio Christian Young, Nick Pitton, Nina Chen, Ryan Ginstrom
                </div>
                <div class="summary">
                    <strong>Summary:</strong> E-commerce marketplaces make use of a number of marketing channels like emails, push notifications, etc. to reach their users and stimulate purchases. Personalized emails especially are a popular touch point for marketers to inform users of latest items in stock, especially for those who stopped visiting the marketplace. Such emails contain personalized recommendations tailored to each user's interests, enticing users to buy relevant items. A common limitation of these emails is that the primary entry point, the title of the email, tends to follow fixed templates, failing to inspire enough interest in the contents. In this work, we explore the potential of large language models (LLMs) for generating thematic titles that reflect the personalized content of the emails. We perform offline simulations and conduct online experiments on the order of millions of users, finding our techniques useful in improving the engagement between customers and our emails. We highlight key findings and learnings as we productionize the safe and automated generation of email titles for millions of users.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T16:31:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20024v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20024v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 Symphony: A Decentralized Multi-Agent Framework for Scalable Collective
  Intelligence</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ji Wang, Kashing Chen, Xinyuan Song, Ke Zhang, Lynn Ai, Eric Yang, Bill Shi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Most existing Large Language Model (LLM)-based agent frameworks rely on centralized orchestration, incurring high deployment costs, rigid communication topologies, and limited adaptability. To address these challenges, we introduce Symphony, a decentralized multi-agent system which enables lightweight LLMs on consumer-grade GPUs to coordinate. Symphony introduces three key mechanisms: (1) a decentralized ledger that records capabilities, (2) a Beacon-selection protocol for dynamic task allocation, and (3) weighted result voting based on CoTs. This design forms a privacy-saving, scalable, and fault-tolerant orchestration with low overhead. Empirically, Symphony outperforms existing baselines on reasoning benchmarks, achieving substantial accuracy gains and demonstrating robustness across models of varying capacities.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T16:27:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span><span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20019v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20019v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in
  Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, Mehrdad Farajtabar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. To address these concerns, we conduct a large-scale study on several SOTA open and closed models. To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of models.Our findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, we investigate the fragility of mathematical reasoning in these models and show that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is because current LLMs cannot perform genuine logical reasoning; they replicate reasoning steps from their training data. Adding a single clause that seems relevant to the question causes significant performance drops (up to 65%) across all state-of-the-art models, even though the clause doesn't contribute to the reasoning chain needed for the final answer. Overall, our work offers a more nuanced understanding of LLMs' capabilities and limitations in mathematical reasoning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T16:24:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.05229v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.05229v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 Scalable Bayesian Structure Learning for Gaussian Graphical Models Using
  Marginal Pseudo-likelihood</h2>
                <div class="authors">
                    <strong>Authors:</strong> Reza Mohammadi, Marit Schoonhoven, Lucas Vogels, S. Ilker Birbil
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Bayesian methods for learning Gaussian graphical models offer a principled framework for quantifying model uncertainty and incorporating prior knowledge. However, their scalability is constrained by the computational cost of jointly exploring graph structures and precision matrices. To address this challenge, we perform inference directly on the graph by integrating out the precision matrix. We adopt a marginal pseudo-likelihood approach, eliminating the need to compute intractable normalizing constants and perform computationally intensive precision matrix sampling. Building on this framework, we develop continuous-time (birth-death) and discrete-time (reversible jump) Markov chain Monte Carlo (MCMC) algorithms that efficiently explore the posterior over graph space. We establish theoretical guarantees for posterior contraction, convergence, and graph selection consistency. The algorithms scale to large graph spaces, enabling parallel exploration for graphs with over 1,000 nodes, while providing uncertainty quantification and supporting flexible prior specification over the graph space. Extensive simulations show substantial computational gains over state-of-the-art Bayesian approaches without sacrificing graph recovery accuracy. Applications to human and mouse gene expression datasets demonstrate the ability of our approach to recover biologically meaningful structures and quantify uncertainty in complex networks. An implementation is available in the R package BDgraph.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T16:23:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>stat.CO</span><span>stat.ML</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2307.00127v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2307.00127v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 Decomposing Behavioral Phase Transitions in LLMs: Order Parameters for
  Emergent Misalignment</h2>
                <div class="authors">
                    <strong>Authors:</strong> Julian Arnold, Niels Lörch
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Fine-tuning LLMs on narrowly harmful datasets can lead to behavior that is broadly misaligned with respect to human values. To understand when and how this emergent misalignment occurs, we develop a comprehensive framework for detecting and characterizing rapid transitions during fine-tuning using both distributional change detection methods as well as order parameters that are formulated in plain English and evaluated by an LLM judge. Using an objective statistical dissimilarity measure, we quantify how the phase transition that occurs during fine-tuning affects multiple aspects of the model. In particular, we assess what percentage of the total distributional change in model outputs is captured by different aspects, such as alignment or verbosity, providing a decomposition of the overall transition. We also find that the actual behavioral transition occurs later in training than indicated by the peak in the gradient norm alone. Our framework enables the automated discovery and quantification of language-based order parameters, which we demonstrate on examples ranging from knowledge questions to politics and ethics.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T16:19:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20015v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20015v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 CataractSurg-80K: Knowledge-Driven Benchmarking for Structured Reasoning
  in Ophthalmic Surgery Planning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yang Meng, Zewen Pan, Yandi Lu, Ruobing Huang, Yanfeng Liao, Jiarui Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Cataract surgery remains one of the most widely performed and effective procedures for vision restoration. Effective surgical planning requires integrating diverse clinical examinations for patient assessment, intraocular lens (IOL) selection, and risk evaluation. Large language models (LLMs) have shown promise in supporting clinical decision-making. However, existing LLMs often lack the domain-specific expertise to interpret heterogeneous ophthalmic data and provide actionable surgical plans. To enhance the model's ability to interpret heterogeneous ophthalmic reports, we propose a knowledge-driven Multi-Agent System (MAS), where each agent simulates the reasoning process of specialist ophthalmologists, converting raw clinical inputs into structured, actionable summaries in both training and deployment stages. Building on MAS, we introduce CataractSurg-80K, the first large-scale benchmark for cataract surgery planning that incorporates structured clinical reasoning. Each case is annotated with diagnostic questions, expert reasoning chains, and structured surgical recommendations. We further introduce Qwen-CSP, a domain-specialized model built on Qwen-4B, fine-tuned through a multi-stage process tailored for surgical planning. Comprehensive experiments show that Qwen-CSP outperforms strong general-purpose LLMs across multiple metrics. Our work delivers a high-quality dataset, a rigorous benchmark, and a domain-adapted LLM to facilitate future research in medical AI reasoning and decision support.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T16:16:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20014v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20014v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 Cross-Platform E-Commerce Product Categorization and Recategorization: A
  Multimodal Hierarchical Classification Approach</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lotte Gross, Rebecca Walter, Nicole Zoppi, Adrien Justus, Alessandro Gambetti, Qiwei Han, Maximilian Kaiser
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This study addresses critical industrial challenges in e-commerce product categorization, namely platform heterogeneity and the structural limitations of existing taxonomies, by developing and deploying a multimodal hierarchical classification framework. Using a dataset of 271,700 products from 40 international fashion e-commerce platforms, we integrate textual features (RoBERTa), visual features (ViT), and joint vision--language representations (CLIP). We investigate fusion strategies, including early, late, and attention-based fusion within a hierarchical architecture enhanced by dynamic masking to ensure taxonomic consistency. Results show that CLIP embeddings combined via an MLP-based late-fusion strategy achieve the highest hierarchical F1 (98.59\%), outperforming unimodal baselines. To address shallow or inconsistent categories, we further introduce a self-supervised ``product recategorization'' pipeline using SimCLR, UMAP, and cascade clustering, which discovered new, fine-grained categories (e.g., subtypes of ``Shoes'') with cluster purities above 86\%. Cross-platform experiments reveal a deployment-relevant trade-off: complex late-fusion methods maximize accuracy with diverse training data, while simpler early-fusion methods generalize more effectively to unseen platforms. Finally, we demonstrate the framework's industrial scalability through deployment in EURWEB's commercial transaction intelligence platform via a two-stage inference pipeline, combining a lightweight RoBERTa stage with a GPU--accelerated multimodal stage to balance cost and accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T16:16:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20013v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20013v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 NAPER: Fault Protection for Real-Time Resource-Constrained Deep Neural
  Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rian Adam Rajagede, Muhammad Husni Santriaji, Muhammad Arya Fikriansyah, Hilal Hudan Nuha, Yanjie Fu, Yan Solihin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Fault tolerance in Deep Neural Networks (DNNs) deployed on resource-constrained systems presents unique challenges for high-accuracy applications with strict timing requirements. Memory bit-flips can severely degrade DNN accuracy, while traditional protection approaches like Triple Modular Redundancy (TMR) often sacrifice accuracy to maintain reliability, creating a three-way dilemma between reliability, accuracy, and timeliness. We introduce NAPER, a novel protection approach that addresses this challenge through ensemble learning. Unlike conventional redundancy methods, NAPER employs heterogeneous model redundancy, where diverse models collectively achieve higher accuracy than any individual model. This is complemented by an efficient fault detection mechanism and a real-time scheduler that prioritizes meeting deadlines by intelligently scheduling recovery operations without interrupting inference. Our evaluations demonstrate NAPER's superiority: 40% faster inference in both normal and fault conditions, maintained accuracy 4.2% higher than TMR-based strategies, and guaranteed uninterrupted operation even during fault recovery. NAPER effectively balances the competing demands of accuracy, reliability, and timeliness in real-time DNN applications
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T16:07:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/IOLTS65288.2025.11116827' target='_blank'>doi</a><a href='http://arxiv.org/abs/2504.06591v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.06591v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 Linear-Time Demonstration Selection for In-Context Learning via Gradient
  Estimation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ziniu Zhang, Zhenshuo Zhang, Dongyue Li, Lu Wang, Jennifer Dy, Hongyang R. Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper introduces an algorithm to select demonstration examples for in-context learning of a query set. Given a set of $n$ examples, how can we quickly select $k$ out of $n$ to best serve as the conditioning for downstream inference? This problem has broad applications in prompt tuning and chain-of-thought reasoning. Since model weights remain fixed during in-context learning, previous work has sought to design methods based on the similarity of token embeddings. This work proposes a new approach based on gradients of the output taken in the input embedding space. Our approach estimates model outputs through a first-order approximation using the gradients. Then, we apply this estimation to multiple randomly sampled subsets. Finally, we aggregate the sampled subset outcomes to form an influence score for each demonstration, and select $k$ most relevant examples. This procedure only requires pre-computing model outputs and gradients once, resulting in a linear-time algorithm relative to model and training set sizes. Extensive experiments across various models and datasets validate the efficiency of our approach. We show that the gradient estimation procedure yields approximations of full inference with less than $\mathbf{1}\%$ error across six datasets. This allows us to scale up subset selection that would otherwise run full inference by up to $\mathbf{37.7}\times$ on models with up to $34$ billion parameters, and outperform existing selection methods based on input embeddings by $\mathbf{11}\%$ on average.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T15:59:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19999v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19999v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 The Coherent Multiplex: Scalable Real-Time Wavelet Coherence
  Architecture</h2>
                <div class="authors">
                    <strong>Authors:</strong> Noah Shore
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Coherent Multiplex is formalized and validated as a scalable, real-time system for identifying, analyzing, and visualizing coherence among multiple time series. Its architecture comprises a fast spectral similarity layer based on cosine similarity metrics of Fourier-transformed signals, and a sparse time-frequency layer for wavelet coherence. The system constructs and evolves a multilayer graph representing inter-signal relationships, enabling low-latency inference and monitoring. A simulation prototype demonstrates functionality across 8 synthetic channels with a high similarity threshold for further computation, with additional opportunities for scaling the architecture up to support thousands of input signals with constrained hardware. Applications discussed include neuroscience, finance, and biomedical signal analysis.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T15:51:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span><span>cs.SY</span><span>eess.SY</span><span>q-fin.MF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19994v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19994v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 MathBuddy: A Multimodal System for Affective Math Tutoring</h2>
                <div class="authors">
                    <strong>Authors:</strong> Debanjana Kar, Leopold Böss, Dacia Braca, Sebastian Maximilian Dennerlein, Nina Christine Hubig, Philipp Wintersberger, Yufang Hou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid adoption of LLM-based conversational systems is already transforming the landscape of educational technology. However, the current state-of-the-art learning models do not take into account the student's affective states. Multiple studies in educational psychology support the claim that positive or negative emotional states can impact a student's learning capabilities. To bridge this gap, we present MathBuddy, an emotionally aware LLM-powered Math Tutor, which dynamically models the student's emotions and maps them to relevant pedagogical strategies, making the tutor-student conversation a more empathetic one. The student's emotions are captured from the conversational text as well as from their facial expressions. The student's emotions are aggregated from both modalities to confidently prompt our LLM Tutor for an emotionally-aware response. We have effectively evaluated our model using automatic evaluation metrics across eight pedagogical dimensions and user studies. We report a massive 23 point performance gain using the win rate and a 3 point gain at an overall level using DAMR scores which strongly supports our hypothesis of improving LLM-based tutor's pedagogical abilities by modeling students' emotions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T15:50:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19993v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19993v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 AgentCoMa: A Compositional Benchmark Mixing Commonsense and Mathematical
  Reasoning in Real-World Scenarios</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lisa Alazraki, Lihu Chen, Ana Brassard, Joe Stacey, Hossein A. Rahmani, Marek Rei
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have achieved high accuracy on complex commonsense and mathematical problems that involve the composition of multiple reasoning steps. However, current compositional benchmarks testing these skills tend to focus on either commonsense or math reasoning, whereas LLM agents solving real-world tasks would require a combination of both. In this work, we introduce an Agentic Commonsense and Math benchmark (AgentCoMa), where each compositional task requires a commonsense reasoning step and a math reasoning step. We test it on 61 LLMs of different sizes, model families, and training strategies. We find that LLMs can usually solve both steps in isolation, yet their accuracy drops by ~30% on average when the two are combined. This is a substantially greater performance gap than the one we observe in prior compositional benchmarks that combine multiple steps of the same reasoning type. In contrast, non-expert human annotators can solve the compositional questions and the individual steps in AgentCoMa with similarly high accuracy. Furthermore, we conduct a series of interpretability studies to better understand the performance gap, examining neuron patterns, attention maps and membership inference. Our work underscores a substantial degree of model brittleness in the context of mixed-type compositional reasoning and offers a test bed for future improvement.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T15:47:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19988v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19988v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 Diffusion Language Models Know the Answer Before Decoding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pengxiang Li, Yefan Zhou, Dilxat Muhtar, Lu Yin, Shilin Yan, Li Shen, Yi Liang, Soroush Vosoughi, Shiwei Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion language models (DLMs) have recently emerged as an alternative to autoregressive approaches, offering parallel sequence generation and flexible token orders. However, their inference remains slower than that of autoregressive models, primarily due to the cost of bidirectional attention and the large number of refinement steps required for high quality outputs. In this work, we highlight and leverage an overlooked property of DLMs early answer convergence: in many cases, the correct answer can be internally identified by half steps before the final decoding step, both under semi-autoregressive and random remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99% of instances, respectively, can be decoded correctly using only half of the refinement steps. Building on this observation, we introduce Prophet, a training-free fast decoding paradigm that enables early commit decoding. Specifically, Prophet dynamically decides whether to continue refinement or to go "all-in" (i.e., decode all remaining tokens in one step), using the confidence gap between the top-2 prediction candidates as the criterion. It integrates seamlessly into existing DLM implementations, incurs negligible overhead, and requires no additional training. Empirical evaluations of LLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the number of decoding steps by up to 3.4x while preserving high generation quality. These results recast DLM decoding as a problem of when to stop sampling, and demonstrate that early decode convergence provides a simple yet powerful mechanism for accelerating DLM inference, complementary to existing speedup techniques. Our code is publicly available at https://github.com/pixeli99/Prophet.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T15:40:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19982v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19982v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text
  Tasks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Luel Hagos Beyene, Vivek Verma, Min Ma, Jesujoba O. Alabi, Fabian David Schmidt, Joyce Nakatumba-Nabende, David Ifeoluwa Adelani
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language models (LLMs) have demonstrated impressive performance on a wide range of tasks, including in multimodal settings such as speech. However, their evaluation is often limited to English and a few high-resource languages. For low-resource languages, there is no standardized evaluation benchmark. In this paper, we address this gap by introducing mSTEB, a new benchmark to evaluate the performance of LLMs on a wide range of tasks covering language identification, text classification, question answering, and translation tasks on both speech and text modalities. We evaluated the performance of leading LLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art open models such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap in performance between high-resource and low-resource languages, especially for languages spoken in Africa and Americas/Oceania. Our findings show that more investment is needed to address their under-representation in LLMs coverage.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T15:27:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span><span>cs.SD</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.08400v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.08400v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 Assessing the Geolocation Capabilities, Limitations and Societal Risks
  of Generative Vision-Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Oliver Grainge, Sania Waheed, Jack Stilgoe, Michael Milford, Shoaib Ehsan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Geo-localization is the task of identifying the location of an image using visual cues alone. It has beneficial applications, such as improving disaster response, enhancing navigation, and geography education. Recently, Vision-Language Models (VLMs) are increasingly demonstrating capabilities as accurate image geo-locators. This brings significant privacy risks, including those related to stalking and surveillance, considering the widespread uses of AI models and sharing of photos on social media. The precision of these models is likely to improve in the future. Despite these risks, there is little work on systematically evaluating the geolocation precision of Generative VLMs, their limits and potential for unintended inferences. To bridge this gap, we conduct a comprehensive assessment of the geolocation capabilities of 25 state-of-the-art VLMs on four benchmark image datasets captured in diverse environments. Our results offer insight into the internal reasoning of VLMs and highlight their strengths, limitations, and potential societal risks. Our findings indicate that current VLMs perform poorly on generic street-level images yet achieve notably high accuracy (61\%) on images resembling social media content, raising significant and urgent privacy concerns.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T15:21:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19967v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19967v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 Constraining Ricci-Cubic Holographic Dark Energy from observational data
  using the MCMC sampling and enhanced Machine learning analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aritra Sanyal, Prabir Rudra
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this work, we find constraints on the parameter space of the Ricci-Cubic Holographic dark energy (RCHDE) from various observational data sets like Hubble data, cosmic-chronometer data, Baryon-acoustic oscillation data, and also data from gamma-ray bursts. RCHDE is formed from the cubic invariant, which in turn is built from the cubic contractions of the Riemann and Ricci tensors. We have used the Markov chain Monte-Carlo (MCMC) sampling technique to find constraints on the model parameters via Bayesian inference. Contour plots have been obtained for the model parameters, showing their marginalized and joint probability distributions. The best-fit regression lines are found for the constrained model and compared with the standard $\Lambda$CDM model to verify and validate the model. To complement this data analysis mechanism, we have also performed an enhanced machine learning analysis using observational Hubble parameter data. This approach serves to validate the model's predictive power through independent, data-driven regression techniques. Different graphical illustrations of the machine learning techniques have been presented to understand the results. These illustrations reveal a strong agreement between the Hubble parameter predictions from the machine learning models, the theoretical RCHDE model, and observational data.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T15:16:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.CO</span><span>gr-qc</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19961v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19961v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 Estimating the dense gas mass of molecular clouds using spatially
  unresolved 3 mm line observations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Antoine Zakardjian, Annie Hughes, Jérôme Pety, Maryvonne Gerin, Pierre Palud, Ivana Beslic, Simon Coudé, Lucas Einig, Helena Mazurek, Jan H. Orkisz, Miriam G. Santa-Maria, Léontine Ségal, Sophia K. Stuber, Sébastien Bardeau, Emeric Bron, Pierre Chainais, Karine Demyk, Victor de Souza Magalhaes, Javier R. Goicoechea, Pierre Gratier, Viviana V. Guzman, David Languignon, François Levrier, Franck Le Petit, Dariusz C. Lis, Harvey S. Liszt, Nicolas Peretto, Antoine Roueff, Evelyne Roueff, Albrecht Sievers, Pierre-Antoine Thouvenin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We aim to develop a new method to infer the sub-beam probability density function (PDF) of H2 column densities and the dense gas mass within molecular clouds using spatially unresolved observations of molecular emission lines in the 3 mm band. We model spatially unresolved line integrated intensity measurements as the average of an emission function weighted by the sub-beam column density PDF. The emission function, which expresses the line integrated intensity as a function of the gas column density, is an empirical fit to high resolution (< 0.05 pc) multi-line observations of the Orion B molecular cloud. The column density PDF is assumed to be parametric, composed of a lognormal distribution at moderate column densities and a power law distribution at higher column densities. To estimate the sub-beam column density PDF, the emission model is combined with a Bayesian inversion algorithm (the Beetroots code), which takes account of thermal noise and calibration errors. We validate our method by demonstrating that it recovers the true column density PDF of the Orion B cloud, reproducing the observed emission line integrated intensities. We apply the method to 12CO(J=1-0), 13CO(J=1-0), C18O(J=1-0), HCN(J=1-0), HCO+(J=1-0) and N2H+(J=1-0) observations of a 700 x 700 pc2 field of view (FoV) in the nearby galaxy M51. On average, the model reproduces the observed intensities within 30%. The column density PDFs obtained for the spiral arm region within our test FoV are dominated by a power-law tail at high column densities, with slopes that are consistent with gravitational collapse. Outside the spiral arm, the column density PDFs are predominantly lognormal, consistent with supersonic isothermal turbulence. We calculate the mass associated with the powerlaw tail of the column density PDFs and observe a strong, linear correlation between this mass and the 24$\mu$m surface brightness.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T15:07:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.GA</span><span>astro-ph.IM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.10145v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.10145v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 Correcting model error bias in estimations of neuronal dynamics from
  time series observations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ian Williams, Joseph D. Taylor, Alain Nogaret
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Neuron models built from experimental data have successfully predicted observed voltage oscillations within and beyond training range. A tantalising prospect is the possibility of estimating the unobserved dynamics of ion channels which is largely inaccessible to experiment, from membrane voltage recordings. The main roadblock here is our lack of knowledge of the equations governing biological neurons which forces us to rely on surrogate models and parameter estimates biassed by model error. Error correction algorithms are therefore needed to infer both observed and unobserved dynamics, and ultimately the actual parameters of a biological neuron. Here we use a recurrent neural network to correct the outputs of a surrogate Hodgkin-Huxley (HH) model. The reservoir-surrogate HH model hybrid was trained on the voltage oscillations of a reference HH model and its driving current waveform. Out of the six reservoir-surrogate model architectures investigated, we identify one that most accurately recovers the reference membrane voltage and ion channel dynamics. The reservoir was thus effective in correcting model error in an externally driven nonlinear oscillator and in reconstructing the dynamics of both observed and unobserved state variables from the reference model mimicking an actual neuron.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T15:02:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>q-bio.NC</span><span>nlin.CD</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19948v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19948v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 Constraint Learning in Multi-Agent Dynamic Games from Demonstrations of
  Local Nash Interactions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhouyu Zhang, Chih-Yuan Chiu, Glen Chou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present an inverse dynamic game-based algorithm to learn parametric constraints from a given dataset of local generalized Nash equilibrium interactions between multiple agents. Specifically, we introduce mixed-integer linear programs (MILP) encoding the Karush-Kuhn-Tucker (KKT) conditions of the interacting agents, which recover constraints consistent with the Nash stationarity of the interaction demonstrations. We establish theoretical guarantees that our method learns inner approximations of the true safe and unsafe sets, as well as limitations of constraint learnability from demonstrations of Nash equilibrium interactions. We also use the interaction constraints recovered by our method to design motion plans that robustly satisfy the underlying constraints. Across simulations and hardware experiments, our methods proved capable of inferring constraints and designing interactive motion plans for various classes of constraints, both convex and non-convex, from interaction demonstrations of agents with nonlinear dynamics.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T15:01:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19945v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19945v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 Socially Interactive Agents for Preserving and Transferring Tacit
  Knowledge in Organizations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Martin Benderoth, Patrick Gebhard, Christian Keller, C. Benjamin Nakhosteen, Stefan Schaffer, Tanja Schneeberger
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper introduces a novel approach to tackle the challenges of preserving and transferring tacit knowledge--deep, experience-based insights that are hard to articulate but vital for decision-making, innovation, and problem-solving. Traditional methods rely heavily on human facilitators, which, while effective, are resource-intensive and lack scalability. A promising alternative is the use of Socially Interactive Agents (SIAs) as AI-driven knowledge transfer facilitators. These agents interact autonomously and socially intelligently with users through multimodal behaviors (verbal, paraverbal, nonverbal), simulating expert roles in various organizational contexts. SIAs engage employees in empathic, natural-language dialogues, helping them externalize insights that might otherwise remain unspoken. Their success hinges on building trust, as employees are often hesitant to share tacit knowledge without assurance of confidentiality and appreciation. Key technologies include Large Language Models (LLMs) for generating context-relevant dialogue, Retrieval-Augmented Generation (RAG) to integrate organizational knowledge, and Chain-of-Thought (CoT) prompting to guide structured reflection. These enable SIAs to actively elicit knowledge, uncover implicit assumptions, and connect insights to broader organizational contexts. Potential applications span onboarding, where SIAs support personalized guidance and introductions, and knowledge retention, where they conduct structured interviews with retiring experts to capture heuristics behind decisions. Success depends on addressing ethical and operational challenges such as data privacy, algorithmic bias, and resistance to AI. Transparency, robust validation, and a culture of trust are essential to mitigate these risks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T14:59:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19942v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19942v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 Principled Detection of Hallucinations in Large Language Models via
  Multiple Testing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiawei Li, Akshayaa Magesh, Venugopal V. Veeravalli
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While Large Language Models (LLMs) have emerged as powerful foundational models to solve a variety of tasks, they have also been shown to be prone to hallucinations, i.e., generating responses that sound confident but are actually incorrect or even nonsensical. In this work, we formulate the problem of detecting hallucinations as a hypothesis testing problem and draw parallels to the problem of out-of-distribution detection in machine learning models. We propose a multiple-testing-inspired method to solve the hallucination detection problem, and provide extensive experimental results to validate the robustness of our approach against state-of-the-art methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T14:55:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18473v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18473v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital
  Payments</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nitish Jaipuria, Lorenzo Gatto, Zijun Kan, Shankey Poddar, Bill Cheung, Diksha Bansal, Ramanan Balakrishnan, Aviral Suri, Jose Estevez
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The proliferation of digital payment platforms has transformed commerce, offering unmatched convenience and accessibility globally. However, this growth has also attracted malicious actors, leading to a corresponding increase in sophisticated social engineering scams. These scams are often initiated and orchestrated on multiple surfaces outside the payment platform, making user and transaction-based signals insufficient for a complete understanding of the scam's methodology and underlying patterns, without which it is very difficult to prevent it in a timely manner. This paper presents CASE (Conversational Agent for Scam Elucidation), a novel Agentic AI framework that addresses this problem by collecting and managing user scam feedback in a safe and scalable manner. A conversational agent is uniquely designed to proactively interview potential victims to elicit intelligence in the form of a detailed conversation. The conversation transcripts are then consumed by another AI system that extracts information and converts it into structured data for downstream usage in automated and manual enforcement mechanisms. Using Google's Gemini family of LLMs, we implemented this framework on Google Pay (GPay) India. By augmenting our existing features with this new intelligence, we have observed a 21% uplift in the volume of scam enforcements. The architecture and its robust evaluation framework are highly generalizable, offering a blueprint for building similar AI-driven systems to collect and manage scam intelligence in other sensitive domains.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T14:47:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19932v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19932v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 InternVL3.5: Advancing Open-Source Multimodal Models in Versatility,
  Reasoning, and Efficiency</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, Zhaokai Wang, Zhe Chen, Hongjie Zhang, Ganlin Yang, Haomin Wang, Qi Wei, Jinhui Yin, Wenhao Li, Erfei Cui, Guanzhou Chen, Zichen Ding, Changyao Tian, Zhenyu Wu, Jingjing Xie, Zehao Li, Bowen Yang, Yuchen Duan, Xuehui Wang, Zhi Hou, Haoran Hao, Tianyi Zhang, Songze Li, Xiangyu Zhao, Haodong Duan, Nianchen Deng, Bin Fu, Yinan He, Yi Wang, Conghui He, Botian Shi, Junjun He, Yingtong Xiong, Han Lv, Lijun Wu, Wenqi Shao, Kaipeng Zhang, Huipeng Deng, Biqing Qi, Jiaye Ge, Qipeng Guo, Wenwei Zhang, Songyang Zhang, Maosong Cao, Junyao Lin, Kexian Tang, Jianfei Gao, Haian Huang, Yuzhe Gu, Chengqi Lyu, Huanze Tang, Rui Wang, Haijun Lv, Wanli Ouyang, Limin Wang, Min Dou, Xizhou Zhu, Tong Lu, Dahua Lin, Jifeng Dai, Weijie Su, Bowen Zhou, Kai Chen, Yu Qiao, Wenhai Wang, Gen Luo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce InternVL 3.5, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inference efficiency along the InternVL series. A key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment. This coarse-to-fine training strategy leads to substantial improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize efficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution of visual tokens without compromising performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs, effectively balancing computational load. These contributions collectively enable InternVL3.5 to achieve up to a +16.0\% gain in overall reasoning performance and a 4.05$\times$ inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as GUI interaction and embodied agency. Notably, our largest model, i.e., InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs across general multimodal, reasoning, text, and agentic tasks -- narrowing the performance gap with leading commercial models like GPT-5. All models and code are publicly released.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T14:39:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18265v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18265v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 Synthesizing High-Quality Programming Tasks with LLM-based Expert and
  Student Agents</h2>
                <div class="authors">
                    <strong>Authors:</strong> Manh Hung Nguyen, Victor-Alexandru Pădurean, Alkis Gotovos, Sebastian Tschiatschek, Adish Singla
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Generative AI is transforming computing education by enabling the automatic generation of personalized content and feedback. We investigate its capabilities in providing high-quality programming tasks to students. Despite promising advancements in task generation, a quality gap remains between AI-generated and expert-created tasks. The AI-generated tasks may not align with target programming concepts, could be incomprehensible to students, or may contain critical issues such as incorrect tests. Existing works often require interventions from human teachers for validation. We address these challenges by introducing PyTaskSyn, a novel synthesis technique that first generates a programming task and then decides whether it meets certain quality criteria to be given to students. The key idea is to break this process into multiple stages performed by expert and student agents simulated using both strong and weaker generative models. Through extensive evaluation, we show that PyTaskSyn significantly improves task quality compared to baseline techniques and showcases the importance of each specialized agent type in our validation pipeline. Additionally, we conducted user studies using our publicly available web application and show that PyTaskSyn can deliver high-quality programming tasks comparable to expert-designed ones while reducing workload and costs, and being more engaging than programming tasks that are available in online resources.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T14:39:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.07655v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.07655v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 Self-Organizing Agent Network for LLM-based Workflow Automation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yiming Xiong, Jian Wang, Bing Li, Yuhan Zhu, Yuqi Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent multi-agent frameworks built upon large language models (LLMs) have demonstrated remarkable capabilities in complex task planning. However, in real-world enterprise environments, business workflows are typically composed through modularization and reuse of numerous subprocesses, resulting in intricate workflows characterized by lengthy and deeply nested execution paths. Such complexity poses significant challenges for LLM-driven orchestration, as extended reasoning chains and state-space explosions severely impact planning effectiveness and the proper sequencing of tool invocations. Therefore, developing an orchestration method with controllable structures capable of handling multi-layer nesting becomes a critical issue. To address this, we propose a novel structure-driven orchestration framework Self-Organizing Agent Network (SOAN). SOAN incrementally builds a formalized agent network by identifying and encapsulating structural units as independent agents, enhancing modularity and clarity in orchestration. Extensive evaluations were performed using multiple benchmarks as well as a real-world enterprise workflow dataset. Experimental results demonstrate that SOAN significantly outperforms state-of-the-art methods in terms of adaptability, fault tolerance, and execution efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T14:37:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.13732v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.13732v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 HEAL: A Hypothesis-Based Preference-Aware Analysis Framework</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yifu Huo, Chenglong Wang, Qiren Zhu, Shunjie Xing, Tong Xiao, Chunliang Zhang, Tongran Liu, Jinbo Zhu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Preference optimization methods like DPO have achieved remarkable performance in LLM alignment. However, the evaluation for these methods relies on a single response and overlooks other potential outputs, which could also be generated in real-world applications within this hypothetical space. To address this issue, this paper presents a \textbf{H}ypothesis-based Pr\textbf{E}ference-aware \textbf{A}na\textbf{L}ysis Framework (HEAL), a novel evaluation paradigm that formulates preference alignment as a re-ranking process within hypothesis spaces. The framework incorporates two complementary metrics: ranking accuracy for evaluating ordinal consistency and preference strength correlation for assessing continuous alignment. To facilitate this framework, we develop UniHypoBench, a unified hypothesis benchmark constructed from diverse instruction-response pairs. Through extensive experiments based on HEAL, with a particular focus on the intrinsic mechanisms of preference learning, we demonstrate that current preference learning methods can effectively capture preferences provided by proxy models while simultaneously suppressing negative samples. These findings contribute to preference learning research through two significant avenues. Theoretically, we introduce hypothesis space analysis as an innovative paradigm for understanding preference alignment. Practically, HEAL offers researchers robust diagnostic tools for refining preference optimization methods, while our empirical results identify promising directions for developing more advanced alignment algorithms capable of comprehensive preference capture.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T14:30:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19922v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19922v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 Your AI Bosses Are Still Prejudiced: The Emergence of Stereotypes in
  LLM-Based Multi-Agent Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jingyu Guo, Yingying Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While stereotypes are well-documented in human social interactions, AI systems are often presumed to be less susceptible to such biases. Previous studies have focused on biases inherited from training data, but whether stereotypes can emerge spontaneously in AI agent interactions merits further exploration. Through a novel experimental framework simulating workplace interactions with neutral initial conditions, we investigate the emergence and evolution of stereotypes in LLM-based multi-agent systems. Our findings reveal that (1) LLM-Based AI agents develop stereotype-driven biases in their interactions despite beginning without predefined biases; (2) stereotype effects intensify with increased interaction rounds and decision-making power, particularly after introducing hierarchical structures; (3) these systems exhibit group effects analogous to human social behavior, including halo effects, confirmation bias, and role congruity; and (4) these stereotype patterns manifest consistently across different LLM architectures. Through comprehensive quantitative analysis, these findings suggest that stereotype formation in AI systems may arise as an emergent property of multi-agent interactions, rather than merely from training data biases. Our work underscores the need for future research to explore the underlying mechanisms of this phenomenon and develop strategies to mitigate its ethical impacts.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T14:25:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19919v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19919v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 Refining Text Generation for Realistic Conversational Recommendation via
  Direct Preference Optimization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Manato Tajiri, Michimasa Inaba
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Conversational Recommender Systems (CRSs) aim to elicit user preferences via natural dialogue to provide suitable item recommendations. However, current CRSs often deviate from realistic human interactions by rapidly recommending items in brief sessions. This work addresses this gap by leveraging Large Language Models (LLMs) to generate dialogue summaries from dialogue history and item recommendation information from item description. This approach enables the extraction of both explicit user statements and implicit preferences inferred from the dialogue context. We introduce a method using Direct Preference Optimization (DPO) to ensure dialogue summary and item recommendation information are rich in information crucial for effective recommendations. Experiments on two public datasets validate our method's effectiveness in fostering more natural and realistic conversational recommendation processes.Our implementation is publicly available at: https://github.com/UEC-InabaLab/Refining-LLM-Text
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T10:57:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19918v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19918v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 GegenNet: Spectral Convolutional Neural Networks for Link Sign
  Prediction in Signed Bipartite Graphs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hewen Wang, Renchi Yang, Xiaokui Xiao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Given a signed bipartite graph (SBG) G with two disjoint node sets U and V, the goal of link sign prediction is to predict the signs of potential links connecting U and V based on known positive and negative edges in G. The majority of existing solutions towards link sign prediction mainly focus on unipartite signed graphs, which are sub-optimal due to the neglect of node heterogeneity and unique bipartite characteristics of SBGs. To this end, recent studies adapt graph neural networks to SBGs by introducing message-passing schemes for both inter-partition (UxV) and intra-partition (UxU or VxV) node pairs. However, the fundamental spectral convolutional operators were originally designed for positive links in unsigned graphs, and thus, are not optimal for inferring missing positive or negative links from known ones in SBGs.   Motivated by this, this paper proposes GegenNet, a novel and effective spectral convolutional neural network model for link sign prediction in SBGs. In particular, GegenNet achieves enhanced model capacity and high predictive accuracy through three main technical contributions: (i) fast and theoretically grounded spectral decomposition techniques for node feature initialization; (ii) a new spectral graph filter based on the Gegenbauer polynomial basis; and (iii) multi-layer sign-aware spectral convolutional networks alternating Gegenbauer polynomial filters with positive and negative edges. Our extensive empirical studies reveal that GegenNet can achieve significantly superior performance (up to a gain of 4.28% in AUC and 11.69% in F1) in link sign prediction compared to 11 strong competitors over 6 benchmark SBG datasets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T14:10:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.SI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19907v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19907v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 Logical Reasoning with Outcome Reward Models for Test-Time Scaling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ramya Keerthy Thatikonda, Wray Buntine, Ehsan Shareghi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Logical reasoning is a critical benchmark for evaluating the capabilities of large language models (LLMs), as it reflects their ability to derive valid conclusions from given premises. While the combination of test-time scaling with dedicated outcome or process reward models has opened up new avenues to enhance LLMs performance in complex reasoning tasks, this space is under-explored in deductive logical reasoning. We present a set of Outcome Reward Models (ORMs) for deductive reasoning. To train the ORMs we mainly generate data using Chain-of-Thought (CoT) with single and multiple samples. Additionally, we propose a novel tactic to further expand the type of errors covered in the training dataset of the ORM. In particular, we propose an echo generation technique that leverages LLMs' tendency to reflect incorrect assumptions made in prompts to extract additional training data, covering previously unexplored error types. While a standard CoT chain may contain errors likely to be made by the reasoner, the echo strategy deliberately steers the model toward incorrect reasoning. We show that ORMs trained on CoT and echo-augmented data demonstrate improved performance on the FOLIO, JustLogic, and ProverQA datasets across four different LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T14:08:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19903v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19903v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 EEGDM: EEG Representation Learning via Generative Diffusion Model</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jia Hong Puah, Sim Kuan Goh, Ziwei Zhang, Zixuan Ye, Chow Khuen Chan, Kheng Seang Lim, Si Lei Fong, Kok Sin Woon, Cuntai Guan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While electroencephalogram (EEG) has been a crucial tool for monitoring the brain and diagnosing neurological disorders (e.g., epilepsy), learning meaningful representations from raw EEG signals remains challenging due to limited annotations and high signal variability. Recently, EEG foundation models (FMs) have shown promising potential by adopting transformer architectures and self-supervised pre-training methods from large language models (e.g., masked prediction) to learn representations from diverse EEG data, followed by fine-tuning on specific EEG tasks. Nonetheless, these large models often incurred high computational costs during both training and inference, with only marginal performance improvements as the model size increases. In this work, we proposed an EEG representation learning framework building upon Generative Diffusion Model (EEGDM). Specifically, we developed a structured state-space model for diffusion pretraining (SSMDP) to better capture the temporal dynamics of EEG signals and trained the model using a Denoising Diffusion Probabilistic Model. Subsequently, the resulting latent EEG representations were then used for downstream classification tasks via our proposed latent fusion transformer (LFT). To evaluate our method, we used multi-event datasets covering both interictal epileptiform discharges and seizure detection, and compared EEGDM with current state-of-the-art approaches, including EEG FMs. Empirical results showed that our method outperformed the existing methods. These findings suggested that EEGDM offered a promising alternative to current FMs. Our code is available at: https://github.com/jhpuah/EEGDM.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T14:04:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.14086v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.14086v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 Truth or Twist? Optimal Model Selection for Reliable Label Flipping
  Evaluation in LLM-based Counterfactuals</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qianli Wang, Van Bach Nguyen, Nils Feldhus, Luis Felipe Villa-Arenas, Christin Seifert, Sebastian Möller, Vera Schmitt
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Counterfactual examples are widely employed to enhance the performance and robustness of large language models (LLMs) through counterfactual data augmentation (CDA). However, the selection of the judge model used to evaluate label flipping, the primary metric for assessing the validity of generated counterfactuals for CDA, yields inconsistent results. To decipher this, we define four types of relationships between the counterfactual generator and judge models: being the same model, belonging to the same model family, being independent models, and having an distillation relationship. Through extensive experiments involving two state-of-the-art LLM-based methods, three datasets, four generator models, and 15 judge models, complemented by a user study (n = 90), we demonstrate that judge models with an independent, non-fine-tuned relationship to the generator model provide the most reliable label flipping evaluations. Relationships between the generator and judge models, which are closely aligned with the user study for CDA, result in better model performance and robustness. Nevertheless, we find that the gap between the most effective judge models and the results obtained from the user study remains considerably large. This suggests that a fully automated pipeline for CDA may be inadequate and requires human intervention.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T14:04:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.13972v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.13972v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 RISE: Two-Stage Rank-Based Identification of High-Dimensional Surrogate
  Markers Applied to Vaccinology</h2>
                <div class="authors">
                    <strong>Authors:</strong> Arthur Hughes, Layla Parast, Rodolphe Thiébaut, Boris P. Hejblum
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In vaccine trials with long-term participant follow-up, it is of great importance to identify surrogate markers that accurately infer long-term immune responses. These markers offer practical advantages such as providing early, indirect evidence of vaccine efficacy, and can accelerate vaccine development while identifying potential biomarkers. High-throughput technologies like RNA-sequencing have emerged as promising tools for understanding complex biological systems and informing new treatment strategies. However, these data are high-dimensional, presenting unique statistical challenges for existing surrogate marker identification methods. We introduce Rank-based Identification of high-dimensional SurrogatE Markers (RISE), a novel approach designed for small sample, high-dimensional settings typical in modern vaccine experiments. RISE employs a non-parametric univariate test to screen variables for promising candidates, followed by surrogate evaluation on independent data. Our simulation studies demonstrate RISE's desirable properties, including type one error rate control and empirical power under various conditions. Applying RISE to a clinical trial for inactivated influenza vaccination, we sought to identify genes whose expression could serve as a surrogate for the induced immune response. This analysis revealed a signature of genes appearing to function as a reasonable surrogate for the neutralising antibody response. Pathways related to innate antiviral signalling and interferon stimulation were strongly represented in this derived surrogate, providing a clear immunological interpretation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:51:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>stat.AP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.03030v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.03030v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 The Power of Regular Constraint Propagation (Technical Report)</h2>
                <div class="authors">
                    <strong>Authors:</strong> Matthew Hague, Artur Jeż, Anthony W. Lin, Oliver Markgraf, Philipp Rümmer
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The past decade has witnessed substantial developments in string solving. Motivated by the complexity of string solving strategies adopted in existing string solvers, we investigate a simple and generic method for solving string constraints: regular constraint propagation. The method repeatedly computes pre- or post-images of regular languages under the string functions present in a string formula, inferring more and more knowledge about the possible values of string variables, until either a conflict is found or satisfiability of the string formula can be concluded. Such a propagation strategy is applicable to string constraints with multiple operations like concatenation, replace, and almost all flavors of string transductions. We demonstrate the generality and effectiveness of this method theoretically and experimentally. On the theoretical side, we show that RCP is sound and complete for a large fragment of string constraints, subsuming both straight-line and chain-free constraints, two of the most expressive decidable fragments for which some modern string solvers provide formal completeness guarantees. On the practical side, we implement regular constraint propagation within the open-source string solver OSTRICH.   Our experimental evaluation shows that this addition significantly improves OSTRICH's performance and makes it competitive with existing solvers. In fact, it substantially outperforms other solvers on random PCP and bioinformatics benchmarks. The results also suggest that incorporating regular constraint propagation alongside other techniques could lead to substantial performance gains for existing solvers.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:50:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LO</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3763165' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.19888v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19888v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 Towards New Benchmark for AI Alignment & Sentiment Analysis in Socially
  Important Issues: A Comparative Study of Human and LLMs in the Context of AGI</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ljubisa Bojic, Dylan Seychell, Milan Cabarkapa
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As general-purpose artificial intelligence systems become increasingly integrated into society and are used for information seeking, content generation, problem solving, textual analysis, coding, and running processes, it is crucial to assess their long-term impact on humans. This research explores the sentiment of large language models (LLMs) and humans toward artificial general intelligence (AGI) using a Likert-scale survey. Seven LLMs, including GPT-4 and Bard, were analyzed and compared with sentiment data from three independent human sample populations. Temporal variations in sentiment were also evaluated over three consecutive days. The results show a diversity in sentiment scores among LLMs, ranging from 3.32 to 4.12 out of 5. GPT-4 recorded the most positive sentiment toward AGI, while Bard leaned toward a neutral sentiment. In contrast, the human samples showed a lower average sentiment of 2.97. The analysis outlines potential conflicts of interest and biases in the sentiment formation of LLMs, and indicates that LLMs could subtly influence societal perceptions. To address the need for regulatory oversight and culturally grounded assessments of AI systems, we introduce the Societal AI Alignment and Sentiment Benchmark (SAAS-AI), which leverages multidimensional prompts and empirically validated societal value frameworks to evaluate language model outputs across temporal, model, and multilingual axes. This benchmark is designed to guide policymakers and AI agencies, including within frameworks such as the EU AI Act, by providing robust, actionable insights into AI alignment with human values, public sentiment, and ethical norms at both national and international levels. Future research should further refine the operationalization of the SAAS-AI benchmark and systematically evaluate its effectiveness through comprehensive empirical testing.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:49:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.02531v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.02531v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 Bangla-Bayanno: A 52K-Pair Bengali Visual Question Answering Dataset
  with LLM-Assisted Translation Refinement</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mohammed Rakibul Hasan, Rafi Majid, Ahanaf Tahmid
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper, we introduce Bangla-Bayanno, an open-ended Visual Question Answering (VQA) Dataset in Bangla, a widely used, low-resource language in multimodal AI research. The majority of existing datasets are either manually annotated with an emphasis on a specific domain, query type, or answer type or are constrained by niche answer formats. In order to mitigate human-induced errors and guarantee lucidity, we implemented a multilingual LLM-assisted translation refinement pipeline. This dataset overcomes the issues of low-quality translations from multilingual sources. The dataset comprises 52,650 question-answer pairs across 4750+ images. Questions are classified into three distinct answer types: nominal (short descriptive), quantitative (numeric), and polar (yes/no). Bangla-Bayanno provides the most comprehensive open-source, high-quality VQA benchmark in Bangla, aiming to advance research in low-resource multimodal learning and facilitate the development of more inclusive AI systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:48:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19887v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19887v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 AI-Powered Detection of Inappropriate Language in Medical School
  Curricula</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chiman Salavati, Shannon Song, Scott A. Hale, Roberto E. Montenegro, Shiri Dori-Hacohen, Fabricio Murai
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The use of inappropriate language -- such as outdated, exclusionary, or non-patient-centered terms -- medical instructional materials can significantly influence clinical training, patient interactions, and health outcomes. Despite their reputability, many materials developed over past decades contain examples now considered inappropriate by current medical standards. Given the volume of curricular content, manually identifying instances of inappropriate use of language (IUL) and its subcategories for systematic review is prohibitively costly and impractical. To address this challenge, we conduct a first-in-class evaluation of small language models (SLMs) fine-tuned on labeled data and pre-trained LLMs with in-context learning on a dataset containing approximately 500 documents and over 12,000 pages. For SLMs, we consider: (1) a general IUL classifier, (2) subcategory-specific binary classifiers, (3) a multilabel classifier, and (4) a two-stage hierarchical pipeline for general IUL detection followed by multilabel classification. For LLMs, we consider variations of prompts that include subcategory definitions and/or shots. We found that both LLama-3 8B and 70B, even with carefully curated shots, are largely outperformed by SLMs. While the multilabel classifier performs best on annotated data, supplementing training with unflagged excerpts as negative examples boosts the specific classifiers' AUC by up to 25%, making them most effective models for mitigating harmful language in medical curricula.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:40:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.CY</span><span>I.2.1; I.2.7</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19883v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19883v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 SinLlama -- A Large Language Model for Sinhala</h2>
                <div class="authors">
                    <strong>Authors:</strong> H. W. K. Aravinda, Rashad Sirajudeen, Samith Karunathilake, Nisansa de Silva, Surangika Ranathunga, Rishemjit Kaur
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Low-resource languages such as Sinhala are often overlooked by open-source Large Language Models (LLMs). In this research, we extend an existing multilingual LLM (Llama-3-8B) to better serve Sinhala. We enhance the LLM tokenizer with Sinhala specific vocabulary and perform continual pre-training on a cleaned 10 million Sinhala corpus, resulting in the SinLlama model. This is the very first decoder-based open-source LLM with explicit Sinhala support. When SinLlama was instruction fine-tuned for three text classification tasks, it outperformed base and instruct variants of Llama-3-8B by a significant margin.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:39:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.09115v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.09115v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 Secure Multi-LLM Agentic AI and Agentification for Edge General
  Intelligence by Zero-Trust: A Survey</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yinqiu Liu, Ruichen Zhang, Haoxiang Luo, Yijing Lin, Geng Sun, Dusit Niyato, Hongyang Du, Zehui Xiong, Yonggang Wen, Abbas Jamalipour, Dong In Kim, Ping Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Agentification serves as a critical enabler of Edge General Intelligence (EGI), transforming massive edge devices into cognitive agents through integrating Large Language Models (LLMs) and perception, reasoning, and acting modules. These agents collaborate across heterogeneous edge infrastructures, forming multi-LLM agentic AI systems that leverage collective intelligence and specialized capabilities to tackle complex, multi-step tasks. However, the collaborative nature of multi-LLM systems introduces critical security vulnerabilities, including insecure inter-LLM communications, expanded attack surfaces, and cross-domain data leakage that traditional perimeter-based security cannot adequately address. To this end, this survey introduces zero-trust security of multi-LLM in EGI, a paradigmatic shift following the ``never trust, always verify'' principle. We begin by systematically analyzing the security risks in multi-LLM systems within EGI contexts. Subsequently, we present the vision of a zero-trust multi-LLM framework in EGI. We then survey key technical progress to facilitate zero-trust multi-LLM systems in EGI. Particularly, we categorize zero-trust security mechanisms into model- and system-level approaches. The former and latter include strong identification, context-aware access control, etc., and proactive maintenance, blockchain-based management, etc., respectively. Finally, we identify critical research directions. This survey serves as the first systematic treatment of zero-trust applied to multi-LLM systems, providing both theoretical foundations and practical strategies.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:33:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19870v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19870v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 Hydra: Structured Cross-Source Enhanced Large Language Model Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Xin Yuan, Liming Zhu, Wenjie Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating external knowledge. Current hybrid RAG system retrieves evidence from both knowledge graphs (KGs) and text documents to support LLM reasoning. However, it faces challenges like handling multi-hop reasoning, multi-entity questions, multi-source verification, and effective graph utilization. To address these limitations, we present Hydra, a training-free framework that unifies graph topology, document semantics, and source reliability to support deep, faithful reasoning in LLMs. Hydra handles multi-hop and multi-entity problems through agent-driven exploration that combines structured and unstructured retrieval, increasing both diversity and precision of evidence. To tackle multi-source verification, Hydra uses a tri-factor cross-source verification (source trustworthiness assessment, cross-source corroboration, and entity-path alignment), to balance topic relevance with cross-modal agreement. By leveraging graph structure, Hydra fuses heterogeneous sources, guides efficient exploration, and prunes noise early. Comprehensive experiments on seven benchmark datasets show that Hydra achieves overall state-of-the-art results on all benchmarks with GPT-3.5, outperforming the strong hybrid baseline ToG-2 by an average of 20.3% and up to 30.1%. Furthermore, Hydra enables smaller models (e.g., Llama-3.1-8B) to achieve reasoning performance comparable to that of GPT-4-Turbo. The source code is available on https://stevetantan.github.io/Hydra/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:30:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.17464v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.17464v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 TrajFusionNet: Pedestrian Crossing Intention Prediction via Fusion of
  Sequential and Visual Trajectory Representations</h2>
                <div class="authors">
                    <strong>Authors:</strong> François G. Landry, Moulay A. Akhloufi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the introduction of vehicles with autonomous capabilities on public roads, predicting pedestrian crossing intention has emerged as an active area of research. The task of predicting pedestrian crossing intention involves determining whether pedestrians in the scene are likely to cross the road or not. In this work, we propose TrajFusionNet, a novel transformer-based model that combines future pedestrian trajectory and vehicle speed predictions as priors for predicting crossing intention. TrajFusionNet comprises two branches: a Sequence Attention Module (SAM) and a Visual Attention Module (VAM). The SAM branch learns from a sequential representation of the observed and predicted pedestrian trajectory and vehicle speed. Complementarily, the VAM branch enables learning from a visual representation of the predicted pedestrian trajectory by overlaying predicted pedestrian bounding boxes onto scene images. By utilizing a small number of lightweight modalities, TrajFusionNet achieves the lowest total inference time (including model runtime and data preprocessing) among current state-of-the-art approaches. In terms of performance, it achieves state-of-the-art results across the three most commonly used datasets for pedestrian crossing intention prediction.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:29:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19866v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19866v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 Input-Time Scaling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rapheal Huang, Weilong Guo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Current Large Language Models (LLMs) are usually post-trained on large-scale carefully curated datasets (data & training scaling) and doing reasoning in test time (inference time scaling). In this work, we present a new scaling paradigm, Input-Time Scaling, to complement previous scaling methods by putting resources on queries (input time). During training and testing, we utilize meta-knowledge from LLMs to refine inputs with different strategies. We also discover a new phenomenon, train-test co-design. It requires us to apply query strategies during training and testing as a whole. Only applying strategies on training or testing would seriously degrade the performance gained. We are also surprised to find that seemingly low data quality datasets can perform better. We can get the best performance even by adding irrelevant information to the queries, with randomly selected 1k examples from a minimally filtered dataset. These findings contradict the widely held inductive bias, "garbage in, garbage out". Curating datasets with seemingly high-quality data can even potentially limit the performance ceiling. In addition, models trained on more data with similar quality (15k VS 1k) perform worse, the intuition of simply scaling the size should also be carefully inspected. The good news is that our findings are compatible with the Less is More phenomenon. 1K examples are enough to invoke high-level reasoning ability. With experiments on Qwen2.5-32B-Instruct, we are able to reach SOTA performance among 32B models on AIME24(76.7%) and AIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with a majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B, the result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate reproducibility and further research, we are working on open-source our datasets, data pipelines, evaluation results, and checkpoints.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:27:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.13654v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.13654v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 Probe the missing baryons in cosmic filaments with Fast Radio Bursts</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jian-Feng Mo, Weishan Zhu, Qi-Rui Yang, Yi Zheng, Long-Long Feng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Cosmic filaments are thought to host a substantial fraction of the missing baryons at redshifts $z<2$. In this study, we probe the baryonic content of these filaments using localized Fast Radio Bursts (FRBs). Filaments are identified from the galaxy distribution in the Dark Energy Spectroscopic Instrument (DESI) imaging surveys using the DisPerSe algorithm. We find tentative evidence ($\sim 3 \sigma$ significance) for a divergence in the relationship between the dispersion measure (DM) contributed by the intergalactic medium and redshift for FRBs whose signals intersect cosmic filaments compared to those that do not, suggesting excess baryons in the filamentary structures. Assuming an isothermal $\beta$-model gas profile with $\beta=2/3$, this discrepancy is best explained by a central baryon overdensity of $\delta = 20.78$, broadly consistent with previous simulation and observational results. The inferred baryon fraction residing in filaments decreases with redshift, from approximately $0.25-0.30\,\Omega_b$ at $z=0.02$ to $0.15-0.30\,\Omega_b$ at $z=0.5$, and $0.03-0.04\,\Omega_b$ at $z=0.8$. These estimates are likely lower bounds, particularly at $z>0.5$, due to the limited number of identified filaments and localized FRBs at higher redshifts. We also examine various factors that may affect the statistical significance of our results. Our method offers an independent approach to tracing baryons in cosmic filaments and underscores the importance of expanding localized FRB samples and deepening galaxy surveys, which are key steps toward refining these estimates and addressing the missing baryon problem.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:25:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.CO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19861v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19861v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 CP4SBI: Local Conformal Calibration of Credible Sets in Simulation-Based
  Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Luben M. C. Cabezas, Vagner S. Santos, Thiago R. Ramos, Pedro L. C. Rodrigues, Rafael Izbicki
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Current experimental scientists have been increasingly relying on simulation-based inference (SBI) to invert complex non-linear models with intractable likelihoods. However, posterior approximations obtained with SBI are often miscalibrated, causing credible regions to undercover true parameters. We develop $\texttt{CP4SBI}$, a model-agnostic conformal calibration framework that constructs credible sets with local Bayesian coverage. Our two proposed variants, namely local calibration via regression trees and CDF-based calibration, enable finite-sample local coverage guarantees for any scoring function, including HPD, symmetric, and quantile-based regions. Experiments on widely used SBI benchmarks demonstrate that our approach improves the quality of uncertainty quantification for neural posterior estimators using both normalizing flows and score-diffusion modeling.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:24:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ML</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17077v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17077v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 When AIOps Become "AI Oops": Subverting LLM-driven IT Operations via
  Telemetry Manipulation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dario Pasquini, Evgenios M. Kornaropoulos, Giuseppe Ateniese, Omer Akgul, Athanasios Theocharis, Petros Efstathopoulos
                </div>
                <div class="summary">
                    <strong>Summary:</strong> AI for IT Operations (AIOps) is transforming how organizations manage complex software systems by automating anomaly detection, incident diagnosis, and remediation. Modern AIOps solutions increasingly rely on autonomous LLM-based agents to interpret telemetry data and take corrective actions with minimal human intervention, promising faster response times and operational cost savings.   In this work, we perform the first security analysis of AIOps solutions, showing that, once again, AI-driven automation comes with a profound security cost. We demonstrate that adversaries can manipulate system telemetry to mislead AIOps agents into taking actions that compromise the integrity of the infrastructure they manage. We introduce techniques to reliably inject telemetry data using error-inducing requests that influence agent behavior through a form of adversarial reward-hacking; plausible but incorrect system error interpretations that steer the agent's decision-making. Our attack methodology, AIOpsDoom, is fully automated--combining reconnaissance, fuzzing, and LLM-driven adversarial input generation--and operates without any prior knowledge of the target system.   To counter this threat, we propose AIOpsShield, a defense mechanism that sanitizes telemetry data by exploiting its structured nature and the minimal role of user-generated content. Our experiments show that AIOpsShield reliably blocks telemetry-based attacks without affecting normal agent performance.   Ultimately, this work exposes AIOps as an emerging attack vector for system compromise and underscores the urgent need for security-aware AIOps design.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:14:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.06394v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.06394v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 Youtu-GraphRAG: Vertically Unified Agents for Graph Retrieval-Augmented
  Complex Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junnan Dong, Siyu An, Yifei Yu, Qian-Wen Zhang, Linhao Luo, Xiao Huang, Yunsheng Wu, Di Yin, Xing Sun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Graph retrieval-augmented generation (GraphRAG) has effectively enhanced large language models in complex reasoning by organizing fragmented knowledge into explicitly structured graphs. Prior efforts have been made to improve either graph construction or graph retrieval in isolation, yielding suboptimal performance, especially when domain shifts occur. In this paper, we propose a vertically unified agentic paradigm, Youtu-GraphRAG, to jointly connect the entire framework as an intricate integration. Specifically, (i) a seed graph schema is introduced to bound the automatic extraction agent with targeted entity types, relations and attribute types, also continuously expanded for scalability over unseen domains; (ii) To obtain higher-level knowledge upon the schema, we develop novel dually-perceived community detection, fusing structural topology with subgraph semantics for comprehensive knowledge organization. This naturally yields a hierarchical knowledge tree that supports both top-down filtering and bottom-up reasoning with community summaries; (iii) An agentic retriever is designed to interpret the same graph schema to transform complex queries into tractable and parallel sub-queries. It iteratively performs reflection for more advanced reasoning; (iv) To alleviate the knowledge leaking problem in pre-trained LLM, we propose a tailored anonymous dataset and a novel 'Anonymity Reversion' task that deeply measures the real performance of the GraphRAG frameworks. Extensive experiments across six challenging benchmarks demonstrate the robustness of Youtu-GraphRAG, remarkably moving the Pareto frontier with up to 90.71% saving of token costs and 16.62% higher accuracy over state-of-the-art baselines. The results indicate our adaptability, allowing seamless domain transfer with minimal intervention on schema.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:13:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19855v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19855v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 Inference on Partially Identified Parameters with Separable Nuisance
  Parameters: a Two-Stage Method</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xunkang Tian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper develops a two-stage method for inference on partially identified parameters in moment inequality models with separable nuisance parameters. In the first stage, the nuisance parameters are estimated separately, and in the second stage, the identified set for the parameters of interest is constructed using a refined chi-squared test with variance correction that accounts for the first-stage estimation error. We establish the asymptotic validity of the proposed method under mild conditions and characterize its finite-sample properties. The method is broadly applicable to models where direct elimination of nuisance parameters is difficult or introduces conservativeness. Its practical performance is illustrated through an application: structural estimation of entry and exit costs in the U.S. vehicle market based on Wollmann (2018).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:10:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>econ.EM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19853v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19853v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 Ego-centric Predictive Model Conditioned on Hand Trajectories</h2>
                <div class="authors">
                    <strong>Authors:</strong> Binjie Zhang, Mike Zheng Shou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In egocentric scenarios, anticipating both the next action and its visual outcome is essential for understanding human-object interactions and for enabling robotic planning. However, existing paradigms fall short of jointly modeling these aspects. Vision-Language-Action (VLA) models focus on action prediction but lack explicit modeling of how actions influence the visual scene, while video prediction models generate future frames without conditioning on specific actions, often resulting in implausible or contextually inconsistent outcomes. To bridge this gap, we propose a unified two-stage predictive framework that jointly models action and visual future in egocentric scenarios, conditioned on hand trajectories. In the first stage, we perform consecutive state modeling to process heterogeneous inputs (visual observations, language, and action history) and explicitly predict future hand trajectories. In the second stage, we introduce causal cross-attention to fuse multi-modal cues, leveraging inferred action signals to guide an image-based Latent Diffusion Model (LDM) for frame-by-frame future video generation. Our approach is the first unified model designed to handle both egocentric human activity understanding and robotic manipulation tasks, providing explicit predictions of both upcoming actions and their visual consequences. Extensive experiments on Ego4D, BridgeData, and RLBench demonstrate that our method outperforms state-of-the-art baselines in both action prediction and future video synthesis.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T07:08:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19852v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19852v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 Tracking World States with Language Models: State-Based Evaluation Using
  Chess</h2>
                <div class="authors">
                    <strong>Authors:</strong> Romain Harang, Jason Naradowsky, Yaswitha Gujju, Yusuke Miyao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) exhibit emergent capabilities in structured domains, suggesting they may implicitly internalize high-fidelity representations of world models. While probing techniques have shown promising signs of this in scientific and game-based settings, they rely on model-specific internal activations, which limit interpretability and generalizability. In this work, we propose a model-agnostic, state-based evaluation framework using chess as a benchmark to assess whether LLMs preserve the semantics of structured environments. Our method analyzes the downstream legal move distributions (state affordances) to estimate semantic fidelity between predicted and actual game states. This approach offers a more meaningful evaluation than conventional string-based metrics by aligning more closely with the strategic and rule-governed nature of chess. Experimental results demonstrate that our metrics capture deficiencies in state-tracking, highlighting limitations of LLMs in maintaining coherent internal models over long sequences. Our framework provides a robust tool for evaluating structured reasoning in LLMs without requiring internal model access, and generalizes to a wide class of symbolic environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:08:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19851v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19851v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 Physics-Informed DeepONet Coupled with FEM for Convective Transport in
  Porous Media with Sharp Gaussian Sources</h2>
                <div class="authors">
                    <strong>Authors:</strong> Erdi Kara, Panos Stinis
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present a hybrid framework that couples finite element methods (FEM) with physics-informed DeepONet to model fluid transport in porous media from sharp, localized Gaussian sources. The governing system consists of a steady-state Darcy flow equation and a time-dependent convection-diffusion equation. Our approach solves the Darcy system using FEM and transfers the resulting velocity field to a physics-informed DeepONet, which learns the mapping from source functions to solute concentration profiles. This modular strategy preserves FEM-level accuracy in the flow field while enabling fast inference for transport dynamics. To handle steep gradients induced by sharp sources, we introduce an adaptive sampling strategy for trunk collocation points. Numerical experiments demonstrate that our method is in good agreement with the reference solutions while offering orders of magnitude speedups over traditional solvers, making it suitable for practical applications in relevant scenarios. Implementation of our proposed method is available at https://github.com/erkara/fem-pi-deeponet.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:03:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19847v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19847v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 SoK: Large Language Model Copyright Auditing via Fingerprinting</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shuo Shao, Yiming Li, Yu He, Hongwei Yao, Wenyuan Yang, Dacheng Tao, Zhan Qin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The broad capabilities and substantial resources required to train Large Language Models (LLMs) make them valuable intellectual property, yet they remain vulnerable to copyright infringement, such as unauthorized use and model theft. LLM fingerprinting, a non-intrusive technique that extracts and compares the distinctive features from LLMs to identify infringements, offers a promising solution to copyright auditing. However, its reliability remains uncertain due to the prevalence of diverse model modifications and the lack of standardized evaluation. In this SoK, we present the first comprehensive study of LLM fingerprinting. We introduce a unified framework and formal taxonomy that categorizes existing methods into white-box and black-box approaches, providing a structured overview of the state of the art. We further propose LeaFBench, the first systematic benchmark for evaluating LLM fingerprinting under realistic deployment scenarios. Built upon mainstream foundation models and comprising 149 distinct model instances, LeaFBench integrates 13 representative post-development techniques, spanning both parameter-altering methods (e.g., fine-tuning, quantization) and parameter-independent mechanisms (e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the strengths and weaknesses of existing methods, thereby outlining future research directions and critical open problems in this emerging field. The code is available at https://github.com/shaoshuo-ss/LeaFBench.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T12:56:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19843v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19843v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hua Farn, Hsuan Su, Shachi H Kumar, Saurav Sahay, Shang-Tse Chen, Hung-yi Lee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Fine-tuning large language models (LLMs) for downstream tasks often leads to catastrophic forgetting, notably degrading the safety of originally aligned models. While some existing methods attempt to restore safety by incorporating additional safety data, the quality of such data typically falls short of that used in the original alignment process. Moreover, these high-quality safety datasets are generally inaccessible, making it difficult to fully recover the model's original safety. We ask: How can we preserve safety while improving downstream task performance without additional safety data? We show that simply merging the weights of pre- and post-fine-tuned models effectively mitigates safety degradation while enhancing performance. Experiments across different downstream tasks and models validate the method's practicality and effectiveness.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T01:13:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.19512v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.19512v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 Benchmarking Hindi LLMs: A New Suite of Datasets and a Comparative
  Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anusha Kamath, Kanishk Singla, Rakesh Paul, Raviraj Joshi, Utkarsh Vaidya, Sanjay Singh Chauhan, Niranjan Wartikar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Evaluating instruction-tuned Large Language Models (LLMs) in Hindi is challenging due to a lack of high-quality benchmarks, as direct translation of English datasets fails to capture crucial linguistic and cultural nuances. To address this, we introduce a suite of five Hindi LLM evaluation datasets: IFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, and BFCL-Hi. These were created using a methodology that combines from-scratch human annotation with a translate-and-verify process. We leverage this suite to conduct an extensive benchmarking of open-source LLMs supporting Hindi, providing a detailed comparative analysis of their current capabilities. Our curation process also serves as a replicable methodology for developing benchmarks in other low-resource languages.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T12:35:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19831v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19831v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize
  Memories via Reinforcement Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sikuan Yan, Xiufeng Yang, Zuchao Huang, Ercong Nie, Zifeng Ding, Zonggen Li, Xiaowen Ma, Hinrich Schütze, Volker Tresp, Yunpu Ma
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of NLP tasks, but they remain fundamentally stateless, constrained by limited context windows that hinder long-horizon reasoning. Recent efforts to address this limitation often augment LLMs with an external memory bank, yet most existing pipelines are static and heuristic-driven, lacking any learned mechanism for deciding what to store, update, or retrieve. We present Memory-R1, a reinforcement learning (RL) framework that equips LLMs with the ability to actively manage and utilize external memory through two specialized agents: a Memory Manager that learns to perform structured memory operations {ADD, UPDATE, DELETE, NOOP}, and an Answer Agent that selects the most relevant entries and reasons over them to produce an answer. Both agents are fine-tuned with outcome-driven RL (PPO and GRPO), enabling adaptive memory management and use with minimal supervision. With as few as 152 question-answer pairs and a corresponding temporal memory bank for training, Memory-R1 outperforms the most competitive existing baseline and demonstrates strong generalization across diverse question types and LLM backbones. Beyond presenting an effective approach, this work provides insights into how RL can unlock more agentic, memory-aware behaviors in LLMs, pointing toward richer, more persistent reasoning systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T12:26:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19828v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19828v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 X-Prompt: Towards Universal In-Context Image Generation in
  Auto-Regressive Vision Language Foundation Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zeyi Sun, Ziyang Chu, Pan Zhang, Tong Wu, Xiaoyi Dong, Yuhang Zang, Yuanjun Xiong, Dahua Lin, Jiaqi Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In-context generation is a key component of large language models' (LLMs) open-task generalization capability. By leveraging a few examples as context, LLMs can perform both in-domain and out-of-domain tasks. Recent advancements in auto-regressive vision-language models (VLMs) built upon LLMs have showcased impressive performance in text-to-image generation. However, the potential of in-context learning for general image generation tasks remains largely unexplored. To address this, we introduce X-Prompt, a purely auto-regressive large-vision language model designed to deliver competitive performance across a wide range of both seen and unseen image generation tasks, all within a unified in-context learning framework. X-Prompt incorporates a specialized design that efficiently compresses valuable features from in-context examples, supporting longer in-context token sequences and improving its ability to generalize to unseen tasks. A unified training task for both text and image prediction enables X-Prompt to handle general image generation with enhanced task awareness from in-context examples. Extensive experiments validate the model's performance across diverse seen image generation tasks and its capacity to generalize to previously unseen tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T12:26:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.LG</span><span>cs.MM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.01824v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.01824v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhiyuan Ning, Tianle Gu, Jiaxin Song, Shixin Hong, Lingyu Li, Huacan Liu, Jie Li, Yixu Wang, Meng Lingyu, Yan Teng, Yingchun Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The widespread adoption and increasing prominence of large language models (LLMs) in global technologies necessitate a rigorous focus on ensuring their safety across a diverse range of linguistic and cultural contexts. The lack of a comprehensive evaluation and diverse data in existing multilingual safety evaluations for LLMs limits their effectiveness, hindering the development of robust multilingual safety alignment. To address this critical gap, we introduce LinguaSafe, a comprehensive multilingual safety benchmark crafted with meticulous attention to linguistic authenticity. The LinguaSafe dataset comprises 45k entries in 12 languages, ranging from Hungarian to Malay. Curated using a combination of translated, transcreated, and natively-sourced data, our dataset addresses the critical need for multilingual safety evaluations of LLMs, filling the void in the safety evaluation of LLMs across diverse under-represented languages from Hungarian to Malay. LinguaSafe presents a multidimensional and fine-grained evaluation framework, with direct and indirect safety assessments, including further evaluations for oversensitivity. The results of safety and helpfulness evaluations vary significantly across different domains and different languages, even in languages with similar resource levels. Our benchmark provides a comprehensive suite of metrics for in-depth safety evaluation, underscoring the critical importance of thoroughly assessing multilingual safety in LLMs to achieve more balanced safety alignment. Our dataset and code are released to the public to facilitate further research in the field of multilingual LLM safety.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T12:21:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.12733v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.12733v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 From Research to Reality: Feasibility of Gradient Inversion Attacks in
  Federated Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Viktor Valadi, Mattias Åkesson, Johan Östman, Salman Toor, Andreas Hellander
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Gradient inversion attacks have garnered attention for their ability to compromise privacy in federated learning. However, many studies consider attacks with the model in inference mode, where training-time behaviors like dropout are disabled and batch normalization relies on fixed statistics. In this work, we systematically analyze how architecture and training behavior affect vulnerability, including the first in-depth study of inference-mode clients, which we show dramatically simplifies inversion. To assess attack feasibility under more realistic conditions, we turn to clients operating in standard training mode. In this setting, we find that successful attacks are only possible when several architectural conditions are met simultaneously: models must be shallow and wide, use skip connections, and, critically, employ pre-activation normalization. We introduce two novel attacks against models in training-mode with varying attacker knowledge, achieving state-of-the-art performance under realistic training conditions. We extend these efforts by presenting the first attack on a production-grade object-detection model. Here, to enable any visibly identifiable leakage, we revert to the lenient inference mode setting and make multiple architectural modifications to increase model vulnerability, with the extent of required changes highlighting the strong inherent robustness of such architectures. We conclude this work by offering the first comprehensive mapping of settings, clarifying which combinations of architectural choices and operational modes meaningfully impact privacy. Our analysis provides actionable insight into when models are likely vulnerable, when they appear robust, and where subtle leakage may persist. Together, these findings reframe how gradient inversion risk should be assessed in future research and deployment scenarios.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T12:07:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19819v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19819v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 T2R-bench: A Benchmark for Generating Article-Level Reports from Real
  World Industrial Tables</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jie Zhang, Changzai Pan, Kaiwen Wei, Sishi Xiong, Yu Zhao, Xiangyu Li, Jiaxin Peng, Xiaoyan Gu, Jian Yang, Wenhan Chang, Zhenhe Wu, Jiang Zhong, Shuangyong Song, Yongxiang Li, Xuelong Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Extensive research has been conducted to explore the capabilities of large language models (LLMs) in table reasoning. However, the essential task of transforming tables information into reports remains a significant challenge for industrial applications. This task is plagued by two critical issues: 1) the complexity and diversity of tables lead to suboptimal reasoning outcomes; and 2) existing table benchmarks lack the capacity to adequately assess the practical application of this task. To fill this gap, we propose the table-to-report task and construct a bilingual benchmark named T2R-bench, where the key information flow from the tables to the reports for this task. The benchmark comprises 457 industrial tables, all derived from real-world scenarios and encompassing 19 industry domains as well as 4 types of industrial tables. Furthermore, we propose an evaluation criteria to fairly measure the quality of report generation. The experiments on 25 widely-used LLMs reveal that even state-of-the-art models like Deepseek-R1 only achieves performance with 62.71 overall score, indicating that LLMs still have room for improvement on T2R-bench. Source code and data will be available after acceptance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T11:55:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19813v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19813v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 Bootstrapping Learned Cost Models with Synthetic SQL Queries</h2>
                <div class="authors">
                    <strong>Authors:</strong> Michael Nidd, Christoph Miksovic, Thomas Gschwind, Francesco Fusco, Andrea Giovannini, Ioana Giurgiu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Having access to realistic workloads for a given database instance is extremely important to enable stress and vulnerability testing, as well as to optimize for cost and performance. Recent advances in learned cost models have shown that when enough diverse SQL queries are available, one can effectively and efficiently predict the cost of running a given query against a specific database engine. In this paper, we describe our experience in exploiting modern synthetic data generation techniques, inspired by the generative AI and LLM community, to create high-quality datasets enabling the effective training of such learned cost models. Initial results show that we can improve a learned cost model's predictive accuracy by training it with 45% fewer queries than when using competitive generation approaches.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T11:50:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19807v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19807v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 A bag of tricks for real-time Mitotic Figure detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Christian Marzahl, Brian Napora
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Mitotic figure (MF) detection in histopathology images is challenging due to large variations in slide scanners, staining protocols, tissue types, and the presence of artifacts. This paper presents a collection of training techniques - a bag of tricks - that enable robust, real-time MF detection across diverse domains. We build on the efficient RTMDet single stage object detector to achieve high inference speed suitable for clinical deployment. Our method addresses scanner variability and tumor heterogeneity via extensive multi-domain training data, balanced sampling, and careful augmentation. Additionally, we employ targeted, hard negative mining on necrotic and debris tissue to reduce false positives. In a grouped 5-fold cross-validation across multiple MF datasets, our model achieves an F1 score between 0.78 and 0.84. On the preliminary test set of the MItosis DOmain Generalization (MIDOG) 2025 challenge, our single-stage RTMDet-S based approach reaches an F1 of 0.81, outperforming larger models and demonstrating adaptability to new, unfamiliar domains. The proposed solution offers a practical trade-off between accuracy and speed, making it attractive for real-world clinical adoption.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T11:45:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19804v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19804v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 Robust Single-Stage Fully Sparse 3D Object Detection via Detachable
  Latent Diffusion</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wentao Qu, Guofeng Mei, Jing Wang, Yujiao Wu, Xiaoshui Huang, Liang Xiao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Denoising Diffusion Probabilistic Models (DDPMs) have shown success in robust 3D object detection tasks. Existing methods often rely on the score matching from 3D boxes or pre-trained diffusion priors. However, they typically require multi-step iterations in inference, which limits efficiency. To address this, we propose a Robust single-stage fully Sparse 3D object Detection Network with a Detachable Latent Framework (DLF) of DDPMs, named RSDNet. Specifically, RSDNet learns the denoising process in latent feature spaces through lightweight denoising networks like multi-level denoising autoencoders (DAEs). This enables RSDNet to effectively understand scene distributions under multi-level perturbations, achieving robust and reliable detection. Meanwhile, we reformulate the noising and denoising mechanisms of DDPMs, enabling DLF to construct multi-type and multi-level noise samples and targets, enhancing RSDNet robustness to multiple perturbations. Furthermore, a semantic-geometric conditional guidance is introduced to perceive the object boundaries and shapes, alleviating the center feature missing problem in sparse representations, enabling RSDNet to perform in a fully sparse detection pipeline. Moreover, the detachable denoising network design of DLF enables RSDNet to perform single-step detection in inference, further enhancing detection efficiency. Extensive experiments on public benchmarks show that RSDNet can outperform existing methods, achieving state-of-the-art detection.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T11:39:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.03252v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.03252v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 Inferring activity from the flow field around active colloidal particles
  using deep learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aditya Mohapatra, Aditya Kumar, Mayurakshi Deb, Siddharth Dhomkar, Rajesh Singh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Active colloidal particles create flow around them due to non-equilibrium process on their surfaces. In this paper, we infer the activity of such colloidal particles from the flow field created by them via deep learning. We first explain our method for one active particle, inferring the $2s$ mode (or the stresslet) and the $3t$ mode (or the source dipole) from the flow field data, along with the position and orientation of the particle. We then apply the method to a system of many active particles. We find excellent agreements between the predictions and the true values of activity. Our method presents a principled way to predict arbitrary activity from the flow field created by active particles.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T11:31:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.soft</span><span>physics.flu-dyn</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1017/jfm.2025.10502' target='_blank'>doi</a><a href='http://arxiv.org/abs/2505.10270v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.10270v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 Agent-as-Judge for Factual Summarization of Long Narratives</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yeonseok Jeong, Minsoo Kim, Seung-won Hwang, Byung-Hak Kim
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have demonstrated near-human performance in summarization tasks based on traditional metrics such as ROUGE and BERTScore. However, these metrics do not adequately capture critical aspects of summarization quality, such as factual accuracy, particularly for long narratives (>100K tokens). Recent advances, such as LLM-as-a-Judge, address the limitations of metrics based on lexical similarity but still exhibit factual inconsistencies, especially in understanding character relationships and states. In this work, we introduce NarrativeFactScore, a novel "Agent-as-a-Judge" framework for evaluating and refining summaries. By leveraging a Character Knowledge Graph (CKG) extracted from input and generated summaries, NarrativeFactScore assesses the factual consistency and provides actionable guidance for refinement, such as identifying missing or erroneous facts. We demonstrate the effectiveness of NarrativeFactScore through a detailed workflow illustration and extensive validation on widely adopted benchmarks, achieving superior performance compared to competitive methods. Our results highlight the potential of agent-driven evaluation systems to improve the factual reliability of LLM-generated summaries.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T11:18:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.09993v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.09993v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 Not Every Gift Comes in Gold Paper or with a Red Ribbon: Exploring Color
  Perception in Text-to-Image Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shay Shomer Chai, Wenxuan Peng, Bharath Hariharan, Hadar Averbuch-Elor
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Text-to-image generation has recently seen remarkable success, granting users with the ability to create high-quality images through the use of text. However, contemporary methods face challenges in capturing the precise semantics conveyed by complex multi-object prompts. Consequently, many works have sought to mitigate such semantic misalignments, typically via inference-time schemes that modify the attention layers of the denoising networks. However, prior work has mostly utilized coarse metrics, such as the cosine similarity between text and image CLIP embeddings, or human evaluations, which are challenging to conduct on a larger-scale. In this work, we perform a case study on colors -- a fundamental attribute commonly associated with objects in text prompts, which offer a rich test bed for rigorous evaluation. Our analysis reveals that pretrained models struggle to generate images that faithfully reflect multiple color attributes-far more so than with single-color prompts-and that neither inference-time techniques nor existing editing methods reliably resolve these semantic misalignments. Accordingly, we introduce a dedicated image editing technique, mitigating the issue of multi-object semantic alignment for prompts containing multiple colors. We demonstrate that our approach significantly boosts performance over a wide range of metrics, considering images generated by various text-to-image diffusion-based techniques.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T11:16:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19791v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19791v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 StableIntrinsic: Detail-preserving One-step Diffusion Model for
  Multi-view Material Estimation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiuchao Wu, Pengfei Zhu, Jiangjing Lyu, Xinguo Liu, Jie Guo, Yanwen Guo, Weiwei Xu, Chengfei Lyu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recovering material information from images has been extensively studied in computer graphics and vision. Recent works in material estimation leverage diffusion model showing promising results. However, these diffusion-based methods adopt a multi-step denoising strategy, which is time-consuming for each estimation. Such stochastic inference also conflicts with the deterministic material estimation task, leading to a high variance estimated results. In this paper, we introduce StableIntrinsic, a one-step diffusion model for multi-view material estimation that can produce high-quality material parameters with low variance. To address the overly-smoothing problem in one-step diffusion, StableIntrinsic applies losses in pixel space, with each loss designed based on the properties of the material. Additionally, StableIntrinsic introduces a Detail Injection Network (DIN) to eliminate the detail loss caused by VAE encoding, while further enhancing the sharpness of material prediction results. The experimental results indicate that our method surpasses the current state-of-the-art techniques by achieving a $9.9\%$ improvement in the Peak Signal-to-Noise Ratio (PSNR) of albedo, and by reducing the Mean Square Error (MSE) for metallic and roughness by $44.4\%$ and $60.0\%$, respectively.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T11:15:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19789v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19789v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 Demonstrating specification gaming in reasoning models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alexander Bondarenko, Denis Volk, Dmitrii Volkov, Jeffrey Ladish
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We demonstrate LLM agent specification gaming by instructing models to win against a chess engine. We find reasoning models like OpenAI o3 and DeepSeek R1 will often hack the benchmark by default, while language models like GPT-4o and Claude 3.5 Sonnet need to be told that normal play won't work to hack.   We improve upon prior work like (Hubinger et al., 2024; Meinke et al., 2024; Weij et al., 2024) by using realistic task prompts and avoiding excess nudging. Our results suggest reasoning models may resort to hacking to solve difficult problems, as observed in OpenAI (2024)'s o1 Docker escape during cyber capabilities testing.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T11:15:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.13295v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.13295v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 Context-Aware Risk Estimation in Home Environments: A Probabilistic
  Framework for Service Robots</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sena Ishii, Akash Chikhalikar, Ankit A. Ravankar, Jose Victorio Salazar Luces, Yasuhisa Hirata
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present a novel framework for estimating accident-prone regions in everyday indoor scenes, aimed at improving real-time risk awareness in service robots operating in human-centric environments. As robots become integrated into daily life, particularly in homes, the ability to anticipate and respond to environmental hazards is crucial for ensuring user safety, trust, and effective human-robot interaction. Our approach models object-level risk and context through a semantic graph-based propagation algorithm. Each object is represented as a node with an associated risk score, and risk propagates asymmetrically from high-risk to low-risk objects based on spatial proximity and accident relationship. This enables the robot to infer potential hazards even when they are not explicitly visible or labeled. Designed for interpretability and lightweight onboard deployment, our method is validated on a dataset with human-annotated risk regions, achieving a binary risk detection accuracy of 75%. The system demonstrates strong alignment with human perception, particularly in scenes involving sharp or unstable objects. These results underline the potential of context-aware risk reasoning to enhance robotic scene understanding and proactive safety behaviors in shared human-robot spaces. This framework could serve as a foundation for future systems that make context-driven safety decisions, provide real-time alerts, or autonomously assist users in avoiding or mitigating hazards within home environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T11:14:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19788v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19788v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large
  Language Models via a Multi-Paradigm Perspective</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yiyao Yu, Yuxiang Zhang, Dongdong Zhang, Xiao Liang, Hengyuan Zhang, Xingxing Zhang, Mahmoud Khademi, Hany Awadalla, Junjie Wang, Yujiu Yang, Furu Wei
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have made notable progress in mathematical reasoning, yet often rely on single-paradigm reasoning, limiting their effectiveness across diverse tasks. We introduce Chain-of-Reasoning (CoR), a novel unified framework integrating multiple reasoning paradigms--Natural Language Reasoning (NLR), Algorithmic Reasoning (AR), and Symbolic Reasoning (SR)--to enable synergistic collaboration. CoR generates multiple potential answers via different reasoning paradigms and synthesizes them into a coherent final solution. We propose a Progressive Paradigm Training (PPT) strategy for models to progressively master these paradigms, leading to CoR-Math-7B. Experimental results demonstrate that CoR-Math-7B significantly outperforms current SOTA models, achieving up to a 41.0% absolute improvement over GPT-4o in theorem proving and a 15.0% improvement over RL-based methods on the MATH benchmark in arithmetic tasks. These results show the enhanced mathematical comprehension ability of our model, enabling zero-shot generalization across tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T10:56:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.11110v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.11110v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 Utility-Focused LLM Annotation for Retrieval and Retrieval-Augmented
  Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hengran Zhang, Minghao Tang, Keping Bi, Jiafeng Guo, Shihao Liu, Daiting Shi, Dawei Yin, Xueqi Cheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper explores the use of large language models (LLMs) for annotating document utility in training retrieval and retrieval-augmented generation (RAG) systems, aiming to reduce dependence on costly human annotations. We address the gap between retrieval relevance and generative utility by employing LLMs to annotate document utility. To effectively utilize multiple positive samples per query, we introduce a novel loss that maximizes their summed marginal likelihood. Using the Qwen-2.5-32B model, we annotate utility on the MS MARCO dataset and conduct retrieval experiments on MS MARCO and BEIR, as well as RAG experiments on MS MARCO QA, NQ, and HotpotQA. Our results show that LLM-generated annotations enhance out-of-domain retrieval performance and improve RAG outcomes compared to models trained solely on human annotations or downstream QA metrics. Furthermore, combining LLM annotations with just 20% of human labels achieves performance comparable to using full human annotations. Our study offers a comprehensive approach to utilizing LLM annotations for initializing QA systems on new corpora.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T10:51:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.05220v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.05220v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 Principled Personas: Defining and Measuring the Intended Effects of
  Persona Prompting on Task Performance</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pedro Henrique Luz de Araujo, Paul Röttger, Dirk Hovy, Benjamin Roth
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Expert persona prompting -- assigning roles such as expert in math to language models -- is widely used for task improvement. However, prior work shows mixed results on its effectiveness, and does not consider when and why personas should improve performance. We analyze the literature on persona prompting for task improvement and distill three desiderata: 1) performance advantage of expert personas, 2) robustness to irrelevant persona attributes, and 3) fidelity to persona attributes. We then evaluate 9 state-of-the-art LLMs across 27 tasks with respect to these desiderata. We find that expert personas usually lead to positive or non-significant performance changes. Surprisingly, models are highly sensitive to irrelevant persona details, with performance drops of almost 30 percentage points. In terms of fidelity, we find that while higher education, specialization, and domain-relatedness can boost performance, their effects are often inconsistent or negligible across tasks. We propose mitigation strategies to improve robustness -- but find they only work for the largest, most capable models. Our findings underscore the need for more careful persona design and for evaluation schemes that reflect the intended effects of persona usage.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T10:40:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19764v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19764v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 DiffArtist: Towards Structure and Appearance Controllable Image
  Stylization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruixiang Jiang, Changwen Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Artistic styles are defined by both their structural and appearance elements. Existing neural stylization techniques primarily focus on transferring appearance-level features such as color and texture, often neglecting the equally crucial aspect of structural stylization. To address this gap, we introduce \textbf{DiffArtist}, the first 2D stylization method to offer fine-grained, simultaneous control over both structure and appearance style strength. This dual controllability is achieved by representing structure and appearance generation as separate diffusion processes, necessitating no further tuning or additional adapters. To properly evaluate this new capability of dual stylization, we further propose a Multimodal LLM-based stylization evaluator that aligns significantly better with human preferences than existing metrics. Extensive analysis shows that DiffArtist achieves superior style fidelity and dual-controllability compared to state-of-the-art methods. Its text-driven, training-free design and unprecedented dual controllability make it a powerful and interactive tool for various creative applications. Project homepage: https://diffusionartist.github.io.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T10:30:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.GR</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3746027.3755010' target='_blank'>doi</a><a href='http://arxiv.org/abs/2407.15842v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.15842v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 Hierarchical Bayesian model updating using Dirichlet process mixtures
  for structural damage localization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Taro Yaoyama, Tatsuya Itoi, Jun Iyama
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Bayesian model updating provides a rigorous probabilistic framework for calibrating finite element (FE) models with quantified uncertainties, thereby enhancing damage assessment, response prediction, and performance evaluation of engineering structures. Recent advances in hierarchical Bayesian model updating (HBMU) enable robust parameter estimation under ill-posed/ill-conditioned settings and in the presence of inherent variability in structural parameters due to environmental and operational conditions. However, most HBMU approaches overlook multimodality in structural parameters that often arises when a structure experiences multiple damage states over its service life. This paper presents an HBMU framework that employs a Dirichlet process (DP) mixture prior on structural parameters (DP-HBMU). DP mixtures are nonparametric Bayesian models that perform clustering without pre-specifying the number of clusters, incorporating damage state classification into FE model updating. We formulate the DP-HBMU framework and devise a Metropolis-within-Gibbs sampler that draws samples from the posterior by embedding Metropolis updates for intractable conditionals due to the FE simulator. The applicability of DP-HBMU to damage localization is demonstrated through both numerical and experimental examples. We consider moment-resisting frame structures with beam-end fractures and apply the method to datasets spanning multiple damage states, from an intact state to moderate or severe damage state. The clusters inferred by DP-HBMU align closely with the assumed or observed damage states. The posterior distributions of stiffness parameters agree with ground truth values or observed fractures while exhibiting substantially reduced uncertainty relative to a non-hierarchical baseline. These results demonstrate the effectiveness of the proposed method in damage localization.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T10:30:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.AP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19753v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19753v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 Fast 3D Diffusion for Scalable Granular Media Synthesis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Muhammad Moeeze Hassan, Régis Cottereau, Filippo Gatti, Patryk Dec
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Simulating granular media, using Discrete Element Method is a computationally intensive task. This is especially true during initialization phase, which dominates total simulation time because of large displacements involved and associated kinetic energy. We overcome this bottleneck with a novel generative pipeline based on 3D diffusion models that directly synthesizes arbitrarily large granular assemblies in their final and physically realistic configurations. The approach frames the problem as a 3D generative modeling task, consisting of a two-stage pipeline. First a diffusion model is trained to generate independent 3D voxel grids representing granular media. Second, a 3D inpainting model, adapted from 2D inpainting techniques using masked inputs, stitches these grids together seamlessly, enabling synthesis of large samples with physically realistic structure. The inpainting model explores several masking strategies for the inputs to the underlying UNets by training the network to infer missing portions of voxel grids from a concatenation of noised tensors, masks, and masked tensors as input channels. The model also adapts a 2D repainting technique of re-injecting noise scheduler output with ground truth to provide a strong guidance to the 3D model. This along with weighted losses ensures long-term coherence over generation of masked regions. Both models are trained on the same binarized 3D occupancy grids extracted from small-scale DEM simulations, achieving linear scaling of computational time with respect to sample size. Quantitatively, a 1.2 m long ballasted rail track synthesis equivalent to a 3-hour DEM simulation, was completed under 20 seconds. The generated voxel grids can also be post-processed to extract grain geometries for DEM-compatibility as well, enabling physically coherent, real-time, scalable granular media synthesis for industrial applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T10:27:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19752v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19752v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 Spotlight Attention: Towards Efficient LLM Generation via Non-linear
  Hashing-based KV Cache Retrieval</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wenhao Li, Yuxin Zhang, Gen Luo, Haiyuan Wan, Ziyang Gong, Fei Chao, Rongrong Ji
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reducing the key-value (KV) cache burden in Large Language Models (LLMs) significantly accelerates inference. Dynamically selecting critical KV caches during decoding helps maintain performance. Existing methods use random linear hashing to identify important tokens, but this approach is inefficient due to the orthogonal distribution of queries and keys within two narrow cones in LLMs. We introduce Spotlight Attention, a novel method that employs non-linear hashing functions to optimize the embedding distribution of queries and keys, enhancing coding efficiency and robustness. We also developed a lightweight, stable training framework using a Bradley-Terry ranking-based loss, enabling optimization of the non-linear hashing module on GPUs with 16GB memory in 8 hours. Experimental results show that Spotlight Attention drastically improves retrieval precision while shortening the length of the hash code at least 5$\times$ compared to traditional linear hashing. Finally, we exploit the computational advantages of bitwise operations by implementing specialized CUDA kernels, achieving hashing retrieval for 512K tokens in under 100$\mu$s on a single A100 GPU, with end-to-end throughput up to 3$\times$ higher than vanilla decoding.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T10:11:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19740v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19740v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 Network Modeling of Asynchronous Change-Points in Multivariate Time
  Series</h2>
                <div class="authors">
                    <strong>Authors:</strong> Carson McKee, Maria Kalli
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This article introduces a novel Bayesian method for asynchronous change-point detection in multivariate time series. This method allows for change-points to occur earlier in some (leading) series followed, after a short delay, by change-points in some other (lagging) series. Such dynamic dependence structure is common in fields such as seismology and neurology where a latent event such as an earthquake or seizure causes certain sensors to register change-points before others. We model these lead-lag dependencies via a latent directed graph and provide a hierarchical prior for learning the graph's structure and parameters. Posterior inference is made tractable by modifying particle MCMC methods designed for univariate change-point problems. We apply our method to both simulated and real datasets from the fields of seismology and neurology. In the simulated data, we find that our method outperforms competing methods in settings where the change-point locations are dependent across series. In the real data applications we show that our model can also uncover interpretable network structure.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T09:57:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>stat.AP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.15801v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.15801v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 Compositional Symbolic Execution for the Next 700 Memory Models
  (Extended Version)</h2>
                <div class="authors">
                    <strong>Authors:</strong> Andreas Lööw, Seung Hoon Park, Daniele Nantes-Sobrinho, Sacha-Élie Ayoun, Opale Sjöstedt, Philippa Gardner
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multiple successful compositional symbolic execution (CSE) tools and platforms exploit separation logic (SL) for compositional verification and/or incorrectness separation logic (ISL) for compositional bug-finding, including VeriFast, Viper, Gillian, CN, and Infer-Pulse. Previous work on the Gillian platform, the only CSE platform that is parametric on the memory model, meaning that it can be instantiated to different memory models, suggests that the ability to use custom memory models allows for more flexibility in supporting analysis of a wide range of programming languages, for implementing custom automation, and for improving performance. However, the literature lacks a satisfactory formal foundation for memory-model-parametric CSE platforms.   In this paper, inspired by Gillian, we provide a new formal foundation for memory-model-parametric CSE platforms. Our foundation advances the state of the art in four ways. First, we mechanise our foundation (in the interactive theorem prover Rocq). Second, we validate our foundation by instantiating it to a broad range of memory models, including models for C and CHERI. Third, whereas previous memory-model-parametric work has only covered SL analyses, we cover both SL and ISL analyses. Fourth, our foundation is based on standard definitions of SL and ISL (including definitions of function specification validity, to ensure sound interoperation with other tools and platforms also based on standard definitions).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T09:41:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PL</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3763151' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.15576v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15576v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 NLKI: A lightweight Natural Language Knowledge Integration Framework for
  Improving Small VLMs in Commonsense VQA Tasks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aritra Dutta, Swapnanil Mukherjee, Deepanway Ghosal, Somak Aditya
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Commonsense visual-question answering often hinges on knowledge that is missing from the image or the question. Small vision-language models (sVLMs) such as ViLT, VisualBERT and FLAVA therefore lag behind their larger generative counterparts. To study the effect of careful commonsense knowledge integration on sVLMs, we present an end-to-end framework (NLKI) that (i) retrieves natural language facts, (ii) prompts an LLM to craft natural language explanations, and (iii) feeds both signals to sVLMs respectively across two commonsense VQA datasets (CRIC, AOKVQA) and a visual-entailment dataset (e-SNLI-VE). Facts retrieved using a fine-tuned ColBERTv2 and an object information-enriched prompt yield explanations that largely cut down hallucinations, while lifting the end-to-end answer accuracy by up to 7% (across 3 datasets), making FLAVA and other models in NLKI match or exceed medium-sized VLMs such as Qwen-2 VL-2B and SmolVLM-2.5B. As these benchmarks contain 10-25% label noise, additional finetuning using noise-robust losses (such as symmetric cross entropy and generalised cross entropy) adds another 2.5% in CRIC, and 5.5% in AOKVQA. Our findings expose when LLM-based commonsense knowledge beats retrieval from commonsense knowledge bases, how noise-aware training stabilises small models in the context of external knowledge augmentation, and why parameter-efficient commonsense reasoning is now within reach for 250M models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T12:05:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19724v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19724v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 Continuously Steering LLMs Sensitivity to Contextual Knowledge with
  Proxy Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yilin Wang, Heng Wang, Yuyang Bai, Minnan Luo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In Large Language Models (LLMs) generation, there exist knowledge conflicts and scenarios where parametric knowledge contradicts knowledge provided in the context. Previous works studied tuning, decoding algorithms, or locating and editing context-aware neurons to adapt LLMs to be faithful to new contextual knowledge. However, they are usually inefficient or ineffective for large models, not workable for black-box models, or unable to continuously adjust LLMs' sensitivity to the knowledge provided in the context. To mitigate these problems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a simple framework that can steer LLMs' sensitivity to contextual knowledge continuously at a lightweight cost. Specifically, we tune two small LMs (i.e. proxy models) and use the difference in their output distributions to shift the original distribution of an LLM without modifying the LLM weights. In the evaluation process, we not only design synthetic data and fine-grained metrics to measure models' sensitivity to contextual knowledge but also use a real conflict dataset to validate CSKS's practical efficacy. Extensive experiments demonstrate that our framework achieves continuous and precise control over LLMs' sensitivity to contextual knowledge, enabling both increased sensitivity and reduced sensitivity, thereby allowing LLMs to prioritize either contextual or parametric knowledge as needed flexibly. Our data and code are available at https://github.com/OliveJuiceLin/CSKS.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T10:00:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19720v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19720v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 Constraining the Cosmological Constant from Stellar Orbits Around Sgr A*
  Using Physics-Informed Neural Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shinsei Eyama, Youhei Masada
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present a novel analytical framework employing Physics-Informed Neural Networks (PINNs) to constrain the cosmological constant $\Lambda$ through the analysis of stellar orbits around the supermassive black hole (SMBH) Sgr A* at the Galactic center. Focusing on the well-observed S2 star, we use an inverse PINN (iPINN) architecture to infer orbital elements and estimate the total precession angle from astrometric data. By isolating the contribution from $\Lambda$, which is defined as the difference between the total precession and the Schwarzschild precession, we derive a stringent upper bound of $\Lambda \leq 5.67 \times 10^{-40}, \mathrm{m}^{-2}$, which is approximately two orders of magnitude tighter than previous estimates obtained using similar data-driven methods. Extension of our analysis to two additional long-period S-stars, S1 and S9, reveals that while the cosmological precession becomes relatively more prominent in such systems, limited orbital coverage introduces significant uncertainties in parameter estimation. Among the cases examined, the constraint derived from S2 remains the most robust. Our results highlight the potential of PINN-based approaches for extracting physical insights from sparse or noisy astronomical data. Future applications to next-generation observational data and further methodological improvements in machine learning are expected to refine the cosmological constraints and enable broader tests of gravitational theories.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T09:29:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.CO</span><span>astro-ph.GA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19719v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19719v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 PhoniTale: Phonologically Grounded Mnemonic Generation for Typologically
  Distant Language Pairs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sana Kang, Myeongseok Gwon, Su Young Kwon, Jaewook Lee, Andrew Lan, Bhiksha Raj, Rita Singh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vocabulary acquisition poses a significant challenge for second-language (L2) learners, especially when learning typologically distant languages such as English and Korean, where phonological and structural mismatches complicate vocabulary learning. Recently, large language models (LLMs) have been used to generate keyword mnemonics by leveraging similar keywords from a learner's first language (L1) to aid in acquiring L2 vocabulary. However, most of this research has focused on native English speakers learning other languages, rather than the reverse. In this paper, we present PhoniTale, a novel cross-lingual mnemonic generation system that retrieves L1 keyword sequence based on phonological similarity and uses LLMs to generate mnemonics. We evaluate PhoniTale using both automated metrics and human evaluations, comparing its output to mnemonics created by humans and by previous automated approaches. To assess practical effectiveness, we also conduct a short-term recall test measuring mnemonic helpfulness. Our findings show that PhoniTale performs comparably to human-authored mnemonics. We also highlight key areas for future improvement in mnemonic quality and methodology.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T09:29:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05444v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05444v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 Edge Agentic AI Framework for Autonomous Network Optimisation in O-RAN</h2>
                <div class="authors">
                    <strong>Authors:</strong> Abdelaziz Salama, Zeinab Nezami, Mohammed M. H. Qazzaz, Maryam Hafeez, Syed Ali Raza Zaidi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The deployment of AI agents within legacy Radio Access Network (RAN) infrastructure poses significant safety and reliability challenges for future 6G networks. This paper presents a novel Edge AI framework for autonomous network optimisation in Open RAN environments, addressing these challenges through three core innovations: (1) a persona-based multi-tools architecture enabling distributed, context-aware decision-making; (2) proactive anomaly detection agent powered by traffic predictive tool; and (3) a safety, aligned reward mechanism that balances performance with operational stability. Integrated into the RAN Intelligent Controller (RIC), our framework leverages multimodal data fusion, including network KPIs, a traffic prediction model, and external information sources, to anticipate and respond to dynamic network conditions. Extensive evaluation using realistic 5G scenarios demonstrates that the edge framework achieves zero network outages under high-stress conditions, compared to 8.4% for traditional fixed-power networks and 3.3% for large language model (LLM) agent-based approaches, while maintaining near real-time responsiveness and consistent QoS. These results establish that, when equipped with the right tools and contextual awareness, AI agents can be safely and effectively deployed in critical network infrastructure, laying the framework for intelligent and autonomous 5G and beyond network operations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T09:28:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.21696v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.21696v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 OPAL: Visibility-aware LiDAR-to-OpenStreetMap Place Recognition via
  Adaptive Radial Fusion</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shuhao Kang, Martin Y. Liao, Yan Xia, Olaf Wysocki, Boris Jutzi, Daniel Cremers
                </div>
                <div class="summary">
                    <strong>Summary:</strong> LiDAR place recognition is a critical capability for autonomous navigation and cross-modal localization in large-scale outdoor environments. Existing approaches predominantly depend on pre-built 3D dense maps or aerial imagery, which impose significant storage overhead and lack real-time adaptability. In this paper, we propose OPAL, a novel framework for LiDAR place recognition that leverages OpenStreetMap (OSM) as a lightweight and up-to-date prior. Our key innovation lies in bridging the domain disparity between sparse LiDAR scans and structured OSM data through two carefully designed components. First, a cross-modal visibility mask that identifies observable regions from both modalities to guide feature alignment. Second, an adaptive radial fusion module that dynamically consolidates radial features into discriminative global descriptors. Extensive experiments on KITTI and KITTI-360 datasets demonstrate OPAL's superiority, achieving 15.98% higher recall at 1m threshold for top-1 retrieved matches, along with 12x faster inference speed compared to the state-of-the-art approach. Code and data are publicly available at: https://github.com/kang-1-2-3/OPAL.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T09:23:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.19258v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.19258v3' target='_blank'>pdf</a>
                </div>
            </div>
            <h1>Keyword: LLM Deployment</h1>

            <div class="paper">
                <h2>#1 (Un)solvable Matrix Models for BPS Correlators</h2>
                <div class="authors">
                    <strong>Authors:</strong> Prokopii Anempodistov, Adolfo Holguin, Vladimir Kazakov, Harish Murali
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We propose and study a family of complex matrix models computing the protected two- and three-point correlation functions in $\mathcal{N}=4$ SYM. Our description allows us to directly relate the eigenvalue density of the matrix model for ``Huge" operators with $ \Delta \sim N^2$ to the shape of droplets in the dual Lin-Lunin-Maldacena (LLM) geometry. We demonstrate how to determine the eigenvalue distribution for various choices of operators such as those of exponential, character, or coherent state type, which then allows us to efficiently compute one-point functions of light chiral primaries in generic LLM backgrounds. In particular, we successfully match the results for light probes with the supergravity calculations of Skenderis and Taylor. We provide a large $N$ formalism for one-point functions of ``Giant" probes, such as operators dual to giant graviton branes in LLM backgrounds, and explicitly apply it for particular backgrounds. We also explicitly compute the correlator of three huge half-BPS operators of exponential type and stacks of determinant operators by reducing them to the known matrix model problems such as the Potts or $O(n)$ model on random planar graphs. Finally, we point out a curious relation between the correlators of $\frac{1}{4}$-BPS and $\frac{1}{8}$-BPS coherent state operators and the Eguchi-Kawai reduction of the Principal Chiral Model in $2D$ and $3D$ correspondingly.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T17:58:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>hep-th</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20094v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20094v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical
  Reinforcement Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Utsav Singh, Pramit Bhattacharyya, Vinay P. Namboodiri
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have shown remarkable abilities in logical reasoning, in-context learning, and code generation. However, translating natural language instructions into effective robotic control policies remains a significant challenge, especially for tasks requiring long-horizon planning and operating under sparse reward conditions. Hierarchical Reinforcement Learning (HRL) provides a natural framework to address this challenge in robotics; however, it typically suffers from non-stationarity caused by the changing behavior of the lower-level policy during training, destabilizing higher-level policy learning. We introduce LGR2, a novel HRL framework that leverages LLMs to generate language-guided reward functions for the higher-level policy. By decoupling high-level reward generation from low-level policy changes, LGR2 fundamentally mitigates the non-stationarity problem in off-policy HRL, enabling stable and efficient learning. To further enhance sample efficiency in sparse environments, we integrate goal-conditioned hindsight experience relabeling. Extensive experiments across simulated and real-world robotic navigation and manipulation tasks demonstrate LGR2 outperforms both hierarchical and non-hierarchical baselines, achieving over 55% success rates on challenging tasks and robust transfer to real robots, without additional fine-tuning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T17:57:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.05881v6' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.05881v6' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 AudioStory: Generating Long-Form Narrative Audio with Large Language
  Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuxin Guo, Teng Wang, Yuying Ge, Shijie Ma, Yixiao Ge, Wei Zou, Ying Shan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in text-to-audio (TTA) generation excel at synthesizing short audio clips but struggle with long-form narrative audio, which requires temporal coherence and compositional reasoning. To address this gap, we propose AudioStory, a unified framework that integrates large language models (LLMs) with TTA systems to generate structured, long-form audio narratives. AudioStory possesses strong instruction-following reasoning generation capabilities. It employs LLMs to decompose complex narrative queries into temporally ordered sub-tasks with contextual cues, enabling coherent scene transitions and emotional tone consistency. AudioStory has two appealing features: (1) Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser collaboration into two specialized components, i.e., a bridging query for intra-event semantic alignment and a residual query for cross-event coherence preservation. (2) End-to-end training: By unifying instruction comprehension and audio generation within a single end-to-end framework, AudioStory eliminates the need for modular training pipelines while enhancing synergy between components. Furthermore, we establish a benchmark AudioStory-10K, encompassing diverse domains such as animated soundscapes and natural sound narratives. Extensive experiments show the superiority of AudioStory on both single-audio generation and narrative audio generation, surpassing prior TTA baselines in both instruction-following ability and audio fidelity. Our code is available at https://github.com/TencentARC/AudioStory
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T17:55:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.MM</span><span>cs.SD</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20088v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20088v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy
  Retriever Poisoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yanbo Dai, Zhenlan Ji, Zongjie Li, Kuan Li, Shuai Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Retrieval-Augmented Generation (RAG) has become a standard approach for improving the reliability of large language models (LLMs). Prior work demonstrates the vulnerability of RAG systems by misleading them into generating attacker-chosen outputs through poisoning the knowledge base. However, this paper uncovers that such attacks could be mitigated by the strong \textit{self-correction ability (SCA)} of modern LLMs, which can reject false context once properly configured. This SCA poses a significant challenge for attackers aiming to manipulate RAG systems.   In contrast to previous poisoning methods, which primarily target the knowledge base, we introduce \textsc{DisarmRAG}, a new poisoning paradigm that compromises the retriever itself to suppress the SCA and enforce attacker-chosen outputs. This compromisation enables the attacker to straightforwardly embed anti-SCA instructions into the context provided to the generator, thereby bypassing the SCA. To this end, we present a contrastive-learning-based model editing technique that performs localized and stealthy edits, ensuring the retriever returns a malicious instruction only for specific victim queries while preserving benign retrieval behavior. To further strengthen the attack, we design an iterative co-optimization framework that automatically discovers robust instructions capable of bypassing prompt-based defenses. We extensively evaluate DisarmRAG across six LLMs and three QA benchmarks. Our results show near-perfect retrieval of malicious instructions, which successfully suppress SCA and achieve attack success rates exceeding 90\% under diverse defensive prompts. Also, the edited retriever remains stealthy under several detection methods, highlighting the urgent need for retriever-centric defenses.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T17:49:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20083v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20083v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 Refining Czech GEC: Insights from a Multi-Experiment Approach</h2>
                <div class="authors">
                    <strong>Authors:</strong> Petr Pechman, Milan Straka, Jana Straková, Jakub Náplava
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present a grammar error correction (GEC) system that achieves state of the art for the Czech language. Our system is based on a neural network translation approach with the Transformer architecture, and its key feature is its real-time synthetic generation pipeline, which dynamically augments sentences with artificial errors by introducing both language-agnostic and Czech-specific errors. We conduct a comprehensive series of experiments, investigating the Czech GEC corpora as bases for synthetic error introduction, several error generation strategies, domain balancing, tokenization granularity, model size, and data scaling during fine-tuning. Additionally, we evaluate the performance of large language models (LLMs) on Czech GEC in both end-user and expert fine-tuning scenarios. Our best-performing model is superior both in performance and computational efficiency. The source code and the trained model links are available on https://github.com/ufal/tsd2025-gec.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T17:43:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1007/978-3-032-02551-7_7' target='_blank'>doi</a><a href='http://arxiv.org/abs/2506.22402v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.22402v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with
  Cognitive-Inspired Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chengzu Li, Wenshan Wu, Huanyu Zhang, Qingtao Li, Zeyu Gao, Yan Xia, José Hernández-Orallo, Ivan Vulić, Furu Wei
                </div>
                <div class="summary">
                    <strong>Summary:</strong> For human cognitive process, spatial reasoning and perception are closely entangled, yet the nature of this interplay remains underexplored in the evaluation of multimodal large language models (MLLMs). While recent MLLM advancements show impressive performance on reasoning, their capacity for human-like spatial cognition remains an open question. In this work, we introduce a systematic evaluation framework to assess the spatial reasoning abilities of state-of-the-art MLLMs relative to human performance. Central to our work is 11Plus-Bench, a high-quality benchmark derived from realistic standardized spatial aptitude tests. 11Plus-Bench also features fine-grained expert annotations of both perceptual complexity and reasoning process, enabling detailed instance-level analysis of model behavior. Through extensive experiments across 14 MLLMs and human evaluation, we find that current MLLMs exhibit early signs of spatial cognition. Despite a large performance gap compared to humans, MLLMs' cognitive profiles resemble those of humans in that cognitive effort correlates strongly with reasoning-related complexity. However, instance-level performance in MLLMs remains largely random, whereas human correctness is highly predictable and shaped by abstract pattern complexity. These findings highlight both emerging capabilities and limitations in current MLLMs' spatial reasoning capabilities and provide actionable insights for advancing model design.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T17:22:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20068v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20068v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 A First Look at Inter-Cell Interference in the Wild</h2>
                <div class="authors">
                    <strong>Authors:</strong> Daqian Ding, Yibo Pi, Cailian Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In cellular networks, inter-cell interference management has been studied for decades, yet its real-world effectiveness remains under-explored. To bridge this gap, we conduct a first measurement study of inter-cell interference for operational 4G/5G networks. Our findings reveal the prevalence of inter-cell interference and a surprising absence of interference coordination among operational base stations. As a result, user equipments experience unnecessary interference, which causes significant signal quality degradation, especially under frequency-selective channel fading. We examine the inter-cell interference issues from four major perspectives: network deployment, channel assignment, time-frequency resource allocation, and network configuration. In none of these dimensions is inter-cell interference effectively managed. Notably, even when spectrum resources are underutilized and simple strategies could effectively mitigate inter-cell interference, base stations consistently prioritize using the same set of time-frequency resources, causing interference across cells. Our measurements reveal substantial opportunities for improving signal quality by inter-cell interference management.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T17:14:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20060v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20060v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 Scaling Decentralized Learning with FLock</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zehua Cheng, Rui Sun, Jiahao Sun, Yike Guo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Fine-tuning the large language models (LLMs) are prevented by the deficiency of centralized control and the massive computing and communication overhead on the decentralized schemes. While the typical standard federated learning (FL) supports data privacy, the central server requirement creates a single point of attack and vulnerability to poisoning attacks. Generalizing the result in this direction to 70B-parameter models in the heterogeneous, trustless environments has turned out to be a huge, yet unbroken bottleneck. This paper introduces FLock, a decentralized framework for secure and efficient collaborative LLM fine-tuning. Integrating a blockchain-based trust layer with economic incentives, FLock replaces the central aggregator with a secure, auditable protocol for cooperation among untrusted parties. We present the first empirical validation of fine-tuning a 70B LLM in a secure, multi-domain, decentralized setting. Our experiments show the FLock framework defends against backdoor poisoning attacks that compromise standard FL optimizers and fosters synergistic knowledge transfer. The resulting models show a >68% reduction in adversarial attack success rates. The global model also demonstrates superior cross-domain generalization, outperforming models trained in isolation on their own specialized data.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T16:50:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.15349v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.15349v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to
  Enhance LLM Safety Guardrail to Potential Attacks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sheng Liu, Qiang Sheng, Danding Wang, Yang Li, Guang Yang, Juan Cao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Despite advances in improving large language model (LLM) to refuse to answer malicious instructions, widely used LLMs remain vulnerable to jailbreak attacks where attackers generate instructions with distributions differing from safety alignment corpora. New attacks expose LLMs' inability to recognize unseen malicious instructions, highlighting a critical distributional mismatch between training data and real-world attacks that forces developers into reactive patching cycles. To tackle this challenge, we propose IMAGINE, a synthesis framework that leverages embedding space distribution analysis to generate jailbreak-like instructions. This approach effectively fills the distributional gap between authentic jailbreak patterns and safety alignment corpora. IMAGINE follows an iterative optimization process that dynamically evolves text generation distributions across iterations, thereby augmenting the coverage of safety alignment data distributions through synthesized data examples. Based on the safety-aligned corpus enhanced through IMAGINE, our framework demonstrates significant decreases in attack success rate on Qwen2.5, Llama3.1, and Llama3.2 without compromising their utility.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T03:02:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20038v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20038v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 Pruning Strategies for Backdoor Defense in LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Santosh Chapagain, Shah Muhammad Hamdi, Soukaina Filali Boubrahimi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Backdoor attacks are a significant threat to the performance and integrity of pre-trained language models. Although such models are routinely fine-tuned for downstream NLP tasks, recent work shows they remain vulnerable to backdoor attacks that survive vanilla fine-tuning. These attacks are difficult to defend because end users typically lack knowledge of the attack triggers. Such attacks consist of stealthy malicious triggers introduced through subtle syntactic or stylistic manipulations, which can bypass traditional detection and remain in the model, making post-hoc purification essential. In this study, we explore whether attention-head pruning can mitigate these threats without any knowledge of the trigger or access to a clean reference model. To this end, we design and implement six pruning-based strategies: (i) gradient-based pruning, (ii) layer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2 sparsification, (iv) randomized ensemble pruning, (v) reinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning. Each method iteratively removes the least informative heads while monitoring validation accuracy to avoid over-pruning. Experimental evaluation shows that gradient-based pruning performs best while defending the syntactic triggers, whereas reinforcement learning and Bayesian pruning better withstand stylistic attacks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T16:34:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3746252.3760946' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.20032v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20032v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 Large Language Models (LLMs) for Electronic Design Automation (EDA)</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kangwei Xu, Denis Schwachhofer, Jason Blocklove, Ilia Polian, Peter Domanski, Dirk Pflüger, Siddharth Garg, Ramesh Karri, Ozgur Sinanoglu, Johann Knechtel, Zhuorui Zhao, Ulf Schlichtmann, Bing Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the growing complexity of modern integrated circuits, hardware engineers are required to devote more effort to the full design-to-manufacturing workflow. This workflow involves numerous iterations, making it both labor-intensive and error-prone. Therefore, there is an urgent demand for more efficient Electronic Design Automation (EDA) solutions to accelerate hardware development. Recently, large language models (LLMs) have shown remarkable advancements in contextual comprehension, logical reasoning, and generative capabilities. Since hardware designs and intermediate scripts can be represented as text, integrating LLM for EDA offers a promising opportunity to simplify and even automate the entire workflow. Accordingly, this paper provides a comprehensive overview of incorporating LLMs into EDA, with emphasis on their capabilities, limitations, and future opportunities. Three case studies, along with their outlook, are introduced to demonstrate the capabilities of LLMs in hardware design, testing, and optimization. Finally, future directions and challenges are highlighted to further explore the potential of LLMs in shaping the next-generation EDA, providing valuable insights for researchers interested in leveraging advanced AI technologies for EDA.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T16:33:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SY</span><span>cs.AI</span><span>cs.AR</span><span>cs.LG</span><span>cs.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20030v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20030v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 SELF: Surrogate-light Feature Selection with Large Language Models in
  Deep Recommender Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pengyue Jia, Zhaocheng Du, Yichao Wang, Xiangyu Zhao, Xiaopeng Li, Yuhao Wang, Qidong Liu, Huifeng Guo, Ruiming Tang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Feature selection is crucial in recommender systems for improving model efficiency and predictive performance. Conventional approaches typically employ surrogate models-such as decision trees or neural networks-to estimate feature importance. However, their effectiveness is inherently constrained, as these models may struggle under suboptimal training conditions, including feature collinearity, high-dimensional sparsity, and insufficient data. In this paper, we propose SELF, an SurrogatE-Light Feature selection method for deep recommender systems. SELF integrates semantic reasoning from Large Language Models (LLMs) with task-specific learning from surrogate models. Specifically, LLMs first produce a semantically informed ranking of feature importance, which is subsequently refined by a surrogate model, effectively integrating general world knowledge with task-specific learning. Comprehensive experiments on three public datasets from real-world recommender platforms validate the effectiveness of SELF.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T16:33:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.08516v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.08516v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 Using item recommendations and LLMs in marketing email titles</h2>
                <div class="authors">
                    <strong>Authors:</strong> Deddy Jobson, Muktti Shukla, Phuong Dinh, Julio Christian Young, Nick Pitton, Nina Chen, Ryan Ginstrom
                </div>
                <div class="summary">
                    <strong>Summary:</strong> E-commerce marketplaces make use of a number of marketing channels like emails, push notifications, etc. to reach their users and stimulate purchases. Personalized emails especially are a popular touch point for marketers to inform users of latest items in stock, especially for those who stopped visiting the marketplace. Such emails contain personalized recommendations tailored to each user's interests, enticing users to buy relevant items. A common limitation of these emails is that the primary entry point, the title of the email, tends to follow fixed templates, failing to inspire enough interest in the contents. In this work, we explore the potential of large language models (LLMs) for generating thematic titles that reflect the personalized content of the emails. We perform offline simulations and conduct online experiments on the order of millions of users, finding our techniques useful in improving the engagement between customers and our emails. We highlight key findings and learnings as we productionize the safe and automated generation of email titles for millions of users.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T16:31:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20024v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20024v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 Symphony: A Decentralized Multi-Agent Framework for Scalable Collective
  Intelligence</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ji Wang, Kashing Chen, Xinyuan Song, Ke Zhang, Lynn Ai, Eric Yang, Bill Shi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Most existing Large Language Model (LLM)-based agent frameworks rely on centralized orchestration, incurring high deployment costs, rigid communication topologies, and limited adaptability. To address these challenges, we introduce Symphony, a decentralized multi-agent system which enables lightweight LLMs on consumer-grade GPUs to coordinate. Symphony introduces three key mechanisms: (1) a decentralized ledger that records capabilities, (2) a Beacon-selection protocol for dynamic task allocation, and (3) weighted result voting based on CoTs. This design forms a privacy-saving, scalable, and fault-tolerant orchestration with low overhead. Empirically, Symphony outperforms existing baselines on reasoning benchmarks, achieving substantial accuracy gains and demonstrating robustness across models of varying capacities.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T16:27:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span><span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20019v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20019v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in
  Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, Mehrdad Farajtabar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. To address these concerns, we conduct a large-scale study on several SOTA open and closed models. To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of models.Our findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, we investigate the fragility of mathematical reasoning in these models and show that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is because current LLMs cannot perform genuine logical reasoning; they replicate reasoning steps from their training data. Adding a single clause that seems relevant to the question causes significant performance drops (up to 65%) across all state-of-the-art models, even though the clause doesn't contribute to the reasoning chain needed for the final answer. Overall, our work offers a more nuanced understanding of LLMs' capabilities and limitations in mathematical reasoning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T16:24:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.05229v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.05229v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 HPC Digital Twins for Evaluating Scheduling Policies, Incentive
  Structures and their Impact on Power and Cooling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Matthias Maiterth, Wesley H. Brewer, Jaya S. Kuruvella, Arunavo Dey, Tanzima Z. Islam, Kevin Menear, Dmitry Duplyakin, Rashadul Kabir, Tapasya Patki, Terry Jones, Feiyi Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Schedulers are critical for optimal resource utilization in high-performance computing. Traditional methods to evaluate schedulers are limited to post-deployment analysis, or simulators, which do not model associated infrastructure. In this work, we present the first-of-its-kind integration of scheduling and digital twins in HPC. This enables what-if studies to understand the impact of parameter configurations and scheduling decisions on the physical assets, even before deployment, or regarching changes not easily realizable in production. We (1) provide the first digital twin framework extended with scheduling capabilities, (2) integrate various top-tier HPC systems given their publicly available datasets, (3) implement extensions to integrate external scheduling simulators. Finally, we show how to (4) implement and evaluate incentive structures, as-well-as (5) evaluate machine learning based scheduling, in such novel digital-twin based meta-framework to prototype scheduling. Our work enables what-if scenarios of HPC systems to evaluate sustainability, and the impact on the simulated system.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T01:16:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span><span>cs.ET</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20016v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20016v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 Decomposing Behavioral Phase Transitions in LLMs: Order Parameters for
  Emergent Misalignment</h2>
                <div class="authors">
                    <strong>Authors:</strong> Julian Arnold, Niels Lörch
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Fine-tuning LLMs on narrowly harmful datasets can lead to behavior that is broadly misaligned with respect to human values. To understand when and how this emergent misalignment occurs, we develop a comprehensive framework for detecting and characterizing rapid transitions during fine-tuning using both distributional change detection methods as well as order parameters that are formulated in plain English and evaluated by an LLM judge. Using an objective statistical dissimilarity measure, we quantify how the phase transition that occurs during fine-tuning affects multiple aspects of the model. In particular, we assess what percentage of the total distributional change in model outputs is captured by different aspects, such as alignment or verbosity, providing a decomposition of the overall transition. We also find that the actual behavioral transition occurs later in training than indicated by the peak in the gradient norm alone. Our framework enables the automated discovery and quantification of language-based order parameters, which we demonstrate on examples ranging from knowledge questions to politics and ethics.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T16:19:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20015v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20015v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 CataractSurg-80K: Knowledge-Driven Benchmarking for Structured Reasoning
  in Ophthalmic Surgery Planning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yang Meng, Zewen Pan, Yandi Lu, Ruobing Huang, Yanfeng Liao, Jiarui Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Cataract surgery remains one of the most widely performed and effective procedures for vision restoration. Effective surgical planning requires integrating diverse clinical examinations for patient assessment, intraocular lens (IOL) selection, and risk evaluation. Large language models (LLMs) have shown promise in supporting clinical decision-making. However, existing LLMs often lack the domain-specific expertise to interpret heterogeneous ophthalmic data and provide actionable surgical plans. To enhance the model's ability to interpret heterogeneous ophthalmic reports, we propose a knowledge-driven Multi-Agent System (MAS), where each agent simulates the reasoning process of specialist ophthalmologists, converting raw clinical inputs into structured, actionable summaries in both training and deployment stages. Building on MAS, we introduce CataractSurg-80K, the first large-scale benchmark for cataract surgery planning that incorporates structured clinical reasoning. Each case is annotated with diagnostic questions, expert reasoning chains, and structured surgical recommendations. We further introduce Qwen-CSP, a domain-specialized model built on Qwen-4B, fine-tuned through a multi-stage process tailored for surgical planning. Comprehensive experiments show that Qwen-CSP outperforms strong general-purpose LLMs across multiple metrics. Our work delivers a high-quality dataset, a rigorous benchmark, and a domain-adapted LLM to facilitate future research in medical AI reasoning and decision support.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T16:16:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20014v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20014v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 Cross-Platform E-Commerce Product Categorization and Recategorization: A
  Multimodal Hierarchical Classification Approach</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lotte Gross, Rebecca Walter, Nicole Zoppi, Adrien Justus, Alessandro Gambetti, Qiwei Han, Maximilian Kaiser
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This study addresses critical industrial challenges in e-commerce product categorization, namely platform heterogeneity and the structural limitations of existing taxonomies, by developing and deploying a multimodal hierarchical classification framework. Using a dataset of 271,700 products from 40 international fashion e-commerce platforms, we integrate textual features (RoBERTa), visual features (ViT), and joint vision--language representations (CLIP). We investigate fusion strategies, including early, late, and attention-based fusion within a hierarchical architecture enhanced by dynamic masking to ensure taxonomic consistency. Results show that CLIP embeddings combined via an MLP-based late-fusion strategy achieve the highest hierarchical F1 (98.59\%), outperforming unimodal baselines. To address shallow or inconsistent categories, we further introduce a self-supervised ``product recategorization'' pipeline using SimCLR, UMAP, and cascade clustering, which discovered new, fine-grained categories (e.g., subtypes of ``Shoes'') with cluster purities above 86\%. Cross-platform experiments reveal a deployment-relevant trade-off: complex late-fusion methods maximize accuracy with diverse training data, while simpler early-fusion methods generalize more effectively to unseen platforms. Finally, we demonstrate the framework's industrial scalability through deployment in EURWEB's commercial transaction intelligence platform via a two-stage inference pipeline, combining a lightweight RoBERTa stage with a GPU--accelerated multimodal stage to balance cost and accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T16:16:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20013v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20013v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real</h2>
                <div class="authors">
                    <strong>Authors:</strong> Prithwish Dan, Kushal Kedia, Angela Chao, Edward Weiyi Duan, Maximus Adrian Pace, Wei-Chiu Ma, Sanjiban Choudhury
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Human videos offer a scalable way to train robot manipulation policies, but lack the action labels needed by standard imitation learning algorithms. Existing cross-embodiment approaches try to map human motion to robot actions, but often fail when the embodiments differ significantly. We propose X-Sim, a real-to-sim-to-real framework that uses object motion as a dense and transferable signal for learning robot policies. X-Sim starts by reconstructing a photorealistic simulation from an RGBD human video and tracking object trajectories to define object-centric rewards. These rewards are used to train a reinforcement learning (RL) policy in simulation. The learned policy is then distilled into an image-conditioned diffusion policy using synthetic rollouts rendered with varied viewpoints and lighting. To transfer to the real world, X-Sim introduces an online domain adaptation technique that aligns real and simulated observations during deployment. Importantly, X-Sim does not require any robot teleoperation data. We evaluate it across 5 manipulation tasks in 2 environments and show that it: (1) improves task progress by 30% on average over hand-tracking and sim-to-real baselines, (2) matches behavior cloning with 10x less data collection time, and (3) generalizes to new camera viewpoints and test-time changes. Code and videos are available at https://portal-cornell.github.io/X-Sim/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T16:04:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.07096v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.07096v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 MathBuddy: A Multimodal System for Affective Math Tutoring</h2>
                <div class="authors">
                    <strong>Authors:</strong> Debanjana Kar, Leopold Böss, Dacia Braca, Sebastian Maximilian Dennerlein, Nina Christine Hubig, Philipp Wintersberger, Yufang Hou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid adoption of LLM-based conversational systems is already transforming the landscape of educational technology. However, the current state-of-the-art learning models do not take into account the student's affective states. Multiple studies in educational psychology support the claim that positive or negative emotional states can impact a student's learning capabilities. To bridge this gap, we present MathBuddy, an emotionally aware LLM-powered Math Tutor, which dynamically models the student's emotions and maps them to relevant pedagogical strategies, making the tutor-student conversation a more empathetic one. The student's emotions are captured from the conversational text as well as from their facial expressions. The student's emotions are aggregated from both modalities to confidently prompt our LLM Tutor for an emotionally-aware response. We have effectively evaluated our model using automatic evaluation metrics across eight pedagogical dimensions and user studies. We report a massive 23 point performance gain using the win rate and a 3 point gain at an overall level using DAMR scores which strongly supports our hypothesis of improving LLM-based tutor's pedagogical abilities by modeling students' emotions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T15:50:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19993v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19993v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 AgentCoMa: A Compositional Benchmark Mixing Commonsense and Mathematical
  Reasoning in Real-World Scenarios</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lisa Alazraki, Lihu Chen, Ana Brassard, Joe Stacey, Hossein A. Rahmani, Marek Rei
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have achieved high accuracy on complex commonsense and mathematical problems that involve the composition of multiple reasoning steps. However, current compositional benchmarks testing these skills tend to focus on either commonsense or math reasoning, whereas LLM agents solving real-world tasks would require a combination of both. In this work, we introduce an Agentic Commonsense and Math benchmark (AgentCoMa), where each compositional task requires a commonsense reasoning step and a math reasoning step. We test it on 61 LLMs of different sizes, model families, and training strategies. We find that LLMs can usually solve both steps in isolation, yet their accuracy drops by ~30% on average when the two are combined. This is a substantially greater performance gap than the one we observe in prior compositional benchmarks that combine multiple steps of the same reasoning type. In contrast, non-expert human annotators can solve the compositional questions and the individual steps in AgentCoMa with similarly high accuracy. Furthermore, we conduct a series of interpretability studies to better understand the performance gap, examining neuron patterns, attention maps and membership inference. Our work underscores a substantial degree of model brittleness in the context of mixed-type compositional reasoning and offers a test bed for future improvement.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T15:47:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19988v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19988v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 Future Deployment and Flexibility of Distributed Energy Resources in the
  Distribution Grids of Switzerland</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lorenzo Zapparoli, Alfredo Oneto, María Parajeles Herrera, Blazhe Gjorgiev, Gabriela Hug, Giovanni Sansavini
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The decarbonization goals worldwide drive the energy transition of power distribution grids, which operate under increasingly volatile conditions and closer to their technical limits. In this context, localized operational data with high temporal and spatial resolution is essential for their effective planning and regulation. Nevertheless, information on grid-connected distributed energy resources, such as electric vehicles, photovoltaic systems, and heat pumps, is often fragmented, inconsistent, and unavailable. This work introduces a comprehensive database of distributed energy resources and non-controllable loads allocated in Switzerland's medium- and low-voltage distribution grid models, covering over 2 million points of connection. Remarkably, this data specifies the flexibility capabilities of the controllable devices, with a set of projections aligned with national forecasts for 2030, 2040, and 2050. The database supports studies on flexibility provision of distributed energy resources, distribution grid resilience, and national energy policy, among other topics. Importantly, its modular structure allows users to extract national- and local-scale information across medium- and low-voltage systems, enabling broad applicability across locations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T15:32:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SY</span><span>cs.SY</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1038/s41597-025-05830-y' target='_blank'>doi</a><a href='http://arxiv.org/abs/2506.08724v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.08724v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 GLSim: Detecting Object Hallucinations in LVLMs via Global-Local
  Similarity</h2>
                <div class="authors">
                    <strong>Authors:</strong> Seongheon Park, Yixuan Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Object hallucination in large vision-language models presents a significant challenge to their safe deployment in real-world applications. Recent works have proposed object-level hallucination scores to estimate the likelihood of object hallucination; however, these methods typically adopt either a global or local perspective in isolation, which may limit detection reliability. In this paper, we introduce GLSim, a novel training-free object hallucination detection framework that leverages complementary global and local embedding similarity signals between image and text modalities, enabling more accurate and reliable hallucination detection in diverse scenarios. We comprehensively benchmark existing object hallucination detection methods and demonstrate that GLSim achieves superior detection performance, outperforming competitive baselines by a significant margin.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T15:30:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19972v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19972v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text
  Tasks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Luel Hagos Beyene, Vivek Verma, Min Ma, Jesujoba O. Alabi, Fabian David Schmidt, Joyce Nakatumba-Nabende, David Ifeoluwa Adelani
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language models (LLMs) have demonstrated impressive performance on a wide range of tasks, including in multimodal settings such as speech. However, their evaluation is often limited to English and a few high-resource languages. For low-resource languages, there is no standardized evaluation benchmark. In this paper, we address this gap by introducing mSTEB, a new benchmark to evaluate the performance of LLMs on a wide range of tasks covering language identification, text classification, question answering, and translation tasks on both speech and text modalities. We evaluated the performance of leading LLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art open models such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap in performance between high-resource and low-resource languages, especially for languages spoken in Africa and Americas/Oceania. Our findings show that more investment is needed to address their under-representation in LLMs coverage.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T15:27:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span><span>cs.SD</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.08400v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.08400v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 Divide, Discover, Deploy: Factorized Skill Learning with Symmetry and
  Style Priors</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rafael Cathomen, Mayank Mittal, Marin Vlastelica, Marco Hutter
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Unsupervised Skill Discovery (USD) allows agents to autonomously learn diverse behaviors without task-specific rewards. While recent USD methods have shown promise, their application to real-world robotics remains underexplored. In this paper, we propose a modular USD framework to address the challenges in the safety, interpretability, and deployability of the learned skills. Our approach employs user-defined factorization of the state space to learn disentangled skill representations. It assigns different skill discovery algorithms to each factor based on the desired intrinsic reward function. To encourage structured morphology-aware skills, we introduce symmetry-based inductive biases tailored to individual factors. We also incorporate a style factor and regularization penalties to promote safe and robust behaviors. We evaluate our framework in simulation using a quadrupedal robot and demonstrate zero-shot transfer of the learned skills to real hardware. Our results show that factorization and symmetry lead to the discovery of structured human-interpretable behaviors, while the style factor and penalties enhance safety and diversity. Additionally, we show that the learned skills can be used for downstream tasks and perform on par with oracle policies trained with hand-crafted rewards.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T15:05:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19953v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19953v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 Socially Interactive Agents for Preserving and Transferring Tacit
  Knowledge in Organizations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Martin Benderoth, Patrick Gebhard, Christian Keller, C. Benjamin Nakhosteen, Stefan Schaffer, Tanja Schneeberger
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper introduces a novel approach to tackle the challenges of preserving and transferring tacit knowledge--deep, experience-based insights that are hard to articulate but vital for decision-making, innovation, and problem-solving. Traditional methods rely heavily on human facilitators, which, while effective, are resource-intensive and lack scalability. A promising alternative is the use of Socially Interactive Agents (SIAs) as AI-driven knowledge transfer facilitators. These agents interact autonomously and socially intelligently with users through multimodal behaviors (verbal, paraverbal, nonverbal), simulating expert roles in various organizational contexts. SIAs engage employees in empathic, natural-language dialogues, helping them externalize insights that might otherwise remain unspoken. Their success hinges on building trust, as employees are often hesitant to share tacit knowledge without assurance of confidentiality and appreciation. Key technologies include Large Language Models (LLMs) for generating context-relevant dialogue, Retrieval-Augmented Generation (RAG) to integrate organizational knowledge, and Chain-of-Thought (CoT) prompting to guide structured reflection. These enable SIAs to actively elicit knowledge, uncover implicit assumptions, and connect insights to broader organizational contexts. Potential applications span onboarding, where SIAs support personalized guidance and introductions, and knowledge retention, where they conduct structured interviews with retiring experts to capture heuristics behind decisions. Success depends on addressing ethical and operational challenges such as data privacy, algorithmic bias, and resistance to AI. Transparency, robust validation, and a culture of trust are essential to mitigate these risks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T14:59:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19942v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19942v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 Principled Detection of Hallucinations in Large Language Models via
  Multiple Testing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiawei Li, Akshayaa Magesh, Venugopal V. Veeravalli
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While Large Language Models (LLMs) have emerged as powerful foundational models to solve a variety of tasks, they have also been shown to be prone to hallucinations, i.e., generating responses that sound confident but are actually incorrect or even nonsensical. In this work, we formulate the problem of detecting hallucinations as a hypothesis testing problem and draw parallels to the problem of out-of-distribution detection in machine learning models. We propose a multiple-testing-inspired method to solve the hallucination detection problem, and provide extensive experimental results to validate the robustness of our approach against state-of-the-art methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T14:55:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18473v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18473v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital
  Payments</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nitish Jaipuria, Lorenzo Gatto, Zijun Kan, Shankey Poddar, Bill Cheung, Diksha Bansal, Ramanan Balakrishnan, Aviral Suri, Jose Estevez
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The proliferation of digital payment platforms has transformed commerce, offering unmatched convenience and accessibility globally. However, this growth has also attracted malicious actors, leading to a corresponding increase in sophisticated social engineering scams. These scams are often initiated and orchestrated on multiple surfaces outside the payment platform, making user and transaction-based signals insufficient for a complete understanding of the scam's methodology and underlying patterns, without which it is very difficult to prevent it in a timely manner. This paper presents CASE (Conversational Agent for Scam Elucidation), a novel Agentic AI framework that addresses this problem by collecting and managing user scam feedback in a safe and scalable manner. A conversational agent is uniquely designed to proactively interview potential victims to elicit intelligence in the form of a detailed conversation. The conversation transcripts are then consumed by another AI system that extracts information and converts it into structured data for downstream usage in automated and manual enforcement mechanisms. Using Google's Gemini family of LLMs, we implemented this framework on Google Pay (GPay) India. By augmenting our existing features with this new intelligence, we have observed a 21% uplift in the volume of scam enforcements. The architecture and its robust evaluation framework are highly generalizable, offering a blueprint for building similar AI-driven systems to collect and manage scam intelligence in other sensitive domains.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T14:47:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19932v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19932v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 InternVL3.5: Advancing Open-Source Multimodal Models in Versatility,
  Reasoning, and Efficiency</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, Zhaokai Wang, Zhe Chen, Hongjie Zhang, Ganlin Yang, Haomin Wang, Qi Wei, Jinhui Yin, Wenhao Li, Erfei Cui, Guanzhou Chen, Zichen Ding, Changyao Tian, Zhenyu Wu, Jingjing Xie, Zehao Li, Bowen Yang, Yuchen Duan, Xuehui Wang, Zhi Hou, Haoran Hao, Tianyi Zhang, Songze Li, Xiangyu Zhao, Haodong Duan, Nianchen Deng, Bin Fu, Yinan He, Yi Wang, Conghui He, Botian Shi, Junjun He, Yingtong Xiong, Han Lv, Lijun Wu, Wenqi Shao, Kaipeng Zhang, Huipeng Deng, Biqing Qi, Jiaye Ge, Qipeng Guo, Wenwei Zhang, Songyang Zhang, Maosong Cao, Junyao Lin, Kexian Tang, Jianfei Gao, Haian Huang, Yuzhe Gu, Chengqi Lyu, Huanze Tang, Rui Wang, Haijun Lv, Wanli Ouyang, Limin Wang, Min Dou, Xizhou Zhu, Tong Lu, Dahua Lin, Jifeng Dai, Weijie Su, Bowen Zhou, Kai Chen, Yu Qiao, Wenhai Wang, Gen Luo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce InternVL 3.5, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inference efficiency along the InternVL series. A key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment. This coarse-to-fine training strategy leads to substantial improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize efficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution of visual tokens without compromising performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs, effectively balancing computational load. These contributions collectively enable InternVL3.5 to achieve up to a +16.0\% gain in overall reasoning performance and a 4.05$\times$ inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as GUI interaction and embodied agency. Notably, our largest model, i.e., InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs across general multimodal, reasoning, text, and agentic tasks -- narrowing the performance gap with leading commercial models like GPT-5. All models and code are publicly released.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T14:39:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18265v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18265v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 Synthesizing High-Quality Programming Tasks with LLM-based Expert and
  Student Agents</h2>
                <div class="authors">
                    <strong>Authors:</strong> Manh Hung Nguyen, Victor-Alexandru Pădurean, Alkis Gotovos, Sebastian Tschiatschek, Adish Singla
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Generative AI is transforming computing education by enabling the automatic generation of personalized content and feedback. We investigate its capabilities in providing high-quality programming tasks to students. Despite promising advancements in task generation, a quality gap remains between AI-generated and expert-created tasks. The AI-generated tasks may not align with target programming concepts, could be incomprehensible to students, or may contain critical issues such as incorrect tests. Existing works often require interventions from human teachers for validation. We address these challenges by introducing PyTaskSyn, a novel synthesis technique that first generates a programming task and then decides whether it meets certain quality criteria to be given to students. The key idea is to break this process into multiple stages performed by expert and student agents simulated using both strong and weaker generative models. Through extensive evaluation, we show that PyTaskSyn significantly improves task quality compared to baseline techniques and showcases the importance of each specialized agent type in our validation pipeline. Additionally, we conducted user studies using our publicly available web application and show that PyTaskSyn can deliver high-quality programming tasks comparable to expert-designed ones while reducing workload and costs, and being more engaging than programming tasks that are available in online resources.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T14:39:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.07655v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.07655v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 Self-Organizing Agent Network for LLM-based Workflow Automation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yiming Xiong, Jian Wang, Bing Li, Yuhan Zhu, Yuqi Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent multi-agent frameworks built upon large language models (LLMs) have demonstrated remarkable capabilities in complex task planning. However, in real-world enterprise environments, business workflows are typically composed through modularization and reuse of numerous subprocesses, resulting in intricate workflows characterized by lengthy and deeply nested execution paths. Such complexity poses significant challenges for LLM-driven orchestration, as extended reasoning chains and state-space explosions severely impact planning effectiveness and the proper sequencing of tool invocations. Therefore, developing an orchestration method with controllable structures capable of handling multi-layer nesting becomes a critical issue. To address this, we propose a novel structure-driven orchestration framework Self-Organizing Agent Network (SOAN). SOAN incrementally builds a formalized agent network by identifying and encapsulating structural units as independent agents, enhancing modularity and clarity in orchestration. Extensive evaluations were performed using multiple benchmarks as well as a real-world enterprise workflow dataset. Experimental results demonstrate that SOAN significantly outperforms state-of-the-art methods in terms of adaptability, fault tolerance, and execution efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T14:37:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.13732v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.13732v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 FARM: Frame-Accelerated Augmentation and Residual Mixture-of-Experts for
  Physics-Based High-Dynamic Humanoid Control</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tan Jing, Shiting Chen, Yangfan Li, Weisheng Xu, Renjing Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Unified physics-based humanoid controllers are pivotal for robotics and character animation, yet models that excel on gentle, everyday motions still stumble on explosive actions, hampering real-world deployment. We bridge this gap with FARM (Frame-Accelerated Augmentation and Residual Mixture-of-Experts), an end-to-end framework composed of frame-accelerated augmentation, a robust base controller, and a residual mixture-of-experts (MoE). Frame-accelerated augmentation exposes the model to high-velocity pose changes by widening inter-frame gaps. The base controller reliably tracks everyday low-dynamic motions, while the residual MoE adaptively allocates additional network capacity to handle challenging high-dynamic actions, significantly enhancing tracking accuracy. In the absence of a public benchmark, we curate the High-Dynamic Humanoid Motion (HDHM) dataset, comprising 3593 physically plausible clips. On HDHM, FARM reduces the tracking failure rate by 42.8\% and lowers global mean per-joint position error by 14.6\% relative to the baseline, while preserving near-perfect accuracy on low-dynamic motions. These results establish FARM as a new baseline for high-dynamic humanoid control and introduce the first open benchmark dedicated to this challenge. The code and dataset will be released at https://github.com/Colin-Jing/FARM.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T14:35:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19926v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19926v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 HEAL: A Hypothesis-Based Preference-Aware Analysis Framework</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yifu Huo, Chenglong Wang, Qiren Zhu, Shunjie Xing, Tong Xiao, Chunliang Zhang, Tongran Liu, Jinbo Zhu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Preference optimization methods like DPO have achieved remarkable performance in LLM alignment. However, the evaluation for these methods relies on a single response and overlooks other potential outputs, which could also be generated in real-world applications within this hypothetical space. To address this issue, this paper presents a \textbf{H}ypothesis-based Pr\textbf{E}ference-aware \textbf{A}na\textbf{L}ysis Framework (HEAL), a novel evaluation paradigm that formulates preference alignment as a re-ranking process within hypothesis spaces. The framework incorporates two complementary metrics: ranking accuracy for evaluating ordinal consistency and preference strength correlation for assessing continuous alignment. To facilitate this framework, we develop UniHypoBench, a unified hypothesis benchmark constructed from diverse instruction-response pairs. Through extensive experiments based on HEAL, with a particular focus on the intrinsic mechanisms of preference learning, we demonstrate that current preference learning methods can effectively capture preferences provided by proxy models while simultaneously suppressing negative samples. These findings contribute to preference learning research through two significant avenues. Theoretically, we introduce hypothesis space analysis as an innovative paradigm for understanding preference alignment. Practically, HEAL offers researchers robust diagnostic tools for refining preference optimization methods, while our empirical results identify promising directions for developing more advanced alignment algorithms capable of comprehensive preference capture.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T14:30:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19922v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19922v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 An assessment of estimation models and investment gaps for the
  deployment of high-speed broadband networks in NUTS3 regions to meet the
  objectives of the European Gigabit Society</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ferrandis Jesus, Ramos Sergio, Feijoo Claudio
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper analyses the deployment of high speed broadband networks in the European Union (EU). Its aim is to assess the investment required to meet the targets set by the European Commission (EC) for 2025, within the framework of the European Gigabit Society (EGS). This plan aims to ensure the availability and take up of very high capacity fixed and wireless networks, in both urban and rural areas, among households and the main socioeconomic drivers. The estimation model presented here uses a methodology supported by data at the local (NUTS3) level to give a bottom up estimation of the investment gap for each of the EGS objectives, using three different scenarios depending on the mix of wired and wireless technologies offered. The methodology and estimation model used in the paper are examined against other examples and assumptions available in the literature. We also offer a dynamic perspective on the analysis of the evolution of this investment gap over the years 2017 2019, which includes an assessment of the usefulness of these estimation models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T14:27:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CE</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1016/j.telpol.2021.102170' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.19921v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19921v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 Your AI Bosses Are Still Prejudiced: The Emergence of Stereotypes in
  LLM-Based Multi-Agent Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jingyu Guo, Yingying Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While stereotypes are well-documented in human social interactions, AI systems are often presumed to be less susceptible to such biases. Previous studies have focused on biases inherited from training data, but whether stereotypes can emerge spontaneously in AI agent interactions merits further exploration. Through a novel experimental framework simulating workplace interactions with neutral initial conditions, we investigate the emergence and evolution of stereotypes in LLM-based multi-agent systems. Our findings reveal that (1) LLM-Based AI agents develop stereotype-driven biases in their interactions despite beginning without predefined biases; (2) stereotype effects intensify with increased interaction rounds and decision-making power, particularly after introducing hierarchical structures; (3) these systems exhibit group effects analogous to human social behavior, including halo effects, confirmation bias, and role congruity; and (4) these stereotype patterns manifest consistently across different LLM architectures. Through comprehensive quantitative analysis, these findings suggest that stereotype formation in AI systems may arise as an emergent property of multi-agent interactions, rather than merely from training data biases. Our work underscores the need for future research to explore the underlying mechanisms of this phenomenon and develop strategies to mitigate its ethical impacts.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T14:25:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19919v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19919v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 Refining Text Generation for Realistic Conversational Recommendation via
  Direct Preference Optimization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Manato Tajiri, Michimasa Inaba
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Conversational Recommender Systems (CRSs) aim to elicit user preferences via natural dialogue to provide suitable item recommendations. However, current CRSs often deviate from realistic human interactions by rapidly recommending items in brief sessions. This work addresses this gap by leveraging Large Language Models (LLMs) to generate dialogue summaries from dialogue history and item recommendation information from item description. This approach enables the extraction of both explicit user statements and implicit preferences inferred from the dialogue context. We introduce a method using Direct Preference Optimization (DPO) to ensure dialogue summary and item recommendation information are rich in information crucial for effective recommendations. Experiments on two public datasets validate our method's effectiveness in fostering more natural and realistic conversational recommendation processes.Our implementation is publicly available at: https://github.com/UEC-InabaLab/Refining-LLM-Text
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T10:57:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19918v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19918v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 Streamlining the Development of Active Learning Methods in Real-World
  Object Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Moussa Kassem Sbeyti, Nadja Klein, Michelle Karg, Christian Wirth, Sahin Albayrak
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Active learning (AL) for real-world object detection faces computational and reliability challenges that limit practical deployment. Developing new AL methods requires training multiple detectors across iterations to compare against existing approaches. This creates high costs for autonomous driving datasets where the training of one detector requires up to 282 GPU hours. Additionally, AL method rankings vary substantially across validation sets, compromising reliability in safety-critical transportation systems. We introduce object-based set similarity ($\mathrm{OSS}$), a metric that addresses these challenges. $\mathrm{OSS}$ (1) quantifies AL method effectiveness without requiring detector training by measuring similarity between training sets and target domains using object-level features. This enables the elimination of ineffective AL methods before training. Furthermore, $\mathrm{OSS}$ (2) enables the selection of representative validation sets for robust evaluation. We validate our similarity-based approach on three autonomous driving datasets (KITTI, BDD100K, CODA) using uncertainty-based AL methods as a case study with two detector architectures (EfficientDet, YOLOv3). This work is the first to unify AL training and evaluation strategies in object detection based on object similarity. $\mathrm{OSS}$ is detector-agnostic, requires only labeled object crops, and integrates with existing AL pipelines. This provides a practical framework for deploying AL in real-world applications where computational efficiency and evaluation reliability are critical. Code is available at https://mos-ks.github.io/publications/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T14:10:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19906v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19906v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 Logical Reasoning with Outcome Reward Models for Test-Time Scaling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ramya Keerthy Thatikonda, Wray Buntine, Ehsan Shareghi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Logical reasoning is a critical benchmark for evaluating the capabilities of large language models (LLMs), as it reflects their ability to derive valid conclusions from given premises. While the combination of test-time scaling with dedicated outcome or process reward models has opened up new avenues to enhance LLMs performance in complex reasoning tasks, this space is under-explored in deductive logical reasoning. We present a set of Outcome Reward Models (ORMs) for deductive reasoning. To train the ORMs we mainly generate data using Chain-of-Thought (CoT) with single and multiple samples. Additionally, we propose a novel tactic to further expand the type of errors covered in the training dataset of the ORM. In particular, we propose an echo generation technique that leverages LLMs' tendency to reflect incorrect assumptions made in prompts to extract additional training data, covering previously unexplored error types. While a standard CoT chain may contain errors likely to be made by the reasoner, the echo strategy deliberately steers the model toward incorrect reasoning. We show that ORMs trained on CoT and echo-augmented data demonstrate improved performance on the FOLIO, JustLogic, and ProverQA datasets across four different LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T14:08:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19903v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19903v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 Truth or Twist? Optimal Model Selection for Reliable Label Flipping
  Evaluation in LLM-based Counterfactuals</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qianli Wang, Van Bach Nguyen, Nils Feldhus, Luis Felipe Villa-Arenas, Christin Seifert, Sebastian Möller, Vera Schmitt
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Counterfactual examples are widely employed to enhance the performance and robustness of large language models (LLMs) through counterfactual data augmentation (CDA). However, the selection of the judge model used to evaluate label flipping, the primary metric for assessing the validity of generated counterfactuals for CDA, yields inconsistent results. To decipher this, we define four types of relationships between the counterfactual generator and judge models: being the same model, belonging to the same model family, being independent models, and having an distillation relationship. Through extensive experiments involving two state-of-the-art LLM-based methods, three datasets, four generator models, and 15 judge models, complemented by a user study (n = 90), we demonstrate that judge models with an independent, non-fine-tuned relationship to the generator model provide the most reliable label flipping evaluations. Relationships between the generator and judge models, which are closely aligned with the user study for CDA, result in better model performance and robustness. Nevertheless, we find that the gap between the most effective judge models and the results obtained from the user study remains considerably large. This suggests that a fully automated pipeline for CDA may be inadequate and requires human intervention.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T14:04:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.13972v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.13972v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 NM-Hebb: Coupling Local Hebbian Plasticity with Metric Learning for More
  Accurate and Interpretable CNNs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Davorin Miličević, Ratko Grbić
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Deep Convolutional Neural Networks (CNNs) achieve high accuracy but often rely on purely global, gradient-based optimisation, which can lead to overfitting, redundant filters, and reduced interpretability. To address these limitations, we propose NM-Hebb, a two-phase training framework that integrates neuro-inspired local plasticity with distance-aware supervision. Phase 1 extends standard supervised training by jointly optimising a cross-entropy objective with two biologically inspired mechanisms: (i) a Hebbian regulariser that aligns the spatial mean of activations with the mean of the corresponding convolutional filter weights, encouraging structured, reusable primitives; and (ii) a learnable neuromodulator that gates an elastic-weight-style consolidation loss, preserving beneficial parameters without freezing the network. Phase 2 fine-tunes the backbone with a pairwise metric-learning loss, explicitly compressing intra-class distances and enlarging inter-class margins in the embedding space. Evaluated on CIFAR-10, CIFAR-100, and TinyImageNet across five backbones (ResNet-18, VGG-11, MobileNet-v2, EfficientNet-V2, DenseNet-121), NM-Hebb achieves consistent gains over baseline and other methods: Top-1 accuracy improves by +2.0-10.0 pp (CIFAR-10), +2.0-9.0 pp (CIFAR-100), and up to +4.3-8.9 pp (TinyImageNet), with Normalised Mutual Information (NMI) increased by up to +0.15. Qualitative visualisations and filter-level analyses further confirm that NM-Hebb produces more structured and selective features, yielding tighter and more interpretable class clusters. Overall, coupling local Hebbian plasticity with metric-based fine-tuning yields CNNs that are not only more accurate but also more interpretable, offering practical benefits for resource-constrained and safety-critical AI deployments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:53:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CV</span><span>I.2.6; I.5.4</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19896v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19896v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 Towards New Benchmark for AI Alignment & Sentiment Analysis in Socially
  Important Issues: A Comparative Study of Human and LLMs in the Context of AGI</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ljubisa Bojic, Dylan Seychell, Milan Cabarkapa
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As general-purpose artificial intelligence systems become increasingly integrated into society and are used for information seeking, content generation, problem solving, textual analysis, coding, and running processes, it is crucial to assess their long-term impact on humans. This research explores the sentiment of large language models (LLMs) and humans toward artificial general intelligence (AGI) using a Likert-scale survey. Seven LLMs, including GPT-4 and Bard, were analyzed and compared with sentiment data from three independent human sample populations. Temporal variations in sentiment were also evaluated over three consecutive days. The results show a diversity in sentiment scores among LLMs, ranging from 3.32 to 4.12 out of 5. GPT-4 recorded the most positive sentiment toward AGI, while Bard leaned toward a neutral sentiment. In contrast, the human samples showed a lower average sentiment of 2.97. The analysis outlines potential conflicts of interest and biases in the sentiment formation of LLMs, and indicates that LLMs could subtly influence societal perceptions. To address the need for regulatory oversight and culturally grounded assessments of AI systems, we introduce the Societal AI Alignment and Sentiment Benchmark (SAAS-AI), which leverages multidimensional prompts and empirically validated societal value frameworks to evaluate language model outputs across temporal, model, and multilingual axes. This benchmark is designed to guide policymakers and AI agencies, including within frameworks such as the EU AI Act, by providing robust, actionable insights into AI alignment with human values, public sentiment, and ethical norms at both national and international levels. Future research should further refine the operationalization of the SAAS-AI benchmark and systematically evaluate its effectiveness through comprehensive empirical testing.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:49:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.02531v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.02531v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 Learning Deployable Locomotion Control via Differentiable Simulation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Clemens Schwarke, Victor Klemm, Joshua Bagajo, Jean-Pierre Sleiman, Ignat Georgiev, Jesus Tordesillas, Marco Hutter
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Differentiable simulators promise to improve sample efficiency in robot learning by providing analytic gradients of the system dynamics. Yet, their application to contact-rich tasks like locomotion is complicated by the inherently non-smooth nature of contact, impeding effective gradient-based optimization. Existing works thus often rely on soft contact models that provide smooth gradients but lack physical accuracy, constraining results to simulation. To address this limitation, we propose a differentiable contact model designed to provide informative gradients while maintaining high physical fidelity. We demonstrate the efficacy of our approach by training a quadrupedal locomotion policy within our differentiable simulator leveraging analytic gradients and successfully transferring the learned policy zero-shot to the real world. To the best of our knowledge, this represents the first successful sim-to-real transfer of a legged locomotion policy learned entirely within a differentiable simulator, establishing the feasibility of using differentiable simulation for real-world locomotion control.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:49:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.02887v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.02887v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 Bangla-Bayanno: A 52K-Pair Bengali Visual Question Answering Dataset
  with LLM-Assisted Translation Refinement</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mohammed Rakibul Hasan, Rafi Majid, Ahanaf Tahmid
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper, we introduce Bangla-Bayanno, an open-ended Visual Question Answering (VQA) Dataset in Bangla, a widely used, low-resource language in multimodal AI research. The majority of existing datasets are either manually annotated with an emphasis on a specific domain, query type, or answer type or are constrained by niche answer formats. In order to mitigate human-induced errors and guarantee lucidity, we implemented a multilingual LLM-assisted translation refinement pipeline. This dataset overcomes the issues of low-quality translations from multilingual sources. The dataset comprises 52,650 question-answer pairs across 4750+ images. Questions are classified into three distinct answer types: nominal (short descriptive), quantitative (numeric), and polar (yes/no). Bangla-Bayanno provides the most comprehensive open-source, high-quality VQA benchmark in Bangla, aiming to advance research in low-resource multimodal learning and facilitate the development of more inclusive AI systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:48:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19887v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19887v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 AI-Powered Detection of Inappropriate Language in Medical School
  Curricula</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chiman Salavati, Shannon Song, Scott A. Hale, Roberto E. Montenegro, Shiri Dori-Hacohen, Fabricio Murai
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The use of inappropriate language -- such as outdated, exclusionary, or non-patient-centered terms -- medical instructional materials can significantly influence clinical training, patient interactions, and health outcomes. Despite their reputability, many materials developed over past decades contain examples now considered inappropriate by current medical standards. Given the volume of curricular content, manually identifying instances of inappropriate use of language (IUL) and its subcategories for systematic review is prohibitively costly and impractical. To address this challenge, we conduct a first-in-class evaluation of small language models (SLMs) fine-tuned on labeled data and pre-trained LLMs with in-context learning on a dataset containing approximately 500 documents and over 12,000 pages. For SLMs, we consider: (1) a general IUL classifier, (2) subcategory-specific binary classifiers, (3) a multilabel classifier, and (4) a two-stage hierarchical pipeline for general IUL detection followed by multilabel classification. For LLMs, we consider variations of prompts that include subcategory definitions and/or shots. We found that both LLama-3 8B and 70B, even with carefully curated shots, are largely outperformed by SLMs. While the multilabel classifier performs best on annotated data, supplementing training with unflagged excerpts as negative examples boosts the specific classifiers' AUC by up to 25%, making them most effective models for mitigating harmful language in medical curricula.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:40:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.CY</span><span>I.2.1; I.2.7</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19883v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19883v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 Generative AI for Testing of Autonomous Driving Systems: A Survey</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qunying Song, He Ye, Mark Harman, Federica Sarro
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Autonomous driving systems (ADS) have been an active area of research, with the potential to deliver significant benefits to society. However, before large-scale deployment on public roads, extensive testing is necessary to validate their functionality and safety under diverse driving conditions. Therefore, different testing approaches are required, and achieving effective and efficient testing of ADS remains an open challenge. Recently, generative AI has emerged as a powerful tool across many domains, and it is increasingly being applied to ADS testing due to its ability to interpret context, reason about complex tasks, and generate diverse outputs. To gain a deeper understanding of its role in ADS testing, we systematically analyzed 91 relevant studies and synthesized their findings into six major application categories, primarily centered on scenario-based testing of ADS. We also reviewed their effectiveness and compiled a wide range of datasets, simulators, ADS, metrics, and benchmarks used for evaluation, while identifying 27 limitations. This survey provides an overview and practical insights into the use of generative AI for testing ADS, highlights existing challenges, and outlines directions for future research in this rapidly evolving field.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:40:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19882v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19882v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 SinLlama -- A Large Language Model for Sinhala</h2>
                <div class="authors">
                    <strong>Authors:</strong> H. W. K. Aravinda, Rashad Sirajudeen, Samith Karunathilake, Nisansa de Silva, Surangika Ranathunga, Rishemjit Kaur
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Low-resource languages such as Sinhala are often overlooked by open-source Large Language Models (LLMs). In this research, we extend an existing multilingual LLM (Llama-3-8B) to better serve Sinhala. We enhance the LLM tokenizer with Sinhala specific vocabulary and perform continual pre-training on a cleaned 10 million Sinhala corpus, resulting in the SinLlama model. This is the very first decoder-based open-source LLM with explicit Sinhala support. When SinLlama was instruction fine-tuned for three text classification tasks, it outperformed base and instruct variants of Llama-3-8B by a significant margin.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:39:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.09115v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.09115v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 Secure Multi-LLM Agentic AI and Agentification for Edge General
  Intelligence by Zero-Trust: A Survey</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yinqiu Liu, Ruichen Zhang, Haoxiang Luo, Yijing Lin, Geng Sun, Dusit Niyato, Hongyang Du, Zehui Xiong, Yonggang Wen, Abbas Jamalipour, Dong In Kim, Ping Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Agentification serves as a critical enabler of Edge General Intelligence (EGI), transforming massive edge devices into cognitive agents through integrating Large Language Models (LLMs) and perception, reasoning, and acting modules. These agents collaborate across heterogeneous edge infrastructures, forming multi-LLM agentic AI systems that leverage collective intelligence and specialized capabilities to tackle complex, multi-step tasks. However, the collaborative nature of multi-LLM systems introduces critical security vulnerabilities, including insecure inter-LLM communications, expanded attack surfaces, and cross-domain data leakage that traditional perimeter-based security cannot adequately address. To this end, this survey introduces zero-trust security of multi-LLM in EGI, a paradigmatic shift following the ``never trust, always verify'' principle. We begin by systematically analyzing the security risks in multi-LLM systems within EGI contexts. Subsequently, we present the vision of a zero-trust multi-LLM framework in EGI. We then survey key technical progress to facilitate zero-trust multi-LLM systems in EGI. Particularly, we categorize zero-trust security mechanisms into model- and system-level approaches. The former and latter include strong identification, context-aware access control, etc., and proactive maintenance, blockchain-based management, etc., respectively. Finally, we identify critical research directions. This survey serves as the first systematic treatment of zero-trust applied to multi-LLM systems, providing both theoretical foundations and practical strategies.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:33:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19870v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19870v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 Hydra: Structured Cross-Source Enhanced Large Language Model Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Xin Yuan, Liming Zhu, Wenjie Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating external knowledge. Current hybrid RAG system retrieves evidence from both knowledge graphs (KGs) and text documents to support LLM reasoning. However, it faces challenges like handling multi-hop reasoning, multi-entity questions, multi-source verification, and effective graph utilization. To address these limitations, we present Hydra, a training-free framework that unifies graph topology, document semantics, and source reliability to support deep, faithful reasoning in LLMs. Hydra handles multi-hop and multi-entity problems through agent-driven exploration that combines structured and unstructured retrieval, increasing both diversity and precision of evidence. To tackle multi-source verification, Hydra uses a tri-factor cross-source verification (source trustworthiness assessment, cross-source corroboration, and entity-path alignment), to balance topic relevance with cross-modal agreement. By leveraging graph structure, Hydra fuses heterogeneous sources, guides efficient exploration, and prunes noise early. Comprehensive experiments on seven benchmark datasets show that Hydra achieves overall state-of-the-art results on all benchmarks with GPT-3.5, outperforming the strong hybrid baseline ToG-2 by an average of 20.3% and up to 30.1%. Furthermore, Hydra enables smaller models (e.g., Llama-3.1-8B) to achieve reasoning performance comparable to that of GPT-4-Turbo. The source code is available on https://stevetantan.github.io/Hydra/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:30:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.17464v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.17464v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 Input-Time Scaling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rapheal Huang, Weilong Guo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Current Large Language Models (LLMs) are usually post-trained on large-scale carefully curated datasets (data & training scaling) and doing reasoning in test time (inference time scaling). In this work, we present a new scaling paradigm, Input-Time Scaling, to complement previous scaling methods by putting resources on queries (input time). During training and testing, we utilize meta-knowledge from LLMs to refine inputs with different strategies. We also discover a new phenomenon, train-test co-design. It requires us to apply query strategies during training and testing as a whole. Only applying strategies on training or testing would seriously degrade the performance gained. We are also surprised to find that seemingly low data quality datasets can perform better. We can get the best performance even by adding irrelevant information to the queries, with randomly selected 1k examples from a minimally filtered dataset. These findings contradict the widely held inductive bias, "garbage in, garbage out". Curating datasets with seemingly high-quality data can even potentially limit the performance ceiling. In addition, models trained on more data with similar quality (15k VS 1k) perform worse, the intuition of simply scaling the size should also be carefully inspected. The good news is that our findings are compatible with the Less is More phenomenon. 1K examples are enough to invoke high-level reasoning ability. With experiments on Qwen2.5-32B-Instruct, we are able to reach SOTA performance among 32B models on AIME24(76.7%) and AIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with a majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B, the result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate reproducibility and further research, we are working on open-source our datasets, data pipelines, evaluation results, and checkpoints.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:27:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.13654v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.13654v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 Multimodal Conditional MeshGAN for Personalized Aneurysm Growth
  Prediction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Long Chen, Ashiv Patel, Mengyun Qiao, Mohammad Yousuf Salmasi, Salah A. Hammouche, Vasilis Stavrinides, Jasleen Nagi, Soodeh Kalaie, Xiao Yun Xu, Wenjia Bai, Declan P. O'Regan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Personalized, accurate prediction of aortic aneurysm progression is essential for timely intervention but remains challenging due to the need to model both subtle local deformations and global anatomical changes within complex 3D geometries. We propose MCMeshGAN, the first multimodal conditional mesh-to-mesh generative adversarial network for 3D aneurysm growth prediction. MCMeshGAN introduces a dual-branch architecture combining a novel local KNN-based convolutional network (KCN) to preserve fine-grained geometric details and a global graph convolutional network (GCN) to capture long-range structural context, overcoming the over-smoothing limitations of deep GCNs. A dedicated condition branch encodes clinical attributes (age, sex) and the target time interval to generate anatomically plausible, temporally controlled predictions, enabling retrospective and prospective modeling. We curated TAAMesh, a new longitudinal thoracic aortic aneurysm mesh dataset consisting of 590 multimodal records (CT scans, 3D meshes, and clinical data) from 208 patients. Extensive experiments demonstrate that MCMeshGAN consistently outperforms state-of-the-art baselines in both geometric accuracy and clinically important diameter estimation. This framework offers a robust step toward clinically deployable, personalized 3D disease trajectory modeling. The source code for MCMeshGAN and the baseline methods is publicly available at https://github.com/ImperialCollegeLondon/MCMeshGAN.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:25:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19862v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19862v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 When AIOps Become "AI Oops": Subverting LLM-driven IT Operations via
  Telemetry Manipulation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dario Pasquini, Evgenios M. Kornaropoulos, Giuseppe Ateniese, Omer Akgul, Athanasios Theocharis, Petros Efstathopoulos
                </div>
                <div class="summary">
                    <strong>Summary:</strong> AI for IT Operations (AIOps) is transforming how organizations manage complex software systems by automating anomaly detection, incident diagnosis, and remediation. Modern AIOps solutions increasingly rely on autonomous LLM-based agents to interpret telemetry data and take corrective actions with minimal human intervention, promising faster response times and operational cost savings.   In this work, we perform the first security analysis of AIOps solutions, showing that, once again, AI-driven automation comes with a profound security cost. We demonstrate that adversaries can manipulate system telemetry to mislead AIOps agents into taking actions that compromise the integrity of the infrastructure they manage. We introduce techniques to reliably inject telemetry data using error-inducing requests that influence agent behavior through a form of adversarial reward-hacking; plausible but incorrect system error interpretations that steer the agent's decision-making. Our attack methodology, AIOpsDoom, is fully automated--combining reconnaissance, fuzzing, and LLM-driven adversarial input generation--and operates without any prior knowledge of the target system.   To counter this threat, we propose AIOpsShield, a defense mechanism that sanitizes telemetry data by exploiting its structured nature and the minimal role of user-generated content. Our experiments show that AIOpsShield reliably blocks telemetry-based attacks without affecting normal agent performance.   Ultimately, this work exposes AIOps as an emerging attack vector for system compromise and underscores the urgent need for security-aware AIOps design.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:14:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.06394v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.06394v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 Youtu-GraphRAG: Vertically Unified Agents for Graph Retrieval-Augmented
  Complex Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junnan Dong, Siyu An, Yifei Yu, Qian-Wen Zhang, Linhao Luo, Xiao Huang, Yunsheng Wu, Di Yin, Xing Sun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Graph retrieval-augmented generation (GraphRAG) has effectively enhanced large language models in complex reasoning by organizing fragmented knowledge into explicitly structured graphs. Prior efforts have been made to improve either graph construction or graph retrieval in isolation, yielding suboptimal performance, especially when domain shifts occur. In this paper, we propose a vertically unified agentic paradigm, Youtu-GraphRAG, to jointly connect the entire framework as an intricate integration. Specifically, (i) a seed graph schema is introduced to bound the automatic extraction agent with targeted entity types, relations and attribute types, also continuously expanded for scalability over unseen domains; (ii) To obtain higher-level knowledge upon the schema, we develop novel dually-perceived community detection, fusing structural topology with subgraph semantics for comprehensive knowledge organization. This naturally yields a hierarchical knowledge tree that supports both top-down filtering and bottom-up reasoning with community summaries; (iii) An agentic retriever is designed to interpret the same graph schema to transform complex queries into tractable and parallel sub-queries. It iteratively performs reflection for more advanced reasoning; (iv) To alleviate the knowledge leaking problem in pre-trained LLM, we propose a tailored anonymous dataset and a novel 'Anonymity Reversion' task that deeply measures the real performance of the GraphRAG frameworks. Extensive experiments across six challenging benchmarks demonstrate the robustness of Youtu-GraphRAG, remarkably moving the Pareto frontier with up to 90.71% saving of token costs and 16.62% higher accuracy over state-of-the-art baselines. The results indicate our adaptability, allowing seamless domain transfer with minimal intervention on schema.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:13:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19855v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19855v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 Tracking World States with Language Models: State-Based Evaluation Using
  Chess</h2>
                <div class="authors">
                    <strong>Authors:</strong> Romain Harang, Jason Naradowsky, Yaswitha Gujju, Yusuke Miyao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) exhibit emergent capabilities in structured domains, suggesting they may implicitly internalize high-fidelity representations of world models. While probing techniques have shown promising signs of this in scientific and game-based settings, they rely on model-specific internal activations, which limit interpretability and generalizability. In this work, we propose a model-agnostic, state-based evaluation framework using chess as a benchmark to assess whether LLMs preserve the semantics of structured environments. Our method analyzes the downstream legal move distributions (state affordances) to estimate semantic fidelity between predicted and actual game states. This approach offers a more meaningful evaluation than conventional string-based metrics by aligning more closely with the strategic and rule-governed nature of chess. Experimental results demonstrate that our metrics capture deficiencies in state-tracking, highlighting limitations of LLMs in maintaining coherent internal models over long sequences. Our framework provides a robust tool for evaluating structured reasoning in LLMs without requiring internal model access, and generalizes to a wide class of symbolic environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T13:08:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19851v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19851v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 SoK: Large Language Model Copyright Auditing via Fingerprinting</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shuo Shao, Yiming Li, Yu He, Hongwei Yao, Wenyuan Yang, Dacheng Tao, Zhan Qin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The broad capabilities and substantial resources required to train Large Language Models (LLMs) make them valuable intellectual property, yet they remain vulnerable to copyright infringement, such as unauthorized use and model theft. LLM fingerprinting, a non-intrusive technique that extracts and compares the distinctive features from LLMs to identify infringements, offers a promising solution to copyright auditing. However, its reliability remains uncertain due to the prevalence of diverse model modifications and the lack of standardized evaluation. In this SoK, we present the first comprehensive study of LLM fingerprinting. We introduce a unified framework and formal taxonomy that categorizes existing methods into white-box and black-box approaches, providing a structured overview of the state of the art. We further propose LeaFBench, the first systematic benchmark for evaluating LLM fingerprinting under realistic deployment scenarios. Built upon mainstream foundation models and comprising 149 distinct model instances, LeaFBench integrates 13 representative post-development techniques, spanning both parameter-altering methods (e.g., fine-tuning, quantization) and parameter-independent mechanisms (e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the strengths and weaknesses of existing methods, thereby outlining future research directions and critical open problems in this emerging field. The code is available at https://github.com/shaoshuo-ss/LeaFBench.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T12:56:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19843v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19843v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hua Farn, Hsuan Su, Shachi H Kumar, Saurav Sahay, Shang-Tse Chen, Hung-yi Lee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Fine-tuning large language models (LLMs) for downstream tasks often leads to catastrophic forgetting, notably degrading the safety of originally aligned models. While some existing methods attempt to restore safety by incorporating additional safety data, the quality of such data typically falls short of that used in the original alignment process. Moreover, these high-quality safety datasets are generally inaccessible, making it difficult to fully recover the model's original safety. We ask: How can we preserve safety while improving downstream task performance without additional safety data? We show that simply merging the weights of pre- and post-fine-tuned models effectively mitigates safety degradation while enhancing performance. Experiments across different downstream tasks and models validate the method's practicality and effectiveness.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T01:13:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.19512v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.19512v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 Benchmarking Hindi LLMs: A New Suite of Datasets and a Comparative
  Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anusha Kamath, Kanishk Singla, Rakesh Paul, Raviraj Joshi, Utkarsh Vaidya, Sanjay Singh Chauhan, Niranjan Wartikar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Evaluating instruction-tuned Large Language Models (LLMs) in Hindi is challenging due to a lack of high-quality benchmarks, as direct translation of English datasets fails to capture crucial linguistic and cultural nuances. To address this, we introduce a suite of five Hindi LLM evaluation datasets: IFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, and BFCL-Hi. These were created using a methodology that combines from-scratch human annotation with a translate-and-verify process. We leverage this suite to conduct an extensive benchmarking of open-source LLMs supporting Hindi, providing a detailed comparative analysis of their current capabilities. Our curation process also serves as a replicable methodology for developing benchmarks in other low-resource languages.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T12:35:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19831v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19831v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize
  Memories via Reinforcement Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sikuan Yan, Xiufeng Yang, Zuchao Huang, Ercong Nie, Zifeng Ding, Zonggen Li, Xiaowen Ma, Hinrich Schütze, Volker Tresp, Yunpu Ma
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of NLP tasks, but they remain fundamentally stateless, constrained by limited context windows that hinder long-horizon reasoning. Recent efforts to address this limitation often augment LLMs with an external memory bank, yet most existing pipelines are static and heuristic-driven, lacking any learned mechanism for deciding what to store, update, or retrieve. We present Memory-R1, a reinforcement learning (RL) framework that equips LLMs with the ability to actively manage and utilize external memory through two specialized agents: a Memory Manager that learns to perform structured memory operations {ADD, UPDATE, DELETE, NOOP}, and an Answer Agent that selects the most relevant entries and reasons over them to produce an answer. Both agents are fine-tuned with outcome-driven RL (PPO and GRPO), enabling adaptive memory management and use with minimal supervision. With as few as 152 question-answer pairs and a corresponding temporal memory bank for training, Memory-R1 outperforms the most competitive existing baseline and demonstrates strong generalization across diverse question types and LLM backbones. Beyond presenting an effective approach, this work provides insights into how RL can unlock more agentic, memory-aware behaviors in LLMs, pointing toward richer, more persistent reasoning systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T12:26:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19828v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19828v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 X-Prompt: Towards Universal In-Context Image Generation in
  Auto-Regressive Vision Language Foundation Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zeyi Sun, Ziyang Chu, Pan Zhang, Tong Wu, Xiaoyi Dong, Yuhang Zang, Yuanjun Xiong, Dahua Lin, Jiaqi Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In-context generation is a key component of large language models' (LLMs) open-task generalization capability. By leveraging a few examples as context, LLMs can perform both in-domain and out-of-domain tasks. Recent advancements in auto-regressive vision-language models (VLMs) built upon LLMs have showcased impressive performance in text-to-image generation. However, the potential of in-context learning for general image generation tasks remains largely unexplored. To address this, we introduce X-Prompt, a purely auto-regressive large-vision language model designed to deliver competitive performance across a wide range of both seen and unseen image generation tasks, all within a unified in-context learning framework. X-Prompt incorporates a specialized design that efficiently compresses valuable features from in-context examples, supporting longer in-context token sequences and improving its ability to generalize to unseen tasks. A unified training task for both text and image prediction enables X-Prompt to handle general image generation with enhanced task awareness from in-context examples. Extensive experiments validate the model's performance across diverse seen image generation tasks and its capacity to generalize to previously unseen tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T12:26:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.LG</span><span>cs.MM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.01824v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.01824v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhiyuan Ning, Tianle Gu, Jiaxin Song, Shixin Hong, Lingyu Li, Huacan Liu, Jie Li, Yixu Wang, Meng Lingyu, Yan Teng, Yingchun Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The widespread adoption and increasing prominence of large language models (LLMs) in global technologies necessitate a rigorous focus on ensuring their safety across a diverse range of linguistic and cultural contexts. The lack of a comprehensive evaluation and diverse data in existing multilingual safety evaluations for LLMs limits their effectiveness, hindering the development of robust multilingual safety alignment. To address this critical gap, we introduce LinguaSafe, a comprehensive multilingual safety benchmark crafted with meticulous attention to linguistic authenticity. The LinguaSafe dataset comprises 45k entries in 12 languages, ranging from Hungarian to Malay. Curated using a combination of translated, transcreated, and natively-sourced data, our dataset addresses the critical need for multilingual safety evaluations of LLMs, filling the void in the safety evaluation of LLMs across diverse under-represented languages from Hungarian to Malay. LinguaSafe presents a multidimensional and fine-grained evaluation framework, with direct and indirect safety assessments, including further evaluations for oversensitivity. The results of safety and helpfulness evaluations vary significantly across different domains and different languages, even in languages with similar resource levels. Our benchmark provides a comprehensive suite of metrics for in-depth safety evaluation, underscoring the critical importance of thoroughly assessing multilingual safety in LLMs to achieve more balanced safety alignment. Our dataset and code are released to the public to facilitate further research in the field of multilingual LLM safety.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T12:21:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.12733v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.12733v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 From Research to Reality: Feasibility of Gradient Inversion Attacks in
  Federated Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Viktor Valadi, Mattias Åkesson, Johan Östman, Salman Toor, Andreas Hellander
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Gradient inversion attacks have garnered attention for their ability to compromise privacy in federated learning. However, many studies consider attacks with the model in inference mode, where training-time behaviors like dropout are disabled and batch normalization relies on fixed statistics. In this work, we systematically analyze how architecture and training behavior affect vulnerability, including the first in-depth study of inference-mode clients, which we show dramatically simplifies inversion. To assess attack feasibility under more realistic conditions, we turn to clients operating in standard training mode. In this setting, we find that successful attacks are only possible when several architectural conditions are met simultaneously: models must be shallow and wide, use skip connections, and, critically, employ pre-activation normalization. We introduce two novel attacks against models in training-mode with varying attacker knowledge, achieving state-of-the-art performance under realistic training conditions. We extend these efforts by presenting the first attack on a production-grade object-detection model. Here, to enable any visibly identifiable leakage, we revert to the lenient inference mode setting and make multiple architectural modifications to increase model vulnerability, with the extent of required changes highlighting the strong inherent robustness of such architectures. We conclude this work by offering the first comprehensive mapping of settings, clarifying which combinations of architectural choices and operational modes meaningfully impact privacy. Our analysis provides actionable insight into when models are likely vulnerable, when they appear robust, and where subtle leakage may persist. Together, these findings reframe how gradient inversion risk should be assessed in future research and deployment scenarios.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T12:07:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19819v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19819v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 T2R-bench: A Benchmark for Generating Article-Level Reports from Real
  World Industrial Tables</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jie Zhang, Changzai Pan, Kaiwen Wei, Sishi Xiong, Yu Zhao, Xiangyu Li, Jiaxin Peng, Xiaoyan Gu, Jian Yang, Wenhan Chang, Zhenhe Wu, Jiang Zhong, Shuangyong Song, Yongxiang Li, Xuelong Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Extensive research has been conducted to explore the capabilities of large language models (LLMs) in table reasoning. However, the essential task of transforming tables information into reports remains a significant challenge for industrial applications. This task is plagued by two critical issues: 1) the complexity and diversity of tables lead to suboptimal reasoning outcomes; and 2) existing table benchmarks lack the capacity to adequately assess the practical application of this task. To fill this gap, we propose the table-to-report task and construct a bilingual benchmark named T2R-bench, where the key information flow from the tables to the reports for this task. The benchmark comprises 457 industrial tables, all derived from real-world scenarios and encompassing 19 industry domains as well as 4 types of industrial tables. Furthermore, we propose an evaluation criteria to fairly measure the quality of report generation. The experiments on 25 widely-used LLMs reveal that even state-of-the-art models like Deepseek-R1 only achieves performance with 62.71 overall score, indicating that LLMs still have room for improvement on T2R-bench. Source code and data will be available after acceptance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T11:55:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19813v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19813v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 Bootstrapping Learned Cost Models with Synthetic SQL Queries</h2>
                <div class="authors">
                    <strong>Authors:</strong> Michael Nidd, Christoph Miksovic, Thomas Gschwind, Francesco Fusco, Andrea Giovannini, Ioana Giurgiu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Having access to realistic workloads for a given database instance is extremely important to enable stress and vulnerability testing, as well as to optimize for cost and performance. Recent advances in learned cost models have shown that when enough diverse SQL queries are available, one can effectively and efficiently predict the cost of running a given query against a specific database engine. In this paper, we describe our experience in exploiting modern synthetic data generation techniques, inspired by the generative AI and LLM community, to create high-quality datasets enabling the effective training of such learned cost models. Initial results show that we can improve a learned cost model's predictive accuracy by training it with 45% fewer queries than when using competitive generation approaches.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T11:50:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19807v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19807v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 A bag of tricks for real-time Mitotic Figure detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Christian Marzahl, Brian Napora
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Mitotic figure (MF) detection in histopathology images is challenging due to large variations in slide scanners, staining protocols, tissue types, and the presence of artifacts. This paper presents a collection of training techniques - a bag of tricks - that enable robust, real-time MF detection across diverse domains. We build on the efficient RTMDet single stage object detector to achieve high inference speed suitable for clinical deployment. Our method addresses scanner variability and tumor heterogeneity via extensive multi-domain training data, balanced sampling, and careful augmentation. Additionally, we employ targeted, hard negative mining on necrotic and debris tissue to reduce false positives. In a grouped 5-fold cross-validation across multiple MF datasets, our model achieves an F1 score between 0.78 and 0.84. On the preliminary test set of the MItosis DOmain Generalization (MIDOG) 2025 challenge, our single-stage RTMDet-S based approach reaches an F1 of 0.81, outperforming larger models and demonstrating adaptability to new, unfamiliar domains. The proposed solution offers a practical trade-off between accuracy and speed, making it attractive for real-world clinical adoption.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T11:45:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19804v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19804v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 Agent-as-Judge for Factual Summarization of Long Narratives</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yeonseok Jeong, Minsoo Kim, Seung-won Hwang, Byung-Hak Kim
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have demonstrated near-human performance in summarization tasks based on traditional metrics such as ROUGE and BERTScore. However, these metrics do not adequately capture critical aspects of summarization quality, such as factual accuracy, particularly for long narratives (>100K tokens). Recent advances, such as LLM-as-a-Judge, address the limitations of metrics based on lexical similarity but still exhibit factual inconsistencies, especially in understanding character relationships and states. In this work, we introduce NarrativeFactScore, a novel "Agent-as-a-Judge" framework for evaluating and refining summaries. By leveraging a Character Knowledge Graph (CKG) extracted from input and generated summaries, NarrativeFactScore assesses the factual consistency and provides actionable guidance for refinement, such as identifying missing or erroneous facts. We demonstrate the effectiveness of NarrativeFactScore through a detailed workflow illustration and extensive validation on widely adopted benchmarks, achieving superior performance compared to competitive methods. Our results highlight the potential of agent-driven evaluation systems to improve the factual reliability of LLM-generated summaries.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T11:18:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.09993v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.09993v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 Demonstrating specification gaming in reasoning models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alexander Bondarenko, Denis Volk, Dmitrii Volkov, Jeffrey Ladish
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We demonstrate LLM agent specification gaming by instructing models to win against a chess engine. We find reasoning models like OpenAI o3 and DeepSeek R1 will often hack the benchmark by default, while language models like GPT-4o and Claude 3.5 Sonnet need to be told that normal play won't work to hack.   We improve upon prior work like (Hubinger et al., 2024; Meinke et al., 2024; Weij et al., 2024) by using realistic task prompts and avoiding excess nudging. Our results suggest reasoning models may resort to hacking to solve difficult problems, as observed in OpenAI (2024)'s o1 Docker escape during cyber capabilities testing.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T11:15:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.13295v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.13295v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 Context-Aware Risk Estimation in Home Environments: A Probabilistic
  Framework for Service Robots</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sena Ishii, Akash Chikhalikar, Ankit A. Ravankar, Jose Victorio Salazar Luces, Yasuhisa Hirata
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present a novel framework for estimating accident-prone regions in everyday indoor scenes, aimed at improving real-time risk awareness in service robots operating in human-centric environments. As robots become integrated into daily life, particularly in homes, the ability to anticipate and respond to environmental hazards is crucial for ensuring user safety, trust, and effective human-robot interaction. Our approach models object-level risk and context through a semantic graph-based propagation algorithm. Each object is represented as a node with an associated risk score, and risk propagates asymmetrically from high-risk to low-risk objects based on spatial proximity and accident relationship. This enables the robot to infer potential hazards even when they are not explicitly visible or labeled. Designed for interpretability and lightweight onboard deployment, our method is validated on a dataset with human-annotated risk regions, achieving a binary risk detection accuracy of 75%. The system demonstrates strong alignment with human perception, particularly in scenes involving sharp or unstable objects. These results underline the potential of context-aware risk reasoning to enhance robotic scene understanding and proactive safety behaviors in shared human-robot spaces. This framework could serve as a foundation for future systems that make context-driven safety decisions, provide real-time alerts, or autonomously assist users in avoiding or mitigating hazards within home environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T11:14:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19788v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19788v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 Error dependencies in the space-based CNEOS fireball database</h2>
                <div class="authors">
                    <strong>Authors:</strong> Eloy Peña-Asensio, Hector Socas-Navarro, Darryl Z. Seligman
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We evaluate the reliability of CNEOS-derived ephemerides of fireball events given the absence of the underlying data. We analyzed 18 events that have both (i) sufficient satellite information to derive orbits and (ii) ground-based observational counterparts. We quantify the uncertainties on these calibrated events using the orbital similarity criterion $D_D$. We also examine the velocity components imbalance and identify discriminants that can indicate the accuracy of an event. We identify two groups in the CNEOS database. CNEOS data produces ephemeris determinations with $D_D$<0.1 for fireballs reported either (i) after late 2017 or (ii) with impact energies above 0.45 kt with 74-78% of events having $D_D$=0.03$\pm$0.02, while ~11% show $D_D$<0.008. Our statistical test confirms these two parameters as the only reliable discriminants that, when combined, explain the two accuracy groups. Daylight, z-velocity component, low altitude, long duration, and latitude might also indicate errors, although the limited dataset may obscure correlations. No clear discriminants are identified for more restrictive $D_D$ cut-offs. We provide estimates of orbital uncertainties for calibrated events. The hyperbolic fireball subset in the CNEOS database appears as an outlier in the velocity imbalance test. Our results confirm that the fidelity of CNEOS fireball data improved significantly from 2018, likely due to the deployment of next-generation space sensors, and show a growing number of high-velocity events. Hyperbolic candidates should be interpreted with caution, as their velocities and inclinations likely reflect measurement errors. Accuracy constraints remain limited by the dataset size, as evidenced by the lack of statistically significant dependence on duration, preventing strong conclusions from being drawn.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T11:11:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.EP</span><span>astro-ph.IM</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1051/0004-6361/202554224' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.01454v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.01454v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large
  Language Models via a Multi-Paradigm Perspective</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yiyao Yu, Yuxiang Zhang, Dongdong Zhang, Xiao Liang, Hengyuan Zhang, Xingxing Zhang, Mahmoud Khademi, Hany Awadalla, Junjie Wang, Yujiu Yang, Furu Wei
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have made notable progress in mathematical reasoning, yet often rely on single-paradigm reasoning, limiting their effectiveness across diverse tasks. We introduce Chain-of-Reasoning (CoR), a novel unified framework integrating multiple reasoning paradigms--Natural Language Reasoning (NLR), Algorithmic Reasoning (AR), and Symbolic Reasoning (SR)--to enable synergistic collaboration. CoR generates multiple potential answers via different reasoning paradigms and synthesizes them into a coherent final solution. We propose a Progressive Paradigm Training (PPT) strategy for models to progressively master these paradigms, leading to CoR-Math-7B. Experimental results demonstrate that CoR-Math-7B significantly outperforms current SOTA models, achieving up to a 41.0% absolute improvement over GPT-4o in theorem proving and a 15.0% improvement over RL-based methods on the MATH benchmark in arithmetic tasks. These results show the enhanced mathematical comprehension ability of our model, enabling zero-shot generalization across tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T10:56:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.11110v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.11110v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 Utility-Focused LLM Annotation for Retrieval and Retrieval-Augmented
  Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hengran Zhang, Minghao Tang, Keping Bi, Jiafeng Guo, Shihao Liu, Daiting Shi, Dawei Yin, Xueqi Cheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper explores the use of large language models (LLMs) for annotating document utility in training retrieval and retrieval-augmented generation (RAG) systems, aiming to reduce dependence on costly human annotations. We address the gap between retrieval relevance and generative utility by employing LLMs to annotate document utility. To effectively utilize multiple positive samples per query, we introduce a novel loss that maximizes their summed marginal likelihood. Using the Qwen-2.5-32B model, we annotate utility on the MS MARCO dataset and conduct retrieval experiments on MS MARCO and BEIR, as well as RAG experiments on MS MARCO QA, NQ, and HotpotQA. Our results show that LLM-generated annotations enhance out-of-domain retrieval performance and improve RAG outcomes compared to models trained solely on human annotations or downstream QA metrics. Furthermore, combining LLM annotations with just 20% of human labels achieves performance comparable to using full human annotations. Our study offers a comprehensive approach to utilizing LLM annotations for initializing QA systems on new corpora.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T10:51:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.05220v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.05220v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 Principled Personas: Defining and Measuring the Intended Effects of
  Persona Prompting on Task Performance</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pedro Henrique Luz de Araujo, Paul Röttger, Dirk Hovy, Benjamin Roth
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Expert persona prompting -- assigning roles such as expert in math to language models -- is widely used for task improvement. However, prior work shows mixed results on its effectiveness, and does not consider when and why personas should improve performance. We analyze the literature on persona prompting for task improvement and distill three desiderata: 1) performance advantage of expert personas, 2) robustness to irrelevant persona attributes, and 3) fidelity to persona attributes. We then evaluate 9 state-of-the-art LLMs across 27 tasks with respect to these desiderata. We find that expert personas usually lead to positive or non-significant performance changes. Surprisingly, models are highly sensitive to irrelevant persona details, with performance drops of almost 30 percentage points. In terms of fidelity, we find that while higher education, specialization, and domain-relatedness can boost performance, their effects are often inconsistent or negligible across tasks. We propose mitigation strategies to improve robustness -- but find they only work for the largest, most capable models. Our findings underscore the need for more careful persona design and for evaluation schemes that reflect the intended effects of persona usage.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T10:40:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19764v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19764v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 DiffArtist: Towards Structure and Appearance Controllable Image
  Stylization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruixiang Jiang, Changwen Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Artistic styles are defined by both their structural and appearance elements. Existing neural stylization techniques primarily focus on transferring appearance-level features such as color and texture, often neglecting the equally crucial aspect of structural stylization. To address this gap, we introduce \textbf{DiffArtist}, the first 2D stylization method to offer fine-grained, simultaneous control over both structure and appearance style strength. This dual controllability is achieved by representing structure and appearance generation as separate diffusion processes, necessitating no further tuning or additional adapters. To properly evaluate this new capability of dual stylization, we further propose a Multimodal LLM-based stylization evaluator that aligns significantly better with human preferences than existing metrics. Extensive analysis shows that DiffArtist achieves superior style fidelity and dual-controllability compared to state-of-the-art methods. Its text-driven, training-free design and unprecedented dual controllability make it a powerful and interactive tool for various creative applications. Project homepage: https://diffusionartist.github.io.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T10:30:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.GR</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3746027.3755010' target='_blank'>doi</a><a href='http://arxiv.org/abs/2407.15842v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.15842v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 Governance-as-a-Service: A Multi-Agent Framework for AI System
  Compliance and Policy Enforcement</h2>
                <div class="authors">
                    <strong>Authors:</strong> Suyash Gaurav, Jukka Heikkonen, Jatin Chaudhary
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As AI systems evolve into distributed ecosystems with autonomous execution, asynchronous reasoning, and multi-agent coordination, the absence of scalable, decoupled governance poses a structural risk. Existing oversight mechanisms are reactive, brittle, and embedded within agent architectures, making them non-auditable and hard to generalize across heterogeneous deployments.   We introduce Governance-as-a-Service (GaaS): a modular, policy-driven enforcement layer that regulates agent outputs at runtime without altering model internals or requiring agent cooperation. GaaS employs declarative rules and a Trust Factor mechanism that scores agents based on compliance and severity-weighted violations. It enables coercive, normative, and adaptive interventions, supporting graduated enforcement and dynamic trust modulation.   To evaluate GaaS, we conduct three simulation regimes with open-source models (LLaMA3, Qwen3, DeepSeek-R1) across content generation and financial decision-making. In the baseline, agents act without governance; in the second, GaaS enforces policies; in the third, adversarial agents probe robustness. All actions are intercepted, evaluated, and logged for analysis. Results show that GaaS reliably blocks or redirects high-risk behaviors while preserving throughput. Trust scores track rule adherence, isolating and penalizing untrustworthy components in multi-agent systems.   By positioning governance as a runtime service akin to compute or storage, GaaS establishes infrastructure-level alignment for interoperable agent ecosystems. It does not teach agents ethics; it enforces them.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T10:16:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18765v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18765v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 Spotlight Attention: Towards Efficient LLM Generation via Non-linear
  Hashing-based KV Cache Retrieval</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wenhao Li, Yuxin Zhang, Gen Luo, Haiyuan Wan, Ziyang Gong, Fei Chao, Rongrong Ji
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reducing the key-value (KV) cache burden in Large Language Models (LLMs) significantly accelerates inference. Dynamically selecting critical KV caches during decoding helps maintain performance. Existing methods use random linear hashing to identify important tokens, but this approach is inefficient due to the orthogonal distribution of queries and keys within two narrow cones in LLMs. We introduce Spotlight Attention, a novel method that employs non-linear hashing functions to optimize the embedding distribution of queries and keys, enhancing coding efficiency and robustness. We also developed a lightweight, stable training framework using a Bradley-Terry ranking-based loss, enabling optimization of the non-linear hashing module on GPUs with 16GB memory in 8 hours. Experimental results show that Spotlight Attention drastically improves retrieval precision while shortening the length of the hash code at least 5$\times$ compared to traditional linear hashing. Finally, we exploit the computational advantages of bitwise operations by implementing specialized CUDA kernels, achieving hashing retrieval for 512K tokens in under 100$\mu$s on a single A100 GPU, with end-to-end throughput up to 3$\times$ higher than vanilla decoding.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T10:11:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19740v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19740v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 Experimental Insights from OpenAirInterface 5G positioning Testbeds:
  Challenges and solutions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mohsen Ahadi, Adeel Malik, Omid Esrafilian, Florian Kaltenberger, Cedric Thienot
                </div>
                <div class="summary">
                    <strong>Summary:</strong> 5G New Radio (NR) is a key enabler of accurate positioning in smart cities and smart factories. This paper presents the experimental results from three 5G positioning testbeds running open-source OpenAirInterface (OAI) gNB and Core Network (CN), using Uplink Time Difference of Arrival (UL-TDoA) with the newly integrated Location Management Function (LMF). The testbeds are deployed across both indoor factories and outdoor scenarios with O-RAN Radio Units (RUs), following a 3GPP-compliant system model. The experiments highlight the impact of synchronization impairments, multipath propagation, and deployment geometry on positioning accuracy. To address these challenges, we propose tailored ToA and TDoA filtering as well as a novel position estimation method based on Particle Swarm Optimization (PSO) within the LMF pipeline. Moreover, we show a beyond-5G framework that leverages non-conventional measurements such as Channel Impulse Response (CIR) to train and test Artificial Intelligence and Machine Learning (AI/ML) models for data-driven positioning. The results demonstrate the feasibility of achieving 1-2 meter positioning accuracy in 90% of cases in different testbeds, offering practical insights for the design of robust 5G positioning systems. Moreover, we publicly release the datasets collected in this work to support the research within the 5G positioning community.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T10:07:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19736v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19736v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 Efficient Human-Aware Task Allocation for Multi-Robot Systems in Shared
  Environments</h2>
                <div class="authors">
                    <strong>Authors:</strong> Maryam Kazemi Eskeri, Ville Kyrki, Dominik Baumann, Tomasz Piotr Kucner
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multi-robot systems are increasingly deployed in applications, such as intralogistics or autonomous delivery, where multiple robots collaborate to complete tasks efficiently. One of the key factors enabling their efficient cooperation is Multi-Robot Task Allocation (MRTA). Algorithms solving this problem optimize task distribution among robots to minimize the overall execution time. In shared environments, apart from the relative distance between the robots and the tasks, the execution time is also significantly impacted by the delay caused by navigating around moving people. However, most existing MRTA approaches are dynamics-agnostic, relying on static maps and neglecting human motion patterns, leading to inefficiencies and delays. In this paper, we introduce \acrfull{method name}. This method leverages Maps of Dynamics (MoDs), spatio-temporal queryable models designed to capture historical human movement patterns, to estimate the impact of humans on the task execution time during deployment. \acrshort{method name} utilizes a stochastic cost function that includes MoDs. Experimental results show that integrating MoDs enhances task allocation performance, resulting in reduced mission completion times by up to $26\%$ compared to the dynamics-agnostic method and up to $19\%$ compared to the baseline. This work underscores the importance of considering human dynamics in MRTA within shared environments and presents an efficient framework for deploying multi-robot systems in environments populated by humans.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T09:53:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19731v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19731v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 NLKI: A lightweight Natural Language Knowledge Integration Framework for
  Improving Small VLMs in Commonsense VQA Tasks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aritra Dutta, Swapnanil Mukherjee, Deepanway Ghosal, Somak Aditya
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Commonsense visual-question answering often hinges on knowledge that is missing from the image or the question. Small vision-language models (sVLMs) such as ViLT, VisualBERT and FLAVA therefore lag behind their larger generative counterparts. To study the effect of careful commonsense knowledge integration on sVLMs, we present an end-to-end framework (NLKI) that (i) retrieves natural language facts, (ii) prompts an LLM to craft natural language explanations, and (iii) feeds both signals to sVLMs respectively across two commonsense VQA datasets (CRIC, AOKVQA) and a visual-entailment dataset (e-SNLI-VE). Facts retrieved using a fine-tuned ColBERTv2 and an object information-enriched prompt yield explanations that largely cut down hallucinations, while lifting the end-to-end answer accuracy by up to 7% (across 3 datasets), making FLAVA and other models in NLKI match or exceed medium-sized VLMs such as Qwen-2 VL-2B and SmolVLM-2.5B. As these benchmarks contain 10-25% label noise, additional finetuning using noise-robust losses (such as symmetric cross entropy and generalised cross entropy) adds another 2.5% in CRIC, and 5.5% in AOKVQA. Our findings expose when LLM-based commonsense knowledge beats retrieval from commonsense knowledge bases, how noise-aware training stabilises small models in the context of external knowledge augmentation, and why parameter-efficient commonsense reasoning is now within reach for 250M models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T12:05:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19724v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19724v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 Continuously Steering LLMs Sensitivity to Contextual Knowledge with
  Proxy Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yilin Wang, Heng Wang, Yuyang Bai, Minnan Luo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In Large Language Models (LLMs) generation, there exist knowledge conflicts and scenarios where parametric knowledge contradicts knowledge provided in the context. Previous works studied tuning, decoding algorithms, or locating and editing context-aware neurons to adapt LLMs to be faithful to new contextual knowledge. However, they are usually inefficient or ineffective for large models, not workable for black-box models, or unable to continuously adjust LLMs' sensitivity to the knowledge provided in the context. To mitigate these problems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a simple framework that can steer LLMs' sensitivity to contextual knowledge continuously at a lightweight cost. Specifically, we tune two small LMs (i.e. proxy models) and use the difference in their output distributions to shift the original distribution of an LLM without modifying the LLM weights. In the evaluation process, we not only design synthetic data and fine-grained metrics to measure models' sensitivity to contextual knowledge but also use a real conflict dataset to validate CSKS's practical efficacy. Extensive experiments demonstrate that our framework achieves continuous and precise control over LLMs' sensitivity to contextual knowledge, enabling both increased sensitivity and reduced sensitivity, thereby allowing LLMs to prioritize either contextual or parametric knowledge as needed flexibly. Our data and code are available at https://github.com/OliveJuiceLin/CSKS.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T10:00:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19720v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19720v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 PhoniTale: Phonologically Grounded Mnemonic Generation for Typologically
  Distant Language Pairs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sana Kang, Myeongseok Gwon, Su Young Kwon, Jaewook Lee, Andrew Lan, Bhiksha Raj, Rita Singh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vocabulary acquisition poses a significant challenge for second-language (L2) learners, especially when learning typologically distant languages such as English and Korean, where phonological and structural mismatches complicate vocabulary learning. Recently, large language models (LLMs) have been used to generate keyword mnemonics by leveraging similar keywords from a learner's first language (L1) to aid in acquiring L2 vocabulary. However, most of this research has focused on native English speakers learning other languages, rather than the reverse. In this paper, we present PhoniTale, a novel cross-lingual mnemonic generation system that retrieves L1 keyword sequence based on phonological similarity and uses LLMs to generate mnemonics. We evaluate PhoniTale using both automated metrics and human evaluations, comparing its output to mnemonics created by humans and by previous automated approaches. To assess practical effectiveness, we also conduct a short-term recall test measuring mnemonic helpfulness. Our findings show that PhoniTale performs comparably to human-authored mnemonics. We also highlight key areas for future improvement in mnemonic quality and methodology.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T09:29:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05444v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05444v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 Edge Agentic AI Framework for Autonomous Network Optimisation in O-RAN</h2>
                <div class="authors">
                    <strong>Authors:</strong> Abdelaziz Salama, Zeinab Nezami, Mohammed M. H. Qazzaz, Maryam Hafeez, Syed Ali Raza Zaidi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The deployment of AI agents within legacy Radio Access Network (RAN) infrastructure poses significant safety and reliability challenges for future 6G networks. This paper presents a novel Edge AI framework for autonomous network optimisation in Open RAN environments, addressing these challenges through three core innovations: (1) a persona-based multi-tools architecture enabling distributed, context-aware decision-making; (2) proactive anomaly detection agent powered by traffic predictive tool; and (3) a safety, aligned reward mechanism that balances performance with operational stability. Integrated into the RAN Intelligent Controller (RIC), our framework leverages multimodal data fusion, including network KPIs, a traffic prediction model, and external information sources, to anticipate and respond to dynamic network conditions. Extensive evaluation using realistic 5G scenarios demonstrates that the edge framework achieves zero network outages under high-stress conditions, compared to 8.4% for traditional fixed-power networks and 3.3% for large language model (LLM) agent-based approaches, while maintaining near real-time responsiveness and consistent QoS. These results establish that, when equipped with the right tools and contextual awareness, AI agents can be safely and effectively deployed in critical network infrastructure, laying the framework for intelligent and autonomous 5G and beyond network operations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T09:28:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.21696v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.21696v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 Fitness Landscape of Large Language Model-Assisted Automated Algorithm
  Search</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fei Liu, Qingfu Zhang, Jialong Shi, Xialiang Tong, Kun Mao, Mingxuan Yuan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Using Large Language Models (LLMs) in an evolutionary or other iterative search framework have demonstrated significant potential in automated algorithm design. However, the underlying fitness landscape, which is critical for understanding its search behavior, remains underexplored. In this paper, we illustrate and analyze the fitness landscape of LLM-assisted Algorithm Search (LAS) using a graph-based approach, where nodes represent algorithms and edges denote transitions between them. We conduct extensive evaluations across six algorithm design tasks and six commonly-used LLMs. Our findings reveal that LAS landscapes are highly multimodal and rugged, particularly in combinatorial optimization tasks, with distinct structural variations across tasks and LLMs. Moreover, we adopt four different methods for algorithm similarity measurement and study their correlations to algorithm performance and operator behaviour. These insights not only deepen our understanding of LAS landscapes but also provide practical insights for designing more effective LAS methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T09:07:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.NE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.19636v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.19636v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 Safety Alignment Should Be Made More Than Just A Few Attention Heads</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chao Huang, Zefeng Zhang, Juewei Yue, Quangang Li, Chuang Zhang, Tingwen Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Current safety alignment for large language models(LLMs) continues to present vulnerabilities, given that adversarial prompting can effectively bypass their safety measures.Our investigation shows that these safety mechanisms predominantly depend on a limited subset of attention heads: removing or ablating these heads can severely compromise model safety. To identify and evaluate these safety-critical components, we introduce RDSHA, a targeted ablation method that leverages the model's refusal direction to pinpoint attention heads mostly responsible for safety behaviors. Further analysis shows that existing jailbreak attacks exploit this concentration by selectively bypassing or manipulating these critical attention heads. To address this issue, we propose AHD, a novel training strategy designed to promote the distributed encoding of safety-related behaviors across numerous attention heads. Experimental results demonstrate that AHD successfully distributes safety-related capabilities across more attention heads. Moreover, evaluations under several mainstream jailbreak attacks show that models trained with AHD exhibit considerably stronger safety robustness, while maintaining overall functional utility.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T09:06:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19697v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19697v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 Explain Before You Answer: A Survey on Compositional Visual Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fucai Ke, Joy Hsu, Zhixi Cai, Zixian Ma, Xin Zheng, Xindi Wu, Sukai Huang, Weiqing Wang, Pari Delir Haghighi, Gholamreza Haffari, Ranjay Krishna, Jiajun Wu, Hamid Rezatofighi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Compositional visual reasoning has emerged as a key research frontier in multimodal AI, aiming to endow machines with the human-like ability to decompose visual scenes, ground intermediate concepts, and perform multi-step logical inference. While early surveys focus on monolithic vision-language models or general multimodal reasoning, a dedicated synthesis of the rapidly expanding compositional visual reasoning literature is still missing. We fill this gap with a comprehensive survey spanning 2023 to 2025 that systematically reviews 260+ papers from top venues (CVPR, ICCV, NeurIPS, ICML, ACL, etc.). We first formalize core definitions and describe why compositional approaches offer advantages in cognitive alignment, semantic fidelity, robustness, interpretability, and data efficiency. Next, we trace a five-stage paradigm shift: from prompt-enhanced language-centric pipelines, through tool-enhanced LLMs and tool-enhanced VLMs, to recently minted chain-of-thought reasoning and unified agentic VLMs, highlighting their architectural designs, strengths, and limitations. We then catalog 60+ benchmarks and corresponding metrics that probe compositional visual reasoning along dimensions such as grounding accuracy, chain-of-thought faithfulness, and high-resolution perception. Drawing on these analyses, we distill key insights, identify open challenges (e.g., limitations of LLM-based reasoning, hallucination, a bias toward deductive reasoning, scalable supervision, tool integration, and benchmark limitations), and outline future directions, including world-model integration, human-AI collaborative reasoning, and richer evaluation protocols. By offering a unified taxonomy, historical roadmap, and critical outlook, this survey aims to serve as a foundational reference and inspire the next generation of compositional visual reasoning research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T08:55:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17298v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17298v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 SegQuant: A Semantics-Aware and Generalizable Quantization Framework for
  Diffusion Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiaji Zhang, Ruichao Sun, Hailiang Zhao, Jiaju Wu, Peng Chen, Hao Li, Yuying Liu, Kingsum Chow, Gang Xiong, Shuiguang Deng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion models have demonstrated exceptional generative capabilities but are computationally intensive, posing significant challenges for deployment in resource-constrained or latency-sensitive environments. Quantization offers an effective means to reduce model size and computational cost, with post-training quantization (PTQ) being particularly appealing due to its compatibility with pre-trained models without requiring retraining or training data. However, existing PTQ methods for diffusion models often rely on architecture-specific heuristics that limit their generalizability and hinder integration with industrial deployment pipelines. To address these limitations, we propose SegQuant, a unified quantization framework that adaptively combines complementary techniques to enhance cross-model versatility. SegQuant consists of a segment-aware, graph-based quantization strategy (SegLinear) that captures structural semantics and spatial heterogeneity, along with a dual-scale quantization scheme (DualScale) that preserves polarity-asymmetric activations, which is crucial for maintaining visual fidelity in generated outputs. SegQuant is broadly applicable beyond Transformer-based diffusion models, achieving strong performance while ensuring seamless compatibility with mainstream deployment tools.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T08:42:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.14811v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.14811v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark</h2>
                <div class="authors">
                    <strong>Authors:</strong> Adil Bahaj, Oumaima Fadi, Mohamed Chetouani, Mounir Ghogho
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) and vision-augmented LLMs (VLMs) have significantly advanced medical informatics, diagnostics, and decision support. However, these models exhibit systematic biases, particularly age bias, compromising their reliability and equity. This is evident in their poorer performance on pediatric-focused text and visual question-answering tasks. This bias reflects a broader imbalance in medical research, where pediatric studies receive less funding and representation despite the significant disease burden in children. To address these issues, a new comprehensive multi-modal pediatric question-answering benchmark, PediatricsMQA, has been introduced. It consists of 3,417 text-based multiple-choice questions (MCQs) covering 131 pediatric topics across seven developmental stages (prenatal to adolescent) and 2,067 vision-based MCQs using 634 pediatric images from 67 imaging modalities and 256 anatomical regions. The dataset was developed using a hybrid manual-automatic pipeline, incorporating peer-reviewed pediatric literature, validated question banks, existing benchmarks, and existing QA resources. Evaluating state-of-the-art open models, we find dramatic performance drops in younger cohorts, highlighting the need for age-aware methods to ensure equitable AI support in pediatric care.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T08:33:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span><span>cs.AI</span><span>cs.CL</span><span>cs.GR</span><span>cs.MM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.16439v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.16439v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 CoCoA: Confidence and Context-Aware Adaptive Decoding for Resolving
  Knowledge Conflicts in Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anant Khandelwal, Manish Gupta, Puneet Agrawal
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Faithful generation in large language models (LLMs) is challenged by knowledge conflicts between parametric memory and external context. Existing contrastive decoding methods tuned specifically to handle conflict often lack adaptability and can degrade performance in low conflict settings. We introduce CoCoA (Confidence- and Context-Aware Adaptive Decoding), a novel token-level algorithm for principled conflict resolution and enhanced faithfulness. CoCoA resolves conflict by utilizing confidence-aware measures (entropy gap and contextual peakedness) and the generalized divergence between the parametric and contextual distributions. Crucially, CoCoA maintains strong performance even in low conflict settings. Extensive experiments across multiple LLMs on diverse Question Answering (QA), Summarization, and Long-Form Question Answering (LFQA) benchmarks demonstrate CoCoA's state-of-the-art performance over strong baselines like AdaCAD. It yields significant gains in QA accuracy, up to 9.2 points on average compared to the strong baseline AdaCAD, and improves factuality in summarization and LFQA by up to 2.5 points on average across key benchmarks. Additionally, it demonstrates superior sensitivity to conflict variations. CoCoA enables more informed, context-aware, and ultimately more faithful token generation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T08:29:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17670v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17670v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 Survey of Specialized Large Language Model</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chenghan Yang, Ruiyu Zhao, Yang Liu, Ling Jiang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid evolution of specialized large language models (LLMs) has transitioned from simple domain adaptation to sophisticated native architectures, marking a paradigm shift in AI development. This survey systematically examines this progression across healthcare, finance, legal, and technical domains. Besides the wide use of specialized LLMs, technical breakthrough such as the emergence of domain-native designs beyond fine-tuning, growing emphasis on parameter efficiency through sparse computation and quantization, increasing integration of multimodal capabilities and so on are applied to recent LLM agent. Our analysis reveals how these innovations address fundamental limitations of general-purpose LLMs in professional applications, with specialized models consistently performance gains on domain-specific benchmarks. The survey further highlights the implications for E-Commerce field to fill gaps in the field.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T08:27:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19667v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19667v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 Leveraging LLMs for Automated Translation of Legacy Code: A Case Study
  on PL/SQL to Java Transformation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lola Solovyeva, Eduardo Carneiro Oliveira, Shiyu Fan, Alper Tuncay, Shamil Gareev, Andrea Capiluppi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The VT legacy system, comprising approximately 2.5 million lines of PL/SQL code, lacks consistent documentation and automated tests, posing significant challenges for refactoring and modernisation. This study investigates the feasibility of leveraging large language models (LLMs) to assist in translating PL/SQL code into Java for the modernised "VTF3" system. By leveraging a dataset comprising 10 PL/SQL-to-Java code pairs and 15 Java classes, which collectively established a domain model for the translated files, multiple LLMs were evaluated. Furthermore, we propose a customized prompting strategy that integrates chain-of-guidance reasoning with $n$-shot prompting. Our findings indicate that this methodology effectively guides LLMs in generating syntactically accurate translations while also achieving functional correctness. However, the findings are limited by the small sample size of available code files and the restricted access to test cases used for validating the correctness of the generated code. Nevertheless, these findings lay the groundwork for scalable, automated solutions in modernising large legacy systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T08:21:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19663v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19663v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 Convert Language Model into a Value-based Strategic Planner</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaoyu Wang, Yue Zhao, Qingqing Gu, Zhonglin Jiang, Xiaokai Chen, Yong Chen, Luo Ji
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Emotional support conversation (ESC) aims to alleviate the emotional distress of individuals through effective conversations. Although large language models (LLMs) have obtained remarkable progress on ESC, most of these studies might not define the diagram from the state model perspective, therefore providing a suboptimal solution for long-term satisfaction. To address such an issue, we leverage the Q-learning on LLMs, and propose a framework called straQ*. Our framework allows a plug-and-play LLM to bootstrap the planning during ESC, determine the optimal strategy based on long-term returns, and finally guide the LLM to response. Substantial experiments on ESC datasets suggest that straQ* outperforms many baselines, including direct inference, self-refine, chain of thought, finetuning, and finite state machines.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T08:21:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.06987v6' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.06987v6' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 Scalable Object Detection in the Car Interior With Vision Foundation
  Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bálint Mészáros, Ahmet Firintepe, Sebastian Schmidt, Stephan Günnemann
                </div>
                <div class="summary">
                    <strong>Summary:</strong> AI tasks in the car interior like identifying and localizing externally introduced objects is crucial for response quality of personal assistants. However, computational resources of on-board systems remain highly constrained, restricting the deployment of such solutions directly within the vehicle. To address this limitation, we propose the novel Object Detection and Localization (ODAL) framework for interior scene understanding. Our approach leverages vision foundation models through a distributed architecture, splitting computational tasks between on-board and cloud. This design overcomes the resource constraints of running foundation models directly in the car. To benchmark model performance, we introduce ODALbench, a new metric for comprehensive assessment of detection and localization.Our analysis demonstrates the framework's potential to establish new standards in this domain. We compare the state-of-the-art GPT-4o vision foundation model with the lightweight LLaVA 1.5 7B model and explore how fine-tuning enhances the lightweight models performance. Remarkably, our fine-tuned ODAL-LLaVA model achieves an ODAL$_{score}$ of 89%, representing a 71% improvement over its baseline performance and outperforming GPT-4o by nearly 20%. Furthermore, the fine-tuned model maintains high detection accuracy while significantly reducing hallucinations, achieving an ODAL$_{SNR}$ three times higher than GPT-4o.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T07:58:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19651v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19651v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 PCR-CA: Parallel Codebook Representations with Contrastive Alignment for
  Multiple-Category App Recommendation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bin Tan, Wangyao Ge, Yidi Wang, Xin Liu, Jeff Burtoft, Hao Fan, Hui Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modern app store recommender systems struggle with multiple-category apps, as traditional taxonomies fail to capture overlapping semantics, leading to suboptimal personalization. We propose PCR-CA (Parallel Codebook Representations with Contrastive Alignment), an end-to-end framework for improved CTR prediction. PCR-CA first extracts compact multimodal embeddings from app text, then introduces a Parallel Codebook VQ-AE module that learns discrete semantic representations across multiple codebooks in parallel -- unlike hierarchical residual quantization (RQ-VAE). This design enables independent encoding of diverse aspects (e.g., gameplay, art style), better modeling multiple-category semantics. To bridge semantic and collaborative signals, we employ a contrastive alignment loss at both the user and item levels, enhancing representation learning for long-tail items. Additionally, a dual-attention fusion mechanism combines ID-based and semantic features to capture user interests, especially for long-tail apps. Experiments on a large-scale dataset show PCR-CA achieves a +0.76% AUC improvement over strong baselines, with +2.15% AUC gains for long-tail apps. Online A/B testing further validates our approach, showing a +10.52% lift in CTR and a +16.30% improvement in CVR, demonstrating PCR-CA's effectiveness in real-world deployment. The new framework has now been fully deployed on the Microsoft Store.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T07:58:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18166v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18166v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 Active Learning for Deep Learning-Based Hemodynamic Parameter Estimation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Patryk Rygiel, Julian Suk, Kak Khee Yeung, Christoph Brune, Jelmer M. Wolterink
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Hemodynamic parameters such as pressure and wall shear stress play an important role in diagnosis, prognosis, and treatment planning in cardiovascular diseases. These parameters can be accurately computed using computational fluid dynamics (CFD), but CFD is computationally intensive. Hence, deep learning methods have been adopted as a surrogate to rapidly estimate CFD outcomes. A drawback of such data-driven models is the need for time-consuming reference CFD simulations for training. In this work, we introduce an active learning framework to reduce the number of CFD simulations required for the training of surrogate models, lowering the barriers to their deployment in new applications. We propose three distinct querying strategies to determine for which unlabeled samples CFD simulations should be obtained. These querying strategies are based on geometrical variance, ensemble uncertainty, and adherence to the physics governing fluid dynamics. We benchmark these methods on velocity field estimation in synthetic coronary artery bifurcations and find that they allow for substantial reductions in annotation cost. Notably, we find that our strategies reduce the number of samples required by up to 50% and make the trained models more robust to difficult cases. Our results show that active learning is a feasible strategy to increase the potential of deep learning-based CFD surrogates.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T07:44:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.03453v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.03453v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 PyVision: Agentic Vision with Dynamic Tooling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shitian Zhao, Haoquan Zhang, Shaoheng Lin, Ming Li, Qilong Wu, Kaipeng Zhang, Chen Wei
                </div>
                <div class="summary">
                    <strong>Summary:</strong> LLMs are increasingly deployed as agents, systems capable of planning, reasoning, and dynamically calling external tools. However, in visual reasoning, prior approaches largely remain limited by predefined workflows and static toolsets. In this report, we present PyVision, an interactive, multi-turn framework that enables MLLMs to autonomously generate, execute, and refine Python-based tools tailored to the task at hand, unlocking flexible and interpretable problem-solving. We develop a taxonomy of the tools created by PyVision and analyze their usage across a diverse set of benchmarks. Quantitatively, PyVision achieves consistent performance gains, boosting GPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini. These results point to a broader shift: dynamic tooling allows models not just to use tools, but to invent them, advancing toward more agentic visual reasoning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T07:42:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.07998v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.07998v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 LLM-based feature generation from text for interpretable machine
  learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Vojtěch Balek, Lukáš Sýkora, Vilém Sklenák, Tomáš Kliegr
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Existing text representations such as embeddings and bag-of-words are not suitable for rule learning due to their high dimensionality and absent or questionable feature-level interpretability. This article explores whether large language models (LLMs) could address this by extracting a small number of interpretable features from text. We demonstrate this process on two datasets (CORD-19 and M17+) containing several thousand scientific articles from multiple disciplines and a target being a proxy for research impact. An evaluation based on testing for the statistically significant correlation with research impact has shown that LLama 2-generated features are semantically meaningful. We consequently used these generated features in text classification to predict the binary target variable representing the citation rate for the CORD-19 dataset and the ordinal 5-class target representing an expert-awarded grade in the M17+ dataset. Machine-learning models trained on the LLM-generated features provided similar predictive performance to the state-of-the-art embedding model SciBERT for scientific text. The LLM used only 62 features compared to 768 features in SciBERT embeddings, and these features were directly interpretable, corresponding to notions such as article methodological rigor, novelty, or grammatical correctness. As the final step, we extract a small number of well-interpretable action rules. Consistently competitive results obtained with the same LLM feature set across both thematically diverse datasets show that this approach generalizes across domains.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T07:41:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.07132v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.07132v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 AirRAG: Autonomous Strategic Planning and Reasoning Steer Retrieval
  Augmented Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wenfeng Feng, Chuzhan Hao, Yuewei Zhang, Guochao Jiang, Jingyi Song, Hao Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Leveraging the autonomous decision-making capabilities of large language models (LLMs) has demonstrated superior performance in reasoning tasks. However, despite the success of iterative or agentic retrieval-augmented generation (RAG) techniques, these methods are often constrained to a single solution space when confronted with complex problems. In this paper, we propose a novel thinking pattern in RAG that integrates autonomous strategic planning with efficient reasoning actions, significantly activating intrinsic reasoning capabilities and expanding the solution space of specific tasks via Monte Carlo Tree Search (MCTS), which we refer to as AirRAG. Specifically, our approach designs five fundamental reasoning actions, which are expanded to a broad tree-based reasoning space using MCTS. The approach also incorporates self-consistency verification to explore potential reasoning paths and inference scaling law. Additionally, computationally optimal strategies are employed to allocate more inference resources to key actions, thereby enhancing overall performance. Experimental results demonstrate the effectiveness of AirRAG, showing significant performance gains on complex question-answering datasets. Furthermore, AirRAG is flexible and lightweight, making it easy to integrate with other advanced technologies and models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T07:38:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.10053v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.10053v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 Using Generative AI to Uncover What Drives Player Enjoyment in PC and VR
  Games</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hisham Abdelqader
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As video games continue to evolve, understanding what drives player enjoyment remains a key challenge. Player reviews provide valuable insights, but their unstructured nature makes large-scale analysis difficult. This study applies generative AI and machine learning, leveraging Microsoft Phi-4 LLM and XGBoost, to quantify and analyze game reviews from Steam and Meta Quest stores. The approach converts qualitative feedback into structured data, enabling comprehensive evaluation of key game design elements, monetization models, and platform-specific trends. The findings reveal distinct patterns in player preferences across PC and VR games, highlighting factors that contribute to higher player satisfaction. By integrating Google Cloud for largescale data storage and processing, this study establishes a scalable framework for game review analysis. The study's insights offer actionable guidance for game developers, helping optimize game mechanics, pricing strategies, and player engagement.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T07:37:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.SI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.16596v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.16596v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 A Symbolic Adversarial Learning Framework for Evolving Fake News
  Generation and Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chong Tian, Qirong Ho, Xiuying Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Rapid LLM advancements heighten fake news risks by enabling the automatic generation of increasingly sophisticated misinformation. Previous detection methods, including fine-tuned small models or LLM-based detectors, often struggle with its dynamically evolving nature. In this work, we propose a novel framework called the Symbolic Adversarial Learning Framework (SALF), which implements an adversarial training paradigm by an agent symbolic learning optimization process, rather than relying on numerical updates. SALF introduces a paradigm where the generation agent crafts deceptive narratives, and the detection agent uses structured debates to identify logical and factual flaws for detection, and they iteratively refine themselves through such adversarial interactions. Unlike traditional neural updates, we represent agents using agent symbolic learning, where learnable weights are defined by agent prompts, and simulate back-propagation and gradient descent by operating on natural language representations of weights, loss, and gradients. Experiments on two multilingual benchmark datasets demonstrate SALF's effectiveness, showing it generates sophisticated fake news that degrades state-of-the-art detection performance by up to 53.4% in Chinese and 34.2% in English on average. SALF also refines detectors, improving detection of refined content by up to 7.7%. We hope our work inspires further exploration into more robust, adaptable fake news detection systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T07:14:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19633v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19633v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 SynthCoder: A Synthetical Strategy to Tune LLMs for Code Completion</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dongjun Yu, Xiao Yan, Zhenrui Li, Jipeng Xiao, Haochuan He, Yongda Yu, Hao Zhang, Guoping Rong, Xiaobo Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Code completion is a prominent application of Large Language Models (LLMs) in software engineering. Due to the near real-time response requirements of this task, base models with small to medium-sized parameters are typically employed, supplemented by various optimization and post-training techniques. However, these optimization methods often have trade-offs, leading to a seesaw effect where performance improvements on certain datasets or metrics are accompanied by degradations on others -- sometimes even falling below the baseline model's performance. This paper proposes SynthCoder, a model that integrates leading industry practices to achieve state-of-the-art performance on the Fill-in-the-Middle (FIM) code completion task. In specific, we first construct a diverse dataset by combining Abstract Syntax Tree (AST) node extraction with heuristics that simulate developer behavior. Then we enrich our training corpus with cross-file contextual information using the BM25 algorithm and call graphs, enhancing the model's ability to perform code completion in both file-level and repository-level scenarios. As the last step, we employ a two-stage training process using the Seed-Coder-8B-Base as the base model. First, we fine-tune the model using Curriculum Learning technology. Following this, we perform alignment using Direct Preference Optimization (DPO) with preference pairs generated through Rejection Sampling. Experimental results demonstrate that our final model excels on mainstream repository-level code completion benchmarks, including aiXcoder, ExecRepoBench, CrossCodeEval, and CoLT. Furthermore, our carefully curated training set effectively mitigates the model's tendency to just repeat existing code, a common issue existing in various code completion models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T07:09:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15495v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15495v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 Semantic Energy: Detecting LLM Hallucination Beyond Entropy</h2>
                <div class="authors">
                    <strong>Authors:</strong> Huan Ma, Jiadong Pan, Jing Liu, Yan Chen, Joey Tianyi Zhou, Guangyu Wang, Qinghua Hu, Hua Wu, Changqing Zhang, Haifeng Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are being increasingly deployed in real-world applications, but they remain susceptible to hallucinations, which produce fluent yet incorrect responses and lead to erroneous decision-making. Uncertainty estimation is a feasible approach to detect such hallucinations. For example, semantic entropy estimates uncertainty by considering the semantic diversity across multiple sampled responses, thus identifying hallucinations. However, semantic entropy relies on post-softmax probabilities and fails to capture the model's inherent uncertainty, causing it to be ineffective in certain scenarios. To address this issue, we introduce Semantic Energy, a novel uncertainty estimation framework that leverages the inherent confidence of LLMs by operating directly on logits of penultimate layer. By combining semantic clustering with a Boltzmann-inspired energy distribution, our method better captures uncertainty in cases where semantic entropy fails. Experiments across multiple benchmarks show that Semantic Energy significantly improves hallucination detection and uncertainty estimation, offering more reliable signals for downstream applications such as hallucination detection.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T07:01:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.14496v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.14496v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 A Scenario-Oriented Survey of Federated Recommender Systems: Techniques,
  Challenges, and Future Directions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yunqi Mi, Jiakui Shen, Guoshuai Zhao, Jialie Shen, Xueming Qian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Extending recommender systems to federated learning (FL) frameworks to protect the privacy of users or platforms while making recommendations has recently gained widespread attention in academia. This is due to the natural coupling of recommender systems and federated learning architectures: the data originates from distributed clients (mostly mobile devices held by users), which are highly related to privacy. In a centralized recommender system (CenRec), the central server collects clients' data, trains the model, and provides the service. Whereas in federated recommender systems (FedRec), the step of data collecting is omitted, and the step of model training is offloaded to each client. The server only aggregates the model and other knowledge, thus avoiding client privacy leakage. Some surveys of federated recommender systems discuss and analyze related work from the perspective of designing FL systems. However, their utility drops by ignoring specific recommendation scenarios' unique characteristics and practical challenges. For example, the statistical heterogeneity issue in cross-domain FedRec originates from the label drift of the data held by different platforms, which is mainly caused by the recommender itself, but not the federated architecture. Therefore, it should focus more on solving specific problems in real-world recommendation scenarios to encourage the deployment FedRec. To this end, this review comprehensively analyzes the coupling of recommender systems and federated learning from the perspective of recommendation researchers and practitioners. We establish a clear link between recommendation scenarios and FL frameworks, systematically analyzing scenario-specific approaches, practical challenges, and potential opportunities. We aim to develop guidance for the real-world deployment of FedRec, bridging the gap between existing research and applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T06:57:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span><span>cs.AI</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19620v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19620v1' target='_blank'>pdf</a>
                </div>
            </div>
            
        </div>
    </body>
    </html>
    