
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>ArXiv Results</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                margin: 0;
                padding: 0;
                display: flex;
                justify-content: center;
                align-items: center;
                min-height: 100vh;
                background-color: #f9f9f9;
            }
            .container {
                width: 17cm;
                padding: 2cm;
                background-color: #fff;
                box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            }
            h1, h2 {
                color: #333;
            }
            .paper {
                border: 1px solid #ccc;
                padding: 1em;
                margin-bottom: 1em;
                border-radius: 8px;
                box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            }
            .authors, .summary, .updated, .tags, .links {
                margin: 0.5em 0;
            }
            .tags span {
                background-color: #f0f0f0;
                border-radius: 4px;
                padding: 0.2em 0.5em;
                margin-right: 0.5em;
            }
            .links a {
                margin-right: 1em;
                text-decoration: none;
                color: #1a73e8;
            }
            .links a:hover {
                text-decoration: underline;
            }
        </style>
    </head>
    <body>
        <div class="container">
    <h1>Keyword: kv cache</h1>

            <div class="paper">
                <h2>#1 StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context
  Modeling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Meng Wei, Chenyang Wan, Xiqian Yu, Tai Wang, Yuqiang Yang, Xiaohan Mao, Chenming Zhu, Wenzhe Cai, Hanqing Wang, Yilun Chen, Xihui Liu, Jiangmiao Pang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vision-and-Language Navigation (VLN) in real-world settings requires agents to process continuous visual streams and generate actions with low latency grounded in language instructions. While Video-based Large Language Models (Video-LLMs) have driven recent progress, current VLN methods based on Video-LLM often face trade-offs among fine-grained visual understanding, long-term context modeling and computational efficiency. We introduce StreamVLN, a streaming VLN framework that employs a hybrid slow-fast context modeling strategy to support multi-modal reasoning over interleaved vision, language and action inputs. The fast-streaming dialogue context facilitates responsive action generation through a sliding-window of active dialogues, while the slow-updating memory context compresses historical visual states using a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN achieves coherent multi-turn dialogue through efficient KV cache reuse, supporting long video streams with bounded context size and inference cost. Experiments on VLN-CE benchmarks demonstrate state-of-the-art performance with stable low latency, ensuring robustness and efficiency in real-world deployment. The project page is: \href{https://streamvln.github.io/}{https://streamvln.github.io/}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:49:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05240v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05240v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 The Case for Instance-Optimized LLMs in OLAP Databases</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bardia Mohammadi, Laurent Bindschaedler
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) can enhance analytics systems with powerful data summarization, cleaning, and semantic transformation capabilities. However, deploying LLMs at scale -- processing millions to billions of rows -- remains prohibitively expensive in computation and memory. We present IOLM-DB, a novel system that makes LLM-enhanced database queries practical through query-specific model optimization. Instead of using general-purpose LLMs, IOLM-DB generates lightweight, specialized models tailored to each query's specific needs using representative data samples. IOLM-DB reduces model footprints by up to 76% and increases throughput by up to 3.31$\times$ while maintaining accuracy through aggressive compression techniques, including quantization, sparsification, and structural pruning. We further show how our approach enables higher parallelism on existing hardware and seamlessly supports caching and batching strategies to reduce overheads. Our prototype demonstrates that leveraging LLM queries inside analytics systems is feasible at scale, opening new possibilities for future OLAP applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T13:10:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04967v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04967v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated
  in a coupled reactive transport HPC simulation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Max LÃ¼bke, Marco De Lucia, Stefan Petri, Bettina Schnor
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Surrogate models can play a pivotal role in enhancing performance in contemporary High-Performance Computing applications. Cache-based surrogates use already calculated simulation results to interpolate or extrapolate further simulation output values. But this approach only pays off if the access time to retrieve the needed values is much faster than the actual simulation. While the most existing key-value stores use a Client-Server architecture with dedicated storage nodes, this is not the most suitable architecture for HPC applications. Instead, we propose a distributed architecture where the parallel processes offer a part of their available memory to build a shared distributed hash table based on MPI. This paper presents three DHT approaches with the special requirements of HPC applications in mind. The presented lock-free design outperforms both DHT versions which use explicit synchronization by coarse-grained resp. fine-grained locking. The lock-free DHT shows very good scaling regarding read and write performance. The runtime of a coupled reactive transport simulation was improved between 14% and 42% using the lock-free DHT as a surrogate model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T09:25:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1007/978-3-031-97635-3_28' target='_blank'>doi</a><a href='http://arxiv.org/abs/2504.14374v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.14374v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 Performance Evaluation of General Purpose Large Language Models for
  Basic Linear Algebra Subprograms Code Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Daichi Mukunoki, Shun-ichiro Hayashi, Tetsuya Hoshino, Takahiro Katagiri
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Generative AI technology based on Large Language Models (LLM) has been developed and applied to assist or automatically generate program codes. In this paper, we evaluate the capability of existing general LLMs for Basic Linear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs provided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model, and o4-mini, one of the o-series of Reasoning models. Both have been released in April 2025. For the routines from level-1 to 3 BLAS, we tried to generate (1) C code without optimization from routine name only, (2) C code with basic performance optimizations (thread parallelization, SIMD vectorization, and cache blocking) from routine name only, and (3) C code with basic performance optimizations based on Fortran reference code. As a result, we found that correct code can be generated in many cases even when only routine name are given. We also confirmed that thread parallelization with OpenMP, SIMD vectorization, and cache blocking can be implemented to some extent, and that the code is faster than the reference code.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T06:33:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.DC</span><span>cs.MS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04697v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04697v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 RAT: Bridging RNN Efficiency and Attention Accuracy in Language Modeling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiuying Wei, Anunay Yadav, Razvan Pascanu, Caglar Gulcehre
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformers have become the cornerstone of modern large-scale language models; however, their dependence on softmax attention poses a major computational bottleneck, particularly in long-context settings. In this work, rather than following prevalent approaches such as linear attention (or SSMs) and local attention, we introduce an intermediate design called \rat between recurrence and attention mechanisms. It partitions the input into chunks, applies a simple linear recurrence within each chunk to capture local dependencies, and then performs softmax attention across chunks to model long-range interactions. By adjusting the size of the chunk, \rat enables flexible trade-offs, combining the strengths of RNN and attention. Empirically, with a chunk size of 16, the \rat layer achieves a \(7\times\) improvement in training speed with 100K token sequences and \(9\times\) in generation at 4K sequence length, while maintaining similar or sometimes even better accuracy compared to standard attention. We demonstrate this by training 1.3B parameter models from scratch and performing large-scale evaluations, including short- and long-context benchmarks, as well as supervised fine-tuning~(SFT). We further propose a hybrid architecture that interleaves \rat with local attention. By combining efficient long-range modeling with strong local interactions, this hybrid design not only improves inference speed and reduces cache memory usage compared to attention, but also consistently enhances performance, for example, achieving an average 1 point gain in commonsense reasoning tasks, up to 4 points on code tasks, and a 1 point Rouge-L increase in a summarization SFT task. Code is available at https://github.com/CLAIRE-Labo/RAT
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-06T15:08:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04416v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04416v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale
  Reconstruction with External Memory</h2>
                <div class="authors">
                    <strong>Authors:</strong> Felix Windisch, Lukas Radl, Thomas KÃ¶hler, Michael Steiner, Dieter Schmalstieg, Markus Steinberger
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Gaussian Splatting has emerged as a high-performance technique for novel view synthesis, enabling real-time rendering and high-quality reconstruction of small scenes. However, scaling to larger environments has so far relied on partitioning the scene into chunks -- a strategy that introduces artifacts at chunk boundaries, complicates training across varying scales, and is poorly suited to unstructured scenarios such as city-scale flyovers combined with street-level views. Moreover, rendering remains fundamentally limited by GPU memory, as all visible chunks must reside in VRAM simultaneously. We introduce A LoD of Gaussians, a framework for training and rendering ultra-large-scale Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our method stores the full scene out-of-core (e.g., in CPU memory) and trains a Level-of-Detail (LoD) representation directly, dynamically streaming only the relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with Sequential Point Trees enables efficient, view-dependent LoD selection, while a lightweight caching and view scheduling system exploits temporal coherence to support real-time streaming and rendering. Together, these innovations enable seamless multi-scale reconstruction and interactive visualization of complex scenes -- from broad aerial views to fine-grained ground-level details.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-05T15:51:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.GR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.01110v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.01110v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiahui Wang, Zuyan Liu, Yongming Rao, Jiwen Lu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multimodal Large Language Models (MLLMs) are commonly derived by extending pre-trained Large Language Models (LLMs) with visual capabilities. In this work, we investigate how MLLMs process visual inputs by analyzing their attention mechanisms. We reveal a surprising sparsity phenomenon: only a small subset (approximately less than 5%) of attention heads in LLMs actively contribute to visual understanding, termed visual heads. To identify these heads efficiently, we design a training-free framework that quantifies head-level visual relevance through targeted response analysis. Building on this discovery, we introduce SparseMM, a KV-Cache optimization strategy that allocates asymmetric computation budgets to heads in LLMs based on their visual scores, leveraging the sparity of visual heads for accelerating the inference of MLLMs. Compared with prior KV-Cache acceleration methods that ignore the particularity of visual, SparseMM prioritizes stress and retaining visual semantics during decoding. Extensive evaluations across mainstream multimodal benchmarks demonstrate that SparseMM achieves superior accuracy-efficiency trade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52% memory reduction during generation while maintaining performance parity on efficiency test. Our project is open sourced at https://github.com/CR400AF-A/SparseMM.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-05T15:40:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.05344v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.05344v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 Heterogeneous Memory Benchmarking Toolkit</h2>
                <div class="authors">
                    <strong>Authors:</strong> Golsana Ghaemi, Gabriel Franco, Kazem Taram, Renato Mancuso
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper presents an open-source kernel-level heterogeneous memory characterization framework (MemScope) for embedded systems. MemScope enables precise characterization of the temporal behavior of available memory modules under configurable contention stress scenarios. MemScope leverages kernel-level control over physical memory allocation, cache maintenance, CPU state, interrupts, and I/O device activity to accurately benchmark heterogeneous memory subsystems. This gives us the privilege to directly map pieces of contiguous physical memory and instantiate allocators, allowing us to finely control cores to create and eliminate interference. Additionally, we can minimize noise and interruptions, guaranteeing more consistent and precise results compared to equivalent user-space solutions. Running our Framework on a Xilinx Zynq UltraScale+ ZCU102 CPU-FPGA platform demonstrates its capability to precisely benchmark bandwidth and latency across various memory types, including PL-side DRAM and BRAM, in a multi-core system.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-05T13:37:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.00901v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.00901v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 Combination generators with optimal cache utilization and communication
  free parallel execution</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xi He, Max. A. Little
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce an efficient and elegant combination generator for producing all combinations of size less than or equal to K, designed for exhaustive generation and combinatorial optimization tasks. This generator can be implemented to achieve what we define as optimal efficiency: constant amortized time, optimal cache utilization, embarrassingly parallel execution, and a recursive structure compatible with pruning-based search. These properties are difficult to satisfy simultaneously in existing generators. For example, classical Gray code or lexicographic generators are typically list-based and sequentially defined, making them difficult to vectorized, inefficient in cache usage, and inherently hard to parallelize. Generators based on unranking methods, while easy to parallelize, are non-recursive. These limitations reduce their applicability in our target applications, where both computational efficiency and recursion are crucial. We adapt Bird's algebra of programming-style calculation to derive our algorithms, a formalism for developing correct-by-construction programs from specifications. As a result, all generators in this paper are first formulated in their clearest specification, and efficient definitions are derived constructively through equational reasoning, resulting in concise and elegant divide-and-conquer definitions. Beyond presenting a combination generator, we extend our approach to construct generators for K-permutations, nested combinations of combinations, and nested permutation-combination structures. To the best of our knowledge, the literature has not previously reported generators for these nested structures. We also develop sequential variants that produce configurations in Gray code-compatible orders -- such as the revolving door ordering -- which are particularly useful for constructing nested generators.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-05T10:11:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DM</span><span>cs.DS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.03980v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.03980v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 PFCS: Prime Factorization Cache System for Deterministic Data
  Relationship Discovery</h2>
                <div class="authors">
                    <strong>Authors:</strong> Duy Le
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Cache systems fundamentally limit modern computing performance due to their inability to precisely capture data relationships. While achieving 85-92% hit rates, traditional systems rely on statistical heuristics that cannot guarantee relationship discovery, leading to suboptimal prefetching and resource waste. We present PFCS (Prime Factorization Cache System), which leverages the mathematical uniqueness of prime factorization to achieve deterministic relationship discovery with zero false positives. PFCS assigns unique primes to data elements and represents relationships as composite numbers, enabling the recovery of perfect relationships through factorization. A comprehensive evaluation across database, ML, and HPC workloads demonstrates an average performance improvement of x 6.2, 98.9% hit rates, and a 38% power reduction compared to state-of-the-art systems. The mathematical foundation provides formal guarantees impossible with approximation-based approaches, establishing a new paradigm for cache system design
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-05T06:55:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.CC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.03919v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.03919v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 A Taxonomy and Comparative Analysis of IPv4 Identifier Selection
  Correctness, Security, and Performance</h2>
                <div class="authors">
                    <strong>Authors:</strong> Joshua J. Daymude, Antonio M. Espinoza, Sean Bergen, Benjamin Mixon-Baca, Jeffrey Knockel, Jedidiah R. Crandall
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The battle for a more secure Internet is waged on many fronts, including the most basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an IPv4 header field as old as the Internet with an equally long history as an exploited side channel for scanning network properties, inferring off-path connections, and poisoning DNS caches. This article taxonomizes the 25-year history of IPID-based exploits and the corresponding changes to IPID selection methods. By mathematically analyzing these methods' correctness and security and empirically evaluating their performance, we reveal recommendations for best practice as well as shortcomings of current operating system implementations, emphasizing the value of systematic evaluations in network security.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-05T01:08:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.06483v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.06483v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 Memory- and compute-optimized geometric multigrid GMGPolar for
  curvilinear coordinate representations -- Applications to fusion plasma</h2>
                <div class="authors">
                    <strong>Authors:</strong> Julian Litz, Philippe Leleux, Carola Kruse, Joscha Gedicke, Martin J. KÃ¼hn
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Tokamak fusion reactors are actively studied as a means of realizing energy production from plasma fusion. However, due to the substantial cost and time required to construct fusion reactors and run physical experiments, numerical experiments are indispensable for understanding plasma physics inside tokamaks, supporting the design and engineering phase, and optimizing future reactor designs. Geometric multigrid methods are optimal solvers for many problems that arise from the discretization of partial differential equations. It has been shown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson equation in linear complexity and with only small memory requirements compared to other state-of-the-art solvers. In this paper, we present a completely refactored and object-oriented version of GMGPolar which offers two different matrix-free implementations. Among other things, we leverage the Sherman-Morrison formula to solve cyclic tridiagonal systems from circular line solvers without additional fill-in and we apply reordering to optimize cache access of circular and radial smoothing operations. With the Give approach, memory requirements are further reduced and speedups of four to seven are obtained for usual test cases. For the Take approach, speedups of 16 to 18 can be attained.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-04T21:09:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MS</span><span>physics.plasm-ph</span><span>68Q25, 65Y20, 65Y05, 65N55, 65N06, 65B99</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.03812v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.03812v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 Quantum Algorithm for the Fixed-Radius Neighbor Search</h2>
                <div class="authors">
                    <strong>Authors:</strong> Luca Cappelli, Claudio Sanavio, Alessandro Andrea Zecchi, Giuseppe Murante, Sauro Succi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The neighbor search is a computationally demanding problem, usually both time- and memory-consuming. The main problem of this kind of algorithms is the long execution time due to cache misses. In this work, we propose a quantum algorithm for the Fixed RAdius Neighbor Search problem (FRANS) based on the fixed-point version of Grover's algorithm. We derive an efficient circuit for solving the FRANS with linear query complexity with the number of particles $N$. The quantum circuit returns the list of all the neighbors' pairs within the fixed radius, together with their distance, avoiding the slow down given by cache miss. We explicitly write the Grover's operator and analyze its gate complexity. The whole algorithm has complexity of $\mathcal{O}(M^{\frac{1}{2}}N^{2})$ in the worst-case scenario, where $M$ is the number of neighboring pairs, and uses $\mathcal{O}(\log N)$ number of qubits. By employing extra ancilla qubits the depth of the circuit can be brought down to $\mathcal{O}(N\log N)$ at the cost of $\mathcal{O}(N)$ qubits for unstructured dataset, or $\mathcal{O}(\text{poly}(\log N))$ qubits for structured datasets. Finally we assess the resilience of the model to the readout error, suggesting an error correction-free strategy to check the accuracy of the results.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-04T10:01:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>quant-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.03445v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.03445v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 Numerical investigation of the effect of high voltage frequency on the
  density of RONS species in the air atmospheric pressure gas discharge</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fariborz Momtazzadeh, Farshad Sohbatzadeh, Hamed Soltani Ahmadi, Ramin Mehrabifard
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the last few decades, studies in various fields of plasma technology have expanded and its application in different processes has increased. Therefore, the achievement of a desirable and practical plasma with specific characteristics is of particular importance. The frequency of the applied voltage is one of the important factors that play a role in the physical and chemical characteristics. In this research, changes in the density of active species produced in an electrical discharge using a dielectric barrier and air working gas have been investigated from a frequency of 500 Hz to 500 kHz, and by applying a constant voltage of 2 kV, have been investigated. For this purpose, 87 different reactions with specific collision cross-sections were defined in COMSOL Multiphysics. Other parameters, including current-voltage waveform, electric field, and species densitywere evaluated. The results show that under completely identical conditions, the electron temperature distribution changes with increasing applied frequency, and the density of reactive oxygen and nitrogen species RONS decreases, but O shows an increasing trend. It should be noted that the simulation results are in good agreement with previous experimental and simulation reports. These results offer valuable insights into optimizing plasma parameters for different applications, potentially resulting in better treatment outcomes across a range of therapeutic domains.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-04T09:03:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.plasm-ph</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1007/S40042-025-01392-9.' target='_blank'>doi</a><a href='http://arxiv.org/abs/2507.03396v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.03396v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token
  Selection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Raghavv Goel, Junyoung Park, Mukul Gagrani, Dalton Jones, Matthew Morse, Harper Langston, Mingu Lee, Chris Lott
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While long context support of large language models has extended their abilities, it also incurs challenges in memory and compute which becomes crucial bottlenecks in resource-restricted devices. Token eviction, a widely adopted post-training methodology designed to alleviate the bottlenecks by evicting less important tokens from the cache, typically uses attention scores as proxy metrics for token importance. However, one major limitation of attention score as a token-wise importance metrics is that it lacks the information about contribution of tokens to the attention output. In this paper, we propose a simple eviction criterion based on the contribution of cached tokens to attention outputs. Our method, CAOTE, optimizes for eviction error due to token eviction, by seamlessly integrating attention scores and value vectors. This is the first method which uses value vector information on top of attention-based eviction scores. Additionally, CAOTE can act as a meta-heuristic method with flexible usage with any token eviction method. We show that CAOTE, when combined with the state-of-the-art attention score-based methods, always improves accuracies on the downstream task, indicating the importance of leveraging information from values during token eviction process.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-04T06:49:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.14051v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.14051v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 Hunyuan-TurboS: Advancing Large Language Models through
  Mamba-Transformer Synergy and Adaptive Chain-of-Thought</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tencent Hunyuan Team, Ao Liu, Botong Zhou, Can Xu, Chayse Zhou, ChenChen Zhang, Chengcheng Xu, Chenhao Wang, Decheng Wu, Dengpeng Wu, Dian Jiao, Dong Du, Dong Wang, Feng Zhang, Fengzong Lian, Guanghui Xu, Guanwei Zhang, Hai Wang, Haipeng Luo, Han Hu, Huilin Xu, Jiajia Wu, Jianchen Zhu, Jianfeng Yan, Jiaqi Zhu, Jihong Zhang, Jinbao Xue, Jun Xia, Junqiang Zheng, Kai Liu, Kai Zhang, Kai Zheng, Kejiao Li, Keyao Wang, Lan Jiang, Lixin Liu, Lulu Wu, Mengyuan Huang, Peijie Yu, Peiqi Wang, Qian Wang, Qianbiao Xiang, Qibin Liu, Qingfeng Sun, Richard Guo, Ruobing Xie, Saiyong Yang, Shaohua Chen, Shihui Hu, Shuai Li, Shuaipeng Li, Shuang Chen, Suncong Zheng, Tao Yang, Tian Zhang, Tinghao Yu, Weidong Han, Weijie Liu, Weijin Zhou, Weikang Wang, Wesleye Chen, Xiao Feng, Xiaoqin Ren, Xingwu Sun, Xiong Kuang, Xuemeng Huang, Xun Cao, Yanfeng Chen, Yang Du, Zhen Yang, Yangyu Tao, Yaping Deng, Yi Shen, Yigeng Hong, Yiqi Chen, Yiqing Huang, Yuchi Deng, Yue Mao, Yulong Wang, Yuyuan Zeng, Zenan Xu, Zhanhui Kang, Zhe Zhao, ZhenXiang Yan, Zheng Fang, Zhichao Hu, Zhongzhi Chen, Zhuoyu Li, Zongwei Li, Alex Yan, Ande Liang, Baitong Liu, Beiping Pan, Bin Xing, Binghong Wu, Bingxin Qu, Bolin Ni, Boyu Wu, Chen Li, Cheng Jiang, Cheng Zhang, Chengjun Liu, Chengxu Yang, Chengzhong Xu, Chiyu Wang, Chong Zha, Daisy Yi, Di Wang, Fanyang Lu, Fei Chen, Feifei Liu, Feng Zheng, Guanghua Yu, Guiyang Li, Guohua Wang, Haisheng Lin, Han Liu, Han Wang, Hao Fei, Hao Lu, Haoqing Jiang, Haoran Sun, Haotian Zhu, Huangjin Dai, Huankui Chen, Huawen Feng, Huihui Cai, Huxin Peng, Jackson Lv, Jiacheng Shi, Jiahao Bu, Jianbo Li, Jianglu Hu, Jiangtao Guan, Jianing Xu, Jianwei Cai, Jiarong Zhang, Jiawei Song, Jie Jiang, Jie Liu, Jieneng Yang, Jihong Zhang, Jin lv, Jing Zhao, Jinjian Li, Jinxing Liu, Jun Zhao, Juntao Guo, Kai Wang, Kan Wu, Lei Fu, Lei He, Lei Wang, Li Liu, Liang Dong, Liya Zhan, Long Cheng, Long Xu, Mao Zheng, Meng Liu, Mengkang Hu, Nanli Chen, Peirui Chen, Peng He, Pengju Pan, Pengzhi Wei, Qi Yang, Qi Yi, Roberts Wang, Rongpeng Chen, Rui Sun, Rui Yang, Ruibin Chen, Ruixu Zhou, Shaofeng Zhang, Sheng Zhang, Shihao Xu, Shuaishuai Chang, Shulin Liu, SiQi Wang, Songjia Feng, Songling Yuan, Tao Zhang, Tianjiao Lang, Tongkai Li, Wei Deng, Wei Li, Weichao Wang, Weigang Zhang, Weixuan Sun, Wen Ouyang, Wenxiang Jiao, Wenzhi Sun, Wenzhuo Jia, Xiang Zhang, Xiangyu He, Xianshun Ren, XiaoYing Zhu, Xiaolong Guo, Xiaoxue Li, Xiaoyu Ma, Xican Lu, Xinhua Feng, Xinting Huang, Xinyu Guan, Xirui Li, Xu Zhang, Xudong Gao, Xun Luo, Xuxiang Qi, Yangkun Chen, Yangyu Tao, Yanling Xiao, Yantao Mai, Yanze Chen, Yao Ding, Yeting Yang, YiFan Song, Yifan Yang, Yijiao Zhu, Yinhe Wu, Yixian Liu, Yong Yang, Yuanjun Cai, Yuanlin Tu, Yue Zhang, Yufei Huang, Yuhang Zhou, Yuhao Jiang, Yuhong Liu, Yuhui Hu, Yujin Lin, Yun Yang, Yunhao Wang, Yusong Zhang, Zekun Wu, Zelong Zhang, Zhan Yu, Zhaoliang Yang, Zhe Zhao, Zheng Li, Zhenyu Huang, Zhiguang Liu, Zhijiang Xu, Zhiqing Kui, Zhiyin Zeng, Zhiyuan Xiong, Zhuo Han, Zifan Wu, Zigang Geng, Zilong Zhao, Ziyan Tang, Ziyuan Zhu, Zonglei Zhu, Zhijiang Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS, a novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It synergistically combines Mamba's long-sequence processing efficiency with Transformer's superior contextual understanding. Hunyuan-TurboS features an adaptive long-short chain-of-thought (CoT) mechanism, dynamically switching between rapid responses for simple queries and deep "thinking" modes for complex problems, optimizing computational resources. Architecturally, this 56B activated (560B total) parameter model employs 128 layers (Mamba2, Attention, FFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear complexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE structure. Pre-trained on 16T high-quality tokens, it supports a 256K context length and is the first industry-deployed large-scale Mamba model. Our comprehensive post-training strategy enhances capabilities via Supervised Fine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method, Multi-round Deliberation Learning for iterative improvement, and a two-stage Large-scale Reinforcement Learning process targeting STEM and general instruction-following. Evaluations show strong performance: overall top 7 rank on LMSYS Chatbot Arena with a score of 1356, outperforming leading models like Gemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves an average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances high performance and efficiency, offering substantial capabilities at lower inference costs than many reasoning models, establishing a new paradigm for efficient large-scale pre-trained models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-04T06:36:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.15431v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.15431v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 Robust and Efficient Embedded Convex Optimization through First-Order
  Adaptive Caching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ishaan Mahajan, Brian Plancher
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in Model Predictive Control (MPC) leveraging a combination of first-order methods, such as the Alternating Direction Method of Multipliers (ADMM), and offline precomputation and caching of select operations, have excitingly enabled real-time MPC on microcontrollers. Unfortunately, these approaches require the use of fixed hyperparameters, limiting their adaptability and overall performance. In this work, we introduce First-Order Adaptive Caching, which precomputes not only select matrix operations but also their sensitivities to hyperparameter variations, enabling online hyperparameter updates without full recomputation of the cache. We demonstrate the effectiveness of our approach on a number of dynamic quadrotor tasks, achieving up to a 63.4% reduction in ADMM iterations over the use of optimized fixed hyperparameters and approaching 70% of the performance of a full cache recomputation, while reducing the computational cost from O(n^3) to O(n^2) complexity. This performance enables us to perform figure-eight trajectories on a 27g tiny quadrotor under wind disturbances. We release our implementation open-source for the benefit of the wider robotics community.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-04T00:16:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>math.OC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.03231v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.03231v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 HGCA: Hybrid GPU-CPU Attention for Long Context LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weishu Deng, Yujie Yang, Peiran Du, Lingfeng Xiang, Zhen Lin, Chen Zhong, Song Jiang, Hui Lu, Jia Rao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Scaling inference for large language models (LLMs) is increasingly constrained by limited GPU memory, especially due to growing key-value (KV) caches required for long-context generation. While existing approaches offload KV caches to CPU memory or apply sparse attention to reduce GPU load, they often underutilize CPU compute resources and compromise accuracy. We present HGCA, a hybrid CPU-GPU attention mechanism that enables scalable, high-throughput LLM inference with near-full attention quality. HGCA performs dense attention on recently generated KV entries retained in GPU memory and parallel sparse attention on selected, salient KV entries in CPU memory. The attention outputs are efficiently merged using log-sum-exp fusion, minimizing PCIe transfer overhead. HGCA also introduces a finegrained, per-head sparsification strategy optimized for CPU execution, preserving contextual relevance while reducing computation. Our implementation seamlessly integrates into existing LLM frameworks without requiring model retraining. Experiments across diverse models and workloads show that HGCA achieves superior scalability, supports longer sequences and larger batch sizes, and outperforms existing sparse attention baselines in both performance and accuracy -- all on commodity GPU hardware.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-03T20:20:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.03153v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.03153v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 Less is Enough: Training-Free Video Diffusion Acceleration via
  Runtime-Adaptive Caching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xin Zhou, Dingkang Liang, Kaijin Chen, Tianrui Feng, Xiwu Chen, Hongkai Lin, Yikang Ding, Feiyang Tan, Hengshuang Zhao, Xiang Bai
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Video generation models have demonstrated remarkable performance, yet their broader adoption remains constrained by slow inference speeds and substantial computational costs, primarily due to the iterative nature of the denoising process. Addressing this bottleneck is essential for democratizing advanced video synthesis technologies and enabling their integration into real-world applications. This work proposes EasyCache, a training-free acceleration framework for video diffusion models. EasyCache introduces a lightweight, runtime-adaptive caching mechanism that dynamically reuses previously computed transformation vectors, avoiding redundant computations during inference. Unlike prior approaches, EasyCache requires no offline profiling, pre-computation, or extensive parameter tuning. We conduct comprehensive studies on various large-scale video generation models, including OpenSora, Wan2.1, and HunyuanVideo. Our method achieves leading acceleration performance, reducing inference time by up to 2.1-3.3$\times$ compared to the original baselines while maintaining high visual fidelity with a significant up to 36% PSNR improvement compared to the previous SOTA method. This improvement makes our EasyCache a efficient and highly accessible solution for high-quality video generation in both research and practical applications. The code is available at https://github.com/H-EmbodVis/EasyCache.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-03T17:59:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.02860v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.02860v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 HybridTier: an Adaptive and Lightweight CXL-Memory Tiering System</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kevin Song, Jiacheng Yang, Zixuan Wang, Jishen Zhao, Sihang Liu, Gennady Pekhimenko
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modern workloads are demanding increasingly larger memory capacity. Compute Express Link (CXL)-based memory tiering has emerged as a promising solution for addressing this problem by utilizing traditional DRAM alongside slow-tier CXL memory devices. We analyze prior tiering systems and observe two challenges for high-performance memory tiering: adapting to skewed but dynamically varying data hotness distributions while minimizing memory and cache overhead due to tiering.   To address these challenges, we propose HybridTier, an adaptive and lightweight tiering system for CXL memory. HybridTier tracks both long-term data access frequency and short-term access momentum \emph{simultaneously} to accurately capture and adapt to shifting hotness distributions. HybridTier reduces the metadata memory overhead by tracking data accesses \emph{probabilistically}, obtaining higher memory efficiency by trading off a small amount of tracking inaccuracy that has a negligible impact on application performance. To reduce cache overhead, HybridTier uses lightweight data structures that optimize for data locality to track data hotness. Our evaluations show that HybridTier outperforms prior systems by up to $91\%$ ($19\%$ geomean), incurring $2.0-7.8\times$ less memory overhead and $1.7-3.5\times$ less cache misses.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-03T17:11:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.OS</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3676642.3736119' target='_blank'>doi</a><a href='http://arxiv.org/abs/2312.04789v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2312.04789v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache
  Compression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Michael R. Metel, Boxing Chen, Mehdi Rezagholizadeh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Several works have developed eviction policies to remove key-value (KV) pairs from the KV cache for more efficient inference. The focus has been on compressing the KV cache after the input prompt has been processed for faster token generation. In settings with limited GPU memory, and when the input context is longer than the generation length, we show that by also compressing the KV cache during the input processing phase, larger batch sizes can be used resulting in significantly higher throughput while still maintaining the original model's accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-03T16:06:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.05693v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.05693v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device
  Speculative Decoding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ramchalam Kinattinkara Ramakrishnan, Zhaocong Yuan, Shaojie Zhuo, Chen Feng, Yicheng Lin, Chenzheng Su, Xiaopeng Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Speculative decoding generally dictates having a small, efficient draft model that is either pretrained or distilled offline to a particular target model series, for instance, Llama or Qwen models. However, within online deployment settings, there are two major challenges: 1) usage of a target model that is incompatible with the draft model; 2) expectation of latency improvements over usage and time. In this work, we propose OmniDraft, a unified framework that enables a single draft model to operate with any target model and adapt dynamically to user data. We introduce an online n-gram cache with hybrid distillation fine-tuning to address the cross-vocabulary mismatch across draft and target models; and further improve decoding speed by leveraging adaptive drafting techniques. OmniDraft is particularly suitable for on-device LLM applications where model cost, efficiency and user customization are the major points of contention. This further highlights the need to tackle the above challenges and motivates the \textit{``one drafter for all''} paradigm. We showcase the proficiency of the OmniDraft framework by performing online learning on math reasoning, coding and text generation tasks. Notably, OmniDraft enables a single Llama-68M model to pair with various target models including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding; and additionally provides up to 1.5-2x speedup.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-03T14:20:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.02659v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.02659v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 Skip-Vision: Efficient and Scalable Acceleration of Vision-Language
  Models via Adaptive Token Skipping</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weili Zeng, Ziyuan Huang, Kaixiang Ji, Yichao Yan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformer-based models have driven significant advancements in Multimodal Large Language Models (MLLMs), yet their computational costs surge drastically when scaling resolution, training data, and model parameters. A key bottleneck stems from the proliferation of visual tokens required for fine-grained image understanding. We propose Skip-Vision, a unified framework addressing both training and inference inefficiencies in vision-language models. On top of conventional token compression approaches, our method introduces two complementary acceleration strategies. For training acceleration, we observe that Feed-Forward Network (FFN) computations on visual tokens induce marginal feature updates. This motivates our Skip-FFN strategy, which bypasses FFN layers for redundant visual tokens. For inference acceleration, we design a selective KV-cache removal mechanism that prunes the skipped key-value pairs during decoding while preserving model performance. Experimental results demonstrate that Skip-Vision reduces training time by up to 35\%, inference FLOPs by 75\%, and latency by 45\%, while achieving comparable or superior performance to existing methods. Our work provides a practical solution for scaling high-performance MLLMs with enhanced efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-03T08:22:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.21817v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.21817v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 Direct Reconstruction of Terahertz-driven Subcycle Electron Emission
  Dynamics</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiakang Mao, Yushan Zeng, Hongyang Li, Liwei Song, Ye Tian, Ruxin Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While field-driven electron emission is theoretically understood down to the subcycle regime, its direct experimental temporal characterization using long-wavelength terahertz (THz) fields remains elusive. Here, by driving a graphite tip with phase-stable quasi-single-cycle THz pulses, we reveal distinct subcycle electron emission dynamics including: (1) At a carrier-envelope phase (CEP) near zero, spectral peaks scale linearly with THz field strength, characteristic of subcycle emission; (2) At the opposite CEP, dominant deceleration fields generate stationary low-energy peaks. Crucially, we develop a pump-probe-free, direct reconstruction method extracting electron pulse profiles solely from measured energy spectra, obtaining durations from 97.3 to 114.3 fs as the field increases (191-290 kV/cm). Phase-resolved simulations further reveal a 71.2% modulation in the cutoff energy and a near-total (99.7%) suppression of the emission current. This work not only validates the Fowler-Nordheim model under THz excitation but also establishes a general framework for the direct temporal characterization of subcycle electron emission, opening pathways for precise electron control in ultrafast electron sources and lightwave nanoelectronics.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-03T07:49:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.optics</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.02397v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.02397v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV
  Cache and Parallel Decoding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, Enze Xie
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion-based large language models (Diffusion LLMs) have shown promise for non-autoregressive text generation with parallel decoding capabilities. However, the practical inference speed of open-sourced Diffusion LLMs often lags behind autoregressive models due to the lack of Key-Value (KV) Cache and quality degradation when decoding multiple tokens simultaneously. To bridge this gap, we introduce a novel block-wise approximate KV Cache mechanism tailored for bidirectional diffusion models, enabling cache reuse with negligible performance drop. Additionally, we identify the root cause of generation quality degradation in parallel decoding as the disruption of token dependencies under the conditional independence assumption. To address this, we propose a confidence-aware parallel decoding strategy that selectively decodes tokens exceeding a confidence threshold, mitigating dependency violations and maintaining generation quality. Experimental results on LLaDA and Dream models across multiple LLM benchmarks demonstrate up to \textbf{27.6$\times$ throughput} improvement with minimal accuracy loss, closing the performance gap with autoregressive models and paving the way for practical deployment of Diffusion LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-03T04:51:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.22618v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.22618v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 PhysicsCorrect: A Training-Free Approach for Stable Neural PDE
  Simulations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xinquan Huang, Paris Perdikaris
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Neural networks have emerged as powerful surrogates for solving partial differential equations (PDEs), offering significant computational speedups over traditional methods. However, these models suffer from a critical limitation: error accumulation during long-term rollouts, where small inaccuracies compound exponentially, eventually causing complete divergence from physically valid solutions. We present PhysicsCorrect, a training-free correction framework that enforces PDE consistency at each prediction step by formulating correction as a linearized inverse problem based on PDE residuals. Our key innovation is an efficient caching strategy that precomputes the Jacobian and its pseudoinverse during an offline warm-up phase, reducing computational overhead by two orders of magnitude compared to standard correction approaches. Across three representative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and the chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction errors by up to 100x while adding negligible inference time (under 5\%). The framework integrates seamlessly with diverse architectures including Fourier Neural Operators, UNets, and Vision Transformers, effectively transforming unstable neural surrogates into reliable simulation tools that bridge the gap between deep learning's computational efficiency and the physical fidelity demanded by practical scientific applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-03T01:22:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.02227v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.02227v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 Autoregressive Image Generation with Linear Complexity: A Spatial-Aware
  Decay Perspective</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuxin Mao, Zhen Qin, Jinxing Zhou, Hui Deng, Xuyang Shen, Bin Fan, Jing Zhang, Yiran Zhong, Yuchao Dai
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Autoregressive (AR) models have garnered significant attention in image generation for their ability to effectively capture both local and global structures within visual data. However, prevalent AR models predominantly rely on the transformer architectures, which are beset by quadratic computational complexity concerning input sequence length and substantial memory overhead due to the necessity of maintaining key-value caches. Although linear attention mechanisms have successfully reduced this burden in language models, our initial experiments reveal that they significantly degrade image generation quality because of their inability to capture critical long-range dependencies in visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a novel attention mechanism that explicitly preserves genuine 2D spatial relationships within the flattened image sequences by computing position-dependent decay factors based on true 2D spatial location rather than 1D sequence positions. Based on this mechanism, we present LASADGen, an autoregressive image generator that enables selective attention to relevant spatial contexts with linear complexity. Experiments on ImageNet show LASADGen achieves state-of-the-art image generation performance and computational efficiency, bridging the gap between linear attention's efficiency and spatial understanding needed for high-quality generation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-02T12:27:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.MM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.01652v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.01652v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 Learned-Database Systems Security</h2>
                <div class="authors">
                    <strong>Authors:</strong> Roei Schuster, Jin Peng Zhou, Thorsten Eisenhofer, Paul Grubbs, Nicolas Papernot
                </div>
                <div class="summary">
                    <strong>Summary:</strong> A learned database system uses machine learning (ML) internally to improve performance. We can expect such systems to be vulnerable to some adversarial-ML attacks. Often, the learned component is shared between mutually-distrusting users or processes, much like microarchitectural resources such as caches, potentially giving rise to highly-realistic attacker models. However, compared to attacks on other ML-based systems, attackers face a level of indirection as they cannot interact directly with the learned model. Additionally, the difference between the attack surface of learned and non-learned versions of the same system is often subtle. These factors obfuscate the de-facto risks that the incorporation of ML carries. We analyze the root causes of potentially-increased attack surface in learned database systems and develop a framework for identifying vulnerabilities that stem from the use of ML. We apply our framework to a broad set of learned components currently being explored in the database community. To empirically validate the vulnerabilities surfaced by our framework, we choose 3 of them and implement and evaluate exploits against these. We show that the use of ML cause leakage of past queries in a database, enable a poisoning attack that causes exponential memory blowup in an index structure and crashes it in seconds, and enable index users to snoop on each others' key distributions by timing queries over their own keys. We find that adversarial ML is an universal threat against learned components in database systems, point to open research gaps in our understanding of learned-systems security, and conclude by discussing mitigations, while noting that data leakage is inherent in systems whose learned component is shared between multiple parties.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-02T10:16:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2212.10318v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2212.10318v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 A new efficient RPKI Design</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haya Schulmann, Niklas Vogel
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Resource Public Key Infrastructure (RPKI) is a critical security mechanism for BGP, but the complexity of its architecture is a growing concern as its adoption scales. Current RPKI design heavily reuses legacy PKI components, such as X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols, all these introduce excessive cryptographic validation, redundant metadata, and inefficiencies in both storage and processing. We show that these design choices, although based on established standards, create significant performance bottlenecks, increase the vulnerability surface, and hinder scalability for wide-scale Internet deployment.   In this paper, we perform the first systematic analysis of the root causes of complexity in RPKI's design and experimentally quantify their real-world impact. We show that over 70% of validation time in RPKI relying parties is spent on certificate parsing and signature verification, much of it unnecessary. Building on this insight, we introduce the improved RPKI (iRPKI), a backwards-compatible redesign that preserves all security guarantees while substantially reducing protocol overhead. iRPKI eliminates EE-certificates and ROA signatures, merges revocation and integrity objects, replaces verbose encodings with Protobuf, and restructures repository metadata for more efficient access. We experimentally demonstrate that our implementation of iRPKI in the Routinator validator achieves a 20x speed-up of processing time, 18x improvement of bandwidth requirements and 8x reduction in cache memory footprint, while also eliminating classes of vulnerabilities that have led to at least 10 vulnerabilities in RPKI software. iRPKI significantly increases the feasibility of deploying RPKI at scale in the Internet, and especially in constrained environments. Our design may be deployed incrementally without impacting existing operations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-02T08:24:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.01465v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.01465v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zheyu Shen, Yexiao He, Ziyao Wang, Yuning Zhang, Guoheng Sun, Wanghao Ye, Ang Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have gained significant attention due to their versatility across a wide array of applications. Fine-tuning LLMs with parameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these models to efficiently adapt to downstream tasks without extensive retraining. Deploying fine-tuned LLMs on multi-tenant edge devices offers substantial benefits, such as reduced latency, enhanced privacy, and personalized responses. However, serving LLMs efficiently on resource-constrained edge devices presents critical challenges, including the complexity of adapter selection for different tasks and memory overhead from frequent adapter swapping. Moreover, given the multiple requests in multi-tenant settings, processing requests sequentially results in underutilization of computational resources and increased latency. This paper introduces EdgeLoRA, an efficient system for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA incorporates three key innovations: (1) an adaptive adapter selection mechanism to streamline the adapter configuration process; (2) heterogeneous memory management, leveraging intelligent adapter caching and pooling to mitigate memory operation overhead; and (3) batch LoRA inference, enabling efficient batch processing to significantly reduce computational latency. Comprehensive evaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly outperforms the status quo (i.e., llama.cpp) in terms of both latency and throughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times boost in throughput. Even more impressively, it can serve several orders of magnitude more adapters simultaneously. These results highlight EdgeLoRA's potential to transform edge deployment of LLMs in multi-tenant scenarios, offering a scalable and efficient solution for resource-constrained environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-02T07:47:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3711875.3729141' target='_blank'>doi</a><a href='http://arxiv.org/abs/2507.01438v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.01438v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV
  Management on a Single Commodity GPU</h2>
                <div class="authors">
                    <strong>Authors:</strong> He Sun, Li Li, Mingjun Xiao, Chengzhong Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Advanced Large Language Models (LLMs) have achieved impressive performance across a wide range of complex and long-context natural language tasks. However, performing long-context LLM inference locally on a commodity GPU (a PC) with privacy concerns remains challenging due to the increasing memory demands of the key-value (KV) cache. Existing systems typically identify important tokens and selectively offload their KV data to GPU and CPU memory. The KV data needs to be offloaded to disk due to the limited memory on a commodity GPU, but the process is bottlenecked by token importance evaluation overhead and the disk's low bandwidth. In this paper, we present LeoAM, the first efficient importance-aware long-context LLM inference system for a single commodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system employs an adaptive KV management strategy that partitions KV data into variable-sized chunks based on the skewed distribution of attention weights across different layers to reduce computational and additional transmission overheads. Moreover, we propose a lightweight KV abstract method, which minimizes transmission latency by storing and extracting the KV abstract of each chunk on disk instead of the full KV data. LeoAM also leverages the dynamic compression and pipeline techniques to further accelerate inference. Experimental results demonstrate that LongInfer achieves an average inference latency speedup of 3.46x, while maintaining comparable LLM response quality. In scenarios with larger batch sizes, it achieves up to a 5.47x speedup.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-02T05:12:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.OS</span><span>cs.CR</span><span>68M20</span><span>C.4</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.20187v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.20187v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shakya Jayakody, Youpeng Zhao, Jun Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Graph convolutional networks (GCNs) are fundamental in various scientific applications, ranging from biomedical protein-protein interactions (PPI) to large-scale recommendation systems. An essential component for modeling graph structures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As the size of graph data continues to scale up, SpGEMMs are often conducted in an out-of-core fashion due to limited GPU memory space in resource-constrained systems. Albeit recent efforts that aim to alleviate the memory constraints of out-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory layout, or performing the computation in sparse format, current systems suffer from both high I/O latency and GPU under-utilization issues.   In this paper, we first identify the problems of existing systems, where sparse format data alignment and memory allocation are the main performance bottlenecks, and propose AIRES, a novel algorithm-system co-design solution to accelerate out-of-core SpGEMM computation for GCNs. Specifically, from the algorithm angle, AIRES proposes to alleviate the data alignment issues on the block level for matrices in sparse formats and develops a tiling algorithm to facilitate row block-wise alignment. On the system level, AIRES employs a three-phase dynamic scheduling that features a dual-way data transfer strategy utilizing a tiered memory system: integrating GPU memory, GPU Direct Storage (GDS), and host memory to reduce I/O latency and improve throughput. Evaluations show that AIRES significantly outperforms the state-of-the-art methods, achieving up to 1.8x lower latency in real-world graph processing benchmarks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-02T00:35:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.02006v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.02006v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile
  Device via Additive Side-Tuning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xingke Yang, Liang Li, Zhiyi Wan, Sicong Li, Hao Wang, Xiaoqi Qi, Jiang Liu, Tomoaki Ohtsuki, Xin Fu, Miao Pan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> There is a huge gap between numerous intriguing applications fostered by on-device large language model (LLM) fine-tuning (FT) from fresh mobile data and the limited resources of a mobile device. While existing server-assisted methods (e.g., split learning or side-tuning) may enable LLM FT on the local mobile device, they suffer from heavy communication burdens of activation transmissions, and may disclose data, labels or fine-tuned models to the server. To address those issues, we develop PAE MobiLLM, a privacy-aware and efficient LLM FT method which can be deployed on the mobile device via server-assisted additive side-tuning. To further accelerate FT convergence and improve computing efficiency, PAE MobiLLM integrates activation caching on the server side, which allows the server to reuse historical activations and saves the mobile device from repeatedly computing forward passes for the recurring data samples. Besides, to reduce communication cost, PAE MobiLLM develops a one-token (i.e., ``pivot'' token) activation shortcut that transmits only a single activation dimension instead of full activation matrices to guide the side network tuning. Last but not least, PAE MobiLLM introduces the additive adapter side-network design which makes the server train the adapter modules based on device-defined prediction differences rather than raw ground-truth labels. In this way, the server can only assist device-defined side-network computing, and learn nothing about data, labels or fine-tuned models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-01T22:27:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.01216v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.01216v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anirud Aggarwal, Abhinav Shrivastava, Matthew Gwilliam
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion-based image generation models excel at producing high-quality synthetic content, but suffer from slow and computationally expensive inference. Prior work has attempted to mitigate this by caching and reusing features within diffusion transformers across inference steps. These methods, however, often rely on rigid heuristics that result in limited acceleration or poor generalization across architectures. We propose Evolutionary Caching to Accelerate Diffusion models (ECAD), a genetic algorithm that learns efficient, per-model, caching schedules forming a Pareto frontier, using only a small set of calibration prompts. ECAD requires no modifications to network parameters or reference images. It offers significant inference speedups, enables fine-grained control over the quality-latency trade-off, and adapts seamlessly to different diffusion models. Notably, ECAD's learned schedules can generalize effectively to resolutions and model variants not seen during calibration. We evaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1$.$dev using multiple metrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k, PartiPrompts), demonstrating consistent improvements over previous approaches. On PixArt-alpha, ECAD identifies a schedule that outperforms the previous state-of-the-art method by 4.47 COCO FID while increasing inference speedup from 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable approach for accelerating diffusion inference. Our project website is available at https://aniaggarwal.github.io/ecad and our code is available at https://github.com/aniaggarwal/ecad.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-01T21:27:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.15682v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.15682v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 FlashDP: Private Training Large Language Models with Efficient DP-SGD</h2>
                <div class="authors">
                    <strong>Authors:</strong> Liangyu Wang, Junxiao Wang, Jie Ren, Zihang Xiang, David E. Keyes, Di Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As large language models (LLMs) increasingly underpin technological advancements, the privacy of their training data emerges as a critical concern. Differential Privacy (DP) serves as a rigorous mechanism to protect this data, yet its integration via Differentially Private Stochastic Gradient Descent (DP-SGD) introduces substantial challenges, primarily due to the complexities of per-sample gradient clipping. Current explicit methods, such as Opacus, necessitate extensive storage for per-sample gradients, significantly inflating memory requirements. Conversely, implicit methods like GhostClip reduce storage needs by recalculating gradients multiple times, which leads to inefficiencies due to redundant computations. This paper introduces FlashDP, an innovative cache-friendly per-layer DP-SGD that consolidates necessary operations into a single task, calculating gradients only once in a fused manner. This approach not only diminishes memory movement by up to \textbf{50\%} but also cuts down redundant computations by \textbf{20\%}, compared to previous methods. Consequently, FlashDP does not increase memory demands and achieves a \textbf{90\%} throughput compared to the Non-DP method on a four-A100 system during the pre-training of the Llama-13B model, while maintaining parity with standard per-layer clipped DP-SGD in terms of accuracy. These advancements establish FlashDP as a pivotal development for efficient and privacy-preserving training of LLMs. FlashDP's code has been open-sourced in https://github.com/kaustpradalab/flashdp.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-01T19:28:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.01154v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.01154v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 Integrating nano- and micrometer-scale energy deposition models for
  mechanistic prediction of radiation-induced DNA damage and cell survival</h2>
                <div class="authors">
                    <strong>Authors:</strong> Giulio Bordieri, Marta Missiggia, Gianluca Lattanzi, Carmen Villagrasa, Yann Perrot, Francesco G. Cordoni
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present an integrated modeling framework that combines the Generalized Stochastic Microdosimetric Model (GSM2), used to predict cell survival fractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for simulating radiation-induced DNA damage in cell populations. This approach enables the generation of spatially and structurally resolved double-strand break (DSB) distributions, capturing key features such as damage complexity and chromosome specificity. A novel application of the DBSCAN clustering algorithm is introduced to group DSBs at the micrometer scale. This allows the identification of physical aggregates of DNA damage and their association with subnuclear domains, providing a direct link to the cell survival probability as predicted by \gsm.   The model was validated using experimental data from HUVEC cells irradiated with 220 kV X-rays and H460 cells exposed to protons over a wide linear energy transfer (LET) range, from approximately 4 keV/{\mu}m to over 20 keV/{\mu}m. Results show excellent agreement between simulations and experimental survival probabilities, making this one of the first consistent multi-scale models to bridge nanodosimetric and microdosimetric representations of radiation with biological outcomes such as cell survival.   By incorporating the inherent stochastic nature of radiation-matter interactions, this framework effectively connects the physical properties of the radiation field to the biological response at the cellular level. Its accuracy across various radiation types and energies supports its potential for use in biologically optimized radiotherapy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-01T16:36:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.bio-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.00929v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.00929v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction
  and Dataflow-flexible Accelerator</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhican Wang, Hongxiang Fan, Haroon Waris, Gang Wang, Zhenyu Li, Jianfei Jiang, Yanan Sun, Guanghui He
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) excel in natural language processing tasks but pose significant computational and memory challenges for edge deployment due to their intensive resource demands. This work addresses the efficiency of LLM inference by algorithm-hardware-dataflow tri-optimizations. We propose a novel voting-based KV cache eviction algorithm, balancing hardware efficiency and algorithm accuracy by adaptively identifying unimportant kv vectors. From a dataflow perspective, we introduce a flexible-product dataflow and a runtime reconfigurable PE array for matrix-vector multiplication. The proposed approach effectively handles the diverse dimensional requirements and solves the challenges of incrementally varying sequence lengths. Additionally, an element-serial scheduling scheme is proposed for nonlinear operations, such as softmax and layer normalization (layernorm). Results demonstrate a substantial reduction in latency, accompanied by a significant decrease in hardware complexity, from O(N) to O(1). The proposed solution is realized in a custom-designed accelerator, VEDA, which outperforms existing hardware platforms. This research represents a significant advancement in LLM inference on resource-constrained edge devices, facilitating real-time processing, enhancing data privacy, and enabling model customization.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-01T14:30:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.00797v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.00797v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 On Hierarchical Coded Caching with Offline Users</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rashid Ummer N. T., B. Sundar Rajan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper studies a two-layer hierarchical network in which some users are offline during the content delivery phase. A two-layer hierarchical network consists of a single server connected to multiple cache-aided mirror sites, and each mirror site is connected to a distinct set of cache-aided users. A scheme for such a hierarchical system with offline users has been proposed recently but considered a special case where all mirror caches have zero memory, which is a significant limitation. We propose an array known as a hierarchical hotplug placement delivery array (HHPDA), which describes the placement and delivery phases of a coded caching scheme for a general two-layer hierarchical network with offline users. Further, we construct a class of HHPDAs using combinatorial t-designs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-01T13:17:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.00727v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.00727v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 Accelerating Loading WebGraphs in ParaGrapher</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mohsen Koohi Esfahani
                </div>
                <div class="summary">
                    <strong>Summary:</strong> ParaGrapher is a graph loading API and library that enables graph processing frameworks to load large-scale compressed graphs with minimal overhead. This capability accelerates the design and implementation of new high-performance graph algorithms and their evaluation on a wide range of graphs and across different frameworks. However, our previous study identified two major limitations in ParaGrapher: inefficient utilization of high-bandwidth storage and reduced decompression bandwidth due to increased compression ratios. To address these limitations, we present two optimizations for ParaGrapher in this paper. To improve storage utilization, particularly for high-bandwidth storage, we introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE (Filesystem in User Space). PG-Fuse optimizes storage access by increasing the size of requested blocks, reducing the number of calls to the underlying filesystem, and caching the received blocks in memory for future calls. To improve the decompression bandwidth, we introduce CompBin, a compact binary representation of the CSR format. CompBin facilitates direct accesses to neighbors while preventing storage usage for unused bytes. Our evaluation on 12 real-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse and CompBin achieve up to 7.6 and 21.8 times speedup, respectively.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-01T12:51:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.00716v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.00716v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 EARN: Efficient Inference Acceleration for LLM-based Generative
  Recommendation by Register Tokens</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chaoqun Yang, Xinyu Lin, Wenjie Wang, Yongqi Li, Teng Sun, Xianjing Han, Tat-Seng Chua
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Model-based generative recommendation (LLMRec) has achieved notable success, but it suffers from high inference latency due to massive computational overhead and memory pressure of KV Cache. Existing KV Cache reduction methods face critical limitations: cache compression offers marginal acceleration given recommendation tasks' short decoding steps, while prompt compression risks discarding vital interaction history. Through systematic analysis of attention patterns in LLMRec, we uncover two pivotal insights: 1) layer-wise attention sparsity inversion where early layers retain dense informative patterns while later layers exhibit high redundancy, and 2) dual attention sinks phenomenon where attention scores concentrate on both head and tail tokens of input sequences. Motivated by these insights, we propose EARN, an efficient inference framework that leverages the early layers to compress information into register tokens placed at the input sequence boundaries, then focuses solely on these tokens in the subsequent layers. Extensive experiments on three datasets, two LLMRec methods and two LLM architectures demonstrate EARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction with better accuracy than the general finetuning approach. Our work bridges the efficiency-effectiveness gap in LLMRec, offering practical deployment advantages for industrial scenarios.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-01T12:42:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.00715v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.00715v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 Structural, dielectric, and ferroelectric characteristics of the
  low-temperature sintered 65PMN-35PT sample for electroceramic applications</h2>
                <div class="authors">
                    <strong>Authors:</strong> B. Ramachandran, N. Sudarshan, G. Mangamma, M. S. Ramachandra Rao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> A single-phase 65PMN-35PT ceramic was synthesized at a relatively low temperature (875 oC) using a modified columbite method. X-ray diffraction analysis confirmed the single-phase formation of perovskite 65PMN-35PT with a tetragonal structure. Morphological studies indicated that the sample consisted of small grains with a size of about 2 micro-m. The dielectric properties of the material demonstrate its relaxor behavior near the ferroelectric transition temperature, TC = 457 K. The saturation and remnant polarization values of approximately 25.9 and 20.1 micro-C cm-2 were achieved for an electrically poled sample. Additionally, the poling induced a negative internal electric field of about -0.2 kV cm-1 was detected due to the presence of ferroelectric nano-grains in this bulk 65PMN-35PT sample. These observed characteristics of the pyrochlore-free 65PMN-35PT ceramic are similar to those of its single-crystal counterpart.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-01T09:47:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.mtrl-sci</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1007/s10832-025-00423-y' target='_blank'>doi</a><a href='http://arxiv.org/abs/2507.00614v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.00614v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 Unleashing the Potential of All Test Samples: Mean-Shift Guided
  Test-Time Adaptation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jizhou Han, Chenhao Ding, SongLin Dong, Yuhang He, Xinyuan Gao, Yihong Gong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Visual-language models (VLMs) like CLIP exhibit strong generalization but struggle with distribution shifts at test time. Existing training-free test-time adaptation (TTA) methods operate strictly within CLIP's original feature space, relying on high-confidence samples while overlooking the potential of low-confidence ones. We propose MS-TTA, a training-free approach that enhances feature representations beyond CLIP's space using a single-step k-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA improves feature compactness and class separability, leading to more stable adaptation. Additionally, a cache of refined embeddings further enhances inference by providing Mean Shift enhanced logits. Extensive evaluations on OOD and cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms state-of-the-art training-free TTA methods, achieving robust adaptation without requiring additional training.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-01T06:22:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.00462v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.00462v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yanting Miao, William Loh, Pacal Poupart, Suraj Kothawade
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent work uses reinforcement learning (RL) to fine-tune text-to-image diffusion models, improving text-image alignment and sample quality. However, existing approaches introduce unnecessary complexity: they cache the full sampling trajectory, depend on differentiable reward models or large preference datasets, or require specialized guidance techniques. Motivated by the "golden noise" hypothesis -- that certain initial noise samples can consistently yield superior alignment -- we introduce Noise PPO, a minimalist RL algorithm that leaves the pre-trained diffusion model entirely frozen and learns a prompt-conditioned initial noise generator. Our approach requires no trajectory storage, reward backpropagation, or complex guidance tricks. Extensive experiments show that optimizing the initial noise distribution consistently improves alignment and sample quality over the original model, with the most significant gains at low inference steps. As the number of inference steps increases, the benefit of noise optimization diminishes but remains present. These findings clarify the scope and limitations of the golden noise hypothesis and reinforce the practical value of minimalist RL fine-tuning for diffusion models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-01T05:46:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.12036v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.12036v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache
  Compression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Payman Behnam, Yaosheng Fu, Ritchie Zhao, Po-An Tsai, Zhiding Yu, Alexey Tumanov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformer-based Large Language Models rely critically on the KV cache to efficiently handle extended contexts during the decode phase. Yet, the size of the KV cache grows proportionally with the input length, burdening both memory bandwidth and capacity as decoding progresses. To address this challenge, we present RocketKV, a training-free KV cache compression strategy containing two consecutive stages. In the first stage, it performs coarse-grain permanent KV cache eviction on the input sequence tokens. In the second stage, it adopts a hybrid sparse attention method to conduct fine-grain top-k sparse attention, approximating the attention scores by leveraging both head and sequence dimensionality reductions. We show that RocketKV provides a compression ratio of up to 400$\times$, end-to-end speedup of up to 3.7$\times$ as well as peak memory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU compared to the full KV cache baseline, while achieving negligible accuracy loss on a variety of long-context tasks. We also propose a variant of RocketKV for multi-turn scenarios, which consistently outperforms other existing methods and achieves accuracy nearly on par with an oracle top-k attention scheme.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-30T19:01:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.14051v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.14051v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 Combinatorial Multi-Access Coded Caching with Private Caches under
  Intersecting Index Constraints</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dhruv Pratap Singh, Anjana A. Mahesh, B. Sundar Rajan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We consider the coded caching system where each user, equipped with a private cache, accesses a distinct r-subset of access caches. A central server housing a library of files populates both private and access caches using uncoded placement. In this work, we focus on a constrained indexing regime, referred to as the intersection class, in which the sets used to index the demands of each user must have a nonempty intersection. This regime models resource-limited IoT scenarios such as edge-assisted IoT systems, where devices with small private caches connect to a small number of shared caches. We provide a necessary and sufficient condition under which the system parameters fall within this intersection class. Under this condition, we propose a centralized coded caching scheme and characterize its rate-memory trade-off. Next, we define a uniform-intersection subclass and establish a condition under which the system belongs to this subclass. Within this subclass, the proposed scheme has a regular structure, with each transmission benefiting the same number of users, and we characterize its rate-memory trade-off. Additionally, we derive an index coding-based lower bound on the minimum achievable worst-case rate under uncoded placement. Finally, we provide numerical comparisons between the rate of the proposed scheme, the new lower bound, and bounds from the original work.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-30T17:07:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.24060v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.24060v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 Full Version: (De/Re)-Composition of Data-Parallel Computations via
  Multi-Dimensional Homomorphisms</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ari Rasch
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We formally introduce a systematic (de/re)-composition approach, based on the algebraic formalism of "Multi-Dimensional Homomorphisms (MDHs)". Our approach is designed as general enough to be applicable to a wide range of data-parallel computations and for various kinds of target parallel architectures. To efficiently target the deep and complex memory and core hierarchies of contemporary architectures, we exploit our introduced (de/re)-composition approach for a correct-by-construction, parametrized cache blocking and parallelization strategy. We show that our approach is powerful enough to express, in the same formalism, the (de/re)-composition strategies of different classes of state-of-the-art approaches (scheduling-based, polyhedral, etc), and we demonstrate that the parameters of our strategies enable systematically generating code that can be fully automatically optimized (auto-tuned) for the particular target architecture and characteristics of the input and output data (e.g., their sizes and memory layouts). Particularly, our experiments confirm that via auto-tuning, we achieve higher performance than state-of-the-art approaches, including hand-optimized solutions provided by vendors (such as NVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a variety of data-parallel computations, including: linear algebra routines, stencil and quantum chemistry computations, data mining algorithms, and computations that recently gained high attention due to their relevance for deep learning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-30T16:23:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PL</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3665643' target='_blank'>doi</a><a href='http://arxiv.org/abs/2405.05118v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.05118v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 Large-scale Neural Network Quantum States for ab initio Quantum
  Chemistry Simulations on Fugaku</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hongtao Xu, Zibo Wu, Mingzhen Li, Weile Jia
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Solving quantum many-body problems is one of the fundamental challenges in quantum chemistry. While neural network quantum states (NQS) have emerged as a promising computational tool, its training process incurs exponentially growing computational demands, becoming prohibitively expensive for large-scale molecular systems and creating fundamental scalability barriers for real-world applications. To address above challenges, we present \ours, a high-performance NQS training framework for \textit{ab initio} electronic structure calculations. First, we propose a scalable sampling parallelism strategy with multi-layers workload division and hybrid sampling scheme, which break the scalability barriers for large-scale NQS training. Then, we introduce multi-level parallelism local energy parallelism, enabling more efficient local energy computation. Last, we employ cache-centric optimization for transformer-based \textit{ansatz} and incorporate it with sampling parallelism strategy, which further speedup up the NQS training and achieve stable memory footprint at scale. Experiments demonstrate that \ours accelerate NQS training with up to 8.41x speedup and attains a parallel efficiency up to 95.8\% when scaling to 1,536 nodes.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-30T12:55:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.23809v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.23809v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 VQ-LLM: High-performance Code Generation for Vector Quantization
  Augmented LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zihan Liu, Xinhao Luo, Junxian Guo, Wentao Ni, Yangjie Zhou, Yue Guan, Cong Guo, Weihao Cui, Yu Feng, Minyi Guo, Yuhao Zhu, Minjia Zhang, Jingwen Leng, Chen Jin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this work, we design and implement VQ-LLM, an efficient fused Vector Quantization (VQ) kernel generation framework. We first introduce a software abstraction called codebook cache to optimize codebook access efficiency and support the integration of VQ with various computations. The codebook cache adaptively stores different entries across the GPU's memory hierarchy, including off-chip global memory, on-chip shared memory, and registers. Centered around the codebook cache, we design an efficient computation engine that optimizes memory traffic during computations involving codebooks. This compute engine adopts the codebook-centric dataflow and fusion optimizations. Additionally, we provide adaptive heuristics to tailor parameter selection in our optimizations to diverse VQ configurations. Our optimizations achieve an average latency reduction of 46.13% compared to unoptimized versions. Compared to existing open-source implementations, our methods decrease latency by 64.36% to 99.1%. A final comparison with state-of-the-art element-wise quantization methods like AWQ and KVQuant shows that our VQ-LLM is practically viable, achieving latencies close or even better latencies to those at equivalent bit-widths, potentially offering greater accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-30T05:54:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/HPCA61900.2025.00112' target='_blank'>doi</a><a href='http://arxiv.org/abs/2503.02236v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.02236v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented
  Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhuocheng Zhang, Yang Feng, Min Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large language model applications, with numerous existing frameworks offering a wide range of functionalities to facilitate the development of RAG systems. However, we have identified several persistent challenges in these frameworks, including difficulties in algorithm reproduction and sharing, lack of new techniques, and high system overhead. To address these limitations, we introduce \textbf{FlexRAG}, an open-source framework specifically designed for research and prototyping. FlexRAG supports text-based, multimodal, and network-based RAG, providing comprehensive lifecycle support alongside efficient asynchronous processing and persistent caching capabilities. By offering a robust and flexible solution, FlexRAG enables researchers to rapidly develop, deploy, and share advanced RAG systems. Our toolkit and resources are available at \href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-30T05:45:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.12494v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.12494v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM
  Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yaoqi Chen, Jinkai Zhang, Baotong Lu, Qianxi Zhang, Chengruidong Zhang, Jingjia Luo, Di Liu, Huiqiang Jiang, Qi Chen, Jing Liu, Bailu Ding, Xiao Yan, Jiawei Jiang, Chen Chen, Mingxing Zhang, Yuqing Yang, Fan Yang, Mao Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The growing context lengths of large language models (LLMs) pose significant challenges for efficient inference, primarily due to GPU memory and bandwidth constraints. We present RetroInfer, a novel system that reconceptualizes the key-value (KV) cache as a vector storage system which exploits the inherent attention sparsity to accelerate long-context LLM inference. At its core is the wave index, an Attention-aWare VEctor index that enables efficient and accurate retrieval of critical tokens through techniques such as tripartite attention approximation, accuracy-bounded attention estimation, and segmented clustering. Complementing this is the wave buffer, which coordinates KV cache placement and overlaps computation and data transfer across GPU and CPU to sustain high throughput. Unlike prior sparsity-based methods that struggle with token selection and hardware coordination, RetroInfer delivers robust performance without compromising model accuracy. Experiments on long-context benchmarks show up to 4.5X speedup over full attention within GPU memory limits and up to 10.5X over sparse attention baselines when KV cache is extended to CPU memory, all while preserving full-attention-level accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-30T05:21:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.02922v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.02922v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent
  Metasurfaces</h2>
                <div class="authors">
                    <strong>Authors:</strong> Geng Sun, Mingzhe Fan, Lei Zhang, Hongyang Pan, Jiahui Li, Chuang Zhang, Linyao Li, Changyuan Zhao, Chau Yuen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Wireless communication systems face significant challenges in meeting the increasing demands for higher data rates and more reliable connectivity in complex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a promising technology for realizing wave-domain signal processing, with mobile SIMs offering superior communication performance compared to their fixed counterparts. In this paper, we investigate a novel unmanned aerial vehicle (UAV)-mounted SIMs (UAV-SIMs) assisted communication system within the low-altitude economy (LAE) networks paradigm, where UAVs function as both base stations that cache SIM-processed data and mobile platforms that flexibly deploy SIMs to enhance uplink communications from ground users. To maximize network capacity, we formulate a UAV-SIM-based joint optimization problem (USBJOP) that comprehensively addresses three critical aspects: the association between UAV-SIMs and users, the three-dimensional positioning of UAV-SIMs, and the phase shifts across multiple SIM layers. Due to the inherent non-convexity and NP-hardness of USBJOP, we decompose it into three sub-optimization problems, \textit{i.e.}, association between UAV-SIMs and users optimization problem (AUUOP), UAV location optimization problem (ULOP), and UAV-SIM phase shifts optimization problem (USPSOP), and solve them using an alternating optimization strategy. Specifically, we transform AUUOP and ULOP into convex forms solvable by the CVX tool, while addressing USPSOP through a generative artificial intelligence (GAI)-based hybrid optimization algorithm. Simulations demonstrate that our proposed approach significantly outperforms benchmark schemes, achieving approximately 1.5 times higher network capacity compared to suboptimal alternatives. Additionally, our proposed GAI method reduces the algorithm runtime by 10\% while maintaining solution quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-30T03:22:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.23488v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.23488v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 CMOS+X: Stacking Persistent Embedded Memories based on Oxide Transistors
  upon GPGPU Platforms</h2>
                <div class="authors">
                    <strong>Authors:</strong> Faaiq Waqar, Ming-Yen Lee, Seongwon Yoon, Seongkwang Lim, Shimeng Yu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In contemporary general-purpose graphics processing units (GPGPUs), the continued increase in raw arithmetic throughput is constrained by the capabilities of the register file (single-cycle) and last-level cache (high bandwidth), which require the delivery of operands at a cadence demanded by wide single-instruction multiple-data (SIMD) lanes. Enhancing the capacity, density, or bandwidth of these memories can unlock substantial performance gains; however, the recent stagnation of SRAM bit-cell scaling leads to inequivalent losses in compute density.   To address the challenges posed by SRAM's scaling and leakage power consumption, this paper explores the potential CMOS+X integration of amorphous oxide semiconductor (AOS) transistors in capacitive, persistent memory topologies (e.g., 1T1C eDRAM, 2T0C/3T0C Gain Cell) as alternative cells in multi-ported and high-bandwidth banked GPGPU memories. A detailed study of the density and energy tradeoffs of back-end-of-line (BEOL) integrated memories utilizing monolithic 3D (M3D)-integrated multiplexed arrays is conducted, while accounting for the macro-level limitations of integrating AOS candidate structures proposed by the device community (an aspect often overlooked in prior work). By exploiting the short lifetime of register operands, we propose a multi-ported AOS gain-cell capable of delivering 3x the read ports in ~76% of the footprint of SRAM with over 70% lower standby power, enabling enhancements to compute capacity, such as larger warp sizes or processor counts. Benchmarks run on a validated NVIDIA Ampere-class GPU model, using a modified version of Accel-Sim, demonstrate improvements of up to 5.2x the performance per watt and an average 8% higher geometric mean instruction per cycle (IPC) on various compute- and memory-bound tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-29T21:55:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.ET</span><span>cs.AR</span><span>B.8.2; B.3.1</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.23405v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.23405v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Giyong Jung, Saeid Gorgin, John Kim, Jungrae Kim
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As AI models outpace the capabilities of single processors, interconnects across chips have become a critical enabler for scalable computing. These processors exchange massive amounts of data at cache-line granularity, prompting the adoption of new interconnect protocols like CXL, NVLink, and UALink, designed for high bandwidth and small payloads. However, the increasing transfer rates of these protocols heighten susceptibility to errors. While mechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction (FEC) are standard for reliable data transmission, scaling chip interconnects to multi-node configurations introduces new challenges, particularly in managing silently dropped flits in switching devices. This paper introduces Implicit Sequence Number (ISN), a novel mechanism that ensures precise flit drop detection and in-order delivery without adding header overhead. Additionally, we propose Reliability Extended Link (RXL), an extension of CXL that incorporates ISN to support scalable, reliable multi-node interconnects while maintaining compatibility with the existing flit structure. By elevating CRC to a transport-layer mechanism for end-to-end data and sequence integrity, and relying on FEC for link-layer error correction and detection, RXL delivers robust reliability and scalability without compromising bandwidth efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-28T13:02:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.01988v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.01988v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in
  Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jianxin Yan, Wangze Ni, Lei Chen, Xuemin Lin, Peng Cheng, Zhan Qin, Kui Ren
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Semantic caching significantly reduces computational costs and improves efficiency by storing and reusing large language model (LLM) responses. However, existing systems rely primarily on matching individual queries, lacking awareness of multi-turn dialogue contexts, which leads to incorrect cache hits when similar queries appear in different conversational settings. This demonstration introduces ContextCache, a context-aware semantic caching system for multi-turn dialogues. ContextCache employs a two-stage retrieval architecture that first executes vector-based retrieval on the current query to identify potential matches and then integrates current and historical dialogue representations through self-attention mechanisms for precise contextual matching. Evaluation of real-world conversations shows that ContextCache improves precision and recall compared to existing methods. Additionally, cached responses exhibit approximately 10 times lower latency than direct LLM invocation, enabling significant computational cost reductions for LLM conversational applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-28T07:25:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.22791v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.22791v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document
  Retrieval</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tuan-Luc Huynh, Thuy-Trang Vu, Weiqing Wang, Yinwei Wei, Trung Le, Dragan Gasevic, Yuan-Fang Li, Thanh-Toan Do
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Differentiable Search Index (DSI) utilizes pre-trained language models to perform indexing and document retrieval via end-to-end learning without relying on external indexes. However, DSI requires full re-training to index new documents, causing significant computational inefficiencies. Continual learning (CL) offers a solution by enabling the model to incrementally update without full re-training. Existing CL solutions in document retrieval rely on memory buffers or generative models for rehearsal, which is infeasible when accessing previous training data is restricted due to privacy concerns. To this end, we introduce PromptDSI, a prompt-based, rehearsal-free continual learning approach for document retrieval. PromptDSI follows the Prompt-based Continual Learning (PCL) framework, using learnable prompts to efficiently index new documents without accessing previous documents or queries. To improve retrieval latency, we remove the initial forward pass of PCL, which otherwise greatly increases training and inference time, with a negligible trade-off in performance. Additionally, we introduce a novel topic-aware prompt pool that employs neural topic embeddings as fixed keys, eliminating the instability of prompt key optimization while maintaining competitive performance with existing PCL prompt pools. In a challenging rehearsal-free continual learning setup, we demonstrate that PromptDSI variants outperform rehearsal-based baselines, match the strong cache-based baseline in mitigating forgetting, and significantly improving retrieval performance on new corpora.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-28T06:24:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span><span>cs.AI</span><span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.12593v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.12593v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 Efficiently Serving Large Multimodal Models Using EPD Disaggregation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Gursimran Singh, Xinglu Wang, Yifan Hu, Timothy Yu, Linzi Xing, Wei Jiang, Zhefeng Wang, Xiaolong Bai, Yi Li, Ying Xiong, Yong Zhang, Zhenan Fan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by handling diverse inputs such as images, audio, and video, but at the cost of adding a multimodal encoding stage that increases both computational and memory overhead. This step negatively affects key Service Level Objectives (SLOs), such as time to first token (TTFT) and time per output token (TPOT). We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates the encoding, prefill, and decode stages onto dedicated resources. Unlike current systems, which bundle encoding and prefill together, our approach decouples these steps, unlocking new opportunities and optimizations. These include a mechanism to cache multimedia tokens for efficient transfer, a novel way to parallelize the encoding load within a request, a module for optimal resource allocation for disaggregated serving, and a novel role-switching method to handle changing workload characteristics. Experimental evaluations with popular LMMs show substantial gains in memory efficiency (up to 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more images per request, and 2.2x larger KV caches. Furthermore, it leads to significant improvements in SLO attainment (up to 90-100% improvement) and TTFT (up to 71% reduction), compared to systems that do not disaggregate. The code is available at https://github.com/vbdi/epdserve.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-28T03:53:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span><span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05460v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05460v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,
  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Danush Khanna, Aditya Kumar Guru, Srivarshinee Sridhar, Zidan Ahmed, Rubhav Bahirwani, Meetu Malhotra, Vinija Jain, Aman Chadha, Amitava Das, Kripabandhu Ghosh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Inference accounts for the majority of latency and energy consumption in large language model (LLM) deployments, often exceeding 90% of total cost. While training-time efficiency has seen extensive progress, runtime optimization remains a key bottleneck, particularly under autoregressive decoding. Existing approaches -- such as pruning, quantization, early exits, and speculative decoding -- often require retraining, architectural changes, or disrupt decoding compatibility. We introduce QuickSilver, a modular, token-level framework that enables semantic adaptivity at inference time without altering model weights or structure. QuickSilver integrates four synergistic mechanisms:   (i) Dynamic Token Halting, which halts computation for tokens with converged representations; (ii) KV Cache Skipping, which selectively suppresses memory writes to reduce attention overhead; and (iii) Contextual Token Fusion, which collapses redundant tokens into shared paths to shrink sequence length.   Unlike speculative decoding or MoE routing, QuickSilver operates entirely on frozen, dense models and requires no auxiliary networks. Applied to GPT-2 and Llama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP reduction with negligible perplexity degradation (<=0.2).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-27T17:10:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>I.2.0; I.2.7</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.22396v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.22396v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient
  Pipeline-Parallel LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yongchao He, Bohan Zhao, Zheng Cao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As inference workloads for large language models (LLMs) scale to meet growing user demand, pipeline parallelism (PP) has become a widely adopted strategy for multi-GPU deployment, particularly in cross-node setups, to improve key-value (KV) cache capacity and inference throughput. However, PP suffers from inherent inefficiencies caused by three types of execution bubbles-load-imbalance, intra-stage, and inter-stage-which limit pipeline saturation. We present SiPipe, a heterogeneous pipeline design that improves throughput by leveraging underutilized CPU resources to offload auxiliary computation and communication. SiPipe incorporates three key techniques-CPU sampling, a token-safe execution model, and structure-aware transmission-to mitigate pipeline bubbles and improve execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1 times higher throughput, 43% lower per-token latency, and up to 23% higher average GPU utilization compared to the state-of-the-art vLLM under the same PP configuration, demonstrating its generality across LLMs and deployment scenarios.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-27T09:27:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.22033v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.22033v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 ChunkKV: Semantic-Preserving KV Cache Compression for Efficient
  Long-Context LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiang Liu, Zhenheng Tang, Peijie Dong, Zeyu Li, Yue Liu, Bo Li, Xuming Hu, Xiaowen Chu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) require significant GPU memory when processing long texts, with the key value (KV) cache consuming up to 70\% of total memory during inference. Although existing compression methods reduce memory by evaluating the importance of individual tokens, they overlook critical semantic relationships between tokens, resulting in fragmented context and degraded performance. We introduce ChunkKV, which fundamentally reimagines KV cache compression by treating semantic chunks - rather than isolated tokens - as basic compression units. This approach preserves complete linguistic structures and contextual integrity, ensuring that essential meaning is retained even under aggressive compression. Our innovation includes a novel layer-wise index reuse technique that exploits the higher cross-layer similarity of preserved indices in ChunkKV, reducing computational overhead and improving throughput by 26.5\%. Comprehensive evaluations on challenging benchmarks: LongBench, Needle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV outperforms state-of-the-art methods by up to 8.7\% in precision while maintaining the same compression ratio. These results confirm that semantic-aware compression significantly enhances both efficiency and performance for long-context LLM inference, providing a simple yet effective solution to the memory bottleneck problem.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-27T09:14:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.00299v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.00299v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 A Survey of LLM Inference Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> James Pan, Guoliang Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The past few years has witnessed specialized large language model (LLM) inference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside rapid LLM adoption via services like ChatGPT. Driving these system design efforts is the unique autoregressive nature of LLM request processing, motivating new techniques for achieving high performance while preserving high inference quality over high-volume and high-velocity workloads. While many of these techniques are discussed across the literature, they have not been analyzed under the framework of a complete inference system, nor have the systems themselves been analyzed and compared.   In this survey, we review these techniques, starting from operators and algorithms for request processing, then moving on to techniques for model optimization and execution, including kernel design, batching, and scheduling, before ending with techniques for memory management, including paged memory, eviction and offloading techniques, quantization, and cache persistence. Through these discussions, we show that these techniques fundamentally rely on load prediction, adaptive mechanisms, and cost reduction in order to overcome the challenges introduced by autoregressive generation and achieve the goals of the system. We then discuss how these techniques can be combined to form single-replica and multi-replica inference systems, including disaggregated inference systems that offer more control over resource allocation and serverless systems that can be deployed over shared hardware infrastructure. We end with a discussion of remaining challenges.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-27T04:38:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.21901v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.21901v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 Round Attention: A Novel Round-Level Attention Mechanism to Accelerate
  LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yaohua Tang, Zhicheng Hu, Kun Cheng, Fan Mo, Qiheng Lv, Hua Wang, Zhi Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The increasing context window size in large language models (LLMs) has improved their ability to handle complex, long-text tasks. However, as the conversation rounds continue, it is required to store a large amount of KV cache in GPU memory, which significantly affects the efficiency and even availability of the model serving systems. This paper analyzes dialogue data from real users on the granularity of round and discovers that the LLM inference manifests a watershed layer, after which the distribution of round-level attention shows notable similarity. Based on this, we propose Round Attention - a novel round-level attention mechanism that selectively processes the KV cache of top-k relevant rounds, where k is dynamically determined through the attention matrix in the watershed layer. Theoretical analysis demonstrates that our method reduces memory usage by 54\% to 82\%, while experimental results confirm that loading sparse critical-round KV cache maintains answer accuracy without performance degradation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-27T03:43:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.15294v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.15294v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual
  Question Answering</h2>
                <div class="authors">
                    <strong>Authors:</strong> Liangyu Zhong, Fabio Rosenthal, Joachim Sicking, Fabian HÃ¼ger, Thorsten Bagdonat, Hanno Gottschalk, Leo Schwinn
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While Multimodal Large Language Models (MLLMs) offer strong perception and reasoning capabilities for image-text input, Visual Question Answering (VQA) focusing on small image details still remains a challenge. Although visual cropping techniques seem promising, recent approaches have several limitations: the need for task-specific fine-tuning, low efficiency due to uninformed exhaustive search, or incompatibility with efficient attention implementations. We address these shortcomings by proposing a training-free visual cropping method, dubbed FOCUS, that leverages MLLM-internal representations to guide the search for the most relevant image region. This is accomplished in four steps: first, we identify the target object(s) in the VQA prompt; second, we compute an object relevance map using the key-value (KV) cache; third, we propose and rank relevant image regions based on the map; and finally, we perform the fine-grained VQA task using the top-ranked region. As a result of this informed search strategy, FOCUS achieves strong performance across four fine-grained VQA datasets and two types of MLLMs. It outperforms three popular visual cropping methods in both accuracy and efficiency, and matches the best-performing baseline, ZoomEye, while requiring 3 - 6.5 x less compute.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-26T18:51:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.21710v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.21710v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 End-to-End Long Document Summarization using Gradient Caching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rohit Saxena, Hao Tang, Frank Keller
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Training transformer-based encoder-decoder models for long document summarization poses a significant challenge due to the quadratic memory consumption during training. Several approaches have been proposed to extend the input length at test time, but training with these approaches is still difficult, requiring truncation of input documents and causing a mismatch between training and test conditions. In this work, we propose CachED (Gradient $\textbf{Cach}$ing for $\textbf{E}$ncoder-$\textbf{D}$ecoder models), an approach that enables end-to-end training of existing transformer-based encoder-decoder models, using the entire document without truncation. Specifically, we apply non-overlapping sliding windows to input documents, followed by fusion in decoder. During backpropagation, the gradients are cached at the decoder and are passed through the encoder in chunks by re-computing the hidden vectors, similar to gradient checkpointing. In the experiments on long document summarization, we extend BART to CachED BART, processing more than 500K tokens during training and achieving superior performance without using any additional parameters.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-26T18:40:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.01805v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.01805v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 From Memories to Maps: Mechanisms of In-Context Reinforcement Learning
  in Transformers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ching Fang, Kanaka Rajan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Humans and animals show remarkable learning efficiency, adapting to new environments with minimal experience. This capability is not well captured by standard reinforcement learning algorithms that rely on incremental value updates. Rapid adaptation likely depends on episodic memory -- the ability to retrieve specific past experiences to guide decisions in novel contexts. Transformers provide a useful setting for studying these questions because of their ability to learn rapidly in-context and because their key-value architecture resembles episodic memory systems in the brain. We train a transformer to in-context reinforcement learn in a distribution of planning tasks inspired by rodent behavior. We then characterize the learning algorithms that emerge in the model. We first find that representation learning is supported by in-context structure learning and cross-context alignment, where representations are aligned across environments with different sensory stimuli. We next demonstrate that the reinforcement learning strategies developed by the model are not interpretable as standard model-free or model-based planning. Instead, we show that in-context reinforcement learning is supported by caching intermediate computations within the model's memory tokens, which are then accessed at decision time. Overall, we find that memory may serve as a computational resource, storing both raw experience and cached computations to support flexible behavior. Furthermore, the representations developed in the model resemble computations associated with the hippocampal-entorhinal system in the brain, suggesting that our findings may be relevant for natural cognition. Taken together, our work offers a mechanistic hypothesis for the rapid adaptation that underlies in-context learning in artificial and natural settings.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-26T17:18:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.19686v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.19686v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 Measurements, simulations, and models of the point-spread function of
  electron-beam lithography</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nikolaj B. Hougs, Kristian S. Knudsen, Marcus Albrechtsen, Taichi Suhara, Christian A. Rosiek, SÃ¸ren Stobbe
                </div>
                <div class="summary">
                    <strong>Summary:</strong> When a sample is exposed using electron-beam lithography, the electrons scatter deep and far in the substrate, resulting in unwanted deposition of dose at both the nano- and the microscale. This proximity effect can be mitigated by proximity effect correction provided that accurate and validated models of the point-spread function of the electron scattering are available. Most works so far considered a double-Gaussian model of the electron point-spread function, which is very inaccurate for modern electron-beam writers with high acceleration voltages. We present measurements of the process point-spread function for chemically semi-amplified resist on silicon and indium phosphide substrates using a 150 kV electron-beam lithography system. We find that the double-Gaussian model deviates from experiments by up to four orders of magnitude. We propose instead a model comprising the sum of a power-law and a Gaussian, which is in excellent agreement with simulations of the electron scattering obtained by a Monte Carlo method. We apply the power-law plus Gaussian model to quantify the electron scattering and proximity effect correction parameters across material stacks, processing, and voltages from 5 kV to 150 kV. We find that the power-law term remains remarkably constant, whereas the long-range dose contributions and the clearing dose are significantly affected by the substrate and the acceleration voltage.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-26T13:22:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.app-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.21236v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.21236v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 Task-Aware KV Compression For Cost-Effective Long Video Understanding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Minghao Qin, Yan Shu, Peitian Zhang, Kun Lun, Huaying Yuan, Juenjie Zhou, Shitao Xiao, Bo Zhao, Zheng Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Long-video understanding (LVU) remains a severe challenge for existing multimodal large language models (MLLMs), primarily due to the prohibitive computational cost. Recent approaches have explored KV compression to mitigate this issue, but they often suffer from significant information loss at high compression ratios. In this paper, we introduce Video-X^2L, which flexibly preserves critical video information for each LVU task. Video-X^2L involves two key operations. The first one is called bi-level KV compression. During the MLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs: low-compression KVs (L-KVs) to capture fine-grained video details and high-compression KVs (H-KVs) to offer compact video representations. The second one is called selective KV re-loading. During the MLLM's decoding stage, Video-X^2L selectively re-loads L-KVs for the most critical video chunks while using H-KVs for other less important ones. This allows the MLLM to fully utilize task-specific information while maintaining the overall compactness. Video-X^2L is simple yet effective: it is free from additional training and directly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L with a variety of popular LVU benchmarks, including VideoMME, MLVU, LongVideoBench, and VNBench. Our experiment result shows that Video-X^2L outperforms existing KV-compression methods by a huge advantage while substantially saving the computation cost.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-26T12:43:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.21184v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.21184v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas
  and Ad-Hoc Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Atonu Ghosh, Sudip Misra
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Minimal infrastructure requirements make LoRa suitable for service delivery in remote areas. Additionally, web applications have become a de-facto standard for modern service delivery. However, Long Range (LoRa) fails to enable HTTP access due to its limited bandwidth, payload size limitations, and high collisions in multi-user setups. We propose LoRaConnect to enable HTTP access over LoRa. The LoRaWeb hardware tethers a WiFi hotspot to which client devices connect and access HTTP resources over LoRa backhaul. It implements caching and synchronization mechanisms to address LoRa's aforementioned limitations. It also implements a message-slicing method in the application layer to overcome LoRa's payload limitations. We evaluate the proposed system using actual hardware in three experimental setups to assess the baseline performance, ideal scenario, and practical application scenario with Frequency Hopping Spread Spectrum (FHSS). Additionally, it implements a ping operation to demonstrate Internet capability and extensible nature. LoRaWeb achieves an average throughput of 1.18 KB/S approximately, with an access delay of only 1.3 S approximately for a 1.5KB webpage in the baseline setup. Moreover, it achieves an access delay of approximately 6.7 S for a 10KB webpage in the ideal case and an average end-to-end delay of only 612 ms approximately in the FHSS-based setup. Comparison with benchmark suggests multi-fold improvement.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-26T05:12:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span><span>cs.CY</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.02469v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.02469v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 The electronic structures, magnetic transition and Fermi surface
  instability of room-temperature altermagnet KV$_{2}$Se$_{2}$O</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuanji Xu, Huiyuan Zhang, Maoyuan Feng, Fuyang Tian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Altermagnetism has recently emerged as a distinct and fundamental class of magnetic order. Exploring its interplay with quantum phenomena such as unconventional superconductivity, density-wave instabilities, and many-body effects represents a compelling frontier. In this work, we theoretically confirm the presence of high-temperature metallic altermagnetism in KV$_2$Se$_2$O. We demonstrate that the anomalous metal-insulator-metal transition arises from a Lifshitz transition associated with Fermi surface reconstruction. The previously reported spin-density wave gap is found to lie below the Fermi level in our study and is now recognized to be attributed to the V-shaped density of states, originating from orbital-selective and sublattice-resolved half-metal-like behavior on a specific V atom. Furthermore, we identify the instability from the nesting of spin-momentum-locked two-dimensional Fermi surfaces, which induces the SDW state. These findings position KV$_2$Se$_2$O as a promising platform for investigating the interplay among altermagnetism, unconventional superconductivity, and density-wave order.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-26T03:13:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.str-el</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.20968v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.20968v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 AirCache: Activating Inter-modal Relevancy KV Cache Compression for
  Efficient Large Vision-Language Model Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kai Huang, Hao Zou, Bochen Wang, Ye Xi, Zhen Xie, Hao Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in Large Visual Language Models (LVLMs) have gained significant attention due to their remarkable reasoning capabilities and proficiency in generalization. However, processing a large number of visual tokens and generating long-context outputs impose substantial computational overhead, leading to excessive demands for key-value (KV) cache. To address this critical bottleneck, we propose AirCache, a novel KV cache compression method aimed at accelerating LVLMs inference. This work systematically investigates the correlations between visual and textual tokens within the attention mechanisms of LVLMs. Our empirical analysis reveals considerable redundancy in cached visual tokens, wherein strategically eliminating these tokens preserves model performance while significantly accelerating context generation. Inspired by these findings, we introduce an elite observation window for assessing the importance of visual components in the KV cache, focusing on stable inter-modal relevancy modeling with enhanced multi-perspective consistency. Additionally, we develop an adaptive layer-wise budget allocation strategy that capitalizes on the strength and skewness of token importance distribution, showcasing superior efficiency compared to uniform allocation. Comprehensive evaluations across multiple LVLMs and benchmarks demonstrate that our method achieves comparable performance to the full cache while retaining only 10% of visual KV cache, thereby reducing decoding latency by 29% to 66% across various batch size and prompt length of inputs. Notably, as cache retention rates decrease, our method exhibits increasing performance advantages over existing approaches.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-26T01:30:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.23956v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.23956v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 Omniwise: Predicting GPU Kernels Performance with LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zixian Wang, Cole Ramos, Muhammad A. Awad, Keith Lowery
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In recent years, the rapid advancement of deep neural networks (DNNs) has revolutionized artificial intelligence, enabling models with unprecedented capabilities in understanding, generating, and processing complex data. These powerful architectures have transformed a wide range of downstream applications, tackling tasks beyond human reach. In this paper, we introduce Omniwise, the first end-to-end, self-supervised fine-tuning pipeline that applies large language models (LLMs) to GPU kernel performance prediction--a novel use case in performance profiling. Omniwise is model-agnostic and lightweight, achieving strong results even with a small 3B-parameter model. It can predict key performance metrics, including memory bandwidth, cache hit rates, GFLOPs, and arithmetic intensity, directly from kernel code without the need for code execution or profiling tools. Our approach achieves over 90% of predictions within 10% relative error on GPU kernels executed on AMD MI250 and MI300X architectures. In addition to the pipeline, we develop an online inference server and a Visual Studio Code plugin that seamlessly integrate LLM-based performance prediction into developers' workflows.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-25T23:36:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.20886v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.20886v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 A3 : an Analytical Low-Rank Approximation Framework for Attention</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jeffrey T. H. Wong, Cheng Zhang, Xinye Cao, Pedro Gimenes, George A. Constantinides, Wayne Luk, Yiren Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models have demonstrated remarkable performance; however, their massive parameter counts make deployment highly expensive. Low-rank approximation offers a promising compression solution, yet existing approaches have two main limitations: (1) They focus on minimizing the output error of individual linear layers, without considering the architectural characteristics of Transformers, and (2) they decompose a large weight matrix into two small low-rank matrices. Consequently, these methods often fall short compared to other compression techniques like pruning and quantization, and introduce runtime overhead such as the extra GEMM kernel launches for decomposed small matrices. To address these limitations, we propose $\tt A^\tt 3$, a post-training low-rank approximation framework. $\tt A^\tt 3$ splits a Transformer layer into three functional components, namely $\tt QK$, $\tt OV$, and $\tt MLP$. For each component, $\tt A^\tt 3$ provides an analytical solution that reduces the hidden dimension size inside each component while minimizing the component's functional loss ($\it i.e.$, error in attention scores, attention outputs, and MLP outputs). This approach directly reduces model sizes, KV cache sizes, and FLOPs without introducing any runtime overheads. In addition, it provides a new narrative in advancing the optimization problem from singular linear layer loss optimization toward improved end-to-end performance. Through extensive experiments, we show that $\tt A^\tt 3$ maintains superior performance compared to SoTAs. For example, under the same reduction budget in computation and memory, our low-rank approximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2, outperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the versatility of $\tt A^\tt 3$, including KV cache compression, quantization, and mixed-rank assignments for enhanced performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-25T23:03:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.12942v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.12942v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 Generative Blocks World: Moving Things Around in Pictures</h2>
                <div class="authors">
                    <strong>Authors:</strong> Vaibhav Vavilala, Seemandhar Jain, Rahul Vasanth, D. A. Forsyth, Anand Bhattad
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We describe Generative Blocks World to interact with the scene of a generated image by manipulating simple geometric abstractions. Our method represents scenes as assemblies of convex 3D primitives, and the same scene can be represented by different numbers of primitives, allowing an editor to move either whole structures or small details. Once the scene geometry has been edited, the image is generated by a flow-based method which is conditioned on depth and a texture hint. Our texture hint takes into account the modified 3D primitives, exceeding texture-consistency provided by existing key-value caching techniques. These texture hints (a) allow accurate object and camera moves and (b) largely preserve the identity of objects depicted. Quantitative and qualitative experiments demonstrate that our approach outperforms prior works in visual fidelity, editability, and compositional generalization.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-25T17:59:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.GR</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.20703v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.20703v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 Semantic Caching for Improving Web Affordability</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hafsa Akbar, Danish Athar, Muhammad Ayain Fida Rana, Chaudhary Hammad Javed, Zartash Afzal Uzmi, Ihsan Ayyub Qazi, Zafar Ayyub Qazi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid growth of web content has led to increasingly large webpages, posing significant challenges for Internet affordability, especially in developing countries where data costs remain prohibitively high. We propose semantic caching using Large Language Models (LLMs) to improve web affordability by enabling reuse of semantically similar images within webpages. Analyzing 50 leading news and media websites, encompassing 4,264 images and over 40,000 image pairs, we demonstrate potential for significant data transfer reduction, with some website categories showing up to 37% of images as replaceable. Our proof-of-concept architecture shows users can achieve approximately 10% greater byte savings compared to exact caching. We evaluate both commercial and open-source multi-modal LLMs for assessing semantic replaceability. GPT-4o performs best with a low Normalized Root Mean Square Error of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA 3.1 model shows comparable performance, highlighting its viability for large-scale applications. This approach offers benefits for both users and website operators, substantially reducing data transmission. We discuss ethical concerns and practical challenges, including semantic preservation, user-driven cache configuration, privacy concerns, and potential resistance from website operators
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-25T13:35:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span><span>F.2.2, I.2.7</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.20420v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.20420v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 Do cell culturing influence the radiosensitizing effect of gold
  nanoparticles part 2: scrutinizing the methodology producing recent evidence</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hans Rabus, Oswald Msosa Mkanda
                </div>
                <div class="summary">
                    <strong>Summary:</strong> When irradiation is performed with gold nanoparticles (AuNPs), a different shape of cells in suspension or adherent to walls may result in different probability of cell survival. In a recent study, differences of up to a factor of 2 were found between the predicted survival of floating and adherent cells. The present work aims to quantify the biases introduced by the simulation setup and the use of voxelized geometry in conjunction with the local effect model for cell survival. The results show that simulated irradiation of a cell near the surface with an incident beam matched to the cell dimensions results in dose values that are by a factor of about 50 lower than the dose to cells deeper in the medium when irradiated with a Co-60 spectrum and lateral beam dimensions in the centimeter range. Furthermore, the number of ionizing photon interactions in gold nanoparticles in a cell near the surface is lower by a factor of about 2 than for cells at 5 mm and 1 cm depth. Using the average dose in voxels of size in the order of 200 nm for assessing cell survival with the local effect model (LEM) leads to an underestimation of the number of lesions from a single ionized AuNP by roughly two orders of magnitude and thus to an overestimation of cell survival. The effect of cell geometry on the survival rate was examined for approximate cell geometries and 100 kV x-ray irradiation, for which the probability of photon interaction in gold nanoparticles is by more than two orders of magnitude higher than for Co-60 irradiation. The results show that the effects are negligible for 5 nm nanoparticles at the concentration of AuNPs considered in preceding work. For 50 nm nanoparticles and thus a thousand times higher mass fraction of gold, significant reduction in cell survival is found, with a clear additional reduction predicted by the LEM as compared to the prediction based on mean dose to the nucleus.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-25T09:44:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.med-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.20283v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.20283v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 MegaFold: System-Level Optimizations for Accelerating Protein Structure
  Prediction Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hoa La, Ahan Gupta, Alex Morehead, Jianlin Cheng, Minjia Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Protein structure prediction models such as AlphaFold3 (AF3) push the frontier of biomolecular modeling by incorporating science-informed architectural changes to the transformer architecture. However, these advances come at a steep system cost, introducing: compute- and memory-intensive operators, 2D attention mechanisms, and retrieval-augmented data pipelines, which collectively hinder the scalability of AF3 training. In this work, we present MegaFold, a cross-platform system to accelerate AF3 training. MegaFold tackles key bottlenecks through ahead-of-time caching to eliminate GPU idle time from the retrieval-augmented data pipeline, Triton-based kernels for memory-efficient EvoAttention on heterogeneous devices, and deep fusion for common and critical small operators in AF3. Evaluation on both NVIDIA H200 and AMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by up to 1.23$\times$ and improves per-iteration training time by up-to 1.73$\times$ and 1.62$\times$ respectively. More importantly, MegaFold enables training on 1.35$\times$ longer sequence lengths compared to PyTorch baselines without running out-of-memory, significantly improving the scalability of modern protein folding models. We open source our code at https://github.com/Supercomputing-System-AI-Lab/MegaFold/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-24T23:30:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>q-bio.BM</span><span>cs.DC</span><span>cs.LG</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.20686v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.20686v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 GainSight: Application-Guided Profiling for Composing Heterogeneous
  On-Chip Memories in AI Hardware Accelerators</h2>
                <div class="authors">
                    <strong>Authors:</strong> Peijing Li, Matthew Hung, Yiming Tan, Konstantin HoÃfeld, Jake Cheng Jiajun, Shuhan Liu, Lixian Yan, Xinxin Wang, H. -S. Philip Wong, Thierry Tambe
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As AI workloads drive soaring memory requirements, higher-density on-chip memory is needed for domain-specific accelerators beyond what current SRAM technology can provide. We motivate that algorithms and application behavior should guide the composition of heterogeneous on-chip memories. However, little work has incorporated dynamic application profiles into these design decisions, and no existing tools are expressly designed for this purpose. We present GainSight, a profiling framework that analyzes fine-grained memory access patterns and data lifetimes in domain-specific accelerators. By instrumenting retargetable architectural simulator backends with application- and device-agnostic analytical frontends, GainSight aligns workload-specific traffic and lifetime metrics with mockups of emerging memory devices, informing system-level heterogeneous memory design. We also present a set of case studies on MLPerf Inference and PolyBench workloads using simulated GPU and systolic array architectures, highlighting the utility of GainSight and the insights it provides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic array scratchpad accesses across profiled workloads are short-lived and suitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory arrays that augment SRAM with GCRAM can reduce active energy consumption by up to 66.8%. To facilitate further research in this domain, GainSight is open source at https://gainsight.stanford.edu/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-24T19:02:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>cs.ET</span><span>B.7.1; B.3.1; C.3; I.6; I.2.6</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.14866v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.14866v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 CronusVLA: Transferring Latent Motion Across Time for Multi-Frame
  Prediction in Manipulation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hao Li, Shuai Yang, Yilun Chen, Yang Tian, Xiaoda Yang, Xinyi Chen, Hanqing Wang, Tai Wang, Feng Zhao, Dahua Lin, Jiangmiao Pang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent vision-language-action (VLA) models built on pretrained vision-language models (VLMs) have demonstrated strong generalization across manipulation tasks. However, they remain constrained by a single-frame observation paradigm and cannot fully benefit from the motion information offered by aggregated multi-frame historical observations, as the large vision-language backbone introduces substantial computational cost and inference latency. We propose CronusVLA, a unified framework that extends single-frame VLA models to the multi-frame paradigm through an efficient post-training stage. CronusVLA comprises three key components: (1) single-frame pretraining on large-scale embodied datasets with autoregressive action tokens prediction, which establishes an embodied vision-language foundation; (2) multi-frame encoding, adapting the prediction of vision-language backbones from discrete action tokens to motion features during post-training, and aggregating motion features from historical frames into a feature chunking; (3) cross-frame decoding, which maps the feature chunking to accurate actions via a shared decoder with cross-attention. By reducing redundant token computation and caching past motion features, CronusVLA achieves efficient inference. As an application of motion features, we further propose an action adaptation mechanism based on feature-action retrieval to improve model performance during finetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with 70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world Franka experiments also show the strong performance and robustness.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-24T17:30:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.19816v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.19816v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 RCStat: A Statistical Framework for using Relative Contextualization in
  Transformers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Debabrata Mahapatra, Shubham Agarwal, Apoorv Saxena, Subrata Mitra
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Prior work on input-token importance in auto-regressive transformers has relied on Softmax-normalized attention weights, which obscure the richer structure of pre-Softmax query-key logits. We introduce RCStat, a statistical framework that harnesses raw attention logits via Relative Contextualization (RC), a random variable measuring contextual alignment between token segments, and derive an efficient upper bound for RC. We demonstrate two applications: (i) Key-Value compression, where RC-based thresholds drive adaptive key-value eviction for substantial cache reduction with minimal quality loss; and (ii) Attribution, where RC yields higher-fidelity token-, sentence-, and chunk-level explanations than post-Softmax methods. Across question answering, summarization, and attribution benchmarks, RCStat achieves significant empirical gains, delivering state-of-the-art compression and attribution performance without any model retraining.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-24T11:55:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.19549v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.19549v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in
  Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zeyu Li, Chuanfu Xiao, Yang Wang, Xiang Liu, Zhenheng Tang, Baotong Lu, Mao Yang, Xinyu Chen, Xiaowen Chu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Quantization has emerged as an effective and lightweight solution to reduce the memory footprint of the KV cache in Large Language Models (LLMs). Nevertheless, minimizing the performance degradation caused by ultra-low-bit KV cache quantization remains a significant challenge. We observe that quantizing the KV cache of different tokens has varying impacts on the quality of attention outputs. To systematically investigate this phenomenon, we perform forward error propagation analysis on attention and propose the Anchor Score (AnS) that quantifies the sensitivity of each token's KV cache to quantization-induced error. Our analysis reveals significant disparities in AnS across tokens, suggesting that preserving a small subset with full precision (FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive quantization scenarios. Based on this insight, we introduce AnTKV, a novel framework that leverages Anchor Token-aware Vector Quantization to compress the KV cache. Furthermore, to support efficient deployment, we design and develop a triton kernel that is fully compatible with FlashAttention, enabling fast online Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context lengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x higher decoding throughput compared to the FP16 baseline. Our experiment results demonstrate that AnTKV matches or outperforms prior works such as KIVI, SKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves significantly lower perplexity under ultra-low-bit quantization on Mistral-7B, with only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of 4.73.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-24T10:45:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.19505v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.19505v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 Mixture of Cache-Conditional Experts for Efficient Mobile Device
  Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Andrii Skliar, Ties van Rozendaal, Romain Lepert, Todor Boinovski, Mart van Baalen, Markus Nagel, Paul Whatmough, Babak Ehteshami Bejnordi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Mixture of Experts (MoE) LLMs have recently gained attention for their ability to enhance performance by selectively engaging specialized subnetworks or "experts" for each input. However, deploying MoEs on memory-constrained devices remains challenging, particularly when generating tokens sequentially with a batch size of one, as opposed to typical high-throughput settings involving long sequences or large batches. In this work, we optimize MoE on memory-constrained devices where only a subset of expert weights fit in DRAM. We introduce a novel cache-aware routing strategy that leverages expert reuse during token generation to improve cache locality. We evaluate our approach on language modeling, MMLU, and GSM8K benchmarks and present on-device results demonstrating 2$\times$ speedups on mobile devices, offering a flexible, training-free solution to extend MoE's applicability across real-world applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-24T09:27:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.00099v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.00099v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments
  with a Hierarchical Spatial-Cognition Long-Short Memory System</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lixuan He, Haoyu Dong, Zhenxing Chen, Yangcheng Yu, Jie Feng, Yong Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vision-and-Language Navigation (VLN) in large-scale urban environments requires embodied agents to ground linguistic instructions in complex scenes and recall relevant experiences over extended time horizons. Prior modular pipelines offer interpretability but lack unified memory, while end-to-end (M)LLM agents excel at fusing vision and language yet remain constrained by fixed context windows and implicit spatial reasoning. We introduce \textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system that can augment any VLN backbone. Mem4Nav fuses a sparse octree for fine-grained voxel indexing with a semantic topology graph for high-level landmark connectivity, storing both in trainable memory tokens embedded via a reversible Transformer. Long-term memory (LTM) compresses and retains historical observations at both octree and graph nodes, while short-term memory (STM) caches recent multimodal entries in relative coordinates for real-time obstacle avoidance and local planning. At each step, STM retrieval sharply prunes dynamic context, and, when deeper history is needed, LTM tokens are decoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and Map2Seq across three backbones (modular, state-of-the-art VLN with prompt-based LLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13 pp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW improvement. Ablations confirm the indispensability of both the hierarchical map and dual memory modules. Our codes are open-sourced via https://github.com/tsinghua-fib-lab/Mem4Nav.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-24T09:00:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.19433v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.19433v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Duong Bach
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The proliferation of multi-agent systems (MAS) in complex, dynamic environments necessitates robust and efficient mechanisms for managing shared knowledge. A critical challenge is ensuring that distributed memories remain synchronized, relevant, and free from the accumulation of outdated or inconsequential data - a process analogous to biological forgetting. This paper introduces the Co-Forgetting Protocol, a novel, comprehensive framework designed to address this challenge by enabling synchronized memory pruning in MAS. The protocol integrates three key components: (1) context-aware semantic voting, where agents utilize a lightweight DistilBERT model to assess the relevance of memory items based on their content and the current operational context; (2) multi-scale temporal decay functions, which assign diminishing importance to memories based on their age and access frequency across different time horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based consensus mechanism, ensuring that decisions to retain or discard memory items are agreed upon by a qualified and fault-tolerant majority of agents, even in the presence of up to f Byzantine (malicious or faulty) agents in a system of N greater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient inter-agent communication and Pinecone for scalable vector embedding storage and similarity search, with SQLite managing metadata. Experimental evaluations in a simulated MAS environment with four agents demonstrate the protocol's efficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88% voting accuracy in forgetting decisions against human-annotated benchmarks, a 92% PBFT consensus success rate under simulated Byzantine conditions, and an 82% cache hit rate for memory access.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-24T06:44:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span><span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.17338v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.17338v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV
  Sparsification</h2>
                <div class="authors">
                    <strong>Authors:</strong> Minghao Qin, Xiangrui Liu, Zhengyang Liang, Yan Shu, Huaying Yuan, Juenjie Zhou, Shitao Xiao, Bo Zhao, Zheng Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multi-modal large language models (MLLMs) models have made significant progress in video understanding over the past few years. However, processing long video inputs remains a major challenge due to high memory and computational costs. This makes it difficult for current models to achieve both strong performance and high efficiency in long video understanding. To address this challenge, we propose Video-XL-2, a novel MLLM that delivers superior cost-effectiveness for long-video understanding based on task-aware KV sparsification. The proposed framework operates with two key steps: chunk-based pre-filling and bi-level key-value decoding. Chunk-based pre-filling divides the visual token sequence into chunks, applying full attention within each chunk and sparse attention across chunks. This significantly reduces computational and memory overhead. During decoding, bi-level key-value decoding selectively reloads either dense or sparse key-values for each chunk based on its relevance to the task. This approach further improves memory efficiency and enhances the model's ability to capture fine-grained information. Video-XL-2 achieves state-of-the-art performance on various long video understanding benchmarks, outperforming existing open-source lightweight models. It also demonstrates exceptional efficiency, capable of processing over 10,000 frames on a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few seconds.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-24T01:19:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.19225v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.19225v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices
  and Tensors</h2>
                <div class="authors">
                    <strong>Authors:</strong> Benjamin Brock, Willow Ahrens, Hameer Abbasi, Timothy A. Davis, Juni Kim, James Kitchen, Spencer Patty, Isaac Virshup, Erik Welch
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Sparse matrices and tensors are ubiquitous throughout multiple subfields of computing. The widespread usage of sparse data has inspired many in-memory and on-disk storage formats, but the only widely adopted storage specifications are the Matrix Market and FROSTT file formats, which both use ASCII text. Due to the inefficiency of text storage, these files typically have larger file sizes and longer parsing times than binary storage formats, which directly store an in-memory representation to disk. This can be a major bottleneck; since sparse computation is often bandwidth-bound, the cost of loading or storing a matrix to disk often exceeds the cost of performing a sparse computation. While it is common practice for practitioners to develop their own, custom, non-portable binary formats for high-performance sparse matrix storage, there is currently no cross-platform binary sparse matrix storage format. We present Binsparse, a cross-platform binary sparse matrix and tensor format specification. Binsparse is a modular, embeddable format, consisting of a JSON descriptor, which describes the matrix or tensor dimensions, type, and format, and a series of binary arrays, which can be stored in all modern binary containers, such as HDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse spanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our Binsparse format on every matrix in the SuiteSparse Matrix Collection and a selection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format shows file size reductions of 2.4x on average without compression and 7.5x with compression. We evaluate our parser's read/write performance against a state-of-the-art Matrix Market parser, demonstrating warm cache mean read speedups of 26.5x without compression and 2.6x with compression, and write speedups of 31x without compression and 1.4x with compression.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-23T22:33:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MS</span><span>cs.DC</span><span>cs.DS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.19175v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.19175v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 CommVQ: Commutative Vector Quantization for KV Cache Compression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junyan Li, Yang Zhang, Muhammad Yusuf Hassan, Talha Chafekar, Tianle Cai, Zhile Ren, Pengsheng Guo, Foroozan Karimzadeh, Colorado Reed, Chong Wang, Chuang Gan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are increasingly used in applications requiring long context lengths, but the key-value (KV) cache often becomes a memory bottleneck on GPUs as context grows. To address this, we propose Commutative Vector Quantization (CommVQ) to significantly reduce memory usage for long-context LLM inference. We first introduce additive quantization with a lightweight encoder and codebook to compress the KV cache, which can be decoded via simple matrix multiplication. To further reduce computational costs during decoding, we design the codebook to be commutative with Rotary Position Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm. This enables efficient integration of decoding into the self-attention mechanism. Our approach achieves high accuracy with additive quantization and low overhead via the RoPE-commutative codebook. Experiments on long-context benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5% with 2-bit quantization, while outperforming state-of-the-art KV cache quantization methods. Notably, it enables 1-bit KV cache quantization with minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context length on a single RTX 4090 GPU. The source code is available at: https://github.com/UMass-Embodied-AGI/CommVQ.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-23T17:50:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.18879v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.18879v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo</h2>
                <div class="authors">
                    <strong>Authors:</strong> Minas Karamanis, UroÅ¡ Seljak
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian inference but suffer from high computational costs due to their reliance on large particle ensembles for accurate estimates. We introduce persistent sampling (PS), an extension of SMC that systematically retains and reuses particles from all prior iterations to construct a growing, weighted ensemble. By leveraging multiple importance sampling and resampling from a mixture of historical distributions, PS mitigates the need for excessively large particle counts, directly addressing key limitations of SMC such as particle impoverishment and mode collapse. Crucially, PS achieves this without additional likelihood evaluations-weights for persistent particles are computed using cached likelihood values. This framework not only yields more accurate posterior approximations but also produces marginal likelihood estimates with significantly lower variance, enhancing reliability in model comparison. Furthermore, the persistent ensemble enables efficient adaptation of transition kernels by leveraging a larger, decorrelated particle pool. Experiments on high-dimensional Gaussian mixtures, hierarchical models, and non-convex targets demonstrate that PS consistently outperforms standard SMC and related variants, including recycled and waste-free SMC, achieving substantial reductions in mean squared error for posterior expectations and evidence estimates, all at reduced computational cost. PS thus establishes itself as a robust, scalable, and efficient alternative for complex Bayesian inference tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-23T07:59:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ML</span><span>cs.LG</span><span>stat.CO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.20722v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.20722v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 FutureFill: Fast Generation from Convolutional Sequence Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Naman Agarwal, Xinyi Chen, Evan Dogariu, Devan Shah, Hubert Strauss, Vlad Feinberg, Daniel Suo, Peter Bartlett, Elad Hazan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We address the challenge of efficient auto-regressive generation in sequence prediction models by introducing FutureFill, a general-purpose fast generation method for any sequence prediction algorithm based on convolutional operators. FutureFill reduces generation time from quadratic to quasilinear in the context length. Moreover, when generating from a prompt, it requires a prefill cache whose size grows only with the number of tokens to be generated, often much smaller than the caches required by standard convolutional or attention based models. We validate our theoretical claims with experiments on synthetic tasks and demonstrate substantial efficiency gains when generating from a deep convolutional sequence prediction model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-23T03:20:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.03766v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.03766v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 RAPID: Long-Context Inference with Retrieval-Augmented Speculative
  Decoding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guanzheng Chen, Qilong Feng, Jinjie Ni, Xin Li, Michael Qizhe Shieh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The emergence of long-context large language models (LLMs) offers a promising alternative to traditional retrieval-augmented generation (RAG) for processing extensive documents. However, the computational overhead of long-context inference presents significant efficiency challenges. While Speculative Decoding (SD) traditionally accelerates inference using smaller draft models, its effectiveness diminishes substantially in long-context scenarios due to memory-bound KV cache operations. We introduce Retrieval-Augmented Speculative Decoding (RAPID), which leverages RAG for both accelerating and enhancing generation quality in long-context inference. RAPID introduces the RAG drafter-a draft LLM operating on shortened retrieval contexts-to speculate on the generation of long-context target LLMs. Our approach enables a new paradigm where same-scale or even larger LLMs can serve as RAG drafters while maintaining computational efficiency. To fully leverage the potentially superior capabilities from stronger RAG drafters, we develop an inference-time knowledge transfer that enriches the target distribution by RAG. Extensive experiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates the strengths of both RAG and long-context LLMs, achieving significant performance improvements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context inference. Our analyses also reveal the robustness of RAPID across various context lengths and retrieval quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-23T03:05:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.20330v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.20330v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 Make It Efficient: Dynamic Sparse Attention for Autoregressive Image
  Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xunzhi Xiang, Qi Fan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Autoregressive conditional image generation models have emerged as a dominant paradigm in text-to-image synthesis. These methods typically convert images into one-dimensional token sequences and leverage the self-attention mechanism, which has achieved remarkable success in natural language processing, to capture long-range dependencies, model global context, and ensure semantic coherence. However, excessively long contexts during inference lead to significant memory overhead caused by KV-cache and computational delays. To alleviate these challenges, we systematically analyze how global semantics, spatial layouts, and fine-grained textures are formed during inference, and propose a novel training-free context optimization method called Adaptive Dynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies historical tokens crucial for maintaining local texture consistency and those essential for ensuring global semantic coherence, thereby efficiently streamlining attention computation. Additionally, we introduce a dynamic KV-cache update mechanism tailored for ADSA, reducing GPU memory consumption during inference by approximately $50\%$. Extensive qualitative and quantitative experiments demonstrate the effectiveness and superiority of our approach in terms of both generation quality and resource efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-23T01:27:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.18226v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.18226v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 Fast Clifford Neural Layers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tianxiang Xia, Max Neuwinger, Lin Xiao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra into neural networks. In this project we focus on optimizing the inference of 2/3D Clifford convolutional layers and multivector activation layers for one core CPU performance.   Overall, by testing on a real network block involving Clifford convolutional layers and multivector activation layers, we observe that our implementation is 30% faster than standard PyTorch implementation in relatively large data + network size (>L2 cache).   We open source our code base at https://github.com/egretwAlker/c-opt-clifford-layers
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-22T20:43:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.NE</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.01040v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.01040v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the
  Limits of Embedding Space Capacity</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuri Kuratov, Mikhail Arkhipov, Aydar Bulatov, Mikhail Burtsev
                </div>
                <div class="summary">
                    <strong>Summary:</strong> A range of recent works addresses the problem of compression of sequence of tokens into a shorter sequence of real-valued vectors to be used as inputs instead of token embeddings or key-value cache. These approaches are focused on reduction of the amount of compute in existing language models rather than minimization of number of bits needed to store text. Despite relying on powerful models as encoders, the maximum attainable lossless compression ratio is typically not higher than x10. This fact is highly intriguing because, in theory, the maximum information capacity of large real-valued vectors is far beyond the presented rates even for 16-bit precision and a modest vector size. In this work, we explore the limits of compression by replacing the encoder with a per-sample optimization procedure. We show that vectors with compression ratios up to x1500 exist, which highlights two orders of magnitude gap between existing and practically attainable solutions. Furthermore, we empirically show that the compression limits are determined not by the length of the input but by the amount of uncertainty to be reduced, namely, the cross-entropy loss on this sequence without any conditioning. The obtained limits highlight the substantial gap between the theoretical capacity of input embeddings and their practical utilization, suggesting significant room for optimization in model design.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-22T15:07:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.13063v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.13063v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 Secure User-friendly Blockchain Modular Wallet Design Using Android &
  OP-TEE</h2>
                <div class="authors">
                    <strong>Authors:</strong> Seongjin Kim, Sanguk Yun, Jungho Jang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Emerging crypto economies still hemorrhage digital assets because legacy wallets leak private keys at almost every layer of the software stack, from user-space libraries to kernel memory dumps. This paper solves that twin crisis of security and interoperability by re-imagining key management as a platform-level service anchored in ARM TrustZone through OP-TEE. Our architecture fractures the traditional monolithic Trusted Application into per-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's single-binary ceiling. A cryptographically sealed firmware-over-the-air pipeline welds each TA set to an Android system image, enabling hot-swap updates while Verified Boot enforces rollback protection. Every package carries a chained signature developer first, registry second so even a compromised supply chain cannot smuggle malicious code past the Secure World's RSA-PSS gatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and GP-compliant crypto APIs ensure secrets never bleed across trust boundaries or timing domains. The Rich Execution Environment can interact only via hardware-mediated Secure Monitor Calls, collapsing the surface exposed to malware in Android space. End-users enjoy a single polished interface yet can install or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap, shrinking both storage footprint and audit scope. For auditors, the composition model slashes duplicated verification effort by quarantining blockchain logic inside narrowly scoped modules that share formally specified interfaces. Our threat analysis spans six adversary layers and shows how the design neutralizes REE malware sniffing, OTA injection, and cross-module side channels without exotic hardware. A reference implementation on AOSP exports a Wallet Manager HAL, custom SELinux domains, and a CI/CD pipeline that vet community modules before release. The result is not merely another hardware wallet but a programmable substrate that can evolve at the velocity of the blockchain ecosystem. By welding radical extensibility to hardware-anchored assurance, the platform closes the security-usability gap that has long stymied mass-market self-custody. We posit that modular TEEs are the missing OS primitive for Web3, much as virtual memory unlocked multi-tasking in classical computing. Together, these contributions sketch a blueprint for multi-chain asset management that is auditable, resilient, and poised for global deployment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-22T10:57:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.17988v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.17988v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training</h2>
                <div class="authors">
                    <strong>Authors:</strong> Maryam Dialameh, Rezaul Karim, Hossein Rajabzadeh, Omar Mohamed Awad, Hyock Ju Kwon, Boxing Chen, Walid Ahmed, Yang Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to improve both the training speed and inference throughput of LLaMA architectures while maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models into shared KV caching across certain layers, significantly reducing KV computational complexity while maintaining or improving language performance. Experimental results demonstrate that ECHO-LLaMA achieves up to 77\% higher token-per-second throughput during training, up to 16\% higher Model FLOPs Utilization (MFU), and up to 14\% lower loss when trained on an equal number of tokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\% higher test-time throughput compared to the baseline. By introducing a computationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable and cost-effective solution for pretraining and finetuning large language models, enabling faster and more resource-efficient training without compromising performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-22T03:46:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.17331v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.17331v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 RPLKG: Robust Prompt Learning with Knowledge Graph</h2>
                <div class="authors">
                    <strong>Authors:</strong> YongTaek Lim, Yewon Kim, Suho Kang, Dokyung Yoon, KyungWoo Song
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large-scale pre-trained models surpass in transferability and robust generalization across diverse datasets. The emergence of multimodal pre-trained models like CLIP has significantly boosted performance in various experiments. However, generalizing to new datasets or domains remains challenging, especially with limited labeled data. Also, existing methods often lack interpretability and impose high computational costs. To address this, we propose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the knowledge graph to curate diverse, interpretable prompt sets automatically. Our method autonomously selects the optimal interpretable prompt based on dataset characteristics, achieving performance improvements over zero-shot learning and competitive performance compared to various prompt learning methods. Also, RPLKG efficiently reuses cached prompt embeddings from a single model pass and optimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast training. Moreover, RPLKG advances few-shot learning effectiveness while enhancing interpretability and efficiency in model adaptation. Our
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-21T08:27:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2304.10805v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2304.10805v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context
  LMs?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Adithya Bhaskar, Alexander Wettig, Tianyu Gao, Yihe Dong, Danqi Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Language models handle increasingly long contexts for tasks such as book summarization, but this leads to growing memory costs for the key-value (KV) cache. Many prior works have proposed ways of discarding KVs from memory, but their approaches are tailored to favorable settings, obscuring caveats like high peak memory and performance degradation, and a fair comparison between methods is difficult. In this paper, we propose the *KV footprint* as a unified metric, which accounts for both the amount of KV entries stored and their lifespan in memory. We evaluate methods based on the smallest footprint they attain while preserving performance in both long-context understanding and generation, with context lengths of up to 128K tokens. This metric reveals the high peak memory of prior KV eviction methods. One class of methods -- *post-fill eviction* -- has a high footprint due to being incompatible with eviction during pre-filling. We adapt these methods to be able to evict KVs during pre-filling, achieving substantially lower KV footprints. We then turn to *recency eviction* methods, wherein we propose PruLong, an end-to-end optimization method for learning which attention heads need to retain the full KV cache and which do not. PruLong saves memory while preserving long-context performance, achieving 12% smaller KV footprint than prior methods while retaining performance in challenging recall tasks. Our paper clarifies the complex tangle of long-context inference methods and paves the way for future development to minimize the KV footprint.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-20T16:21:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.17121v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.17121v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 PUL: Pre-load in Software for Caches Wouldn't Always Play Along</h2>
                <div class="authors">
                    <strong>Authors:</strong> Arthur Bernhardt, Sajjad Tamimi, Florian Stock, Andreas Koch, Ilia Petrov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Memory latencies and bandwidth are major factors, limiting system performance and scalability. Modern CPUs aim at hiding latencies by employing large caches, out-of-order execution, or complex hardware prefetchers. However, software-based prefetching exhibits higher efficiency, improving with newer CPU generations.   In this paper we investigate software-based, post-Moore systems that offload operations to intelligent memories. We show that software-based prefetching has even higher potential in near-data processing settings by maximizing compute utilization through compute/IO interleaving.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-20T13:09:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.16976v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.16976v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 Serving Large Language Models on Huawei CloudMatrix384</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pengfei Zuo, Huimin Lin, Junbo Deng, Nan Zou, Xingkun Yang, Yingyu Diao, Weifeng Gao, Ke Xu, Zhangyu Chen, Shirui Lu, Zhao Qiu, Peiyang Li, Xianyu Chang, Zhengzhong Yu, Fangzheng Miao, Jia Zheng, Ying Li, Yuan Feng, Bei Wang, Zaijian Zong, Mosong Zhou, Wenli Zhou, Houjiang Chen, Xingyu Liao, Yipeng Li, Wenxiao Zhang, Ping Zhu, Yinggang Wang, Chuanjie Xiao, Depeng Liang, Dong Cao, Juncheng Liu, Yongqiang Yang, Xiaolong Bai, Yi Li, Huaguo Xie, Huatao Wu, Zhibin Yu, Lv Chen, Hu Liu, Yujun Ding, Haipei Zhu, Jing Xia, Yi Xiong, Zhou Yu, Heng Liao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid evolution of large language models (LLMs), driven by growing parameter scales, adoption of mixture-of-experts (MoE) architectures, and expanding context lengths, imposes unprecedented demands on AI infrastructure. Traditional AI clusters face limitations in compute intensity, memory bandwidth, inter-chip communication, and latency, compounded by variable workloads and strict service-level objectives. Addressing these issues requires fundamentally redesigned hardware-software integration. This paper introduces Huawei CloudMatrix, a next-generation AI datacenter architecture, realized in the production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910 NPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified Bus (UB) network, enabling direct all-to-all communication and dynamic pooling of resources. These features optimize performance for communication-intensive operations, such as large-scale MoE expert parallelism and distributed key-value cache access. To fully leverage CloudMatrix384, we propose CloudMatrix-Infer, an advanced LLM serving solution incorporating three core innovations: a peer-to-peer serving architecture that independently scales prefill, decode, and caching; a large-scale expert parallelism strategy supporting EP320 via efficient UB-based token dispatch; and hardware-aware optimizations including specialized operators, microbatch-based pipelining, and INT8 quantization. Evaluation with the DeepSeek-R1 model shows CloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of 6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms TPOT). It effectively balances throughput and latency, sustaining 538 tokens/s per NPU even under stringent 15 ms latency constraints, while INT8 quantization maintains model accuracy across benchmarks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-19T12:27:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span><span>cs.AR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.12708v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.12708v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 A block Recycled GMRES method with investigations into aspects of solver
  performance</h2>
                <div class="authors">
                    <strong>Authors:</strong> Michael L. Parks, Kirk M. Soodhalter, Daniel B. Szyld
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We propose a block Krylov subspace version of the GCRO-DR method proposed in [Parks et al.; SISC 2005], which is an iterative method allowing for the efficient minimization of the the residual over an augmented Krylov subspace. We offer a clean derivation of our proposed method and discuss methods of selecting recycling subspaces at restart as well as implementation decisions in the context of high-performance computing. Numerical experiments are split into those demonstrating convergence properties and those demonstrating the data movement and cache efficiencies of the dominant operations of the method, measured using processor monitoring code from Intel.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-19T10:23:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.NA</span><span>cs.NA</span><span>65F10</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/1604.01713v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/1604.01713v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 Characterization of discharge capillaries via benchmarked hydrodynamic
  plasma simulations</h2>
                <div class="authors">
                    <strong>Authors:</strong> S. M. Mewes, G. J. Boyle, R. D'Arcy, J. M. Garland, M. Huck, H. Jones, G. Loisch, A. R. Maier, J. Osterhoff, T. Parikh, S. Wesch, J. C. Wood, M. ThÃ©venet
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Plasma accelerators utilize strong electric fields in plasma waves to accelerate charged particles, making them a compact alternative to radiofrequency technologies. Discharge capillaries are plasma sources used in plasma accelerator research to provide acceleration targets, or as plasma lenses to capture or focus accelerated beams. They have applications for beam-driven and laser-driven plasma accelerators and can sustain high repetition rates for extended periods of time. Despite these advantages, high-fidelity simulations of discharge capillaries remain challenging due to the range of mechanisms involved and the difficulty to diagnose them in experiments. In this work, we utilize hydrodynamic plasma simulations to examine the discharge process of a plasma cell and discuss implications for future accelerator systems. The simulation model is validated with experimental measurements in a 50-mm-long, 1-mm-wide plasma capillary operating a 12-27 kV discharge at 2-12mbar hydrogen pressure. For 20 kV at 8.7mbar the discharge is shown to deposit 178mJ of energy in the plasma. Potential difficulties with the common density measurement method using H{\alpha} emission spectroscopy are discussed. This simulation model enables investigations of repeatability, heat flow management and fine tailoring of the plasma profile with discharges.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-19T10:17:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.plasm-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.16192v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.16192v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 Resource Allocation for Twin Maintenance and Computing Task Processing
  in Digital Twin Vehicular Edge Computing Network</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yu Xie, Qiong Wu, Pingyi Fan, Nan Cheng, Wen Chen, Jiangzhou Wang, Khaled B. Letaief
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As a promising technology, vehicular edge computing (VEC) can provide computing and caching services by deploying VEC servers near vehicles. However, VEC networks still face challenges such as high vehicle mobility. Digital twin (DT), an emerging technology, can predict, estimate, and analyze real-time states by digitally modeling objects in the physical world. By integrating DT with VEC, a virtual vehicle DT can be created in the VEC server to monitor the real-time operating status of vehicles. However, maintaining the vehicle DT model requires ongoing attention from the VEC server, which also needs to offer computing services for the vehicles. Therefore, effective allocation and scheduling of VEC server resources are crucial. This study focuses on a general VEC network with a single VEC service and multiple vehicles, examining the two types of delays caused by twin maintenance and computational processing within the network. By transforming the problem using satisfaction functions, we propose an optimization problem aimed at maximizing each vehicle's resource utility to determine the optimal resource allocation strategy. Given the non-convex nature of the issue, we employ multi-agent Markov decision processes to reformulate the problem. Subsequently, we propose the twin maintenance and computing task processing resource collaborative scheduling (MADRL-CSTC) algorithm, which leverages multi-agent deep reinforcement learning. Through experimental comparisons with alternative algorithms, it demonstrates that our proposed approach is effective in terms of resource allocation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-06-19T07:29:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.07575v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.07575v2' target='_blank'>pdf</a>
                </div>
            </div>
            <h1>Keyword: LLM Inference</h1>

            <div class="paper">
                <h2>#1 Spatio-Temporal LLM: Reasoning about Environments and Actions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haozhen Zheng, Beitong Tian, Mingyuan Wu, Zhenggang Tang, Klara Nahrstedt, Alex Schwing
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Despite the significant recent progress of Multimodal Large Language Models (MLLMs), MLLMs still struggle to correctly answer prompts that require a holistic spatio-temporal understanding. Specifically, it is challenging to address prompts that refer to 1) the entirety of an environment that an agent equipped with an MLLM can operate in; and simultaneously also refer to 2) recent actions that just happened and are encoded in a video clip. However, such a holistic spatio-temporal understanding is important for agents operating in the real world. To address this issue, we first develop a framework to collect a large-scale dataset. Using the collected "Reasoning about Environments and Actions" (REA) dataset, we show that recent methods indeed struggle to correctly answer the prompts. To improve, we develop a "spatio-temporal LLM" (ST-LLM), a model equipped with projectors to improve both spatial understanding of an environment and temporal understanding of recent observations. On the collected REA data, we show that the proposed method significantly improves results compared to prior work. Code and data are available at https://zoezheng126.github.io/STLLM-website/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:59:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05258v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05258v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuanzhe Hu, Yu Wang, Julian McAuley
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to the lack of benchmarks. We term agents with memory mechanisms as memory agents. In this paper, we identify four core competencies essential for memory agents: accurate retrieval, test-time learning, long-range understanding, and conflict resolution. Existing datasets either rely on limited context lengths or are tailored for static, long-context settings like book-based QA, which do not reflect the interactive, multi-turn nature of memory agents that incrementally accumulate information. Furthermore, no existing benchmarks cover all four competencies. Therefore, we introduce MemoryAgentBench, a new benchmark specifically designed for memory agents. Our benchmark combines reformulated existing datasets with newly constructed ones, covering the above four memory competencies, providing a systematic and challenging testbed for assessing memory quality. We evaluate a diverse set of memory agents, ranging from simple context-based and retrieval-augmented generation (RAG) systems to advanced agents with external memory modules and tool integration. Empirical results reveal that current methods fall short of mastering all four competencies, underscoring the need for further research into comprehensive memory mechanisms for LLM agents.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:59:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05257v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05257v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for
  Visual Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yana Wei, Liang Zhao, Jianjian Sun, Kangheng Lin, Jisheng Yin, Jingcheng Hu, Yinmin Zhang, En Yu, Haoran Lv, Zejia Weng, Jia Wang, Chunrui Han, Yuang Peng, Qi Han, Zheng Ge, Xiangyu Zhang, Daxin Jiang, Vishal M. Patel
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The remarkable reasoning capability of large language models (LLMs) stems from cognitive behaviors that emerge through reinforcement with verifiable rewards. This work investigates how to transfer this principle to Multimodal LLMs (MLLMs) to unlock advanced visual reasoning. We introduce a two-stage paradigm built on Qwen2.5-VL-7B: a massive linguistic cold-start fine-tuning, followed by multimodal reinforcement learning (RL) spanning nearly 1,000 steps, surpassing all previous open-source efforts in scale. This pioneering work reveals three fundamental insights: 1) Behavior transfer emerges surprisingly early in cold start due to linguistic mental imagery. 2) Cold start broadly memorizes visual behaviors, while RL critically discerns and scales up effective patterns. 3) Transfer strategically favors high-utility behaviors such as visual reflection. Our resulting model, Open-Vision-Reasoner (OVR), achieves state-of-the-art performance on a suite of reasoning benchmarks, including 95.3% on MATH500, 51.8% on MathVision and 54.6% on MathVerse. We release our model, data, and training dynamics to catalyze the development of more capable, behavior-aligned multimodal reasoners.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:59:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05255v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05255v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 From Marginal to Joint Predictions: Evaluating Scene-Consistent
  Trajectory Prediction Approaches for Automated Driving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fabian Konstantinidis, Ariel Dallari Guerreiro, Raphael Trumpp, Moritz Sackmann, Ulrich Hofmann, Marco Caccamo, Christoph Stiller
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Accurate motion prediction of surrounding traffic participants is crucial for the safe and efficient operation of automated vehicles in dynamic environments. Marginal prediction models commonly forecast each agent's future trajectories independently, often leading to sub-optimal planning decisions for an automated vehicle. In contrast, joint prediction models explicitly account for the interactions between agents, yielding socially and physically consistent predictions on a scene level. However, existing approaches differ not only in their problem formulation but also in the model architectures and implementation details used, making it difficult to compare them. In this work, we systematically investigate different approaches to joint motion prediction, including post-processing of the marginal predictions, explicitly training the model for joint predictions, and framing the problem as a generative task. We evaluate each approach in terms of prediction accuracy, multi-modality, and inference efficiency, offering a comprehensive analysis of the strengths and limitations of each approach. Several prediction examples are available at https://frommarginaltojointpred.github.io/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:58:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.LG</span><span>cs.MA</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05254v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05254v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 Response Attack: Exploiting Contextual Priming to Jailbreak Large
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ziqi Miao, Lijun Li, Yuan Xiong, Zhenhua Liu, Pengyu Zhu, Jing Shao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Contextual priming, where earlier stimuli covertly bias later judgments, offers an unexplored attack surface for large language models (LLMs). We uncover a contextual priming vulnerability in which the previous response in the dialogue can steer its subsequent behavior toward policy-violating content. Building on this insight, we propose Response Attack, which uses an auxiliary LLM to generate a mildly harmful response to a paraphrased version of the original malicious query. They are then formatted into the dialogue and followed by a succinct trigger prompt, thereby priming the target model to generate harmful content. Across eight open-source and proprietary LLMs, RA consistently outperforms seven state-of-the-art jailbreak techniques, achieving higher attack success rates. To mitigate this threat, we construct and release a context-aware safety fine-tuning dataset, which significantly reduces the attack success rate while preserving model capabilities. The code and data are available at https://github.com/Dtc7w3PQ/Response-Attack.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:56:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05248v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05248v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 Modeling Latent Partner Strategies for Adaptive Zero-Shot Human-Agent
  Collaboration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Benjamin Li, Shuyang Shi, Lucia Romero, Huao Li, Yaqi Xie, Woojun Kim, Stefanos Nikolaidis, Michael Lewis, Katia Sycara, Simon Stepputtis
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In collaborative tasks, being able to adapt to your teammates is a necessary requirement for success. When teammates are heterogeneous, such as in human-agent teams, agents need to be able to observe, recognize, and adapt to their human partners in real time. This becomes particularly challenging in tasks with time pressure and complex strategic spaces where the dynamics can change rapidly. In this work, we introduce TALENTS, a strategy-conditioned cooperator framework that learns to represent, categorize, and adapt to a range of partner strategies, enabling ad-hoc teamwork. Our approach utilizes a variational autoencoder to learn a latent strategy space from trajectory data. This latent space represents the underlying strategies that agents employ. Subsequently, the system identifies different types of strategy by clustering the data. Finally, a cooperator agent is trained to generate partners for each type of strategy, conditioned on these clusters. In order to adapt to previously unseen partners, we leverage a fixed-share regret minimization algorithm that infers and adjusts the estimated partner strategy dynamically. We assess our approach in a customized version of the Overcooked environment, posing a challenging cooperative cooking task that demands strong coordination across a wide range of possible strategies. Using an online user study, we show that our agent outperforms current baselines when working with unfamiliar human partners.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:53:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05244v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05244v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 Macroscopic Quantum States and Universal Correlations in a
  Disorder-Order Interface Propagating over a 1D Ground State</h2>
                <div class="authors">
                    <strong>Authors:</strong> Vanja MariÄ, Florent Ferro, Maurizio Fagotti
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We consider translationally invariant quantum spin-$\frac{1}{2}$ chains with local interactions and a discrete symmetry that is spontaneously broken at zero temperature. We envision experimenters switching off the couplings between two parts of the system and preparing them in independent equilibrium states. One side of the chain is prepared in a disordered phase, and the other in a symmetry-breaking ground state. When the couplings are switched back on, time evolution ensues. We argue that in integrable systems the front separating the ordered region recedes at the maximal velocity of quasiparticle excitations over the ground state. We infer that, generically, the order parameters should vary on a subdiffusive scale of order $t^{1/3}$, where $t$ is time, and their fluctuations should exhibit the same scaling. This interfacial region exhibits full range correlations, indicating that it cannot be decomposed into nearly uncorrelated subsystems. Using the transverse-field Ising chain as a case study, we demonstrate that all order parameters follow the same universal scaling functions. Through an analysis of the skew information, we uncover that the breakdown of cluster decomposition has a quantum contribution: each subsystem within the interfacial region, with extent comparable to the region, exists in a macroscopic quantum state.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:51:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.stat-mech</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1103/982k-5jmn' target='_blank'>doi</a><a href='http://arxiv.org/abs/2410.10645v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.10645v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context
  Modeling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Meng Wei, Chenyang Wan, Xiqian Yu, Tai Wang, Yuqiang Yang, Xiaohan Mao, Chenming Zhu, Wenzhe Cai, Hanqing Wang, Yilun Chen, Xihui Liu, Jiangmiao Pang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vision-and-Language Navigation (VLN) in real-world settings requires agents to process continuous visual streams and generate actions with low latency grounded in language instructions. While Video-based Large Language Models (Video-LLMs) have driven recent progress, current VLN methods based on Video-LLM often face trade-offs among fine-grained visual understanding, long-term context modeling and computational efficiency. We introduce StreamVLN, a streaming VLN framework that employs a hybrid slow-fast context modeling strategy to support multi-modal reasoning over interleaved vision, language and action inputs. The fast-streaming dialogue context facilitates responsive action generation through a sliding-window of active dialogues, while the slow-updating memory context compresses historical visual states using a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN achieves coherent multi-turn dialogue through efficient KV cache reuse, supporting long video streams with bounded context size and inference cost. Experiments on VLN-CE benchmarks demonstrate state-of-the-art performance with stable low latency, ensuring robustness and efficiency in real-world deployment. The project page is: \href{https://streamvln.github.io/}{https://streamvln.github.io/}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:49:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05240v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05240v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 OPeRA: A Dataset of Observation, Persona, Rationale, and Action for
  Evaluating LLMs on Human Online Shopping Behavior Simulation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ziyi Wang, Yuxuan Lu, Wenbo Li, Amirali Amini, Bo Sun, Yakov Bart, Weimin Lyu, Jiri Gesi, Tian Wang, Jing Huang, Yu Su, Upol Ehsan, Malihe Alikhani, Toby Jia-Jun Li, Lydia Chilton, Dakuo Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Can large language models (LLMs) accurately simulate the next web action of a specific user? While LLMs have shown promising capabilities in generating ``believable'' human behaviors, evaluating their ability to mimic real user behaviors remains an open challenge, largely due to the lack of high-quality, publicly available datasets that capture both the observable actions and the internal reasoning of an actual human user. To address this gap, we introduce OPERA, a novel dataset of Observation, Persona, Rationale, and Action collected from real human participants during online shopping sessions. OPERA is the first public dataset that comprehensively captures: user personas, browser observations, fine-grained web actions, and self-reported just-in-time rationales. We developed both an online questionnaire and a custom browser plugin to gather this dataset with high fidelity. Using OPERA, we establish the first benchmark to evaluate how well current LLMs can predict a specific user's next action and rationale with a given persona and <observation, action, rationale> history. This dataset lays the groundwork for future research into LLM agents that aim to act as personalized digital twins for human.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:44:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.05606v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.05606v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 The Super Weight in Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mengxia Yu, De Wang, Qi Shan, Colorado J Reed, Alvin Wan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent works have shown a surprising result: a small fraction of Large Language Model (LLM) parameter outliers are disproportionately important to the quality of the model. LLMs contain billions of parameters, so these small fractions, such as 0.01%, translate to hundreds of thousands of parameters. In this work, we present an even more surprising finding: Pruning as few as a single parameter can destroy an LLM's ability to generate text -- increasing perplexity by 3 orders of magnitude and reducing zero-shot accuracy to guessing. We propose a data-free method for identifying such parameters, termed super weights, using a single forward pass through the model. We additionally find that these super weights induce correspondingly rare and large activation outliers, termed super activations. When preserved with high precision, super activations can improve simple round-to-nearest quantization to become competitive with state-of-the-art methods. For weight quantization, we similarly find that by preserving the super weight and clipping other weight outliers, round-to-nearest quantization can scale to much larger block sizes than previously considered. To facilitate further research into super weights, we provide an index of super weight coordinates for common, openly available LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:42:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.07191v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.07191v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 On the Role of Feedback in Test-Time Scaling of Agentic AI Workflows</h2>
                <div class="authors">
                    <strong>Authors:</strong> Souradip Chakraborty, Mohammadreza Pourreza, Ruoxi Sun, Yiwen Song, Nino Scherrer, Furong Huang, Amrit Singh Bedi, Ahmad Beirami, Jindong Gu, Hamid Palangi, Tomas Pfister
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Agentic AI workflows (systems that autonomously plan and act) are becoming widespread, yet their task success rate on complex tasks remains low. A promising solution is inference-time alignment, which uses extra compute at test time to improve performance. Inference-time alignment relies on three components: sampling, evaluation, and feedback. While most prior work studies sampling and automatic evaluation, feedback remains underexplored. To study the role of feedback, we introduce Iterative Agent Decoding (IAD), a procedure that repeatedly inserts feedback extracted from different forms of critiques (reward models or AI-generated textual feedback) between decoding steps. Through IAD, we analyze feedback along four dimensions: (1) its role in the accuracy-compute trade-offs with limited inference budget, (2) quantifying the gains over diversity-only baselines such as best-of-N sampling, (3) effectiveness of composing feedback from reward models versus textual critique, and (4) robustness to noisy or low-quality feedback. Across Sketch2Code, Text2SQL, Intercode, and WebShop, we show that IAD with proper integration of high fidelity feedback leads to consistent gains up to 10 percent absolute performance improvement over various baselines such as best-of-N. Our findings underscore feedback as a crucial knob for inference-time alignment of agentic AI workflows with limited inference budget.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-08T03:19:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.01931v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.01931v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 Self-Supervised Real-Time Tracking of Military Vehicles in Low-FPS UAV
  Footage</h2>
                <div class="authors">
                    <strong>Authors:</strong> Markiyan Kostiv, Anatolii Adamovskyi, Yevhen Cherniavskyi, Mykyta Varenyk, Ostap Viniavskyi, Igor Krashenyi, Oles Dobosevych
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multi-object tracking (MOT) aims to maintain consistent identities of objects across video frames. Associating objects in low-frame-rate videos captured by moving unmanned aerial vehicles (UAVs) in actual combat scenarios is complex due to rapid changes in object appearance and position within the frame. The task becomes even more challenging due to image degradation caused by cloud video streaming and compression algorithms. We present how instance association learning from single-frame annotations can overcome these challenges. We show that global features of the scene provide crucial context for low-FPS instance association, allowing our solution to be robust to distractors and gaps in detections. We also demonstrate that such a tracking approach maintains high association quality even when reducing the input image resolution and latent representation size for faster inference. Finally, we present a benchmark dataset of annotated military vehicles collected from publicly available data sources. This paper was initially presented at the NATO Science and Technology Organization Symposium (ICMCIS) organized by the Information Systems Technology (IST)Scientific and Technical Committee, IST-209-RSY - the ICMCIS, held in Oeiras, Portugal, 13-14 May 2025.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:39:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/ICMCIS64378.2025.11047873' target='_blank'>doi</a><a href='http://arxiv.org/abs/2507.05229v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05229v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 Extended Inductive Reasoning for Personalized Preference Inference from
  Behavioral Signals</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jia-Nan Li, Jian Guan, Wei Wu, Rui Yan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have demonstrated significant success in complex reasoning tasks such as math and coding. In contrast to these tasks where deductive reasoning predominates, inductive reasoning-the ability to derive general rules from incomplete evidence, remains underexplored. This paper investigates extended inductive reasoning in LLMs through the lens of personalized preference inference, a critical challenge in LLM alignment where current approaches struggle to capture diverse user preferences. The task demands strong inductive reasoning capabilities as user preferences are typically embedded implicitly across various interaction forms, requiring models to synthesize consistent preference patterns from scattered signals. We propose AlignXplore, a model that leverages extended reasoning chains to enable systematic preference inference from behavioral signals in users' interaction histories. Such explicit preference articulation enables efficient streaming inference: when new behavioral signals emerge, the model can directly build upon previously inferred preference descriptions rather than reprocessing historical signals from scratch, while also supporting iterative refinement to the inferred preferences. We develop AlignXplore by combining cold-start training based on synthetic data with subsequent online reinforcement learning. Through extensive experiments, we demonstrate that AlignXplore achieves substantial improvements over the backbone model by an average of 15.49\% on in-domain and out-of-domain benchmarks, while maintaining strong generalization ability across different input formats and downstream models. Further analyses establish best practices for preference inference learning through systematic comparison of reward modeling strategies, while revealing the emergence of human-like inductive reasoning patterns during training.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:38:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.18071v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.18071v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 Cascade: Token-Sharded Private LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rahul Thomas, Louai Zahran, Erica Choi, Akilesh Potti, Micah Goldblum, Arka Pal
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As LLMs continue to increase in parameter size, the computational resources required to run them are available to fewer parties. Therefore, third-party inference services -- where LLMs are hosted by third parties with significant computational resources -- are becoming increasingly popular. However, third party inference raises critical concerns about user data privacy. To mitigate these risks, privacy researchers have developed provably secure schemes for third-party inference, such as Secure Multi-Party Computation (SMPC). However, SMPC protocols have significant computational and communication overhead, and do not scale to large models. In this work, we propose a new multi-party inference protocol, Cascade, that avoids these punitive costs by leveraging sharding in the sequence dimension to maintain privacy, trading off cryptographic privacy guarantees for increased performance and scalability. We demonstrate that Cascade is resistant to a generalization of a recent attack that is highly effective against other statistical privacy schemes, and that it is further resistant to learning-based attacks. As Cascade is orders of magnitude faster than existing schemes, our findings offer practical solutions for secure deployment of modern state-of-the-art LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:37:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05228v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05228v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 QuEst: Enhancing Estimates of Quantile-Based Distributional Measures
  Using Model Predictions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhun Deng, Thomas P Zollo, Benjamin Eyre, Amogh Inamdar, David Madras, Richard Zemel
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As machine learning models grow increasingly competent, their predictions can supplement scarce or expensive data in various important domains. In support of this paradigm, algorithms have emerged to combine a small amount of high-fidelity observed data with a much larger set of imputed model outputs to estimate some quantity of interest. Yet current hybrid-inference tools target only means or single quantiles, limiting their applicability for many critical domains and use cases. We present QuEst, a principled framework to merge observed and imputed data to deliver point estimates and rigorous confidence intervals for a wide family of quantile-based distributional measures. QuEst covers a range of measures, from tail risk (CVaR) to population segments such as quartiles, that are central to fields such as economics, sociology, education, medicine, and more. We extend QuEst to multidimensional metrics, and introduce an additional optimization technique to further reduce variance in this and other hybrid estimators. We demonstrate the utility of our framework through experiments in economic modeling, opinion polling, and language model auto-evaluation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:33:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>stat.ML</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05220v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05220v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yanming Wan, Jiaxing Wu, Marwa Abdulhai, Lior Shani, Natasha Jaques
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Effective conversational agents like large language models (LLMs) must personalize their interactions to adapt to user preferences, personalities, and attributes across diverse domains like education and healthcare. Current methods like Reinforcement Learning from Human Feedback (RLHF), often prioritize helpfulness and safety but fall short in fostering truly empathetic, adaptive, and personalized dialogues. Existing personalization approaches typically rely on extensive user history, limiting their effectiveness for new or context-limited users. To address these limitations, we propose leveraging a user model to incorporate a curiosity-based intrinsic reward into multi-turn RLHF. This novel reward mechanism encourages the LLM agent to actively infer user traits by optimizing conversations to improve its user model's accuracy. Consequently, the agent delivers more personalized interactions by learning more about the user. We demonstrate our method's effectiveness in two distinct domains: significantly improving personalization performance in a conversational recommendation task, and personalizing conversations for different learning styles in an educational setting. We show improved generalization capabilities compared to traditional multi-turn RLHF, all while maintaining conversation quality. Our method offers a promising solution for creating more personalized, adaptive, and engaging conversational agents.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:32:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.03206v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.03206v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 Testing the ubiquitous presence of very high energy emission in
  gamma-ray bursts with the MAGIC telescopes</h2>
                <div class="authors">
                    <strong>Authors:</strong> S. Abe, J. Abhir, A. Abhishek, V. A. Acciari, A. Aguasca-Cabot, I. Agudo, T. Aniello, S. Ansoldi, L. A. Antonelli, A. Arbet Engels, C. Arcaro, T. T. H. Arnesen, K. Asano, A. Babic, C. Bakshi, U. Barres de Almeida, J. A. Barrio, L. Barrios-Jimenez, I. Batkovic, J. Baxter, J. Becerra Gonzalez, W. Bednarek, E. Bernardini, J. Bernete, A. Berti, J. Besenrieder, C. Bigongiari, A. Biland, O. Blanch, G. Bonnoli, Å½. BoÅ¡njak, E. Bronzini, I. Burelli, A. Campoy-Ordaz, A. Carosi, R. Carosi, M. Carretero-Castrillo, A. J. Castro-Tirado, D. Cerasole, G. Ceribella, Y. Chai, A. Cifuentes, J. L. Contreras, J. Cortina, S. Covino, G. D'Amico, P. Da Vela, F. Dazzi, A. De Angelis, B. De Lotto, R. de Menezes, M. Delfino, J. Delgado, C. Delgado Mendez, F. Di Pierro, R. Di Tria, L. Di Venere, A. Dinesh, D. Dominis Prester, A. Donini, D. Dorner, M. Doro, L. Eisenberger, D. Elsaesser, J. Escudero, L. FariÃ±a, A. Fattorini, L. Foffano, L. Font, S. FrÃ¶se, S. Fukami, Y. Fukazawa, R. J. GarcÃ­a LÃ³pez, S. GarcÃ­a Soto, M. Garczarczyk, S. Gasparyan, M. Gaug, J. G. Giesbrecht Paiva, N. Giglietto, F. Giordano, P. Gliwny, N. Godinovic, T. Gradetzke, R. Grau, D. Green, J. G. Green, P. GÃ¼nther, D. Hadasch, A. Hahn, T. Hassan, L. Heckmann, J. Herrera Llorente, D. Hrupec, R. Imazawa, S. Inoue, D. Israyelyan, J. Jahanvi, I. JimÃ©nez MartÃ­nez, J. JimÃ©nez Quiles, J. Jormanainen, S. Kankkunen, T. Kayanoki, J. Konrad, P. M. Kouch, H. Kubo, J. Kushida, M. LÃ¡inez, A. Lamastra, E. Lindfors, S. Lombardi, F. Longo, R. LÃ³pez-Coto, M. LÃ³pez-Moya, A. LÃ³pez-Oramas, S. Loporchio, L. Lulic, E. Lyard, P. Majumdar, M. Makariev, M. Mallamaci, G. Maneva, M. Manganaro, S. Mangano, K. Mannheim, S. Marchesi, M. Mariotti, M. MartÃ­nez, P. MaruÅ¡evec, A. Mas-Aguilar, D. Mazin, S. Menchiari, J. MÃ©ndez Gallego, S. Menon, D. Miceli, J. M. Miranda, R. Mirzoyan, M. Molero GonzÃ¡lez, E. Molina, H. A. Mondal, A. Moralejo, E. Moretti, T. Nakamori, C. Nanci, L. Nava, V. Neustroev, L. Nickel, M. Nievas Rosillo, C. Nigro, L. Nikolic, K. Nilsson, K. Nishijima, K. Noda, S. Nozaki, A. Okumura, J. Otero-Santos, S. Paiano, D. Paneque, R. Paoletti, J. M. Paredes, M. Peresano, M. Persic, M. Pihet, G. Pirola, F. Podobnik, P. G. Prada Moroni, E. Prandini, M. RibÃ³, J. Rico, C. Righi, N. Sahakyan, T. Saito, F. G. Saturni, K. Schmitz, F. Schmuckermaier, A. Sciaccaluga, G. Silvestri, A. Simongini, J. Sitarek, V. Sliusar, D. Sobczynska, A. Stamerra, J. StriÅ¡kovic, D. Strom, M. Strzys, Y. Suda, H. Tajima, M. Takahashi, R. Takeishi, P. Temnikov, K. Terauchi, T. Terzic, M. Teshima, A. Tutone, S. Ubach, J. van Scherpenberg, M. Vazquez Acosta, S. Ventura, G. Verna, I. Viale, A. Vigliano, C. F. Vigorito, E. Visentin, V. Vitale, I. Vovk, R. Walter, F. Wersig, M. Will, T. Yamamoto, P. K. H. Yeung
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Gamma-ray bursts (GRBs) are the most powerful transient objects in the Universe, and they are a primary target for the MAGIC Collaboration. Recognizing the challenges of observing these elusive objects with Imaging Atmospheric Cherenkov Telescopes (IACTs), we implemented a dedicated observational strategy that included an automated procedure for rapid re-pointing to transient sources. Since 2013, this automated procedure has enabled MAGIC to observe GRBs at a rate of approximately ten per year, which led to the successful detection of two GRBs at very high energies (VHE; E > 100 GeV). We present a comprehensive analysis of 42 non-detected GRBs (4 short GRBs) observed by MAGIC from 2013 to 2019. We derived upper limits (ULs) on the observed energy flux as well as on the intrinsic energy flux corrected for absorption by the extragalactic background light (EBL) from the MAGIC observations in selected energy and time intervals. We conducted a comprehensive study of their properties to investigate the reasons for these non-detections, including the possible peculiar properties of TeV-detected GRBs. We find that strong EBL absorption significantly hinders TeV detection for the majority of GRBs in our sample. For a subset of 6 GRBs with redshift z < 2, we compared the UL on the intrinsic flux in the VHE domain with the simultaneous X-ray flux, which is observed to be at the same level in the current population of TeV-detected GRBs. Based on these inferred MAGIC ULs, we conclude that a VHE component with a luminosity comparable to the simultaneously observed X-ray luminosity cannot be ruled out for this sample.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:29:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.HE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05215v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05215v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 Reviewing Scientific Papers for Critical Problems With Reasoning LLMs:
  Baseline Approaches and Automatic Evaluation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tianmai M. Zhang, Neil F. Abernethy
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in large language models have sparked interest in utilizing them to aid the peer review process of scientific publication amid the peer review crisis. However, having AI models generate full reviews in the same way as human reviewers risks exacerbating the irresponsible use of LLM-generated reviews. As an alternative, we propose adopting LLMs as manuscript quality checkers. We introduce several baseline approaches and an extendable automatic evaluation framework using top reasoning LLMs as judges to tackle the difficulty of recruiting domain experts for manual evaluation. Utilizing papers withdrawn from arXiv, we validated our proposed methods with several leading reasoning LLMs from multiple vendors and assessed their performance and API costs for identifying critical errors and unsoundness problems in scientific papers. o3 exhibited the best problem identification performance among all models at a modest cost. This paper provides insights into document-based scientific understanding/reasoning and lays a foundation for future applications. Our dataset, code, and model outputs are publicly available.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:28:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.23824v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.23824v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 All in One: Visual-Description-Guided Unified Point Cloud Segmentation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zongyan Han, Mohamed El Amine Boudjoghra, Jiahua Dong, Jinhong Wang, Rao Muhammad Anwer
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Unified segmentation of 3D point clouds is crucial for scene understanding, but is hindered by its sparse structure, limited annotations, and the challenge of distinguishing fine-grained object classes in complex environments. Existing methods often struggle to capture rich semantic and contextual information due to limited supervision and a lack of diverse multimodal cues, leading to suboptimal differentiation of classes and instances. To address these challenges, we propose VDG-Uni3DSeg, a novel framework that integrates pre-trained vision-language models (e.g., CLIP) and large language models (LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual descriptions and reference images from the internet, our method incorporates rich multimodal cues, facilitating fine-grained class and instance separation. We further design a Semantic-Visual Contrastive Loss to align point features with multimodal queries and a Spatial Enhanced Module to model scene-wide relationships efficiently. Operating within a closed-set paradigm that utilizes multimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art results in semantic, instance, and panoptic segmentation, offering a scalable and practical solution for 3D understanding. Our code is available at https://github.com/Hanzy1996/VDG-Uni3DSeg.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:22:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05211v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05211v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 Identification of Causal Effects with a Bunching Design</h2>
                <div class="authors">
                    <strong>Authors:</strong> Carolina Caetano, Gregorio Caetano, Leonard Goff, Eric Nielsen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We show that causal effects can be identified when there is bunching in the distribution of a continuous treatment variable, without imposing any parametric assumptions. This yields a new nonparametric method for overcoming selection bias in the absence of instrumental variables, panel data, or other popular research designs for causal inference. The method leverages the change of variables theorem from integration theory, relating the selection bias to the ratio of the density of the treatment and the density of the part of the outcome that varies with confounders. At the bunching point, the treatment level is constant, so the variation in the outcomes is due entirely to unobservables, allowing us to identify the denominator. Our main result identifies the average causal response to the treatment among individuals who marginally select into the bunching point. We further show that under additional smoothness assumptions on the selection bias, treatment effects away from the bunching point may also be identified. We propose estimators based on standard software packages and apply the method to estimate the effect of maternal smoking during pregnancy on birth weight.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:20:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>econ.EM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05210v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05210v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 Hierarchical Subtraction with Neural Density Estimators as a General
  Solution to Overlapping Gravitational Wave Signals</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qian Hu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Overlapping gravitational wave (GW) signals are expected in the third-generation (3G) GW detectors, leading to one of the major challenges in GW data analysis. Inference of overlapping GW sources is complicated - it has been reported that hierarchical inference with signal subtraction may amplify errors, while joint estimation, though more accurate, is computationally expensive. However, in this work, we show that hierarchical subtraction can achieve accurate results with a sufficient number of iterations, and on the other hand, neural density estimators, being able to generate posterior samples rapidly, make it possible to perform signal subtraction and inference repeatedly. We further develop likelihood-based resampling to accelerate the convergence of the iterative subtraction. Our method provides fast and accurate inference for overlapping GW signals and is highly adaptable to various source types and time separations, offering a potential general solution for overlapping GW signal analysis.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:19:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>gr-qc</span><span>astro-ph.HE</span><span>astro-ph.IM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05209v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05209v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 In-Context Learning as an Effective Estimator of Functional Correctness
  of LLM-Generated Code</h2>
                <div class="authors">
                    <strong>Authors:</strong> Susmita Das, Madhusudan Ghosh, Priyanka Swami, Debasis Ganguly, Gul Calikli
                </div>
                <div class="summary">
                    <strong>Summary:</strong> When applying LLM-based code generation to software development projects that follow a feature-driven or rapid application development approach, it becomes necessary to estimate the functional correctness of the generated code in the absence of test cases. Just as a user selects a relevant document from a ranked list of retrieved ones, a software generation workflow requires a developer to choose (and potentially refine) a generated solution from a ranked list of alternative solutions, ordered by their posterior likelihoods. This implies that estimating the quality of a ranked list -- akin to estimating "relevance" for query performance prediction (QPP) in IR -- is also crucial for generative software development, where quality is defined in terms of "functional correctness". In this paper, we propose an in-context learning (ICL) based approach for code quality estimation. Our findings demonstrate that providing few-shot examples of functionally correct code from a training set enhances the performance of existing QPP approaches as well as a zero-shot-based approach for code quality estimation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:01:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3726302.3730212' target='_blank'>doi</a><a href='http://arxiv.org/abs/2507.05200v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05200v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 Train-before-Test Harmonizes Language Model Rankings</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guanhua Zhang, Ricardo Dominguez-Olmedo, Moritz Hardt
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Existing language model benchmarks provide contradictory model rankings, even for benchmarks that aim to capture similar skills. This dilemma of conflicting rankings hampers model selection, clouds model comparisons, and adds confusion to a growing ecosystem of competing models. Recent work attributed ranking disagreement to the phenomenon of training on the test task: As released, different models exhibit a different level of preparation for any given test task. A candidate solution to the problem is train-before-test: Give each model the same benchmark-specific finetuning before evaluation. Our primary contribution is a broad empirical evaluation of train-before-test across 24 benchmarks and 61 models. We show that train-before-test significantly improves ranking agreement consistently across all benchmarks. Whereas rankings have little external validity to start with, they enjoy a significant degree of external validity when applying train-before-test: Model rankings transfer gracefully from one benchmark to the other. Even within the same model family, train-before-test reduces strong ranking disagreement to near-perfect agreement. In addition, train-before-test reduces the model-score matrix to essentially rank one, revealing new insights into the latent factors of benchmark performance. Our work supports the recommendation to make train-before-test a default component of LLM benchmarking.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T16:54:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05195v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05195v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 NativQA Framework: Enabling LLMs with Native, Local, and Everyday
  Knowledge</h2>
                <div class="authors">
                    <strong>Authors:</strong> Firoj Alam, Md Arid Hasan, Sahinur Rahman Laskar, Mucahid Kutlu, Kareem Darwish, Shammur Absar Chowdhury
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid advancement of large language models (LLMs) has raised concerns about cultural bias, fairness, and their applicability in diverse linguistic and underrepresented regional contexts. To enhance and benchmark the capabilities of LLMs, there is a need to develop large-scale resources focused on multilingual, local, and cultural contexts. In this study, we propose the NativQA framework, which can seamlessly construct large-scale, culturally and regionally aligned QA datasets in native languages. The framework utilizes user-defined seed queries and leverages search engines to collect location-specific, everyday information. It has been evaluated across 39 locations in 24 countries and in 7 languages -- ranging from extremely low-resource to high-resource languages -- resulting in over 300K Question-Answer (QA) pairs. The developed resources can be used for LLM benchmarking and further fine-tuning. The framework has been made publicly available for the community (https://gitlab.com/nativqa/nativqa-framework).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T16:43:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>68T50</span><span>F.2.2; I.2.7</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.05995v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.05995v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 From Fragments to Facts: A Curriculum-Driven DPO Approach for Generating
  Hindi News Veracity Explanations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pulkit Bansal, Raghvendra Kumar, Shakti Singh, Sriparna Saha, Adam Jatowt
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In an era of rampant misinformation, generating reliable news explanations is vital, especially for under-represented languages like Hindi. Lacking robust automated tools, Hindi faces challenges in scaling misinformation detection. To bridge this gap, we propose a novel framework integrating Direct Preference Optimization (DPO) with curriculum learning to align machine-generated explanations with human reasoning. Fact-checked explanations from credible sources serve as preferred responses, while LLM outputs highlight system limitations and serve as non-preferred responses. To refine task-specific alignment, we introduce two key parameters -- Actuality and Finesse -- into the DPO loss function, enhancing explanation quality and consistency. Experiments with LLMs (Mistral, Llama, Gemma) and PLMs (mBART, mT5) confirm the framework's effectiveness in generating coherent, contextually relevant explanations. This scalable approach combats misinformation and extends automated explanation generation to low-resource languages.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T16:34:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05179v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05179v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jonathan Hyun, Nicholas R Waytowich, Boyuan Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Despite rapid progress in large language model (LLM)-based multi-agent systems, current benchmarks fall short in evaluating their scalability, robustness, and coordination capabilities in complex, dynamic, real-world tasks. Existing environments typically focus on small-scale, fully observable, or low-complexity domains, limiting their utility for developing and assessing next-generation multi-agent Agentic AI frameworks. We introduce CREW-Wildfire, an open-source benchmark designed to close this gap. Built atop the human-AI teaming CREW simulation platform, CREW-Wildfire offers procedurally generated wildfire response scenarios featuring large maps, heterogeneous agents, partial observability, stochastic dynamics, and long-horizon planning objectives. The environment supports both low-level control and high-level natural language interactions through modular Perception and Execution modules. We implement and evaluate several state-of-the-art LLM-based multi-agent Agentic AI frameworks, uncovering significant performance gaps that highlight the unsolved challenges in large-scale coordination, communication, spatial reasoning, and long-horizon planning under uncertainty. By providing more realistic complexity, scalable architecture, and behavioral evaluation metrics, CREW-Wildfire establishes a critical foundation for advancing research in scalable multi-agent Agentic intelligence. All code, environments, data, and baselines will be released to support future research in this emerging domain.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T16:33:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MA</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05178v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05178v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 Mass Proxy Quality of Massive Halo Properties in the IllustrisTNG and
  FLAMINGO Simulations: I. Hot Gas</h2>
                <div class="authors">
                    <strong>Authors:</strong> Eddie Aljamal, August E. Evrard, Arya Farahi, Annalisa Pillepich, Dylan Nelson, Joop Schaye, Matthieu Schaller, Joey Braspenning
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We examine scale and redshift dependence of mass-property relations (MPRs) for five hot gas properties of two large group- and cluster-scale halo samples realized by the IllustrisTNG, TNG-Cluster and FLAMINGO cosmological hydrodynamical simulations. For intrinsic properties of i) hot gas mass ($M_{\rm gas}$), ii) spectroscopic-like temperature ($T_{\rm sl}$), iii) soft-band X-ray luminosity ($L_{\rm X}$), and iv) X-ray ($Y_{\rm X}$) and v) Sunyaev-Zel'dovich ($Y_{\rm SZ}$) thermal energies, we use MPR parameters to infer mass proxy quality (MPQ) -- the implied scatter in total halo mass conditioned on a property -- for halos with $M_{\rm 500c} \geq 10^{13}{\, {\rm M}_\odot}$ at redshifts, $z \in \{0, 0.5, 1, 2\}$. We find: (1) in general, scaling relation slopes and covariance display moderate to strong dependence on halo mass, with redshift dependence secondary, (2) for halos with $M_{\rm 500c} > 10^{14}{\, {\rm M}_\odot}$, scalings of $M_{\rm gas}$ and $Y_{\rm SZ}$ simplify toward self-similar slope and constant intrinsic scatter (5 and 10%, respectively) nearly independent of scale, making both measures ideal for cluster finding and characterization to $z=2$, (3) halo mass-conditioned likelihoods of hot gas mass and thermal energy at fixed halo mass closely follow a log-normal form, (4) despite normalization differences ranging up to $0.4$ dex, there is good qualitative, and often quantitative, agreement between the scale-dependent slopes and property covariance of the two simulations. Slopes show appreciable redshift dependence at the group scale, while redshift dependence of the scatter is exhibited by low mass FLAMINGO halos only, (5) property correlations are largely consistent between the simulations, with values that mainly agree with existing empirical measurements. We close with a literature survey placing our MPR slopes and intrinsic scatter estimates into context.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T16:31:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.GA</span><span>astro-ph.CO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05176v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05176v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 Transfer Attack for Bad and Good: Explain and Boost Adversarial
  Transferability across Multimodal Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hao Cheng, Erjia Xiao, Jiayan Yang, Jinhao Duan, Yichi Wang, Jiahang Cao, Qiang Zhang, Le Yang, Kaidi Xu, Jindong Gu, Renjing Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multimodal Large Language Models (MLLMs) demonstrate exceptional performance in cross-modality interaction, yet they also suffer adversarial vulnerabilities. In particular, the transferability of adversarial examples remains an ongoing challenge. In this paper, we specifically analyze the manifestation of adversarial transferability among MLLMs and identify the key factors that influence this characteristic. We discover that the transferability of MLLMs exists in cross-LLM scenarios with the same vision encoder and indicate \underline{\textit{two key Factors}} that may influence transferability. We provide two semantic-level data augmentation methods, Adding Image Patch (AIP) and Typography Augment Transferability Method (TATM), which boost the transferability of adversarial examples across MLLMs. To explore the potential impact in the real world, we utilize two tasks that can have both negative and positive societal impacts: \ding{182} Harmful Content Insertion and \ding{183} Information Protection.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T16:28:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2405.20090v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.20090v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 NOVA: Navigation via Object-Centric Visual Autonomy for High-Speed
  Target Tracking in Unstructured GPS-Denied Environments</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alessandro Saviolo, Giuseppe Loianno
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Autonomous aerial target tracking in unstructured and GPS-denied environments remains a fundamental challenge in robotics. Many existing methods rely on motion capture systems, pre-mapped scenes, or feature-based localization to ensure safety and control, limiting their deployment in real-world conditions. We introduce NOVA, a fully onboard, object-centric framework that enables robust target tracking and collision-aware navigation using only a stereo camera and an IMU. Rather than constructing a global map or relying on absolute localization, NOVA formulates perception, estimation, and control entirely in the target's reference frame. A tightly integrated stack combines a lightweight object detector with stereo depth completion, followed by histogram-based filtering to infer robust target distances under occlusion and noise. These measurements feed a visual-inertial state estimator that recovers the full 6-DoF pose of the robot relative to the target. A nonlinear model predictive controller (NMPC) plans dynamically feasible trajectories in the target frame. To ensure safety, high-order control barrier functions are constructed online from a compact set of high-risk collision points extracted from depth, enabling real-time obstacle avoidance without maps or dense representations. We validate NOVA across challenging real-world scenarios, including urban mazes, forest trails, and repeated transitions through buildings with intermittent GPS loss and severe lighting changes that disrupt feature-based localization. Each experiment is repeated multiple times under similar conditions to assess resilience, showing consistent and reliable performance. NOVA achieves agile target following at speeds exceeding 50 km/h. These results show that high-speed vision-based tracking is possible in the wild using only onboard sensing, with no reliance on external localization or environment assumptions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T16:28:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.18689v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.18689v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 Semantic Frame Interpolation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yijia Hong, Jiangning Zhang, Ran Yi, Yuji Wang, Weijian Cao, Xiaobin Hu, Zhucun Xue, Yabiao Wang, Chengjie Wang, Lizhuang Ma
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Generating intermediate video content of varying lengths based on given first and last frames, along with text prompt information, offers significant research and application potential. However, traditional frame interpolation tasks primarily focus on scenarios with a small number of frames, no text control, and minimal differences between the first and last frames. Recent community developers have utilized large video models represented by Wan to endow frame-to-frame capabilities. However, these models can only generate a fixed number of frames and often fail to produce satisfactory results for certain frame lengths, while this setting lacks a clear official definition and a well-established benchmark. In this paper, we first propose a new practical Semantic Frame Interpolation (SFI) task from the perspective of academic definition, which covers the above two settings and supports inference at multiple frame rates. To achieve this goal, we propose a novel SemFi model building upon Wan2.1, which incorporates a Mixture-of-LoRA module to ensure the generation of high-consistency content that aligns with control conditions across various frame length limitations. Furthermore, we propose SFI-300K, the first general-purpose dataset and benchmark specifically designed for SFI. To support this, we collect and process data from the perspective of SFI, carefully designing evaluation metrics and methods to assess the model's performance across multiple dimensions, encompassing image and video, and various aspects, including consistency and diversity. Through extensive experiments on SFI-300K, we demonstrate that our method is particularly well-suited to meet the requirements of the SFI task.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T16:25:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05173v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05173v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 CycleVAR: Repurposing Autoregressive Model for Unsupervised One-Step
  Image Translation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yi Liu, Shengqian Li, Zuzeng Lin, Feng Wang, Si Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The current conditional autoregressive image generation methods have shown promising results, yet their potential remains largely unexplored in the practical unsupervised image translation domain, which operates without explicit cross-domain correspondences. A critical limitation stems from the discrete quantization inherent in traditional Vector Quantization-based frameworks, which disrupts gradient flow between the Variational Autoencoder decoder and causal Transformer, impeding end-to-end optimization during adversarial training in image space. To tackle this issue, we propose using Softmax Relaxed Quantization, a novel approach that reformulates codebook selection as a continuous probability mixing process via Softmax, thereby preserving gradient propagation. Building upon this differentiable foundation, we introduce CycleVAR, which reformulates image-to-image translation as image-conditional visual autoregressive generation by injecting multi-scale source image tokens as contextual prompts, analogous to prefix-based conditioning in language models. CycleVAR exploits two modes to generate the target image tokens, including (1) serial multi-step generation, enabling iterative refinement across scales, and (2) parallel one-step generation synthesizing all resolution outputs in a single forward pass. Experimental findings indicate that the parallel one-step generation mode attains superior translation quality with quicker inference speed than the serial multi-step mode in unsupervised scenarios. Furthermore, both quantitative and qualitative results indicate that CycleVAR surpasses previous state-of-the-art unsupervised image translation models, \textit{e}.\textit{g}., CycleGAN-Turbo.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T16:20:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.23347v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.23347v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 AI Generated Text Detection Using Instruction Fine-tuned Large Language
  and Transformer-Based Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chinnappa Guggilla, Budhaditya Roy, Trupti Ramdas Chavan, Abdul Rahman, Edward Bowen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) possess an extraordinary capability to produce text that is not only coherent and contextually relevant but also strikingly similar to human writing. They adapt to various styles and genres, producing content that is both grammatically correct and semantically meaningful. Recently, LLMs have been misused to create highly realistic phishing emails, spread fake news, generate code to automate cyber crime, and write fraudulent scientific articles. Additionally, in many real-world applications, the generated content including style and topic and the generator model are not known beforehand. The increasing prevalence and sophistication of artificial intelligence (AI)-generated texts have made their detection progressively more challenging. Various attempts have been made to distinguish machine-generated text from human-authored content using linguistic, statistical, machine learning, and ensemble-based approaches. This work focuses on two primary objectives Task-A, which involves distinguishing human-written text from machine-generated text, and Task-B, which attempts to identify the specific LLM model responsible for the generation. Both of these tasks are based on fine tuning of Generative Pre-trained Transformer (GPT_4o-mini), Large Language Model Meta AI (LLaMA) 3 8B, and Bidirectional Encoder Representations from Transformers (BERT). The fine-tuned version of GPT_4o-mini and the BERT model has achieved accuracies of 0.9547 for Task-A and 0.4698 for Task-B.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T16:13:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05157v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05157v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 Scalable Multi-Task Learning for Particle Collision Event Reconstruction
  with Heterogeneous Graph Neural Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> William Sutcliffe, Marta Calvi, Simone Capelli, Jonas Eschle, JuliÃ¡n GarcÃ­a PardiÃ±as, Abhijit Mathad, Azusa Uzuki, Nicola Serra
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The growing luminosity frontier at the Large Hadron Collider is challenging the reconstruction and analysis of particle collision events. Increased particle multiplicities are straining latency and storage requirements at the data acquisition stage, while new complications are emerging, including higher background levels and more frequent particle vertex misassociations. This in turn necessitates the development of more holistic and scalable reconstruction methods that take advantage of recent advances in machine learning. We propose a novel Heterogeneous Graph Neural Network (HGNN) architecture featuring unique representations for diverse particle collision relationships and integrated graph pruning layers for scalability. Trained with a multi-task paradigm in an environment mimicking the LHCb experiment, this HGNN significantly improves beauty hadron reconstruction performance. Notably, it concurrently performs particle vertex association and graph pruning within a single framework. We quantify reconstruction and pruning performance, demonstrate enhanced inference time scaling with event complexity, and mitigate potential performance loss using a weighted message passing scheme.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T16:11:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.data-an</span><span>cs.LG</span><span>hep-ex</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.21844v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.21844v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 Distributional Diffusion Models with Scoring Rules</h2>
                <div class="authors">
                    <strong>Authors:</strong> Valentin De Bortoli, Alexandre Galashov, J. Swaroop Guntupalli, Guangyao Zhou, Kevin Murphy, Arthur Gretton, Arnaud Doucet
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion models generate high-quality synthetic data. They operate by defining a continuous-time forward process which gradually adds Gaussian noise to data until fully corrupted. The corresponding reverse process progressively "denoises" a Gaussian sample into a sample from the data distribution. However, generating high-quality outputs requires many discretization steps to obtain a faithful approximation of the reverse process. This is expensive and has motivated the development of many acceleration methods. We propose to accomplish sample generation by learning the posterior {\em distribution} of clean data samples given their noisy versions, instead of only the mean of this distribution. This allows us to sample from the probability transitions of the reverse process on a coarse time scale, significantly accelerating inference with minimal degradation of the quality of the output. This is accomplished by replacing the standard regression loss used to estimate conditional means with a scoring rule. We validate our method on image and robot trajectory generation, where we consistently outperform standard diffusion models at few discretization steps.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T16:03:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>stat.ML</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.02483v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.02483v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 Pseudo-likelihood produces associative memories able to generalize, even
  for asymmetric couplings</h2>
                <div class="authors">
                    <strong>Authors:</strong> Francesco D'Amico, Dario Bocchi, Luca Maria Del Bono, Saverio Rossi, Matteo Negri
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Energy-based probabilistic models learned by maximizing the likelihood of the data are limited by the intractability of the partition function. A widely used workaround is to maximize the pseudo-likelihood, which replaces the global normalization with tractable local normalizations. Here we show that, in the zero-temperature limit, a network trained to maximize pseudo-likelihood naturally implements an associative memory: if the training set is small, patterns become fixed-point attractors whose basins of attraction exceed those of any classical Hopfield rule. We explain quantitatively this effect on uncorrelated random patterns. Moreover, we show that, for different structured datasets coming from computer science (random feature model, MNIST), physics (spin glasses) and biology (proteins), as the number of training examples increases the learned network goes beyond memorization, developing meaningful attractors with non-trivial correlations with test examples, thus showing the ability to generalize. Our results therefore reveal pseudo-likelihood works both as an efficient inference tool and as a principled mechanism for memory and generalization.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T15:57:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.stat-mech</span><span>cond-mat.dis-nn</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05147v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05147v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 PropMolFlow: Property-guided Molecule Generation with Geometry-Complete
  Flow Matching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Cheng Zeng, Jirui Jin, George Karypis, Mark Transtrum, Ellad B. Tadmor, Richard G. Hennig, Adrian Roitberg, Stefano Martiniani, Mingjie Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Molecule generation is advancing rapidly in chemical discovery and drug design. Flow matching methods have recently set the state of the art (SOTA) in unconditional molecule generation, surpassing score-based diffusion models. However, diffusion models still lead in property-guided generation. In this work, we introduce PropMolFlow, a novel approach for property-guided molecule generation based on geometry-complete SE(3)-equivariant flow matching. Integrating five different property embedding methods with a Gaussian expansion of scalar properties, PropMolFlow achieves competitive performance against previous SOTA diffusion models in conditional molecule generation across various properties while preserving the stability and validity of the generated molecules, consistent with its unconditional counterpart. Additionally, it enables faster inference with significantly fewer time steps compared to baseline models. We highlight the importance of validating the properties of generated molecules through DFT calculations performed at the same level of theory as the training data. Specifically, our analysis identifies properties that require DFT validation and others where a pretrained SE(3) geometric vector perceptron regressors provide sufficiently accurate predictions on generated molecules. Furthermore, we introduce a new property metric to assess the model's ability to propose molecules with underrepresented property values, assessing its capacity for out-of-distribution generalization. Our findings reveal shortcomings in existing structural metrics, which mistakenly validate open-shell molecules or molecules with invalid valence-charge configurations, underscoring the need for improved evaluation frameworks. Overall, this work paves the way for developing targeted property-guided generation methods, enhancing the design of molecular generative models for diverse applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T15:56:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.chem-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.21469v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.21469v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 Hardware-efficient tractable probabilistic inference for TinyML
  Neurosymbolic AI applications</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jelin Leslin, Martin Trapp, Martin Andraud
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Neurosymbolic AI (NSAI) has recently emerged to mitigate limitations associated with deep learning (DL) models, e.g. quantifying their uncertainty or reason with explicit rules. Hence, TinyML hardware will need to support these symbolic models to bring NSAI to embedded scenarios. Yet, although symbolic models are typically compact, their sparsity and computation resolution contrasts with low-resolution and dense neuro models, which is a challenge on resource-constrained TinyML hardware severely limiting the size of symbolic models that can be computed. In this work, we remove this bottleneck leveraging a tight hardware/software integration to present a complete framework to compute NSAI with TinyML hardware. We focus on symbolic models realized with tractable probabilistic circuits (PCs), a popular subclass of probabilistic models for hardware integration. This framework: (1) trains a specific class of hardware-efficient \emph{deterministic} PCs, chosen for the symbolic task; (2) \emph{compresses} this PC until it can be computed on TinyML hardware with minimal accuracy degradation, using our $n^{th}$-root compression technique, and (3) \emph{deploys} the complete NSAI model on TinyML hardware. Compared to a 64b precision baseline necessary for the PC without compression, our workflow leads to significant hardware reduction on FPGA (up to 82.3\% in FF, 52.6\% in LUTs, and 18.0\% in Flash usage) and an average inference speedup of 4.67x on ESP32 microcontroller.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T15:51:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05141v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05141v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 Interpretable Mnemonic Generation for Kanji Learning via
  Expectation-Maximization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jaewook Lee, Alexander Scarlatos, Andrew Lan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Learning Japanese vocabulary is a challenge for learners from Roman alphabet backgrounds due to script differences. Japanese combines syllabaries like hiragana with kanji, which are logographic characters of Chinese origin. Kanji are also complicated due to their complexity and volume. Keyword mnemonics are a common strategy to aid memorization, often using the compositional structure of kanji to form vivid associations. Despite recent efforts to use large language models (LLMs) to assist learners, existing methods for LLM-based keyword mnemonic generation function as a black box, offering limited interpretability. We propose a generative framework that explicitly models the mnemonic construction process as driven by a set of common rules, and learn them using a novel Expectation-Maximization-type algorithm. Trained on learner-authored mnemonics from an online platform, our method learns latent structures and compositional rules, enabling interpretable and systematic mnemonics generation. Experiments show that our method performs well in the cold-start setting for new learners while providing insight into the mechanisms behind effective mnemonic creation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T15:49:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05137v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05137v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 SMART: Simulated Students Aligned with Item Response Theory for Question
  Difficulty Prediction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alexander Scarlatos, Nigel Fernandez, Christopher Ormerod, Susan Lottridge, Andrew Lan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Item (question) difficulties play a crucial role in educational assessments, enabling accurate and efficient assessment of student abilities and personalization to maximize learning outcomes. Traditionally, estimating item difficulties can be costly, requiring real students to respond to items, followed by fitting an item response theory (IRT) model to get item difficulty estimates. This approach cannot be applied to the cold-start setting for previously unseen items either. In this work, we present SMART (Simulated Students Aligned with IRT), a novel method for aligning simulated students with instructed ability, which can then be used in simulations to predict the difficulty of open-ended items. We achieve this alignment using direct preference optimization (DPO), where we form preference pairs based on how likely responses are under a ground-truth IRT model. We perform a simulation by generating thousands of responses, evaluating them with an LLM-based scoring model, and fit the resulting data to an IRT model to obtain item difficulty estimates. Through extensive experiments on a real-world student response dataset, we show that SMART outperforms other item difficulty prediction methods by leveraging its improved ability alignment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T15:41:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.CY</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05129v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05129v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 Practical considerations for Gaussian Process modeling for causal
  inference quasi-experimental studies with panel data</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sofia L. Vega, Rachel C. Nethery
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Estimating causal effects in quasi-experiments with spatio-temporal panel data often requires adjusting for unmeasured confounding that varies across space and time. Gaussian Processes (GPs) offer a flexible, nonparametric modeling approach that can account for such complex dependencies through carefully chosen covariance kernels. In this paper, we provide a practical and interpretable framework for applying GPs to causal inference in panel data settings. We demonstrate how GPs generalize popular methods such as synthetic control and vertical regression, and we show that the GP posterior mean can be represented as a weighted average of observed outcomes, where the weights reflect spatial and temporal similarity. To support applied use, we explore how different kernel choices impact both estimation performance and interpretability, offering guidance for selecting between separable and nonseparable kernels. Through simulations and application to Hurricane Katrina mortality data, we illustrate how GP models can be used to estimate counterfactual outcomes and quantify treatment effects. All code and materials are made publicly available to support reproducibility and encourage adoption. Our results suggest that GPs are a promising and interpretable tool for addressing unmeasured spatio-temporal confounding in quasi-experimental studies.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T15:40:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>stat.AP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05128v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05128v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 An Evaluation of Large Language Models on Text Summarization Tasks Using
  Prompt Engineering Techniques</h2>
                <div class="authors">
                    <strong>Authors:</strong> Walid Mohamed Aly, Taysir Hassan A. Soliman, Amr Mohamed AbdelAziz
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) continue to advance natural language processing with their ability to generate human-like text across a range of tasks. Despite the remarkable success of LLMs in Natural Language Processing (NLP), their performance in text summarization across various domains and datasets has not been comprehensively evaluated. At the same time, the ability to summarize text effectively without relying on extensive training data has become a crucial bottleneck. To address these issues, we present a systematic evaluation of six LLMs across four datasets: CNN/Daily Mail and NewsRoom (news), SAMSum (dialog), and ArXiv (scientific). By leveraging prompt engineering techniques including zero-shot and in-context learning, our study evaluates the performance using the ROUGE and BERTScore metrics. In addition, a detailed analysis of inference times is conducted to better understand the trade-off between summarization quality and computational efficiency. For Long documents, introduce a sentence-based chunking strategy that enables LLMs with shorter context windows to summarize extended inputs in multiple stages. The findings reveal that while LLMs perform competitively on news and dialog tasks, their performance on long scientific documents improves significantly when aided by chunking strategies. In addition, notable performance variations were observed based on model parameters, dataset properties, and prompt design. These results offer actionable insights into how different LLMs behave across task types, contributing to ongoing research in efficient, instruction-based NLP systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T15:34:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05123v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05123v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 VerifyLLM: LLM-Based Pre-Execution Task Plan Verification for Robots</h2>
                <div class="authors">
                    <strong>Authors:</strong> Danil S. Grigorev, Alexey K. Kovalev, Aleksandr I. Panov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the field of robotics, researchers face a critical challenge in ensuring reliable and efficient task planning. Verifying high-level task plans before execution significantly reduces errors and enhance the overall performance of these systems. In this paper, we propose an architecture for automatically verifying high-level task plans before their execution in simulator or real-world environments. Leveraging Large Language Models (LLMs), our approach consists of two key steps: first, the conversion of natural language instructions into Linear Temporal Logic (LTL), followed by a comprehensive analysis of action sequences. The module uses the reasoning capabilities of the LLM to evaluate logical coherence and identify potential gaps in the plan. Rigorous testing on datasets of varying complexity demonstrates the broad applicability of the module to household tasks. We contribute to improving the reliability and efficiency of task planning and addresses the critical need for robust pre-execution verification in autonomous systems. The code is available at https://verifyllm.github.io.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T15:31:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05118v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05118v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 VOTE: Vision-Language-Action Optimization with Trajectory Ensemble
  Voting</h2>
                <div class="authors">
                    <strong>Authors:</strong> Juyi Lin, Amir Taherin, Arash Akbari, Arman Akbari, Lei Lu, Guangyu Chen, Taskin Padir, Xiaomeng Yang, Weiwei Chen, Yiqian Li, Xue Lin, David Kaeli, Pu Zhao, Yanzhi Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent large-scale Vision Language Action (VLA) models have shown superior performance in robotic manipulation tasks guided by natural language. However, their generalization remains limited when applied to novel objects or unfamiliar environments that lie outside the training distribution. To address this, many existing approaches integrate additional components such as depth estimation, segmentation, or even diffusion to improve generalization, at the cost of adding significant computation overhead, resulting in low efficiency. This motivates the exploration of efficient action prediction methods, which are independent of additional high-level visual representations or diffusion techniques. In this work, we propose VOTE, an efficient and general framework for the optimization and acceleration of VLA models. In details, we propose a novel tokenizer-free fine-tuning approach for parallel accurate action prediction, which reduces computational overhead and accelerates inference speed. Additionally, we adopt an ensemble voting strategy for the action sampling, which significantly improves model performance and enhances generalization. Experimental results show that our method achieves state-of-the-art performance with 35$\times$ faster inference and 145 Hz throughput. All the details and codes will be open-sourced.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T15:30:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05116v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05116v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 Sequential multiple importance sampling for high-dimensional Bayesian
  inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Li Binbin, He Xiao, Liao Zihan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper introduces a sequential multiple importance sampling (SeMIS) algorithm for high-dimensional Bayesian inference. The method estimates Bayesian evidence using all generated samples from each proposal distribution while obtaining posterior samples through an importance-resampling scheme. A key innovation of SeMIS is the use of a softly truncated prior distribution as the intermediate proposal, providing a new way bridging prior and posterior distributions. By enabling samples from high-likelihood regions to traverse low-probability zones, SeMIS enhances mode mixing in challenging inference problems. Comparative evaluations against subset simulation (SuS) and adaptive Bayesian updating with structural reliability methods (aBUS) demonstrate that SeMIS achieves superior performance in evidence estimation (lower bias and variance) and posterior sampling (higher effective sample sizes and closer approximation to the true posterior), particularly for multimodal distributions. The efficacy of SeMIS is further validated in a high-dimensional finite element model updating application, where it successfully localizes structural damages by quantifying stiffness loss. The proposed algorithm not only advances Bayesian computation for complex posterior distributions but also provides a robust tool for uncertainty quantification in civil engineering systems, offering new possibilities for probabilistic structural health monitoring.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T15:30:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>cs.NA</span><span>math.NA</span><span>math.ST</span><span>stat.TH</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05114v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05114v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 Rule Learning for Knowledge Graph Reasoning under Agnostic Distribution
  Shift</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shixuan Liu, Yue He, Yunfei Wang, Hao Zou, Haoxiang Cheng, Wenjing Yang, Peng Cui, Zhong Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Knowledge graph (KG) reasoning remains a critical research area focused on inferring missing knowledge by analyzing relationships among observed facts. Despite its success, a key limitation of existing KG reasoning methods is their dependence on the I.I.D assumption. This assumption can easily be violated due to unknown sample selection bias during training or agnostic distribution shifts during testing, significantly compromising model performance and reliability. To facilitate the deployment of KG reasoning in wild environments, this study investigates learning logical rules from KGs affected by unknown selection bias. Additionally, we address test sets with agnostic distribution shifts, formally defining this challenge as out-of-distribution (OOD) KG reasoning-a previously underexplored problem. To solve the issue, we propose the Stable Rule Learning (StableRule) framework, an end-to-end methodology that integrates feature decorrelation with rule learning network, to enhance OOD generalization performance. By leveraging feature decorrelation, the StableRule framework mitigates the adverse effects of covariate shifts arising in OOD scenarios, thereby improving the robustness of the rule learning component in effectively deriving logical rules. Extensive experiments on seven benchmark KGs demonstrate the framework's superior effectiveness and stability across diverse heterogeneous environments, underscoring its practical significance for real-world applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-08T03:40:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05110v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05110v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 Transformation Discriminant Analysis for Constructing Optimal Biomarker
  Combinations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ainesh Sewak, Sandra Siegfried, Torsten Hothorn
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Accurate diagnostic tests are essential for effective screening and treatment. However, individual biomarkers often fail to provide sufficient diagnostic accuracy, as they typically capture only one aspect of the complex disease process. Combining multiple biomarkers, each capturing a distinct mechanism, can help constructing more informative diagnostic tests. In practice, logistic regression is used as the default to combine biomarkers, but it can perform poorly when biomarker distributions exhibit skewness or differ across disease groups. Nonparametric methods provide more flexibility but generally require large sample sizes that are infrequently available in biomedical research.   We propose a novel framework called transformation discriminant analysis which combines biomarkers through the likelihood ratio function to construct theoretically optimal diagnostic scores. Transformation discriminant analysis balances between flexibility and efficiency. It can accommodate a wide range of distributional shapes and disease-specific dependence structures while remaining fully parametric. This allows for likelihood inference and strong performance even in small-sample settings.   We evaluate TDA through simulations and benchmark its performance against commonly used methods. Finally, we illustrate its utility in constructing an optimal diagnostic test for hepatocellular carcinoma, a disease with no single ideal biomarker. An open-source R implementation is provided for reproducibility and broader application.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T15:21:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.03004v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.03004v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 The Hidden Threat in Plain Text: Attacking RAG Data Loaders</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alberto Castagnaro, Umberto Salviati, Mauro Conti, Luca Pajola, Simeone Pizzi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have transformed human-machine interaction since ChatGPT's 2022 debut, with Retrieval-Augmented Generation (RAG) emerging as a key framework that enhances LLM outputs by integrating external knowledge. However, RAG's reliance on ingesting external documents introduces new vulnerabilities. This paper exposes a critical security gap at the data loading stage, where malicious actors can stealthily corrupt RAG pipelines by exploiting document ingestion.   We propose a taxonomy of 9 knowledge-based poisoning attacks and introduce two novel threat vectors -- Content Obfuscation and Content Injection -- targeting common formats (DOCX, HTML, PDF). Using an automated toolkit implementing 19 stealthy injection techniques, we test five popular data loaders, finding a 74.4% attack success rate across 357 scenarios. We further validate these threats on six end-to-end RAG systems -- including white-box pipelines and black-box services like NotebookLM and OpenAI Assistants -- demonstrating high success rates and critical vulnerabilities that bypass filters and silently compromise output integrity. Our results emphasize the urgent need to secure the document ingestion process in RAG systems against covert content manipulations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T15:13:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05093v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05093v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 GaussRender: Learning 3D Occupancy with Gaussian Rendering</h2>
                <div class="authors">
                    <strong>Authors:</strong> LoÃ¯ck Chambon, Eloi Zablocki, Alexandre Boulch, MickaÃ«l Chen, Matthieu Cord
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Understanding the 3D geometry and semantics of driving scenes is critical for safe autonomous driving. Recent advances in 3D occupancy prediction have improved scene representation but often suffer from visual inconsistencies, leading to floating artifacts and poor surface localization. Existing voxel-wise losses (e.g., cross-entropy) fail to enforce visible geometric coherence. In this paper, we propose GaussRender, a module that improves 3D occupancy learning by enforcing projective consistency. Our key idea is to project both predicted and ground-truth 3D occupancy into 2D camera views, where we apply supervision. Our method penalizes 3D configurations that produce inconsistent 2D projections, thereby enforcing a more coherent 3D structure. To achieve this efficiently, we leverage differentiable rendering with Gaussian splatting. GaussRender seamlessly integrates with existing architectures while maintaining efficiency and requiring no inference-time modifications. Extensive evaluations on multiple benchmarks (SurroundOcc-nuScenes, Occ3D-nuScenes, SSCBench-KITTI360) demonstrate that GaussRender significantly improves geometric fidelity across various 3D occupancy models (TPVFormer, SurroundOcc, Symphonies), achieving state-of-the-art results, particularly on surface-sensitive metrics such as RayIoU. The code is open-sourced at https://github.com/valeoai/GaussRender.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T15:05:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.05040v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.05040v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 Computation-Aware Gaussian Processes: Model Selection And Linear-Time
  Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jonathan Wenger, Kaiwen Wu, Philipp Hennig, Jacob R. Gardner, Geoff Pleiss, John P. Cunningham
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Model selection in Gaussian processes scales prohibitively with the size of the training dataset, both in time and memory. While many approximations exist, all incur inevitable approximation error. Recent work accounts for this error in the form of computational uncertainty, which enables -- at the cost of quadratic complexity -- an explicit tradeoff between computation and precision. Here we extend this development to model selection, which requires significant enhancements to the existing approach, including linear-time scaling in the size of the dataset. We propose a novel training loss for hyperparameter optimization and demonstrate empirically that the resulting method can outperform SGPR, CGGP and SVGP, state-of-the-art methods for GP model selection, on medium to large-scale datasets. Our experiments show that model selection for computation-aware GPs trained on 1.8 million data points can be done within a few hours on a single GPU. As a result of this work, Gaussian processes can be trained on large-scale datasets without significantly compromising their ability to quantify uncertainty -- a fundamental prerequisite for optimal decision-making.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T14:51:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>stat.ML</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.01036v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.01036v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 ICAS: Detecting Training Data from Autoregressive Image Generative
  Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hongyao Yu, Yixiang Qiu, Yiheng Yang, Hao Fang, Tianqu Zhuang, Jiaxin Hong, Bin Chen, Hao Wu, Shu-Tao Xia
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Autoregressive image generation has witnessed rapid advancements, with prominent models such as scale-wise visual auto-regression pushing the boundaries of visual synthesis. However, these developments also raise significant concerns regarding data privacy and copyright. In response, training data detection has emerged as a critical task for identifying unauthorized data usage in model training. To better understand the vulnerability of autoregressive image generative models to such detection, we conduct the first study applying membership inference to this domain. Our approach comprises two key components: implicit classification and an adaptive score aggregation strategy. First, we compute the implicit token-wise classification score within the query image. Then we propose an adaptive score aggregation strategy to acquire a final score, which places greater emphasis on the tokens with lower scores. A higher final score indicates that the sample is more likely to be involved in the training set. To validate the effectiveness of our method, we adapt existing detection algorithms originally designed for LLMs to visual autoregressive models. Extensive experiments demonstrate the superiority of our method in both class-conditional and text-to-image scenarios. Moreover, our approach exhibits strong robustness and generalization under various data transformations. Furthermore, sufficient experiments suggest two novel key findings: (1) A linear scaling law on membership inference, exposing the vulnerability of large foundation models. (2) Training data from scale-wise visual autoregressive models is easier to detect than other autoregressive paradigms.Our code is available at https://github.com/Chrisqcwx/ImageAR-MIA.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T14:50:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05068v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05068v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 Quantifying Resolution Limits in Pedestal Profile Measurements with
  Gaussian Process Regression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Norman M. Cao, David R. Hatch, Craig Michoski, Todd A. Oliver, David Eldon, Andrew Oakleigh Nelson, Matthew Waller
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Edge transport barriers (ETBs) in magnetically confined fusion plasmas, commonly known as pedestals, play a crucial role in achieving high confinement plasmas. However, their defining characteristic, a steep rise in plasma pressure over short length scales, makes them challenging to diagnose experimentally. In this work, we use Gaussian Process Regression (GPR) to develop first-principles metrics for quantifying the spatiotemporal resolution limits of inferring differentiable profiles of temperature, pressure, or other quantities from experimental measurements. Although we focus on pedestals, the methods are fully general and can be applied to any setting involving the inference of profiles from discrete measurements. First, we establish a correspondence between GPR and low-pass filtering, giving an explicit expression for the effective `cutoff frequency' associated with smoothing incurred by GPR. Second, we introduce a novel information-theoretic metric, \(N_{eff}\), which measures the effective number of data points contributing to the inferred value of a profile or its derivative. These metrics enable a quantitative assessment of the trade-off between `over-fitting' and `over-regularization', providing both practitioners and consumers of GPR with a systematic way to evaluate the credibility of inferred profiles. We apply these tools to develop practical advice for using GPR in both time-independent and time-dependent settings, and demonstrate their usage on inferring pedestal profiles using measurements from the DIII-D tokamak.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T14:49:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.plasm-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05067v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05067v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 Replacing thinking with tool usage enables reasoning in small language
  models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Corrado Rainone, Tim Bakker, Roland Memisevic
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances have established a new machine learning paradigm based on scaling up compute at inference time as well as at training time. In that line of work, a combination of Supervised Fine-Tuning (SFT) on synthetic demonstrations and Reinforcement Learning with Verifiable Rewards (RLVR) is used for training Large Language Models to expend extra compute during inference in the form of "thoughts" expressed in natural language. In this paper, we propose to instead format these tokens as a multi-turn interaction trace with a stateful tool. At each turn, the new state of the tool is appended to the context of the model, whose job is to generate the tokens necessary to control the tool via a custom DSL. We benchmark this approach on the problem of repairing malfunctioning Python code, and show that this constrained setup allows for faster sampling of experience and a denser reward signal, allowing even models of size up to 3B parameters to learn how to proficiently expend additional compute on the task.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T14:49:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05065v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05065v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 A COMPASS to Model Comparison and Simulation-Based Inference in Galactic
  Chemical Evolution</h2>
                <div class="authors">
                    <strong>Authors:</strong> Berkay Gunes, Sven Buder, Tobias Buck
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present COMPASS, a novel simulation-based inference framework that combines score-based diffusion models with transformer architectures to jointly perform parameter estimation and Bayesian model comparison across competing Galactic Chemical Evolution (GCE) models. COMPASS handles high-dimensional, incomplete, and variable-size stellar abundance datasets. Applied to high-precision elemental abundance measurements, COMPASS evaluates 40 combinations of nucleosynthetic yield tables. The model strongly favours Asymptotic Giant Branch yields from NuGrid and core-collapse SN yields used in the IllustrisTNG simulation, achieving near-unity cumulative posterior probability. Using the preferred model, we infer a steep high-mass IMF slope and an elevated Supernova Ia normalization, consistent with prior solar neighbourhood studies but now derived from fully amortized Bayesian inference. Our results demonstrate that modern SBI methods can robustly constrain uncertain physics in astrophysical simulators and enable principled model selection when analysing complex, simulation-based data.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-08T06:11:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.GA</span><span>astro-ph.IM</span><span>cs.LG</span><span>physics.comp-ph</span><span>physics.data-an</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05060v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05060v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 What Shapes User Trust in ChatGPT? A Mixed-Methods Study of User
  Attributes, Trust Dimensions, Task Context, and Societal Perceptions among
  University Students</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kadija Bouyzourn, Alexandra Birch
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This mixed-methods inquiry examined four domains that shape university students' trust in ChatGPT: user attributes, seven delineated trust dimensions, task context, and perceived societal impact. Data were collected through a survey of 115 UK undergraduate and postgraduate students and four complementary semi-structured interviews. Behavioural engagement outweighed demographics: frequent use increased trust, whereas self-reported understanding of large-language-model mechanics reduced it. Among the dimensions, perceived expertise and ethical risk were the strongest predictors of overall trust; ease of use and transparency had secondary effects, while human-likeness and reputation were non-significant. Trust was highly task-contingent; highest for coding and summarising, lowest for entertainment and citation generation, yet confidence in ChatGPT's referencing ability, despite known inaccuracies, was the single strongest correlate of global trust, indicating automation bias. Computer-science students surpassed peers only in trusting the system for proofreading and writing, suggesting technical expertise refines rather than inflates reliance. Finally, students who viewed AI's societal impact positively reported the greatest trust, whereas mixed or negative outlooks dampened confidence. These findings show that trust in ChatGPT hinges on task verifiability, perceived competence, ethical alignment and direct experience, and they underscore the need for transparency, accuracy cues and user education when deploying LLMs in academic settings.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T14:29:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05046v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05046v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 MoLink: Distributed and Efficient Serving Framework for Large Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lewei Jin, Yongqi Chen, Kui Zhang, Yifan Zhuo, Yi Gao, Bowei Yang, Zhengong Cai, Wei Dong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models represent a groundbreaking shift in generative AI. Yet, these advances come with a significant challenge: the high cost of model serving. To mitigate these costs, consumer-grade GPUs emerge as a more affordable alternative. This presents an opportunity for more cost-efficient LLM serving by leveraging these GPUs.   However, it is non-trivial to achieve high-efficiency LLM serving on consumer-grade GPUs, mainly due to two challenges: 1) these GPUs are often deployed in limited network conditions; 2) these GPUs often exhibit heterogeneity in host systems. To address these challenges, we present MoLink, a distributed LLM serving system for large models. It incorporates several key techniques, enabling efficient LLM serving on heterogeneous and weakly connected consumer-grade GPUs. Our experiments demonstrate that it achieves throughput improvements of up to 458\% and cost-profit margin improvements of up to 151\%, compared to state-of-the-art systems. MoLink allows users on Windows, Linux, and containerized VMs to seamlessly integrate GPUs with just a few lines of code over Ethernet or public networks. Currently, it supports 18 mainstream architectures of open-source large language models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T14:27:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05043v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05043v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic
  Empirical Study</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuqi Zhu, Yi Zhong, Jintian Zhang, Ziheng Zhang, Shuofei Qiao, Yujie Luo, Lun Du, Da Zheng, Huajun Chen, Ningyu Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate models across three dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMs' analytical reasoning capabilities.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T14:20:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.IR</span><span>cs.LG</span><span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.19794v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.19794v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 Inference and Learning of Nonlinear LFR State-Space Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Merijn Floren, Jean-Philippe NoÃ«l, Jan Swevers
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Estimating the parameters of nonlinear block-oriented state-space models from input-output data typically involves solving a highly non-convex optimization problem, which is prone to poor local minima and slow convergence. This paper presents a computationally efficient initialization method for nonlinear linear fractional representation (NL-LFR) models using periodic data. By first inferring the latent signals and subsequently estimating the model parameters, the approach generates initial estimates for use in a later nonlinear optimization step. The proposed method shows robustness against poor local minima, and achieves a twofold error reduction compared to the state-of-the-art on a challenging benchmark dataset.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T14:16:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SY</span><span>cs.SY</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/LCSYS.2025.3580354' target='_blank'>doi</a><a href='http://arxiv.org/abs/2503.14409v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.14409v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 A Framework for Synthetic Audio Conversations Generation using Large
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kaung Myat Kyaw, Jonathan Hoyin Chan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper, we introduce ConversaSynth, a framework designed to generate synthetic conversation audio using large language models (LLMs) with multiple persona settings. The framework first creates diverse and coherent text-based dialogues across various topics, which are then converted into audio using text-to-speech (TTS) systems. Our experiments demonstrate that ConversaSynth effectively generates highquality synthetic audio datasets, which can significantly enhance the training and evaluation of models for audio tagging, audio classification, and multi-speaker speech recognition. The results indicate that the synthetic datasets generated by ConversaSynth exhibit substantial diversity and realism, making them suitable for developing robust, adaptable audio-based AI systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T13:57:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SD</span><span>cs.AI</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/WI-IAT62293.2024.00056' target='_blank'>doi</a><a href='http://arxiv.org/abs/2409.00946v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.00946v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 Relative Overfitting and Accept-Reject Framework</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yanxin Liu, Yunqi Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The scaling of Large Language Models (LLMs) currently faces significant challenges. Model assembly is widely considered a promising solution to break through these performance bottlenecks. However, current ensembling methods are primarily guided by the statistical expectation that combining multiple models over large samples will lead to performance gains. We propose an ensemble framework that transitions from such stochastic, sample-dependent methods to a regular, controllable approach based on fine-grained model segmentation. This regularity governs how models are segmented to ensure performance improvement, how the magnitude of this improvement varies with model selection, and what factors determine its theoretical maximum. To formalize this pattern, we introduce the concept of'relative overfitting,' which is derived from the performance discrepancies between constituent models and builds a bridge between ensemble outcomes and the inherent attributes of these models. We detail the patterns of this framework within the domain of NLP and briefly describe its extensibility to other fields, such as computer vision (CV) and AI for science. Our approach was validated using both custom-built and pre-trained mainstream models across diverse benchmarks, including language modeling, long-context tasks, and question-answering (QA). The results indicate that the ensemble rules we proposed are generally effective and that we provide a rigorous proof of these rules in certain experimental scenarios. The proposed framework offers a new perspective for understanding ensemble theory and provides a systematic approach to addressing the performance bottlenecks of LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T13:56:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.07783v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.07783v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 Co-DETECT: Collaborative Discovery of Edge Cases in Text Classification</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chenfei Xiong, Jingwei Ni, Yu Fan, VilÃ©m Zouhar, Donya Rooein, Lorena Calvo-BartolomÃ©, Alexander Hoyle, Zhijing Jin, Mrinmaya Sachan, Markus Leippold, Dirk Hovy, Mennatallah El-Assady, Elliott Ash
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce Co-DETECT (Collaborative Discovery of Edge cases in TExt ClassificaTion), a novel mixed-initiative annotation framework that integrates human expertise with automatic annotation guided by large language models (LLMs). Co-DETECT starts with an initial, sketch-level codebook and dataset provided by a domain expert, then leverages the LLM to annotate the data and identify edge cases that are not well described by the initial codebook. Specifically, Co-DETECT flags challenging examples, induces high-level, generalizable descriptions of edge cases, and assists user in incorporating edge case handling rules to improve the codebook. This iterative process enables more effective handling of nuanced phenomena through compact, generalizable annotation rules. Extensive user study, qualitative and quantitative analyses prove the effectiveness of Co-DETECT.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T13:48:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05010v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05010v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 Multi-modal Representations for Fine-grained Multi-label Critical View
  of Safety Recognition</h2>
                <div class="authors">
                    <strong>Authors:</strong> Britty Baby, Vinkle Srivastav, Pooja P. Jain, Kun Yuan, Pietro Mascagni, Nicolas Padoy
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Critical View of Safety (CVS) is crucial for safe laparoscopic cholecystectomy, yet assessing CVS criteria remains a complex and challenging task, even for experts. Traditional models for CVS recognition depend on vision-only models learning with costly, labor-intensive spatial annotations. This study investigates how text can be harnessed as a powerful tool for both training and inference in multi-modal surgical foundation models to automate CVS recognition. Unlike many existing multi-modal models, which are primarily adapted for multi-class classification, CVS recognition requires a multi-label framework. Zero-shot evaluation of existing multi-modal surgical models shows a significant performance gap for this task. To address this, we propose CVS-AdaptNet, a multi-label adaptation strategy that enhances fine-grained, binary classification across multiple labels by aligning image embeddings with textual descriptions of each CVS criterion using positive and negative prompts. By adapting PeskaVLP, a state-of-the-art surgical foundation model, on the Endoscapes-CVS201 dataset, CVS-AdaptNet achieves 57.6 mAP, improving over the ResNet50 image-only baseline (51.5 mAP) by 6 points. Our results show that CVS-AdaptNet's multi-label, multi-modal framework, enhanced by textual prompts, boosts CVS recognition over image-only methods. We also propose text-specific inference methods, that helps in analysing the image-text alignment. While further work is needed to match state-of-the-art spatial annotation-based methods, this approach highlights the potential of adapting generalist models to specialized surgical tasks. Code: https://github.com/CAMMA-public/CVS-AdaptNet
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T13:44:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05007v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05007v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility
  Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiangbo Yu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity to operate according to internal rules without external control. Accordingly, autonomous vehicles (AuVs) are defined as systems capable of perceiving their environment and executing preprogrammed tasks independently of external input. However, both research and real-world deployments increasingly showcase vehicles that demonstrate behaviors beyond this definition (including the SAE levels 1 to 6), such as interaction with humans and machines, goal adaptation, contextual reasoning, external tool use, and long-term planning, particularly with the integration of large language models (LLMs) and agentic AI systems. These developments reveal a conceptual gap between technical autonomy and the broader cognitive and social capabilities needed for future human-centered mobility systems. To address this, we introduce the concept of agentic vehicles (AgVs), referring to vehicles that integrate agentic AI to reason, adapt, and interact within complex environments. This paper presents a systems-level framework to characterize AgVs, focusing on their cognitive and communicative layers and differentiating them from conventional AuVs. It synthesizes relevant advances in agentic AI, robotics, multi-agent systems, and human-machine interaction, and highlights how agentic AI, through high-level reasoning and tool use, can function not merely as computational tools but as interactive agents embedded in mobility ecosystems. The paper concludes by identifying key challenges in the development and governance of AgVs, including safety, real-time control, public acceptance, ethical alignment, and regulatory frameworks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T13:34:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span><span>cs.CE</span><span>cs.CL</span><span>cs.HC</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04996v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04996v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 TLB-VFI: Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame
  Interpolation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zonglin Lyu, Chen Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Video Frame Interpolation (VFI) aims to predict the intermediate frame $I_n$ (we use n to denote time in videos to avoid notation overload with the timestep $t$ in diffusion models) based on two consecutive neighboring frames $I_0$ and $I_1$. Recent approaches apply diffusion models (both image-based and video-based) in this task and achieve strong performance. However, image-based diffusion models are unable to extract temporal information and are relatively inefficient compared to non-diffusion methods. Video-based diffusion models can extract temporal information, but they are too large in terms of training scale, model size, and inference time. To mitigate the above issues, we propose Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI), an efficient video-based diffusion model. By extracting rich temporal information from video inputs through our proposed 3D-wavelet gating and temporal-aware autoencoder, our method achieves 20% improvement in FID on the most challenging datasets over recent SOTA of image-based diffusion models. Meanwhile, due to the existence of rich temporal information, our method achieves strong performance while having 3times fewer parameters. Such a parameter reduction results in 2.3x speed up. By incorporating optical flow guidance, our method requires 9000x less training data and achieves over 20x fewer parameters than video-based diffusion models. Codes and results are available at our project page: https://zonglinl.github.io/tlbvfi_page.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T13:25:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04984v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04984v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 FUDOKI: Discrete Flow-based Unified Understanding and Generation via
  Kinetic-Optimal Velocities</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jin Wang, Yao Lai, Aoxue Li, Shifeng Zhang, Jiacheng Sun, Ning Kang, Chengyue Wu, Zhenguo Li, Ping Luo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid progress of large language models (LLMs) has catalyzed the emergence of multimodal large language models (MLLMs) that unify visual understanding and image generation within a single framework. However, most existing MLLMs rely on autoregressive (AR) architectures, which impose inherent limitations on future development, such as the raster-scan order in image generation and restricted reasoning abilities in causal context modeling. In this work, we challenge the dominance of AR-based approaches by introducing FUDOKI, a unified multimodal model purely based on discrete flow matching, as an alternative to conventional AR paradigms. By leveraging metric-induced probability paths with kinetic optimal velocities, our framework goes beyond the previous masking-based corruption process, enabling iterative refinement with self-correction capability and richer bidirectional context integration during generation. To mitigate the high cost of training from scratch, we initialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to the discrete flow matching paradigm. Experimental results show that FUDOKI achieves performance comparable to state-of-the-art AR-based MLLMs across both visual understanding and image generation tasks, highlighting its potential as a foundation for next-generation unified multimodal models. Furthermore, we show that applying test-time scaling techniques to FUDOKI yields significant performance gains, further underscoring its promise for future enhancement through reinforcement learning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T13:23:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.20147v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.20147v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 Autonomous Microscopy Experiments through Large Language Model Agents</h2>
                <div class="authors">
                    <strong>Authors:</strong> Indrajeet Mandal, Jitendra Soni, Mohd Zaki, Morten M. Smedskjaer, Katrin Wondraczek, Lothar Wondraczek, Nitya Nand Gosvami, N. M. Anoop Krishnan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) are revolutionizing self driving laboratories (SDLs) for materials research, promising unprecedented acceleration of scientific discovery. However, current SDL implementations rely on rigid protocols that fail to capture the adaptability and intuition of expert scientists in dynamic experimental settings. We introduce Artificially Intelligent Lab Assistant (AILA), a framework automating atomic force microscopy through LLM driven agents. Further, we develop AFMBench a comprehensive evaluation suite challenging AI agents across the complete scientific workflow from experimental design to results analysis. We find that state of the art models struggle with basic tasks and coordination scenarios. Notably, Claude 3.5 sonnet performs unexpectedly poorly despite excelling in materials domain question answering (QA) benchmarks, revealing that domain specific QA proficiency does not necessarily translate to effective agentic capabilities. Additionally, we observe that LLMs can deviate from instructions, raising safety alignment concerns for SDL applications. Our ablations reveal that multi agent frameworks outperform single-agent architectures. We also observe significant prompt fragility, where slight modifications in prompt structure cause substantial performance variations in capable models like GPT 4o. Finally, we evaluate AILA's effectiveness in increasingly advanced experiments AFM calibration, feature detection, mechanical property measurement, graphene layer counting, and indenter detection. Our findings underscore the necessity for rigorous benchmarking protocols and prompt engineering strategies before deploying AI laboratory assistants in scientific research environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T13:21:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span><span>cond-mat.mtrl-sci</span><span>cs.AI</span><span>physics.ins-det</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.10385v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.10385v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 Can Video LLMs Refuse to Answer? Alignment for Answerability in Video
  Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Eunseop Yoon, Hee Suk Yoon, Mark A. Hasegawa-Johnson, Chang D. Yoo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the broader context of deep learning, Multimodal Large Language Models have achieved significant breakthroughs by leveraging powerful Large Language Models as a backbone to align different modalities into the language space. A prime exemplification is the development of Video Large Language Models (Video-LLMs). While numerous advancements have been proposed to enhance the video understanding capabilities of these models, they are predominantly trained on questions generated directly from video content. However, in real-world scenarios, users often pose questions that extend beyond the informational scope of the video, highlighting the need for Video-LLMs to assess the relevance of the question. We demonstrate that even the best-performing Video-LLMs fail to reject unfit questions-not necessarily due to a lack of video understanding, but because they have not been trained to identify and refuse such questions. To address this limitation, we propose alignment for answerability, a framework that equips Video-LLMs with the ability to evaluate the relevance of a question based on the input video and appropriately decline to answer when the question exceeds the scope of the video, as well as an evaluation framework with a comprehensive set of metrics designed to measure model behavior before and after alignment. Furthermore, we present a pipeline for creating a dataset specifically tailored for alignment for answerability, leveraging existing video-description paired datasets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T13:19:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04976v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04976v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 The Case for Instance-Optimized LLMs in OLAP Databases</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bardia Mohammadi, Laurent Bindschaedler
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) can enhance analytics systems with powerful data summarization, cleaning, and semantic transformation capabilities. However, deploying LLMs at scale -- processing millions to billions of rows -- remains prohibitively expensive in computation and memory. We present IOLM-DB, a novel system that makes LLM-enhanced database queries practical through query-specific model optimization. Instead of using general-purpose LLMs, IOLM-DB generates lightweight, specialized models tailored to each query's specific needs using representative data samples. IOLM-DB reduces model footprints by up to 76% and increases throughput by up to 3.31$\times$ while maintaining accuracy through aggressive compression techniques, including quantization, sparsification, and structural pruning. We further show how our approach enables higher parallelism on existing hardware and seamlessly supports caching and batching strategies to reduce overheads. Our prototype demonstrates that leveraging LLM queries inside analytics systems is feasible at scale, opening new possibilities for future OLAP applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T13:10:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04967v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04967v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 InterGSEdit: Interactive 3D Gaussian Splatting Editing with 3D
  Geometry-Consistent Attention Prior</h2>
                <div class="authors">
                    <strong>Authors:</strong> Minghao Wen, Shengjie Wu, Kangkan Wang, Dong Liang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> 3D Gaussian Splatting based 3D editing has demonstrated impressive performance in recent years. However, the multi-view editing often exhibits significant local inconsistency, especially in areas of non-rigid deformation, which lead to local artifacts, texture blurring, or semantic variations in edited 3D scenes. We also found that the existing editing methods, which rely entirely on text prompts make the editing process a "one-shot deal", making it difficult for users to control the editing degree flexibly. In response to these challenges, we present InterGSEdit, a novel framework for high-quality 3DGS editing via interactively selecting key views with users' preferences. We propose a CLIP-based Semantic Consistency Selection (CSCS) strategy to adaptively screen a group of semantically consistent reference views for each user-selected key view. Then, the cross-attention maps derived from the reference views are used in a weighted Gaussian Splatting unprojection to construct the 3D Geometry-Consistent Attention Prior ($GAP^{3D}$). We project $GAP^{3D}$ to obtain 3D-constrained attention, which are fused with 2D cross-attention via Attention Fusion Network (AFN). AFN employs an adaptive attention strategy that prioritizes 3D-constrained attention for geometric consistency during early inference, and gradually prioritizes 2D cross-attention maps in diffusion for fine-grained features during the later inference. Extensive experiments demonstrate that InterGSEdit achieves state-of-the-art performance, delivering consistent, high-fidelity 3DGS editing with improved user experience.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T13:04:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04961v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04961v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 Open-Set Gait Recognition from Sparse mmWave Radar Point Clouds</h2>
                <div class="authors">
                    <strong>Authors:</strong> Riccardo Mazzieri, Jacopo Pegoraro, Michele Rossi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The adoption of Millimeter-Wave (mmWave) radar devices for human sensing, particularly gait recognition, has recently gathered significant attention due to their efficiency, resilience to environmental conditions, and privacy-preserving nature. In this work, we tackle the challenging problem of Open-set Gait Recognition (OSGR) from sparse mmWave radar point clouds. Unlike most existing research, which assumes a closed-set scenario, our work considers the more realistic open-set case, where unknown subjects might be present at inference time, and should be correctly recognized by the system. Point clouds are well-suited for edge computing applications with resource constraints, but are more significantly affected by noise and random fluctuations than other representations, like the more common micro-Doppler signature. This is the first work addressing open-set gait recognition with sparse point cloud data. To do so, we propose a novel neural network architecture that combines supervised classification with unsupervised reconstruction of the point clouds, creating a robust, rich, and highly regularized latent space of gait features. To detect unknown subjects at inference time, we introduce a probabilistic novelty detection algorithm that leverages the structured latent space and offers a tunable trade-off between inference speed and prediction accuracy. Along with this paper, we release mmGait10, an original human gait dataset featuring over five hours of measurements from ten subjects, under varied walking modalities. Extensive experimental results show that our solution attains F1-Score improvements by 24% over state-of-the-art methods adapted for point clouds, on average, and across multiple openness levels.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T13:04:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.07435v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.07435v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 Boosting Temporal Sentence Grounding via Causal Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kefan Tang, Lihuo He, Jisheng Dang, Xinbo Gao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Temporal Sentence Grounding (TSG) aims to identify relevant moments in an untrimmed video that semantically correspond to a given textual query. Despite existing studies having made substantial progress, they often overlook the issue of spurious correlations between video and textual queries. These spurious correlations arise from two primary factors: (1) inherent biases in the textual data, such as frequent co-occurrences of specific verbs or phrases, and (2) the model's tendency to overfit to salient or repetitive patterns in video content. Such biases mislead the model into associating textual cues with incorrect visual moments, resulting in unreliable predictions and poor generalization to out-of-distribution examples. To overcome these limitations, we propose a novel TSG framework, causal intervention and counterfactual reasoning that utilizes causal inference to eliminate spurious correlations and enhance the model's robustness. Specifically, we first formulate the TSG task from a causal perspective with a structural causal model. Then, to address unobserved confounders reflecting textual biases toward specific verbs or phrases, a textual causal intervention is proposed, utilizing do-calculus to estimate the causal effects. Furthermore, visual counterfactual reasoning is performed by constructing a counterfactual scenario that focuses solely on video features, excluding the query and fused multi-modal features. This allows us to debias the model by isolating and removing the influence of the video from the overall effect. Experiments on public datasets demonstrate the superiority of the proposed method. The code is available at https://github.com/Tangkfan/CICR.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T13:01:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.MM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04958v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04958v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code
  Generation Evaluation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chenchen Zhang, Yuhang Li, Can Xu, Jiaheng Liu, Ao Liu, Shihui Hu, Dengpeng Wu, Guanhua Huang, Kejiao Li, Qi Yi, Ruibin Xiong, Haotian Zhu, Yuanxing Zhang, Yuhao Jiang, Yue Zhang, Zenan Xu, Bohui Zhai, Guoxiang He, Hebin Li, Jie Zhao, Le Zhang, Lingyun Tan, Pengyu Guo, Xianshu Pang, Yang Ruan, Zhifeng Zhang, Zhonghu Wang, Ziyan Xu, Zuopu Yin, Wiggin Zhou, Chayse Zhou, Fengzong Lian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The generative capabilities of Large Language Models (LLMs) are rapidly expanding from static code to dynamic, interactive visual artifacts. This progress is bottlenecked by a critical evaluation gap: established benchmarks focus on algorithmic correctness and are blind to the visual fidelity and interactive integrity that define modern user experiences. To bridge this gap, we introduce ArtifactsBench, a new benchmark and paradigm for the automated, multimodal evaluation of visual code generation. Our framework programmatically renders each generated artifact and captures its dynamic behavior through temporal screenshots. This visual evidence, alongside the source code, is then assessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a fine-grained, per-task checklist to ensure holistic and reproducible scoring. We construct a new benchmark of 1,825 diverse tasks and evaluate over 30 leading LLMs. Our automated evaluation achieves a striking 94.4% ranking consistency with WebDev Arena, the gold-standard for human preference in web development, and over 90% pairwise agreement with human experts. This establishes ArtifactsBench as the first framework to reliably automate the assessment of human-perceived quality at scale. Our analysis provides a high-resolution map of the current SOTA, revealing that generalist models often outperform domain-specific ones. We open-source ArtifactsBench, including the benchmark, evaluation harness, and baseline results at https://artifactsbenchmark.github.io/, to provide the community with a scalable and accurate tool to accelerate the development of user-centric generative models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T12:53:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04952v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04952v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 What is emergence, after all?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Abbas K. Rizi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The term emergence is increasingly used across scientific disciplines to describe phenomena that arise from interactions among a system's components but cannot be readily inferred by examining those components in isolation. While often invoked to explain higher-level behaviors, such as flocking, synchronization, or collective intelligence, the term is frequently used without precision, sometimes giving rise to ambiguity or even mystique. In this perspective paper, we clarify the scientific meaning of emergence as a measurable, physically grounded phenomenon. Through concrete examples, such as temperature, magnetism, and herd immunity in social networks, we review how collective behavior can arise from local interactions that are constrained by global boundaries. By disentangling emergence from vague overuse, we emphasize its role as a rigorous tool for understanding complex systems. Our goal is to show that emergence, when properly framed, offers not mysticism but insight.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T12:52:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.soc-ph</span><span>cond-mat.stat-mech</span><span>physics.hist-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04951v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04951v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 SIGIR 2025 -- LiveRAG Challenge Report</h2>
                <div class="authors">
                    <strong>Authors:</strong> David Carmel, Simone Filice, Guy Horowitz, Yoelle Maarek, Oren Somekh, Ran Tavory, Mehdi Ghissassi, Edo Liberty, Roy Miara
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The LiveRAG Challenge at SIGIR 2025, held between March and May 2025, provided a competitive platform for advancing Retrieval-Augmented Generation (RAG) technologies. Participants from academia and industry were invited to develop a RAG-based question-answering system using a fixed corpus (Fineweb-10BT) and a common open-source LLM (Falcon3-10B-Instruct). The goal was to facilitate challenging comparisons of retrieval and prompting strategies. During the Live Challenge Day, 70 teams from 27 different countries provided answers and supportive information to 500 unseen questions within a strict two-hour time window. Evaluation was conducted in two stages: first an automated LLM-as-a-judge approach was used to compute correctness and faithfulness score, then a manual review of top ranked submissions was conducted. The finalists were announced on June 12, 2025, with prizes awarded during the LiveRAG Workshop at SIGIR 2025 in Padua, Italy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-08T06:37:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.IR</span><span>H.3.3</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04942v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04942v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 LIFT: Automating Symbolic Execution Optimization with Large Language
  Models for AI Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruoxi Wang, Kun Li, Minghui Xu, Yue Zhang, Kaidi Xu, Chunchi Liu, Yinhao Xiao, Xiuzhen Cheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Dynamic Symbolic Execution (DSE) is a key technique in program analysis, widely used in software testing, vulnerability discovery, and formal verification. In distributed AI systems, DSE plays a crucial role in identifying hard-to-detect bugs, especially those arising from complex network communication patterns. However, traditional approaches to symbolic execution are often hindered by scalability issues and inefficiencies, particularly in large-scale systems. This paper introduces LIFT (Large-language-model Integrated Functional-equivalent-IR Transformation), a novel framework that leverages Large Language Models (LLMs) to automate the optimization of Intermediate Representations (IRs) in symbolic execution. LIFT addresses the challenges of symbolic execution by providing a scalable, context-sensitive solution for IR transformation. The framework consists of two phases: IR Analysis and Optimization, where LLMs optimize time-intensive IR blocks, and Symbolic Execution and Validation, which includes benchmarking and semantic verification to ensure correctness and generalizability. Experiments on real-world binaries demonstrated significant performance improvements, including a 53.5\% reduction in execution time for bigtest and a 10.24\% reduction for random, along with reductions in IR statements, PUT instructions, and temporary variables. These results demonstrate that LLMs simplify IRs while maintaining functional correctness, enhancing symbolic execution in distributed AI systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T12:26:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04931v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04931v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 Object-centric Denoising Diffusion Models for Physical Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Moritz Lange, Raphael C. Engelhardt, Wolfgang Konen, Andrew Melnik, Laurenz Wiskott
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reasoning about the trajectories of multiple, interacting objects is integral to physical reasoning tasks in machine learning. This involves conditions imposed on the objects at different time steps, for instance initial states or desired goal states. Existing approaches in physical reasoning generally rely on autoregressive modeling, which can only be conditioned on initial states, but not on later states. In fields such as planning for reinforcement learning, similar challenges are being addressed with denoising diffusion models. In this work, we propose an object-centric denoising diffusion model architecture for physical reasoning that is translation equivariant over time, permutation equivariant over objects, and can be conditioned on arbitrary time steps for arbitrary objects. We demonstrate how this model can solve tasks with multiple conditions and examine its performance when changing object numbers and trajectory lengths during inference.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T12:06:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04920v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04920v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 Leadership Detection via Time-Lagged Correlation-Based Network Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Thayanne FranÃ§a da Silva, JosÃ© Everardo Bessa Maia
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Understanding leadership dynamics in collective behavior is a key challenge in animal ecology, swarm robotics, and intelligent transportation. Traditional information-theoretic approaches, including Transfer Entropy (TE) and Time-Lagged Mutual Information (TLMI), have been widely used to infer leader-follower relationships but face critical limitations in noisy or short-duration datasets due to their reliance on robust probability estimations. This study proposes a method based on dynamic network inference using time-lagged correlations across multiple kinematic variables: velocity, acceleration, and direction. Our approach constructs directed influence graphs over time, enabling the identification of leadership patterns without the need for large volumes of data or parameter-sensitive discretization. We validate our method through two multi-agent simulations in NetLogo: a modified Vicsek model with informed leaders and a predator-prey model featuring coordinated and independent wolf groups. Experimental results demonstrate that the network-based method outperforms TE and TLMI in scenarios with limited spatiotemporal observations, ranking true leaders at the top of influence metrics more consistently than TE and TLMI.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T12:04:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MA</span><span>cs.AI</span><span>nlin.AO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04917v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04917v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 EAP4EMSIG -- Enhancing Event-Driven Microscopy for Microfluidic
  Single-Cell Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nils Friederich, Angelo Jovin Yamachui Sitcheu, Annika Nassal, Erenus Yildiz, Matthias Pesch, Maximilian Beichter, Lukas Scholtes, Bahar Akbaba, Thomas Lautenschlager, Oliver Neumann, Dietrich Kohlheyer, Hanno Scharr, Johannes Seiffarth, Katharina NÃ¶h, Ralf Mikut
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Microfluidic Live-Cell Imaging (MLCI) yields data on microbial cell factories. However, continuous acquisition is challenging as high-throughput experiments often lack real-time insights, delaying responses to stochastic events. We introduce three components in the Experiment Automation Pipeline for Event-Driven Microscopy to Smart Microfluidic Single-Cell Analysis (EAP4EMSIG): a fast, accurate Multi-Layer Perceptron (MLP)-based autofocusing method predicting the focus offset, an evaluation of real-time segmentation methods and a real-time data analysis dashboard. Our MLP-based autofocusing achieves a Mean Absolute Error (MAE) of 0.105 $\mu$m with inference times from 87 ms. Among eleven evaluated Deep Learning (DL) segmentation methods, Cellpose reached a Panoptic Quality (PQ) of 93.36 %, while a distance-based method was fastest (121 ms, Panoptic Quality 93.02 %).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T12:01:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>q-bio.QM</span><span>cs.AI</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.00047v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.00047v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 Do LLMs Understand the Safety of Their Inputs? Training-Free Moderation
  via Latent Prototypes</h2>
                <div class="authors">
                    <strong>Authors:</strong> Maciej ChrabÄszcz, Filip Szatkowski, Bartosz WÃ³jcik, Jan DubiÅski, Tomasz TrzciÅski, Sebastian Cygert
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the rise of LLMs, ensuring model safety and alignment has become a critical concern. While modern instruction-finetuned LLMs incorporate alignment during training, they still frequently require moderation tools to prevent unsafe behavior. The most common approach to moderation are guard models that flag unsafe inputs. However, guards require costly training and are typically limited to fixed-size, pre-trained options, making them difficult to adapt to evolving risks and resource constraints. We hypothesize that instruction-finetuned LLMs already encode safety-relevant information internally and explore training-free safety assessment methods that work with off-the-shelf models. We show that simple prompting allows models to recognize harmful inputs they would otherwise mishandle. We also demonstrate that safe and unsafe prompts are distinctly separable in the models' latent space. Building on this, we introduce the Latent Prototype Moderator (LPM), a training-free moderation method that uses Mahalanobis distance in latent space to assess input safety. LPM is a lightweight, customizable add-on that generalizes across model families and sizes. Our method matches or exceeds state-of-the-art guard models across multiple safety benchmarks, offering a practical and flexible solution for scalable LLM moderation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T11:43:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.16174v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.16174v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 MARBLE: A Multi-Agent Rule-Based LLM Reasoning Engine for Accident
  Severity Prediction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kaleem Ullah Qasim, Jiashu Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Accident severity prediction plays a critical role in transportation safety systems but is a persistently difficult task due to incomplete data, strong feature dependencies, and severe class imbalance in which rare but high-severity cases are underrepresented and hard to detect. Existing methods often rely on monolithic models or black box prompting, which struggle to scale in noisy, real-world settings and offer limited interpretability. To address these challenges, we propose MARBLE a multiagent rule based LLM engine that decomposes the severity prediction task across a team of specialized reasoning agents, including an interchangeable ML-backed agent. Each agent focuses on a semantic subset of features (e.g., spatial, environmental, temporal), enabling scoped reasoning and modular prompting without the risk of prompt saturation. Predictions are coordinated through either rule-based or LLM-guided consensus mechanisms that account for class rarity and confidence dynamics. The system retains structured traces of agent-level reasoning and coordination outcomes, supporting in-depth interpretability and post-hoc performance diagnostics. Across both UK and US datasets, MARBLE consistently outperforms traditional machine learning classifiers and state-of-the-art (SOTA) prompt-based reasoning methods including Chain-of-Thought (CoT), Least-to-Most (L2M), and Tree-of-Thought (ToT) achieving nearly 90% accuracy where others plateau below 48%. This performance redefines the practical ceiling for accident severity classification under real world noise and extreme class imbalance. Our results position MARBLE as a generalizable and interpretable framework for reasoning under uncertainty in safety-critical applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T11:27:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span><span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04893v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04893v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement
  Learning Framework</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chen Wang, Lai Wei, Yanzhi Zhang, Chenyang Shao, Zedong Dan, Weiran Huang, Yue Wang, Yuzhi Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in reinforcement learning (RL) have significantly enhanced the reasoning capabilities of large language models (LLMs). Group Relative Policy Optimization (GRPO), an efficient variant of PPO that lowers RL's computational cost, still faces limited exploration, low sample efficiency and instability, constraining its performance on complex reasoning tasks. To address these limitations, we introduce EFRame, an Exploration-Filter-Replay framework that systematically augments GRPO along three critical dimensions. EFRame performs additional rollouts to explore high-quality trajectories, applies online filtering to eliminate low-quality samples that introduce noise and variance, and leverages experience replay to repeatedly exploit rare but informative samples. EFRame establishes a complete and stable learning cycle, guiding the model through a structured transition from exploration to convergence. Our experiments across a variety of reasoning benchmarks demonstrate that EFRame not only improves the robustness and efficiency of training, but also enables access to deeper reasoning capabilities that remain unattainable under vanilla GRPO. Furthermore, EFRame not only enables fine-grained categorization of training samples for deeper insight into their contributions, but also introduces an efficient and precise mechanism for entropy control, which is critical for balancing exploration and convergence in RL training. Our code is available at https://github.com/597358816/EFRame.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T11:27:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.22200v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.22200v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen
  Visual Unicode Representations</h2>
                <div class="authors">
                    <strong>Authors:</strong> A. Bochkov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Understanding the locus of semantic representation in large language models (LLMs) is crucial for interpretability and architectural innovation. The dominant paradigm posits that trainable input embeddings serve as foundational "meaning vectors." This paper challenges that view. We construct Transformer models where the embedding layer is entirely frozen, with vectors derived not from data, but from the visual structure of Unicode glyphs. These non-semantic, precomputed visual embeddings are fixed throughout training. Our method is compatible with any tokenizer, including a novel Unicode-centric tokenizer we introduce to ensure universal text coverage. Despite the absence of trainable, semantically initialized embeddings, our models converge, generate coherent text, and, critically, outperform architecturally identical models with trainable embeddings on the MMLU reasoning benchmark. We attribute this to "representational interference" in conventional models, where the embedding layer is burdened with learning both structural and semantic features. Our results indicate that high-level semantics are not inherent to input embeddings but are an emergent property of the Transformer's compositional architecture and data scale. This reframes the role of embeddings from meaning containers to structural primitives. We release all code and models to foster further research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T11:17:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04886v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04886v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 Building Open-Retrieval Conversational Question Answering Systems by
  Generating Synthetic Data and Decontextualizing User Questions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Christos Vlachos, Nikolaos Stylianou, Alexandra Fiotaki, Spiros Methenitis, Elisavet Palogiannidi, Themos Stafylakis, Ion Androutsopoulos
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We consider open-retrieval conversational question answering (OR-CONVQA), an extension of question answering where system responses need to be (i) aware of dialog history and (ii) grounded in documents (or document fragments) retrieved per question. Domain-specific OR-CONVQA training datasets are crucial for real-world applications, but hard to obtain. We propose a pipeline that capitalizes on the abundance of plain text documents in organizations (e.g., product documentation) to automatically produce realistic OR-CONVQA dialogs with annotations. Similarly to real-world humanannotated OR-CONVQA datasets, we generate in-dialog question-answer pairs, self-contained (decontextualized, e.g., no referring expressions) versions of user questions, and propositions (sentences expressing prominent information from the documents) the system responses are grounded in. We show how the synthetic dialogs can be used to train efficient question rewriters that decontextualize user questions, allowing existing dialog-unaware retrievers to be utilized. The retrieved information and the decontextualized question are then passed on to an LLM that generates the system's response.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T11:16:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04884v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04884v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 DoPI: Doctor-like Proactive Interrogation LLM for Traditional Chinese
  Medicine</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zewen Sun, Ruoxiang Huang, Jiahe Feng, Rundong Kong, Yuqian Wang, Hengyu Liu, Ziqi Gong, Yuyuan Qin, Yingxue Wang, Yu Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Enhancing interrogation capabilities in Traditional Chinese Medicine (TCM) diagnosis through multi-turn dialogues and knowledge graphs presents a significant challenge for modern AI systems. Current large language models (LLMs), despite their advancements, exhibit notable limitations in medical applications, particularly in conducting effective multi-turn dialogues and proactive questioning. These shortcomings hinder their practical application and effectiveness in simulating real-world diagnostic scenarios. To address these limitations, we propose DoPI, a novel LLM system specifically designed for the TCM domain. The DoPI system introduces a collaborative architecture comprising a guidance model and an expert model. The guidance model conducts multi-turn dialogues with patients and dynamically generates questions based on a knowledge graph to efficiently extract critical symptom information. Simultaneously, the expert model leverages deep TCM expertise to provide final diagnoses and treatment plans. Furthermore, this study constructs a multi-turn doctor-patient dialogue dataset to simulate realistic consultation scenarios and proposes a novel evaluation methodology that does not rely on manually collected real-world consultation data. Experimental results show that the DoPI system achieves an accuracy rate of 84.68 percent in interrogation outcomes, significantly enhancing the model's communication ability during diagnosis while maintaining professional expertise.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T11:04:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04877v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04877v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 NTSFormer: A Self-Teaching Graph Transformer for Multimodal Cold-Start
  Node Classification</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jun Hu, Yufei He, Yuan Li, Bryan Hooi, Bingsheng He
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Cold-start node classification on multimodal graphs is challenging because cold-start nodes are isolated (i.e., no edges) and often have missing modalities (e.g., absent text or image features). Existing methods address structural isolation by degrading graph learning models to MLPs for cold-start inference, using a teacher model (with graph access) to guide the MLP. However, this results in limited model capacity in the student, which is further challenged when modalities are missing. In this paper, we propose Neighbor-to-Self Graph Transformer (NTSFormer), a unified Graph Transformer framework that jointly tackles the isolation and missing-modality issues via a self-teaching paradigm. Specifically, NTSFormer uses a cold-start attention mask to simultaneously make two predictions for each node: a "student" prediction based only on self-information (i.e., the node's own features), and a "teacher" prediction incorporating both self and neighbor information. This enables the model to supervise itself without degrading to an MLP, thereby fully leveraging the Transformer's capacity to handle missing modalities. To handle diverse graph information and missing modalities, NTSFormer performs a one-time multimodal graph pre-computation that converts structural and feature data into token sequences, which are then processed by a Mixture-of-Experts (MoE) Input Projection and Transformer layers for effective fusion. Experimental results on public datasets show that NTSFormer achieves superior performance on multimodal cold-start node classification tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T10:56:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04870v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04870v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 Supporting Software Formal Verification with Large Language Models: An
  Experimental Study</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weiqi Wang, Marie Farrell, Lucas C. Cordeiro, Liping Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Formal methods have been employed for requirements verification for a long time. However, it is difficult to automatically derive properties from natural language requirements. SpecVerify addresses this challenge by integrating large language models (LLMs) with formal verification tools, providing a more flexible mechanism for expressing requirements. This framework combines Claude 3.5 Sonnet with the ESBMC verifier to form an automated workflow. Evaluated on nine cyber-physical systems from Lockheed Martin, SpecVerify achieves 46.5% verification accuracy, comparable to NASA's CoCoSim, but with lower false positives. Our framework formulates assertions that extend beyond the expressive power of LTL and identifies falsifiable cases that are missed by more traditional methods. Counterexample analysis reveals CoCoSim's limitations stemming from model connection errors and numerical approximation issues. While SpecVerify advances verification automation, our comparative study of Claude, ChatGPT, and Llama shows that high-quality requirements documentation and human monitoring remain critical, as models occasionally misinterpret specifications. Our results demonstrate that LLMs can significantly reduce the barriers to formal verification, while highlighting the continued importance of human-machine collaboration in achieving optimal results.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T10:30:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04857v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04857v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 NotebookLM: An LLM with RAG for active learning and collaborative
  tutoring</h2>
                <div class="authors">
                    <strong>Authors:</strong> Eugenio Tufino
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This study explores NotebookLM, a Google Gemini powered AI platform that integrates Retrieval Augmented Generation (RAG), as a collaborative physics tutor, an area of research that is developing quickly. In our implementation, NotebookLM was configured as an AI physics collaborative tutor to support students in solving conceptually oriented physics problems using a collaborative, Socratic approach. When deployed as a collaborative tutor, the system restricts student interaction to a chat only interface, promoting controlled and guided engagement. By grounding its responses in teacher provided source documents, NotebookLM helps mitigate one of the major shortcomings of standard large language models--hallucinations--thereby ensuring more traceable and reliable answers. Our experiments demonstrate NotebookLM's potential as a low cost, easily implemented RAG based tool for personalized and traceable AI assisted physics learning in diverse educational settings. Furthermore, NotebookLM also functions as a valuable study tool for both teachers and students by generating targeted questions, study guides, and supplementary materials that enhance both classroom instruction and independent research. While limitations remain, particularly regarding legal restrictions, the current text only mode of interaction, and the intrinsic reliability challenges of statistical models, this work presents a promising example of a grounded AI application in physics education.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T10:27:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.ed-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.09720v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.09720v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 $\textit{Grahak-Nyay:}$ Consumer Grievance Redressal through Large
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shrey Ganatra, Swapnil Bhattacharyya, Harshvivek Kashid, Spandan Anaokar, Shruti Nair, Reshma Sekhar, Siddharth Manohar, Rahul Hemrajani, Pushpak Bhattacharyya
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Access to consumer grievance redressal in India is often hindered by procedural complexity, legal jargon, and jurisdictional challenges. To address this, we present $\textbf{Grahak-Nyay}$ (Justice-to-Consumers), a chatbot that streamlines the process using open-source Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG). Grahak-Nyay simplifies legal complexities through a concise and up-to-date knowledge base. We introduce three novel datasets: $\textit{GeneralQA}$ (general consumer law), $\textit{SectoralQA}$ (sector-specific knowledge) and $\textit{SyntheticQA}$ (for RAG evaluation), along with $\textit{NyayChat}$, a dataset of 300 annotated chatbot conversations. We also introduce $\textit{Judgments}$ data sourced from Indian Consumer Courts to aid the chatbot in decision making and to enhance user trust. We also propose $\textbf{HAB}$ metrics ($\textbf{Helpfulness, Accuracy, Brevity}$) to evaluate chatbot performance. Legal domain experts validated Grahak-Nyay's effectiveness. Code and datasets will be released.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T10:26:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04854v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04854v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 Dialogue-Based Multi-Dimensional Relationship Extraction from Novels</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuchen Yan, Hanjie Zhao, Senbin Zhu, Hongde Liu, Zhihong Zhang, Yuxiang Jia
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Relation extraction is a crucial task in natural language processing, with broad applications in knowledge graph construction and literary analysis. However, the complex context and implicit expressions in novel texts pose significant challenges for automatic character relationship extraction. This study focuses on relation extraction in the novel domain and proposes a method based on Large Language Models (LLMs). By incorporating relationship dimension separation, dialogue data construction, and contextual learning strategies, the proposed method enhances extraction performance. Leveraging dialogue structure information, it improves the model's ability to understand implicit relationships and demonstrates strong adaptability in complex contexts. Additionally, we construct a high-quality Chinese novel relation extraction dataset to address the lack of labeled resources and support future research. Experimental results show that our method outperforms traditional baselines across multiple evaluation metrics and successfully facilitates the automated construction of character relationship networks in novels.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T10:20:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04852v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04852v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 Spec-TOD: A Specialized Instruction-Tuned LLM Framework for Efficient
  Task-Oriented Dialogue Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Quang-Vinh Nguyen, Quang-Chieu Nguyen, Hoang Pham, Khac-Hoai Nam Bui
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Task-oriented dialogue (TOD) systems facilitate goal-driven interactions between users and machines. While recent advances in deep learning have improved the performance, TOD systems often struggle in low-resource scenarios with limited labeled data. To address this challenge, we propose Spec-TOD, a novel framework designed to train an end-to-end TOD system with limited data. Spec-TOD introduces two main innovations: (i) a novel specialized end-to-end TOD framework that incorporates explicit task instructions for instruction-tuned large language models (LLMs), and (ii) an efficient training strategy that leverages lightweight, specialized LLMs to achieve strong performance with minimal supervision. Experiments on the MultiWOZ dataset, a widely used TOD benchmark, demonstrate that Spec-TOD achieves competitive results while significantly reducing the need for labeled data. These findings highlight the potential of the proposed framework in advancing efficient and effective TOD systems in low-resource settings.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T10:03:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04841v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04841v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 RewardAnything: Generalizable Principle-Following Reward Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhuohao Yu, Jiali Zeng, Weizheng Gu, Yidong Wang, Jindong Wang, Fandong Meng, Jie Zhou, Yue Zhang, Shikun Zhang, Wei Ye
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reward Models, essential for guiding Large Language Model optimization, are typically trained on fixed preference datasets, resulting in rigid alignment to single, implicit preference distributions. This prevents adaptation to diverse real-world needs-from conciseness in one task to detailed explanations in another. The standard practice of collecting task-specific preference data and retraining reward models is resource-intensive, often producing biased rewards, and limits practical application. We introduce generalizable, principle-following reward models. We propose that RMs should understand and adhere to dynamically provided natural language specifications of reward principles, similar to instruction-following in LLMs. To measure this capability, we develop RABench, a comprehensive benchmark for RMs focusing on generalization across diverse principles. Evaluations on RABench reveal poor generalization of current RMs. As a solution, we present RewardAnything, a novel RM designed and trained to explicitly follow natural language principles. We achieve SotA performance with RewardAnything in traditional RM benchmark simply by specifying a well-defined principle, and results on RABench show we excel in adapting to novel principles without retraining. Furthermore, RewardAnything integrates seamlessly with existing RLHF methods and we show by a case study on how to automatically and efficiently align LLMs with only natural language principles.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T09:53:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.03637v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.03637v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 Harnessing Pairwise Ranking Prompting Through Sample-Efficient Ranking
  Distillation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junru Wu, Le Yan, Zhen Qin, Honglei Zhuang, Paul Suganthan G. C., Tianqi Liu, Zhe Dong, Xuanhui Wang, Harrie Oosterhuis
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While Pairwise Ranking Prompting (PRP) with Large Language Models (LLMs) is one of the most effective zero-shot document ranking methods, it has a quadratic computational complexity with respect to the number of documents to be ranked, as it requires an enumeration over all possible document pairs. Consequently, the outstanding ranking performance of PRP has remained unreachable for most real-world ranking applications.   In this work, we propose to harness the effectiveness of PRP through pairwise distillation. Specifically, we distill a pointwise student ranker from pairwise teacher labels generated by PRP, resulting in an efficient student model that retains the performance of PRP with substantially lower computational costs. Furthermore, we find that the distillation process can be made sample-efficient: with only 2% of pairs, we are able to obtain the same performance as using all pairs for teacher labels. Thus, our novel approach provides a solution to harness the ranking performance of PRP without incurring high computational costs during both distillation and serving.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T09:38:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04820v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04820v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 Fast-VGAN: Lightweight Voice Conversion with Explicit Control of F0 and
  Duration Parameters</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mathilde Abrassart, Nicolas Obin, Axel Roebel
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Precise control over speech characteristics, such as pitch, duration, and speech rate, remains a significant challenge in the field of voice conversion. The ability to manipulate parameters like pitch and syllable rate is an important element for effective identity conversion, but can also be used independently for voice transformation, achieving goals that were historically addressed by vocoder-based methods.   In this work, we explore a convolutional neural network-based approach that aims to provide means for modifying fundamental frequency (F0), phoneme sequences, intensity, and speaker identity. Rather than relying on disentanglement techniques, our model is explicitly conditioned on these factors to generate mel spectrograms, which are then converted into waveforms using a universal neural vocoder. Accordingly, during inference, F0 contours, phoneme sequences, and speaker embeddings can be freely adjusted, allowing for intuitively controlled voice transformations.   We evaluate our approach on speaker conversion and expressive speech tasks using both perceptual and objective metrics. The results suggest that the proposed method offers substantial flexibility, while maintaining high intelligibility and speaker similarity.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T09:36:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SD</span><span>cs.AI</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04817v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04817v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 SwiftSeg: Efficient Training-Free Open-Vocabulary Segmentation via
  Hierarchical Attention Refinement Method</h2>
                <div class="authors">
                    <strong>Authors:</strong> Quang-Huy Che, Vinh-Tiep Nguyen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Open-vocabulary semantic segmentation (OVSS) aims to segment objects from arbitrary text categories without requiring densely annotated datasets. Although contrastive learning based models enable zero-shot segmentation, they often lose fine spatial precision at pixel level, due to global representation bias. In contrast, diffusion-based models naturally encode fine-grained spatial features via attention mechanisms that capture both global context and local details. However, they often face challenges in balancing the number of iterations with the quality of the segmentation. In this work, we propose FastSeg, a novel and efficient training-free framework with only (1+1)-step of reverse process of a pretrained diffusion model (e.g., Stable Diffusion). Moreover, instead of running multiple times for different classes, FastSeg performs segmentation for all classes at once. To further enhance the segmentation quality, FastSeg introduces three key components: (i) a dual-prompt mechanism for discriminative, class-aware attention extraction, (ii) a Hierarchical Attention Refinement Method (HARD) that enhances fused cross-attention using scale-aligned selfattention maps, and (iii) a Test-Time Flipping (TTF) scheme designed to improve spatial consistency. Extensive experiments show that FastSeg achieves state-of-the-art training-free performance, obtaining 43.8% average mIoU across PASCAL VOC, PASCAL Context, and COCO Object benchmarks while maintaining superior inference efficiency. Our results demonstrate that FastSeg provides a strong foundation for extendability, bridging the gap between segmentation quality and inference efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T09:26:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.23323v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.23323v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 Application and Evaluation of Large Language Models for Forecasting the
  Impact of Traffic Incidents</h2>
                <div class="authors">
                    <strong>Authors:</strong> George Jagadeesh, Srikrishna Iyer, Michal Polanowski, Kai Xin Thia
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This study examines the feasibility of applying large language models (LLMs) for forecasting the impact of traffic incidents on the traffic flow. The use of LLMs for this task has several advantages over existing machine learning-based solutions such as not requiring a large training dataset and the ability to utilize free-text incident logs. We propose a fully LLM-based solution that predicts the incident impact using a combination of traffic features and LLM-extracted incident features. A key ingredient of this solution is an effective method of selecting examples for the LLM's in-context learning. We evaluate the performance of three advanced LLMs and two state-of-the-art machine learning models on a real traffic incident dataset. The results show that the best-performing LLM matches the accuracy of the most accurate machine learning model, despite the former not having been trained on this prediction task. The findings indicate that LLMs are a practically viable option for traffic incident impact prediction.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T09:22:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04803v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04803v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 Training-Free Query Optimization via LLM-Based Plan Similarity</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nikita Vasilenko, Alexander Demin, Vladimir Boorlakov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language model (LLM) embeddings offer a promising new avenue for database query optimization. In this paper, we explore how pre-trained execution plan embeddings can guide SQL query execution without the need for additional model training. We introduce LLM-PM (LLM-based Plan Mapping), a framework that embeds the default execution plan of a query, finds its k nearest neighbors among previously executed plans, and recommends database hintsets based on neighborhood voting. A lightweight consistency check validates the selected hint, while a fallback mechanism searches the full hint space when needed. Evaluated on the JOB-CEB benchmark using OpenGauss, LLM-PM achieves an average speed-up of 21% query latency reduction. This work highlights the potential of LLM-powered embeddings to deliver practical improvements in query performance and opens new directions for training-free, embedding-based optimizer guidance systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T09:14:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.05853v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.05853v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 Qwen vs. Gemma Integration with Whisper: A Comparative Study in
  Multilingual SpeechLLM Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tuan Nguyen, Long-Vu Hoang, Huy-Dat Tran
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper presents our system for the MLC-SLM Challenge 2025, focusing on multilingual speech recognition and language modeling with large language models (LLMs). Our approach combines a fine-tuned Whisper-large-v3 encoder with efficient projector architectures and various decoder configurations. We employ a three-stage training methodology that progressively optimizes the encoder, projector, and LLM components. Our system achieves competitive performance with a private test average WER/CER result of 16.63% using the Gemma3-12B and 18.6% using the Qwen2.5-7B as decoder-only language model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T09:09:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.SD</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.13596v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.13596v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 Combined P-value Functions for Compatible Effect Estimation and
  Hypothesis Testing in Drug Regulation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Samuel Pawel, MaÅgorzata Roos, Leonhard Held
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The two-trials rule in drug regulation requires statistically significant results from two pivotal trials to demonstrate efficacy. However, it is unclear how the effect estimates from both trials should be combined to quantify the drug effect. Fixed-effect meta-analysis is commonly used but may yield confidence intervals that exclude the value of no effect even when the two-trials rule is not fulfilled. We systematically address this by recasting the two-trials rule and meta-analysis in a unified framework of combined p-value functions, where they are variants of Wilkinson's and Stouffer's combination methods, respectively. This allows us to obtain compatible combined p-values, effect estimates, and confidence intervals, which we derive in closed-form. Additionally, we provide new results for Edgington's, Fisher's, Pearson's, and Tippett's p-value combination methods. When both trials have the same true effect, all methods can consistently estimate it, although some show bias. When true effects differ, the two-trials rule and Pearson's method are conservative (converging to the less extreme effect), Fisher's and Tippett's methods are anti-conservative (converging to the more extreme effect), and Edgington's method and meta-analysis are balanced (converging to a weighted average). Notably, Edgington's confidence intervals asymptotically always include the individual trial effects, while meta-analytic confidence intervals shrink to a point at the weighted average effect. We conclude that all of these methods may be appropriate depending on the estimand of interest. We implement combined p-value function inference for two trials in the R package twotrials, allowing researchers to easily perform compatible hypothesis testing and effect estimation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T09:08:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.10246v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.10246v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 Retrodicting Chaotic Systems: An Algorithmic Information Theory Approach</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kamal Dingle, Boumediene Hamzi, Marcus Hutter, Houman Owhadi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Making accurate inferences about data is a key task in science and mathematics. Here we study the problem of \emph{retrodiction}, inferring past values of a series, in the context of chaotic dynamical systems. Specifically, we are interested in inferring the starting value $x_0$ in the series $x_0,x_1,x_2,\dots,x_n$ given the value of $x_n$, and the associated function $f$ which determines the series as $f(x_i)=x_{i+1}$. Even in the deterministic case this is a challenging problem, due to mixing and the typically exponentially many candidate past values in the pre-image of any given value $x_n$ (e.g., a current observation). We study this task from the perspective of algorithmic information theory, which motivates two approaches: One to search for the `simplest' value in the set of candidates, and one to look for the value in the lowest density region of the candidates. We test these methods numerically on the logistic map, Tent map, Bernoulli map, and Julia/Mandelbrot map, which are well-studied maps in chaos theory. The methods aid in retrodiction by assigning low ranks to candidates which are more likely to be the true starting value, which works well in some parameter and map cases, but the methods are not effective in all cases, and several open problems remain.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T08:57:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.DS</span><span>nlin.CD</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04780v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04780v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 FurniMAS: Language-Guided Furniture Decoration using Multi-Agent System</h2>
                <div class="authors">
                    <strong>Authors:</strong> Toan Nguyen, Tri Le, Quang Nguyen, Anh Nguyen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Furniture decoration is an important task in various industrial applications. However, achieving a high-quality decorative result is often time-consuming and requires specialized artistic expertise. To tackle these challenges, we explore how multi-agent systems can assist in automating the decoration process. We propose FurniMAS, a multi-agent system for automatic furniture decoration. Specifically, given a human prompt and a household furniture item such as a working desk or a TV stand, our system suggests relevant assets with appropriate styles and materials, and arranges them on the item, ensuring the decorative result meets functionality, aesthetic, and ambiance preferences. FurniMAS assembles a hybrid team of LLM-based and non-LLM agents, each fulfilling distinct roles in a typical decoration project. These agents collaborate through communication, logical reasoning, and validation to transform the requirements into the final outcome. Extensive experiments demonstrate that our FurniMAS significantly outperforms other baselines in generating high-quality 3D decor.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T08:45:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04770v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04770v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 ABench-Physics: Benchmarking Physical Reasoning in LLMs via
  High-Difficulty and Dynamic Physics Problems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yiming Zhang, Yingfan Ma, Yanmei Gu, Zhengkai Yang, Yihong Zhuang, Feng Wang, Zenan Huang, Yuanyuan Wang, Chao Huang, Bowen Song, Cheng Lin, Junbo Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have shown impressive performance in domains such as mathematics and programming, yet their capabilities in physics remain underexplored and poorly understood. Physics poses unique challenges that demand not only precise computation but also deep conceptual understanding and physical modeling skills. Existing benchmarks often fall short due to limited difficulty, multiple-choice formats, and static evaluation settings that fail to capture physical modeling ability. In this paper, we introduce ABench-Physics, a novel benchmark designed to rigorously evaluate LLMs' physical reasoning and generalization capabilities. ABench-Physics consists of two components: Phy_A, a static set of 400 graduate- or Olympiad-level problems; and Phy_B, a dynamic subset of 100 problems equipped with an automatic variation engine to test model robustness across changing conditions. All questions require precise numerical answers, with strict formatting and tolerance constraints. Our evaluation of several state-of-the-art LLMs reveals substantial performance gaps, highlighting persistent limitations in physical reasoning, especially in generalization to dynamic variants. ABench-Physics provides a challenging and diagnostic framework for advancing scientific reasoning in LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T08:43:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04766v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04766v1' target='_blank'>pdf</a>
                </div>
            </div>
            <h1>Keyword: LLM Deployment</h1>

            <div class="paper">
                <h2>#1 Spatio-Temporal LLM: Reasoning about Environments and Actions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haozhen Zheng, Beitong Tian, Mingyuan Wu, Zhenggang Tang, Klara Nahrstedt, Alex Schwing
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Despite the significant recent progress of Multimodal Large Language Models (MLLMs), MLLMs still struggle to correctly answer prompts that require a holistic spatio-temporal understanding. Specifically, it is challenging to address prompts that refer to 1) the entirety of an environment that an agent equipped with an MLLM can operate in; and simultaneously also refer to 2) recent actions that just happened and are encoded in a video clip. However, such a holistic spatio-temporal understanding is important for agents operating in the real world. To address this issue, we first develop a framework to collect a large-scale dataset. Using the collected "Reasoning about Environments and Actions" (REA) dataset, we show that recent methods indeed struggle to correctly answer the prompts. To improve, we develop a "spatio-temporal LLM" (ST-LLM), a model equipped with projectors to improve both spatial understanding of an environment and temporal understanding of recent observations. On the collected REA data, we show that the proposed method significantly improves results compared to prior work. Code and data are available at https://zoezheng126.github.io/STLLM-website/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:59:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05258v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05258v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuanzhe Hu, Yu Wang, Julian McAuley
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to the lack of benchmarks. We term agents with memory mechanisms as memory agents. In this paper, we identify four core competencies essential for memory agents: accurate retrieval, test-time learning, long-range understanding, and conflict resolution. Existing datasets either rely on limited context lengths or are tailored for static, long-context settings like book-based QA, which do not reflect the interactive, multi-turn nature of memory agents that incrementally accumulate information. Furthermore, no existing benchmarks cover all four competencies. Therefore, we introduce MemoryAgentBench, a new benchmark specifically designed for memory agents. Our benchmark combines reformulated existing datasets with newly constructed ones, covering the above four memory competencies, providing a systematic and challenging testbed for assessing memory quality. We evaluate a diverse set of memory agents, ranging from simple context-based and retrieval-augmented generation (RAG) systems to advanced agents with external memory modules and tool integration. Empirical results reveal that current methods fall short of mastering all four competencies, underscoring the need for further research into comprehensive memory mechanisms for LLM agents.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:59:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05257v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05257v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for
  Visual Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yana Wei, Liang Zhao, Jianjian Sun, Kangheng Lin, Jisheng Yin, Jingcheng Hu, Yinmin Zhang, En Yu, Haoran Lv, Zejia Weng, Jia Wang, Chunrui Han, Yuang Peng, Qi Han, Zheng Ge, Xiangyu Zhang, Daxin Jiang, Vishal M. Patel
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The remarkable reasoning capability of large language models (LLMs) stems from cognitive behaviors that emerge through reinforcement with verifiable rewards. This work investigates how to transfer this principle to Multimodal LLMs (MLLMs) to unlock advanced visual reasoning. We introduce a two-stage paradigm built on Qwen2.5-VL-7B: a massive linguistic cold-start fine-tuning, followed by multimodal reinforcement learning (RL) spanning nearly 1,000 steps, surpassing all previous open-source efforts in scale. This pioneering work reveals three fundamental insights: 1) Behavior transfer emerges surprisingly early in cold start due to linguistic mental imagery. 2) Cold start broadly memorizes visual behaviors, while RL critically discerns and scales up effective patterns. 3) Transfer strategically favors high-utility behaviors such as visual reflection. Our resulting model, Open-Vision-Reasoner (OVR), achieves state-of-the-art performance on a suite of reasoning benchmarks, including 95.3% on MATH500, 51.8% on MathVision and 54.6% on MathVerse. We release our model, data, and training dynamics to catalyze the development of more capable, behavior-aligned multimodal reasoners.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:59:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05255v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05255v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 Judging the Judges: Can Large Vision-Language Models Fairly Evaluate
  Chart Comprehension and Reasoning?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Md Tahmid Rahman Laskar, Mohammed Saidul Islam, Ridwan Mahbub, Ahmed Masry, Mizanur Rahman, Amran Bhuiyan, Mir Tafseer Nayeem, Shafiq Joty, Enamul Hoque, Jimmy Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Charts are ubiquitous as they help people understand and reason with data. Recently, various downstream tasks, such as chart question answering, chart2text, and fact-checking, have emerged. Large Vision-Language Models (LVLMs) show promise in tackling these tasks, but their evaluation is costly and time-consuming, limiting real-world deployment. While using LVLMs as judges to assess the chart comprehension capabilities of other LVLMs could streamline evaluation processes, challenges like proprietary datasets, restricted access to powerful models, and evaluation costs hinder their adoption in industrial settings. To this end, we present a comprehensive evaluation of 13 open-source LVLMs as judges for diverse chart comprehension and reasoning tasks. We design both pairwise and pointwise evaluation tasks covering criteria like factual correctness, informativeness, and relevancy. Additionally, we analyze LVLM judges based on format adherence, positional consistency, length bias, and instruction-following. We focus on cost-effective LVLMs (<10B parameters) suitable for both research and commercial use, following a standardized evaluation protocol and rubric to measure the LVLM judge's accuracy. Experimental results reveal notable variability: while some open LVLM judges achieve GPT-4-level evaluation performance (about 80% agreement with GPT-4 judgments), others struggle (below ~10% agreement). Our findings highlight that state-of-the-art open-source LVLMs can serve as cost-effective automatic evaluators for chart-related tasks, though biases such as positional preference and length bias persist.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:58:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.08468v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.08468v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 Response Attack: Exploiting Contextual Priming to Jailbreak Large
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ziqi Miao, Lijun Li, Yuan Xiong, Zhenhua Liu, Pengyu Zhu, Jing Shao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Contextual priming, where earlier stimuli covertly bias later judgments, offers an unexplored attack surface for large language models (LLMs). We uncover a contextual priming vulnerability in which the previous response in the dialogue can steer its subsequent behavior toward policy-violating content. Building on this insight, we propose Response Attack, which uses an auxiliary LLM to generate a mildly harmful response to a paraphrased version of the original malicious query. They are then formatted into the dialogue and followed by a succinct trigger prompt, thereby priming the target model to generate harmful content. Across eight open-source and proprietary LLMs, RA consistently outperforms seven state-of-the-art jailbreak techniques, achieving higher attack success rates. To mitigate this threat, we construct and release a context-aware safety fine-tuning dataset, which significantly reduces the attack success rate while preserving model capabilities. The code and data are available at https://github.com/Dtc7w3PQ/Response-Attack.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:56:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05248v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05248v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context
  Modeling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Meng Wei, Chenyang Wan, Xiqian Yu, Tai Wang, Yuqiang Yang, Xiaohan Mao, Chenming Zhu, Wenzhe Cai, Hanqing Wang, Yilun Chen, Xihui Liu, Jiangmiao Pang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vision-and-Language Navigation (VLN) in real-world settings requires agents to process continuous visual streams and generate actions with low latency grounded in language instructions. While Video-based Large Language Models (Video-LLMs) have driven recent progress, current VLN methods based on Video-LLM often face trade-offs among fine-grained visual understanding, long-term context modeling and computational efficiency. We introduce StreamVLN, a streaming VLN framework that employs a hybrid slow-fast context modeling strategy to support multi-modal reasoning over interleaved vision, language and action inputs. The fast-streaming dialogue context facilitates responsive action generation through a sliding-window of active dialogues, while the slow-updating memory context compresses historical visual states using a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN achieves coherent multi-turn dialogue through efficient KV cache reuse, supporting long video streams with bounded context size and inference cost. Experiments on VLN-CE benchmarks demonstrate state-of-the-art performance with stable low latency, ensuring robustness and efficiency in real-world deployment. The project page is: \href{https://streamvln.github.io/}{https://streamvln.github.io/}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:49:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05240v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05240v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 OPeRA: A Dataset of Observation, Persona, Rationale, and Action for
  Evaluating LLMs on Human Online Shopping Behavior Simulation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ziyi Wang, Yuxuan Lu, Wenbo Li, Amirali Amini, Bo Sun, Yakov Bart, Weimin Lyu, Jiri Gesi, Tian Wang, Jing Huang, Yu Su, Upol Ehsan, Malihe Alikhani, Toby Jia-Jun Li, Lydia Chilton, Dakuo Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Can large language models (LLMs) accurately simulate the next web action of a specific user? While LLMs have shown promising capabilities in generating ``believable'' human behaviors, evaluating their ability to mimic real user behaviors remains an open challenge, largely due to the lack of high-quality, publicly available datasets that capture both the observable actions and the internal reasoning of an actual human user. To address this gap, we introduce OPERA, a novel dataset of Observation, Persona, Rationale, and Action collected from real human participants during online shopping sessions. OPERA is the first public dataset that comprehensively captures: user personas, browser observations, fine-grained web actions, and self-reported just-in-time rationales. We developed both an online questionnaire and a custom browser plugin to gather this dataset with high fidelity. Using OPERA, we establish the first benchmark to evaluate how well current LLMs can predict a specific user's next action and rationale with a given persona and <observation, action, rationale> history. This dataset lays the groundwork for future research into LLM agents that aim to act as personalized digital twins for human.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:44:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.05606v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.05606v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 The Super Weight in Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mengxia Yu, De Wang, Qi Shan, Colorado J Reed, Alvin Wan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent works have shown a surprising result: a small fraction of Large Language Model (LLM) parameter outliers are disproportionately important to the quality of the model. LLMs contain billions of parameters, so these small fractions, such as 0.01%, translate to hundreds of thousands of parameters. In this work, we present an even more surprising finding: Pruning as few as a single parameter can destroy an LLM's ability to generate text -- increasing perplexity by 3 orders of magnitude and reducing zero-shot accuracy to guessing. We propose a data-free method for identifying such parameters, termed super weights, using a single forward pass through the model. We additionally find that these super weights induce correspondingly rare and large activation outliers, termed super activations. When preserved with high precision, super activations can improve simple round-to-nearest quantization to become competitive with state-of-the-art methods. For weight quantization, we similarly find that by preserving the super weight and clipping other weight outliers, round-to-nearest quantization can scale to much larger block sizes than previously considered. To facilitate further research into super weights, we provide an index of super weight coordinates for common, openly available LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:42:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.07191v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.07191v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 Extended Inductive Reasoning for Personalized Preference Inference from
  Behavioral Signals</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jia-Nan Li, Jian Guan, Wei Wu, Rui Yan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have demonstrated significant success in complex reasoning tasks such as math and coding. In contrast to these tasks where deductive reasoning predominates, inductive reasoning-the ability to derive general rules from incomplete evidence, remains underexplored. This paper investigates extended inductive reasoning in LLMs through the lens of personalized preference inference, a critical challenge in LLM alignment where current approaches struggle to capture diverse user preferences. The task demands strong inductive reasoning capabilities as user preferences are typically embedded implicitly across various interaction forms, requiring models to synthesize consistent preference patterns from scattered signals. We propose AlignXplore, a model that leverages extended reasoning chains to enable systematic preference inference from behavioral signals in users' interaction histories. Such explicit preference articulation enables efficient streaming inference: when new behavioral signals emerge, the model can directly build upon previously inferred preference descriptions rather than reprocessing historical signals from scratch, while also supporting iterative refinement to the inferred preferences. We develop AlignXplore by combining cold-start training based on synthetic data with subsequent online reinforcement learning. Through extensive experiments, we demonstrate that AlignXplore achieves substantial improvements over the backbone model by an average of 15.49\% on in-domain and out-of-domain benchmarks, while maintaining strong generalization ability across different input formats and downstream models. Further analyses establish best practices for preference inference learning through systematic comparison of reward modeling strategies, while revealing the emergence of human-like inductive reasoning patterns during training.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:38:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.18071v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.18071v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 Cascade: Token-Sharded Private LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rahul Thomas, Louai Zahran, Erica Choi, Akilesh Potti, Micah Goldblum, Arka Pal
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As LLMs continue to increase in parameter size, the computational resources required to run them are available to fewer parties. Therefore, third-party inference services -- where LLMs are hosted by third parties with significant computational resources -- are becoming increasingly popular. However, third party inference raises critical concerns about user data privacy. To mitigate these risks, privacy researchers have developed provably secure schemes for third-party inference, such as Secure Multi-Party Computation (SMPC). However, SMPC protocols have significant computational and communication overhead, and do not scale to large models. In this work, we propose a new multi-party inference protocol, Cascade, that avoids these punitive costs by leveraging sharding in the sequence dimension to maintain privacy, trading off cryptographic privacy guarantees for increased performance and scalability. We demonstrate that Cascade is resistant to a generalization of a recent attack that is highly effective against other statistical privacy schemes, and that it is further resistant to learning-based attacks. As Cascade is orders of magnitude faster than existing schemes, our findings offer practical solutions for secure deployment of modern state-of-the-art LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:37:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05228v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05228v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yanming Wan, Jiaxing Wu, Marwa Abdulhai, Lior Shani, Natasha Jaques
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Effective conversational agents like large language models (LLMs) must personalize their interactions to adapt to user preferences, personalities, and attributes across diverse domains like education and healthcare. Current methods like Reinforcement Learning from Human Feedback (RLHF), often prioritize helpfulness and safety but fall short in fostering truly empathetic, adaptive, and personalized dialogues. Existing personalization approaches typically rely on extensive user history, limiting their effectiveness for new or context-limited users. To address these limitations, we propose leveraging a user model to incorporate a curiosity-based intrinsic reward into multi-turn RLHF. This novel reward mechanism encourages the LLM agent to actively infer user traits by optimizing conversations to improve its user model's accuracy. Consequently, the agent delivers more personalized interactions by learning more about the user. We demonstrate our method's effectiveness in two distinct domains: significantly improving personalization performance in a conversational recommendation task, and personalizing conversations for different learning styles in an educational setting. We show improved generalization capabilities compared to traditional multi-turn RLHF, all while maintaining conversation quality. Our method offers a promising solution for creating more personalized, adaptive, and engaging conversational agents.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:32:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.03206v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.03206v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 Bridging Prediction and Intervention Problems in Social Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lydia T. Liu, Inioluwa Deborah Raji, Angela Zhou, Luke Guerdan, Jessica Hullman, Daniel Malinsky, Bryan Wilder, Simone Zhang, Hammaad Adam, Amanda Coston, Ben Laufer, Ezinne Nwankwo, Michael Zanger-Tishler, Eli Ben-Michael, Solon Barocas, Avi Feller, Marissa Gerchick, Talia Gillis, Shion Guha, Daniel Ho, Lily Hu, Kosuke Imai, Sayash Kapoor, Joshua Loftus, Razieh Nabi, Arvind Narayanan, Ben Recht, Juan Carlos Perdomo, Matthew Salganik, Mark Sendak, Alexander Tolbert, Berk Ustun, Suresh Venkatasubramanian, Angelina Wang, Ashia Wilson
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Many automated decision systems (ADS) are designed to solve prediction problems -- where the goal is to learn patterns from a sample of the population and apply them to individuals from the same population. In reality, these prediction systems operationalize holistic policy interventions in deployment. Once deployed, ADS can shape impacted population outcomes through an effective policy change in how decision-makers operate, while also being defined by past and present interactions between stakeholders and the limitations of existing organizational, as well as societal, infrastructure and context. In this work, we consider the ways in which we must shift from a prediction-focused paradigm to an interventionist paradigm when considering the impact of ADS within social systems. We argue this requires a new default problem setup for ADS beyond prediction, to instead consider predictions as decision support, final decisions, and outcomes. We highlight how this perspective unifies modern statistical frameworks and other tools to study the design, implementation, and evaluation of ADS systems, and point to the research directions necessary to operationalize this paradigm shift. Using these tools, we characterize the limitations of focusing on isolated prediction tasks, and lay the foundation for a more intervention-oriented approach to developing and deploying ADS.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:29:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CY</span><span>stat.AP</span><span>stat.ML</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05216v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05216v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 Reviewing Scientific Papers for Critical Problems With Reasoning LLMs:
  Baseline Approaches and Automatic Evaluation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tianmai M. Zhang, Neil F. Abernethy
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in large language models have sparked interest in utilizing them to aid the peer review process of scientific publication amid the peer review crisis. However, having AI models generate full reviews in the same way as human reviewers risks exacerbating the irresponsible use of LLM-generated reviews. As an alternative, we propose adopting LLMs as manuscript quality checkers. We introduce several baseline approaches and an extendable automatic evaluation framework using top reasoning LLMs as judges to tackle the difficulty of recruiting domain experts for manual evaluation. Utilizing papers withdrawn from arXiv, we validated our proposed methods with several leading reasoning LLMs from multiple vendors and assessed their performance and API costs for identifying critical errors and unsoundness problems in scientific papers. o3 exhibited the best problem identification performance among all models at a modest cost. This paper provides insights into document-based scientific understanding/reasoning and lays a foundation for future applications. Our dataset, code, and model outputs are publicly available.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:28:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.23824v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.23824v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 All in One: Visual-Description-Guided Unified Point Cloud Segmentation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zongyan Han, Mohamed El Amine Boudjoghra, Jiahua Dong, Jinhong Wang, Rao Muhammad Anwer
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Unified segmentation of 3D point clouds is crucial for scene understanding, but is hindered by its sparse structure, limited annotations, and the challenge of distinguishing fine-grained object classes in complex environments. Existing methods often struggle to capture rich semantic and contextual information due to limited supervision and a lack of diverse multimodal cues, leading to suboptimal differentiation of classes and instances. To address these challenges, we propose VDG-Uni3DSeg, a novel framework that integrates pre-trained vision-language models (e.g., CLIP) and large language models (LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual descriptions and reference images from the internet, our method incorporates rich multimodal cues, facilitating fine-grained class and instance separation. We further design a Semantic-Visual Contrastive Loss to align point features with multimodal queries and a Spatial Enhanced Module to model scene-wide relationships efficiently. Operating within a closed-set paradigm that utilizes multimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art results in semantic, instance, and panoptic segmentation, offering a scalable and practical solution for 3D understanding. Our code is available at https://github.com/Hanzy1996/VDG-Uni3DSeg.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:22:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05211v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05211v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 MedGemma Technical Report</h2>
                <div class="authors">
                    <strong>Authors:</strong> Andrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri, Atilla Kiraly, Madeleine Traverse, Timo Kohlberger, Shawn Xu, Fayaz Jamil, CÃ­an Hughes, Charles Lau, Justin Chen, Fereshteh Mahvar, Liron Yatziv, Tiffany Chen, Bram Sterling, Stefanie Anna Baby, Susanna Maria Baby, Jeremy Lai, Samuel Schmidgall, Lu Yang, Kejia Chen, Per Bjornsson, Shashir Reddy, Ryan Brush, Kenneth Philbrick, Howard Hu, Howard Yang, Richa Tiwari, Sunny Jansen, Preeti Singh, Yun Liu, Shekoofeh Azizi, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre RamÃ©, Morgane Riviere, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Elena Buchatskaya, Jean-Baptiste Alayrac, Dmitry, Lepikhin, Vlad Feinberg, Sebastian Borgeaud, Alek Andreev, Cassidy Hardin, Robert Dadashi, LÃ©onard Hussenot, Armand Joulin, Olivier Bachem, Yossi Matias, Katherine Chou, Avinatan Hassidim, Kavi Goel, Clement Farabet, Joelle Barral, Tris Warkentin, Jonathon Shlens, David Fleet, Victor Cotruta, Omar Sanseviero, Gus Martins, Phoebe Kirk, Anand Rao, Shravya Shetty, David F. Steiner, Can Kirmizibayrak, Rory Pilgrim, Daniel Golden, Lin Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Artificial intelligence (AI) has significant potential in healthcare applications, but its training and deployment faces challenges due to healthcare's diverse data, complex tasks, and the need to preserve privacy. Foundation models that perform well on medical tasks and require less task-specific tuning data are critical to accelerate the development of healthcare AI applications. We introduce MedGemma, a collection of medical vision-language foundation models based on Gemma 3 4B and 27B. MedGemma demonstrates advanced medical understanding and reasoning on images and text, significantly exceeding the performance of similar-sized generative models and approaching the performance of task-specific models, while maintaining the general capabilities of the Gemma 3 base models. For out-of-distribution tasks, MedGemma achieves 2.6-10% improvement on medical multimodal question answering, 15.5-18.1% improvement on chest X-ray finding classification, and 10.8% improvement on agentic evaluations compared to the base models. Fine-tuning MedGemma further improves performance in subdomains, reducing errors in electronic health record information retrieval by 50% and reaching comparable performance to existing specialized state-of-the-art methods for pneumothorax classification and histopathology patch classification. We additionally introduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP. MedSigLIP powers the visual understanding capabilities of MedGemma and as an encoder achieves comparable or better performance than specialized medical image encoders. Taken together, the MedGemma collection provides a strong foundation of medical image and text capabilities, with potential to significantly accelerate medical research and development of downstream applications. The MedGemma collection, including tutorials and model weights, can be found at https://goo.gle/medgemma.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:01:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05201v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05201v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 In-Context Learning as an Effective Estimator of Functional Correctness
  of LLM-Generated Code</h2>
                <div class="authors">
                    <strong>Authors:</strong> Susmita Das, Madhusudan Ghosh, Priyanka Swami, Debasis Ganguly, Gul Calikli
                </div>
                <div class="summary">
                    <strong>Summary:</strong> When applying LLM-based code generation to software development projects that follow a feature-driven or rapid application development approach, it becomes necessary to estimate the functional correctness of the generated code in the absence of test cases. Just as a user selects a relevant document from a ranked list of retrieved ones, a software generation workflow requires a developer to choose (and potentially refine) a generated solution from a ranked list of alternative solutions, ordered by their posterior likelihoods. This implies that estimating the quality of a ranked list -- akin to estimating "relevance" for query performance prediction (QPP) in IR -- is also crucial for generative software development, where quality is defined in terms of "functional correctness". In this paper, we propose an in-context learning (ICL) based approach for code quality estimation. Our findings demonstrate that providing few-shot examples of functionally correct code from a training set enhances the performance of existing QPP approaches as well as a zero-shot-based approach for code quality estimation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T17:01:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3726302.3730212' target='_blank'>doi</a><a href='http://arxiv.org/abs/2507.05200v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05200v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 Train-before-Test Harmonizes Language Model Rankings</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guanhua Zhang, Ricardo Dominguez-Olmedo, Moritz Hardt
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Existing language model benchmarks provide contradictory model rankings, even for benchmarks that aim to capture similar skills. This dilemma of conflicting rankings hampers model selection, clouds model comparisons, and adds confusion to a growing ecosystem of competing models. Recent work attributed ranking disagreement to the phenomenon of training on the test task: As released, different models exhibit a different level of preparation for any given test task. A candidate solution to the problem is train-before-test: Give each model the same benchmark-specific finetuning before evaluation. Our primary contribution is a broad empirical evaluation of train-before-test across 24 benchmarks and 61 models. We show that train-before-test significantly improves ranking agreement consistently across all benchmarks. Whereas rankings have little external validity to start with, they enjoy a significant degree of external validity when applying train-before-test: Model rankings transfer gracefully from one benchmark to the other. Even within the same model family, train-before-test reduces strong ranking disagreement to near-perfect agreement. In addition, train-before-test reduces the model-score matrix to essentially rank one, revealing new insights into the latent factors of benchmark performance. Our work supports the recommendation to make train-before-test a default component of LLM benchmarking.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T16:54:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05195v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05195v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 NativQA Framework: Enabling LLMs with Native, Local, and Everyday
  Knowledge</h2>
                <div class="authors">
                    <strong>Authors:</strong> Firoj Alam, Md Arid Hasan, Sahinur Rahman Laskar, Mucahid Kutlu, Kareem Darwish, Shammur Absar Chowdhury
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid advancement of large language models (LLMs) has raised concerns about cultural bias, fairness, and their applicability in diverse linguistic and underrepresented regional contexts. To enhance and benchmark the capabilities of LLMs, there is a need to develop large-scale resources focused on multilingual, local, and cultural contexts. In this study, we propose the NativQA framework, which can seamlessly construct large-scale, culturally and regionally aligned QA datasets in native languages. The framework utilizes user-defined seed queries and leverages search engines to collect location-specific, everyday information. It has been evaluated across 39 locations in 24 countries and in 7 languages -- ranging from extremely low-resource to high-resource languages -- resulting in over 300K Question-Answer (QA) pairs. The developed resources can be used for LLM benchmarking and further fine-tuning. The framework has been made publicly available for the community (https://gitlab.com/nativqa/nativqa-framework).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T16:43:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>68T50</span><span>F.2.2; I.2.7</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.05995v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.05995v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 From Fragments to Facts: A Curriculum-Driven DPO Approach for Generating
  Hindi News Veracity Explanations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pulkit Bansal, Raghvendra Kumar, Shakti Singh, Sriparna Saha, Adam Jatowt
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In an era of rampant misinformation, generating reliable news explanations is vital, especially for under-represented languages like Hindi. Lacking robust automated tools, Hindi faces challenges in scaling misinformation detection. To bridge this gap, we propose a novel framework integrating Direct Preference Optimization (DPO) with curriculum learning to align machine-generated explanations with human reasoning. Fact-checked explanations from credible sources serve as preferred responses, while LLM outputs highlight system limitations and serve as non-preferred responses. To refine task-specific alignment, we introduce two key parameters -- Actuality and Finesse -- into the DPO loss function, enhancing explanation quality and consistency. Experiments with LLMs (Mistral, Llama, Gemma) and PLMs (mBART, mT5) confirm the framework's effectiveness in generating coherent, contextually relevant explanations. This scalable approach combats misinformation and extends automated explanation generation to low-resource languages.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T16:34:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05179v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05179v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jonathan Hyun, Nicholas R Waytowich, Boyuan Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Despite rapid progress in large language model (LLM)-based multi-agent systems, current benchmarks fall short in evaluating their scalability, robustness, and coordination capabilities in complex, dynamic, real-world tasks. Existing environments typically focus on small-scale, fully observable, or low-complexity domains, limiting their utility for developing and assessing next-generation multi-agent Agentic AI frameworks. We introduce CREW-Wildfire, an open-source benchmark designed to close this gap. Built atop the human-AI teaming CREW simulation platform, CREW-Wildfire offers procedurally generated wildfire response scenarios featuring large maps, heterogeneous agents, partial observability, stochastic dynamics, and long-horizon planning objectives. The environment supports both low-level control and high-level natural language interactions through modular Perception and Execution modules. We implement and evaluate several state-of-the-art LLM-based multi-agent Agentic AI frameworks, uncovering significant performance gaps that highlight the unsolved challenges in large-scale coordination, communication, spatial reasoning, and long-horizon planning under uncertainty. By providing more realistic complexity, scalable architecture, and behavioral evaluation metrics, CREW-Wildfire establishes a critical foundation for advancing research in scalable multi-agent Agentic intelligence. All code, environments, data, and baselines will be released to support future research in this emerging domain.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T16:33:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MA</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05178v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05178v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 Transfer Attack for Bad and Good: Explain and Boost Adversarial
  Transferability across Multimodal Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hao Cheng, Erjia Xiao, Jiayan Yang, Jinhao Duan, Yichi Wang, Jiahang Cao, Qiang Zhang, Le Yang, Kaidi Xu, Jindong Gu, Renjing Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multimodal Large Language Models (MLLMs) demonstrate exceptional performance in cross-modality interaction, yet they also suffer adversarial vulnerabilities. In particular, the transferability of adversarial examples remains an ongoing challenge. In this paper, we specifically analyze the manifestation of adversarial transferability among MLLMs and identify the key factors that influence this characteristic. We discover that the transferability of MLLMs exists in cross-LLM scenarios with the same vision encoder and indicate \underline{\textit{two key Factors}} that may influence transferability. We provide two semantic-level data augmentation methods, Adding Image Patch (AIP) and Typography Augment Transferability Method (TATM), which boost the transferability of adversarial examples across MLLMs. To explore the potential impact in the real world, we utilize two tasks that can have both negative and positive societal impacts: \ding{182} Harmful Content Insertion and \ding{183} Information Protection.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T16:28:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2405.20090v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.20090v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 NOVA: Navigation via Object-Centric Visual Autonomy for High-Speed
  Target Tracking in Unstructured GPS-Denied Environments</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alessandro Saviolo, Giuseppe Loianno
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Autonomous aerial target tracking in unstructured and GPS-denied environments remains a fundamental challenge in robotics. Many existing methods rely on motion capture systems, pre-mapped scenes, or feature-based localization to ensure safety and control, limiting their deployment in real-world conditions. We introduce NOVA, a fully onboard, object-centric framework that enables robust target tracking and collision-aware navigation using only a stereo camera and an IMU. Rather than constructing a global map or relying on absolute localization, NOVA formulates perception, estimation, and control entirely in the target's reference frame. A tightly integrated stack combines a lightweight object detector with stereo depth completion, followed by histogram-based filtering to infer robust target distances under occlusion and noise. These measurements feed a visual-inertial state estimator that recovers the full 6-DoF pose of the robot relative to the target. A nonlinear model predictive controller (NMPC) plans dynamically feasible trajectories in the target frame. To ensure safety, high-order control barrier functions are constructed online from a compact set of high-risk collision points extracted from depth, enabling real-time obstacle avoidance without maps or dense representations. We validate NOVA across challenging real-world scenarios, including urban mazes, forest trails, and repeated transitions through buildings with intermittent GPS loss and severe lighting changes that disrupt feature-based localization. Each experiment is repeated multiple times under similar conditions to assess resilience, showing consistent and reliable performance. NOVA achieves agile target following at speeds exceeding 50 km/h. These results show that high-speed vision-based tracking is possible in the wild using only onboard sensing, with no reliance on external localization or environment assumptions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T16:28:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.18689v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.18689v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 LAID: Lightweight AI-Generated Image Detection in Spatial and Spectral
  Domains</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nicholas Chivaran, Jianbing Ni
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The recent proliferation of photorealistic AI-generated images (AIGI) has raised urgent concerns about their potential misuse, particularly on social media platforms. Current state-of-the-art AIGI detection methods typically rely on large, deep neural architectures, creating significant computational barriers to real-time, large-scale deployment on platforms like social media. To challenge this reliance on computationally intensive models, we introduce LAID, the first framework -- to our knowledge -- that benchmarks and evaluates the detection performance and efficiency of off-the-shelf lightweight neural networks. In this framework, we comprehensively train and evaluate selected models on a representative subset of the GenImage dataset across spatial, spectral, and fusion image domains. Our results demonstrate that lightweight models can achieve competitive accuracy, even under adversarial conditions, while incurring substantially lower memory and computation costs compared to current state-of-the-art methods. This study offers valuable insight into the trade-off between efficiency and performance in AIGI detection and lays a foundation for the development of practical, scalable, and trustworthy detection systems. The source code of LAID can be found at: https://github.com/nchivar/LAID.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T16:18:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05162v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05162v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 AI Generated Text Detection Using Instruction Fine-tuned Large Language
  and Transformer-Based Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chinnappa Guggilla, Budhaditya Roy, Trupti Ramdas Chavan, Abdul Rahman, Edward Bowen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) possess an extraordinary capability to produce text that is not only coherent and contextually relevant but also strikingly similar to human writing. They adapt to various styles and genres, producing content that is both grammatically correct and semantically meaningful. Recently, LLMs have been misused to create highly realistic phishing emails, spread fake news, generate code to automate cyber crime, and write fraudulent scientific articles. Additionally, in many real-world applications, the generated content including style and topic and the generator model are not known beforehand. The increasing prevalence and sophistication of artificial intelligence (AI)-generated texts have made their detection progressively more challenging. Various attempts have been made to distinguish machine-generated text from human-authored content using linguistic, statistical, machine learning, and ensemble-based approaches. This work focuses on two primary objectives Task-A, which involves distinguishing human-written text from machine-generated text, and Task-B, which attempts to identify the specific LLM model responsible for the generation. Both of these tasks are based on fine tuning of Generative Pre-trained Transformer (GPT_4o-mini), Large Language Model Meta AI (LLaMA) 3 8B, and Bidirectional Encoder Representations from Transformers (BERT). The fine-tuned version of GPT_4o-mini and the BERT model has achieved accuracies of 0.9547 for Task-A and 0.4698 for Task-B.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T16:13:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05157v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05157v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 Interpretable Mnemonic Generation for Kanji Learning via
  Expectation-Maximization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jaewook Lee, Alexander Scarlatos, Andrew Lan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Learning Japanese vocabulary is a challenge for learners from Roman alphabet backgrounds due to script differences. Japanese combines syllabaries like hiragana with kanji, which are logographic characters of Chinese origin. Kanji are also complicated due to their complexity and volume. Keyword mnemonics are a common strategy to aid memorization, often using the compositional structure of kanji to form vivid associations. Despite recent efforts to use large language models (LLMs) to assist learners, existing methods for LLM-based keyword mnemonic generation function as a black box, offering limited interpretability. We propose a generative framework that explicitly models the mnemonic construction process as driven by a set of common rules, and learn them using a novel Expectation-Maximization-type algorithm. Trained on learner-authored mnemonics from an online platform, our method learns latent structures and compositional rules, enabling interpretable and systematic mnemonics generation. Experiments show that our method performs well in the cold-start setting for new learners while providing insight into the mechanisms behind effective mnemonic creation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T15:49:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05137v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05137v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 SMART: Simulated Students Aligned with Item Response Theory for Question
  Difficulty Prediction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alexander Scarlatos, Nigel Fernandez, Christopher Ormerod, Susan Lottridge, Andrew Lan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Item (question) difficulties play a crucial role in educational assessments, enabling accurate and efficient assessment of student abilities and personalization to maximize learning outcomes. Traditionally, estimating item difficulties can be costly, requiring real students to respond to items, followed by fitting an item response theory (IRT) model to get item difficulty estimates. This approach cannot be applied to the cold-start setting for previously unseen items either. In this work, we present SMART (Simulated Students Aligned with IRT), a novel method for aligning simulated students with instructed ability, which can then be used in simulations to predict the difficulty of open-ended items. We achieve this alignment using direct preference optimization (DPO), where we form preference pairs based on how likely responses are under a ground-truth IRT model. We perform a simulation by generating thousands of responses, evaluating them with an LLM-based scoring model, and fit the resulting data to an IRT model to obtain item difficulty estimates. Through extensive experiments on a real-world student response dataset, we show that SMART outperforms other item difficulty prediction methods by leveraging its improved ability alignment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T15:41:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.CY</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05129v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05129v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 Incremental Firmware Update Over-the-Air for Low-Power IoT Devices over
  LoRaWAN</h2>
                <div class="authors">
                    <strong>Authors:</strong> Andrea De Simone, Giovanna Turvani, Fabrizio Riente
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Efficiently supporting remote firmware updates in Internet of Things (IoT) devices remains a significant challenge due to the limitations of many IoT communication protocols, which often make it impractical to transmit full firmware images. Techniques such as firmware partitioning have been introduced to mitigate this issue, but they frequently fall short, especially in battery-powered systems where time and energy constraints are critical. As a result, physical maintenance interventions are still commonly required, which is costly and inconvenient in large-scale deployments. In this work, we present a lightweight and innovative method that addresses this challenge by generating highly compact delta patches, enabling firmware reconstruction directly on the device. Our algorithm is specifically optimized for low-power devices, minimizing both memory usage and computational overhead. Compared to existing solutions, our approach significantly reduces the data volume needed for updates while maintaining performance comparable to more complex alternatives. Experimental evaluations confirm that our method yields substantial time and energy savings, making it particularly well-suited for battery-powered IoT nodes. Although our implementation targets the LoRaWAN protocol, the approach is flexible and can be adapted to other IoT communication technologies.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T15:37:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SY</span><span>cs.NI</span><span>cs.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.13764v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.13764v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 An Evaluation of Large Language Models on Text Summarization Tasks Using
  Prompt Engineering Techniques</h2>
                <div class="authors">
                    <strong>Authors:</strong> Walid Mohamed Aly, Taysir Hassan A. Soliman, Amr Mohamed AbdelAziz
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) continue to advance natural language processing with their ability to generate human-like text across a range of tasks. Despite the remarkable success of LLMs in Natural Language Processing (NLP), their performance in text summarization across various domains and datasets has not been comprehensively evaluated. At the same time, the ability to summarize text effectively without relying on extensive training data has become a crucial bottleneck. To address these issues, we present a systematic evaluation of six LLMs across four datasets: CNN/Daily Mail and NewsRoom (news), SAMSum (dialog), and ArXiv (scientific). By leveraging prompt engineering techniques including zero-shot and in-context learning, our study evaluates the performance using the ROUGE and BERTScore metrics. In addition, a detailed analysis of inference times is conducted to better understand the trade-off between summarization quality and computational efficiency. For Long documents, introduce a sentence-based chunking strategy that enables LLMs with shorter context windows to summarize extended inputs in multiple stages. The findings reveal that while LLMs perform competitively on news and dialog tasks, their performance on long scientific documents improves significantly when aided by chunking strategies. In addition, notable performance variations were observed based on model parameters, dataset properties, and prompt design. These results offer actionable insights into how different LLMs behave across task types, contributing to ongoing research in efficient, instruction-based NLP systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T15:34:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05123v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05123v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 VerifyLLM: LLM-Based Pre-Execution Task Plan Verification for Robots</h2>
                <div class="authors">
                    <strong>Authors:</strong> Danil S. Grigorev, Alexey K. Kovalev, Aleksandr I. Panov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the field of robotics, researchers face a critical challenge in ensuring reliable and efficient task planning. Verifying high-level task plans before execution significantly reduces errors and enhance the overall performance of these systems. In this paper, we propose an architecture for automatically verifying high-level task plans before their execution in simulator or real-world environments. Leveraging Large Language Models (LLMs), our approach consists of two key steps: first, the conversion of natural language instructions into Linear Temporal Logic (LTL), followed by a comprehensive analysis of action sequences. The module uses the reasoning capabilities of the LLM to evaluate logical coherence and identify potential gaps in the plan. Rigorous testing on datasets of varying complexity demonstrates the broad applicability of the module to household tasks. We contribute to improving the reliability and efficiency of task planning and addresses the critical need for robust pre-execution verification in autonomous systems. The code is available at https://verifyllm.github.io.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T15:31:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05118v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05118v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 Rule Learning for Knowledge Graph Reasoning under Agnostic Distribution
  Shift</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shixuan Liu, Yue He, Yunfei Wang, Hao Zou, Haoxiang Cheng, Wenjing Yang, Peng Cui, Zhong Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Knowledge graph (KG) reasoning remains a critical research area focused on inferring missing knowledge by analyzing relationships among observed facts. Despite its success, a key limitation of existing KG reasoning methods is their dependence on the I.I.D assumption. This assumption can easily be violated due to unknown sample selection bias during training or agnostic distribution shifts during testing, significantly compromising model performance and reliability. To facilitate the deployment of KG reasoning in wild environments, this study investigates learning logical rules from KGs affected by unknown selection bias. Additionally, we address test sets with agnostic distribution shifts, formally defining this challenge as out-of-distribution (OOD) KG reasoning-a previously underexplored problem. To solve the issue, we propose the Stable Rule Learning (StableRule) framework, an end-to-end methodology that integrates feature decorrelation with rule learning network, to enhance OOD generalization performance. By leveraging feature decorrelation, the StableRule framework mitigates the adverse effects of covariate shifts arising in OOD scenarios, thereby improving the robustness of the rule learning component in effectively deriving logical rules. Extensive experiments on seven benchmark KGs demonstrate the framework's superior effectiveness and stability across diverse heterogeneous environments, underscoring its practical significance for real-world applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-08T03:40:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05110v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05110v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 The Hidden Threat in Plain Text: Attacking RAG Data Loaders</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alberto Castagnaro, Umberto Salviati, Mauro Conti, Luca Pajola, Simeone Pizzi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have transformed human-machine interaction since ChatGPT's 2022 debut, with Retrieval-Augmented Generation (RAG) emerging as a key framework that enhances LLM outputs by integrating external knowledge. However, RAG's reliance on ingesting external documents introduces new vulnerabilities. This paper exposes a critical security gap at the data loading stage, where malicious actors can stealthily corrupt RAG pipelines by exploiting document ingestion.   We propose a taxonomy of 9 knowledge-based poisoning attacks and introduce two novel threat vectors -- Content Obfuscation and Content Injection -- targeting common formats (DOCX, HTML, PDF). Using an automated toolkit implementing 19 stealthy injection techniques, we test five popular data loaders, finding a 74.4% attack success rate across 357 scenarios. We further validate these threats on six end-to-end RAG systems -- including white-box pipelines and black-box services like NotebookLM and OpenAI Assistants -- demonstrating high success rates and critical vulnerabilities that bypass filters and silently compromise output integrity. Our results emphasize the urgent need to secure the document ingestion process in RAG systems against covert content manipulations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T15:13:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05093v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05093v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 ViPSN 2.0: A Reconfigurable Battery-free IoT Platform for Vibration
  Energy Harvesting</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xin Li, Mianxin Xiao, Xi Shen, Jiaqing Chu, Weifeng Huang, Jiashun Li, Yaoyi Li, Mingjing Cai, Jiaming Chen, Xinming Zhang, Daxing Zhang, Congsi Wang, Hong Tang, Bao Zhao, Qitao Lu, Yilong Wang, Jianjun Wang, Minyi Xu, Shitong Fang, Xuanyu Huang. Chaoyang Zhao, Zicheng Liu, Yaowen Yang, Guobiao Hu, Junrui Liang, Wei-Hsin Liao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vibration energy harvesting is a promising solution for powering battery-free IoT systems; however, the instability of ambient vibrations presents significant challenges, such as limited harvested energy, intermittent power supply, and poor adaptability to various applications. To address these challenges, this paper proposes ViPSN2.0, a modular and reconfigurable IoT platform that supports multiple vibration energy harvesters (piezoelectric, electromagnetic, and triboelectric) and accommodates sensing tasks with varying application requirements through standardized hot-swappable interfaces. ViPSN~2.0 incorporates an energy-indication power management framework tailored to various application demands, including light-duty discrete sampling, heavy-duty high-power sensing, and complex-duty streaming tasks, thereby effectively managing fluctuating energy availability. The platform's versatility and robustness are validated through three representative applications: ViPSN-Beacon, enabling ultra-low-power wireless beacon transmission from a single transient fingertip press; ViPSN-LoRa, supporting high-power, long-range wireless communication powered by wave vibrations in actual marine environments; and ViPSN-Cam, enabling intermittent image capture and wireless transfer. Experimental results demonstrate that ViPSN~2.0 can reliably meet a wide range of requirements in practical battery-free IoT deployments under energy-constrained conditions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T15:06:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05081v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05081v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 ICAS: Detecting Training Data from Autoregressive Image Generative
  Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hongyao Yu, Yixiang Qiu, Yiheng Yang, Hao Fang, Tianqu Zhuang, Jiaxin Hong, Bin Chen, Hao Wu, Shu-Tao Xia
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Autoregressive image generation has witnessed rapid advancements, with prominent models such as scale-wise visual auto-regression pushing the boundaries of visual synthesis. However, these developments also raise significant concerns regarding data privacy and copyright. In response, training data detection has emerged as a critical task for identifying unauthorized data usage in model training. To better understand the vulnerability of autoregressive image generative models to such detection, we conduct the first study applying membership inference to this domain. Our approach comprises two key components: implicit classification and an adaptive score aggregation strategy. First, we compute the implicit token-wise classification score within the query image. Then we propose an adaptive score aggregation strategy to acquire a final score, which places greater emphasis on the tokens with lower scores. A higher final score indicates that the sample is more likely to be involved in the training set. To validate the effectiveness of our method, we adapt existing detection algorithms originally designed for LLMs to visual autoregressive models. Extensive experiments demonstrate the superiority of our method in both class-conditional and text-to-image scenarios. Moreover, our approach exhibits strong robustness and generalization under various data transformations. Furthermore, sufficient experiments suggest two novel key findings: (1) A linear scaling law on membership inference, exposing the vulnerability of large foundation models. (2) Training data from scale-wise visual autoregressive models is easier to detect than other autoregressive paradigms.Our code is available at https://github.com/Chrisqcwx/ImageAR-MIA.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T14:50:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05068v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05068v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 What Shapes User Trust in ChatGPT? A Mixed-Methods Study of User
  Attributes, Trust Dimensions, Task Context, and Societal Perceptions among
  University Students</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kadija Bouyzourn, Alexandra Birch
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This mixed-methods inquiry examined four domains that shape university students' trust in ChatGPT: user attributes, seven delineated trust dimensions, task context, and perceived societal impact. Data were collected through a survey of 115 UK undergraduate and postgraduate students and four complementary semi-structured interviews. Behavioural engagement outweighed demographics: frequent use increased trust, whereas self-reported understanding of large-language-model mechanics reduced it. Among the dimensions, perceived expertise and ethical risk were the strongest predictors of overall trust; ease of use and transparency had secondary effects, while human-likeness and reputation were non-significant. Trust was highly task-contingent; highest for coding and summarising, lowest for entertainment and citation generation, yet confidence in ChatGPT's referencing ability, despite known inaccuracies, was the single strongest correlate of global trust, indicating automation bias. Computer-science students surpassed peers only in trusting the system for proofreading and writing, suggesting technical expertise refines rather than inflates reliance. Finally, students who viewed AI's societal impact positively reported the greatest trust, whereas mixed or negative outlooks dampened confidence. These findings show that trust in ChatGPT hinges on task verifiability, perceived competence, ethical alignment and direct experience, and they underscore the need for transparency, accuracy cues and user education when deploying LLMs in academic settings.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T14:29:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05046v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05046v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 MoLink: Distributed and Efficient Serving Framework for Large Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lewei Jin, Yongqi Chen, Kui Zhang, Yifan Zhuo, Yi Gao, Bowei Yang, Zhengong Cai, Wei Dong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models represent a groundbreaking shift in generative AI. Yet, these advances come with a significant challenge: the high cost of model serving. To mitigate these costs, consumer-grade GPUs emerge as a more affordable alternative. This presents an opportunity for more cost-efficient LLM serving by leveraging these GPUs.   However, it is non-trivial to achieve high-efficiency LLM serving on consumer-grade GPUs, mainly due to two challenges: 1) these GPUs are often deployed in limited network conditions; 2) these GPUs often exhibit heterogeneity in host systems. To address these challenges, we present MoLink, a distributed LLM serving system for large models. It incorporates several key techniques, enabling efficient LLM serving on heterogeneous and weakly connected consumer-grade GPUs. Our experiments demonstrate that it achieves throughput improvements of up to 458\% and cost-profit margin improvements of up to 151\%, compared to state-of-the-art systems. MoLink allows users on Windows, Linux, and containerized VMs to seamlessly integrate GPUs with just a few lines of code over Ethernet or public networks. Currently, it supports 18 mainstream architectures of open-source large language models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T14:27:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05043v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05043v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic
  Empirical Study</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuqi Zhu, Yi Zhong, Jintian Zhang, Ziheng Zhang, Shuofei Qiao, Yujie Luo, Lun Du, Da Zheng, Huajun Chen, Ningyu Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate models across three dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMs' analytical reasoning capabilities.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T14:20:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.IR</span><span>cs.LG</span><span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.19794v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.19794v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 A Framework for Synthetic Audio Conversations Generation using Large
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kaung Myat Kyaw, Jonathan Hoyin Chan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper, we introduce ConversaSynth, a framework designed to generate synthetic conversation audio using large language models (LLMs) with multiple persona settings. The framework first creates diverse and coherent text-based dialogues across various topics, which are then converted into audio using text-to-speech (TTS) systems. Our experiments demonstrate that ConversaSynth effectively generates highquality synthetic audio datasets, which can significantly enhance the training and evaluation of models for audio tagging, audio classification, and multi-speaker speech recognition. The results indicate that the synthetic datasets generated by ConversaSynth exhibit substantial diversity and realism, making them suitable for developing robust, adaptable audio-based AI systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T13:57:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SD</span><span>cs.AI</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/WI-IAT62293.2024.00056' target='_blank'>doi</a><a href='http://arxiv.org/abs/2409.00946v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.00946v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 Relative Overfitting and Accept-Reject Framework</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yanxin Liu, Yunqi Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The scaling of Large Language Models (LLMs) currently faces significant challenges. Model assembly is widely considered a promising solution to break through these performance bottlenecks. However, current ensembling methods are primarily guided by the statistical expectation that combining multiple models over large samples will lead to performance gains. We propose an ensemble framework that transitions from such stochastic, sample-dependent methods to a regular, controllable approach based on fine-grained model segmentation. This regularity governs how models are segmented to ensure performance improvement, how the magnitude of this improvement varies with model selection, and what factors determine its theoretical maximum. To formalize this pattern, we introduce the concept of'relative overfitting,' which is derived from the performance discrepancies between constituent models and builds a bridge between ensemble outcomes and the inherent attributes of these models. We detail the patterns of this framework within the domain of NLP and briefly describe its extensibility to other fields, such as computer vision (CV) and AI for science. Our approach was validated using both custom-built and pre-trained mainstream models across diverse benchmarks, including language modeling, long-context tasks, and question-answering (QA). The results indicate that the ensemble rules we proposed are generally effective and that we provide a rigorous proof of these rules in certain experimental scenarios. The proposed framework offers a new perspective for understanding ensemble theory and provides a systematic approach to addressing the performance bottlenecks of LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T13:56:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.07783v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.07783v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 Co-DETECT: Collaborative Discovery of Edge Cases in Text Classification</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chenfei Xiong, Jingwei Ni, Yu Fan, VilÃ©m Zouhar, Donya Rooein, Lorena Calvo-BartolomÃ©, Alexander Hoyle, Zhijing Jin, Mrinmaya Sachan, Markus Leippold, Dirk Hovy, Mennatallah El-Assady, Elliott Ash
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce Co-DETECT (Collaborative Discovery of Edge cases in TExt ClassificaTion), a novel mixed-initiative annotation framework that integrates human expertise with automatic annotation guided by large language models (LLMs). Co-DETECT starts with an initial, sketch-level codebook and dataset provided by a domain expert, then leverages the LLM to annotate the data and identify edge cases that are not well described by the initial codebook. Specifically, Co-DETECT flags challenging examples, induces high-level, generalizable descriptions of edge cases, and assists user in incorporating edge case handling rules to improve the codebook. This iterative process enables more effective handling of nuanced phenomena through compact, generalizable annotation rules. Extensive user study, qualitative and quantitative analyses prove the effectiveness of Co-DETECT.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T13:48:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.05010v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.05010v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 Exploring O-RAN Compression Techniques in Decentralized Distributed MIMO
  Systems: Reducing Fronthaul Load</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mostafa Rahmani, Junbo Zhao, Vida Ranjbar, Ahmed Al-Tahmeesschi, Hamed Ahmadi, Sofie Pollin, Alister G. Burr
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper explores the application of uplink fronthaul compression techniques within Open RAN (O-RAN) to mitigate fronthaul load in decentralized distributed MIMO (DD-MIMO) systems. With the ever-increasing demand for high data rates and system scalability, the fronthaul load becomes a critical bottleneck. Our method uses O-RAN compression techniques to efficiently compress the fronthaul signals. The goal is to greatly lower the fronthaul load while having little effect on the overall system performance, as shown by Block Error Rate (BLER) curves. Through rigorous link-level simulations, we compare our quantization strategies against a benchmark scenario with no quantization, providing insights into the trade-offs between fronthaul data rate reduction and link performance integrity. The results demonstrate that our proposed quantization techniques not only lower the fronthaul load but also maintain a competitive link quality, making them a viable solution for enhancing the efficiency of next-generation wireless networks. This study underscores the potential of quantization in O-RAN contexts to achieve optimal balance between system capacity and performance, paving the way for more scalable and robust DD-MIMO deployments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T13:36:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04997v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04997v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility
  Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiangbo Yu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity to operate according to internal rules without external control. Accordingly, autonomous vehicles (AuVs) are defined as systems capable of perceiving their environment and executing preprogrammed tasks independently of external input. However, both research and real-world deployments increasingly showcase vehicles that demonstrate behaviors beyond this definition (including the SAE levels 1 to 6), such as interaction with humans and machines, goal adaptation, contextual reasoning, external tool use, and long-term planning, particularly with the integration of large language models (LLMs) and agentic AI systems. These developments reveal a conceptual gap between technical autonomy and the broader cognitive and social capabilities needed for future human-centered mobility systems. To address this, we introduce the concept of agentic vehicles (AgVs), referring to vehicles that integrate agentic AI to reason, adapt, and interact within complex environments. This paper presents a systems-level framework to characterize AgVs, focusing on their cognitive and communicative layers and differentiating them from conventional AuVs. It synthesizes relevant advances in agentic AI, robotics, multi-agent systems, and human-machine interaction, and highlights how agentic AI, through high-level reasoning and tool use, can function not merely as computational tools but as interactive agents embedded in mobility ecosystems. The paper concludes by identifying key challenges in the development and governance of AgVs, including safety, real-time control, public acceptance, ethical alignment, and regulatory frameworks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T13:34:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span><span>cs.CE</span><span>cs.CL</span><span>cs.HC</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04996v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04996v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 FUDOKI: Discrete Flow-based Unified Understanding and Generation via
  Kinetic-Optimal Velocities</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jin Wang, Yao Lai, Aoxue Li, Shifeng Zhang, Jiacheng Sun, Ning Kang, Chengyue Wu, Zhenguo Li, Ping Luo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid progress of large language models (LLMs) has catalyzed the emergence of multimodal large language models (MLLMs) that unify visual understanding and image generation within a single framework. However, most existing MLLMs rely on autoregressive (AR) architectures, which impose inherent limitations on future development, such as the raster-scan order in image generation and restricted reasoning abilities in causal context modeling. In this work, we challenge the dominance of AR-based approaches by introducing FUDOKI, a unified multimodal model purely based on discrete flow matching, as an alternative to conventional AR paradigms. By leveraging metric-induced probability paths with kinetic optimal velocities, our framework goes beyond the previous masking-based corruption process, enabling iterative refinement with self-correction capability and richer bidirectional context integration during generation. To mitigate the high cost of training from scratch, we initialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to the discrete flow matching paradigm. Experimental results show that FUDOKI achieves performance comparable to state-of-the-art AR-based MLLMs across both visual understanding and image generation tasks, highlighting its potential as a foundation for next-generation unified multimodal models. Furthermore, we show that applying test-time scaling techniques to FUDOKI yields significant performance gains, further underscoring its promise for future enhancement through reinforcement learning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T13:23:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.20147v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.20147v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 Autonomous Microscopy Experiments through Large Language Model Agents</h2>
                <div class="authors">
                    <strong>Authors:</strong> Indrajeet Mandal, Jitendra Soni, Mohd Zaki, Morten M. Smedskjaer, Katrin Wondraczek, Lothar Wondraczek, Nitya Nand Gosvami, N. M. Anoop Krishnan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) are revolutionizing self driving laboratories (SDLs) for materials research, promising unprecedented acceleration of scientific discovery. However, current SDL implementations rely on rigid protocols that fail to capture the adaptability and intuition of expert scientists in dynamic experimental settings. We introduce Artificially Intelligent Lab Assistant (AILA), a framework automating atomic force microscopy through LLM driven agents. Further, we develop AFMBench a comprehensive evaluation suite challenging AI agents across the complete scientific workflow from experimental design to results analysis. We find that state of the art models struggle with basic tasks and coordination scenarios. Notably, Claude 3.5 sonnet performs unexpectedly poorly despite excelling in materials domain question answering (QA) benchmarks, revealing that domain specific QA proficiency does not necessarily translate to effective agentic capabilities. Additionally, we observe that LLMs can deviate from instructions, raising safety alignment concerns for SDL applications. Our ablations reveal that multi agent frameworks outperform single-agent architectures. We also observe significant prompt fragility, where slight modifications in prompt structure cause substantial performance variations in capable models like GPT 4o. Finally, we evaluate AILA's effectiveness in increasingly advanced experiments AFM calibration, feature detection, mechanical property measurement, graphene layer counting, and indenter detection. Our findings underscore the necessity for rigorous benchmarking protocols and prompt engineering strategies before deploying AI laboratory assistants in scientific research environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T13:21:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span><span>cond-mat.mtrl-sci</span><span>cs.AI</span><span>physics.ins-det</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.10385v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.10385v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 Can Video LLMs Refuse to Answer? Alignment for Answerability in Video
  Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Eunseop Yoon, Hee Suk Yoon, Mark A. Hasegawa-Johnson, Chang D. Yoo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the broader context of deep learning, Multimodal Large Language Models have achieved significant breakthroughs by leveraging powerful Large Language Models as a backbone to align different modalities into the language space. A prime exemplification is the development of Video Large Language Models (Video-LLMs). While numerous advancements have been proposed to enhance the video understanding capabilities of these models, they are predominantly trained on questions generated directly from video content. However, in real-world scenarios, users often pose questions that extend beyond the informational scope of the video, highlighting the need for Video-LLMs to assess the relevance of the question. We demonstrate that even the best-performing Video-LLMs fail to reject unfit questions-not necessarily due to a lack of video understanding, but because they have not been trained to identify and refuse such questions. To address this limitation, we propose alignment for answerability, a framework that equips Video-LLMs with the ability to evaluate the relevance of a question based on the input video and appropriately decline to answer when the question exceeds the scope of the video, as well as an evaluation framework with a comprehensive set of metrics designed to measure model behavior before and after alignment. Furthermore, we present a pipeline for creating a dataset specifically tailored for alignment for answerability, leveraging existing video-description paired datasets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T13:19:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04976v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04976v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 Silent Failures in Stateless Systems: Rethinking Anomaly Detection for
  Serverless Computing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chanh Nguyen, Erik Elmroth, Monowar Bhuyan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Serverless computing has redefined cloud application deployment by abstracting infrastructure and enabling on-demand, event-driven execution, thereby enhancing developer agility and scalability. However, maintaining consistent application performance in serverless environments remains a significant challenge. The dynamic and transient nature of serverless functions makes it difficult to distinguish between benign and anomalous behavior, which in turn undermines the effectiveness of traditional anomaly detection methods. These conventional approaches, designed for stateful and long-running services, struggle in serverless settings where executions are short-lived, functions are isolated, and observability is limited.   In this first comprehensive vision paper on anomaly detection for serverless systems, we systematically explore the unique challenges posed by this paradigm, including the absence of persistent state, inconsistent monitoring granularity, and the difficulty of correlating behaviors across distributed functions. We further examine a range of threats that manifest as anomalies, from classical Denial-of-Service (DoS) attacks to serverless-specific threats such as Denial-of-Wallet (DoW) and cold start amplification. Building on these observations, we articulate a research agenda for next-generation detection frameworks that address the need for context-aware, multi-source data fusion, real-time, lightweight, privacy-preserving, and edge-cloud adaptive capabilities.   Through the identification of key research directions and design principles, we aim to lay the foundation for the next generation of anomaly detection in cloud-native, serverless ecosystems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T13:12:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04969v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04969v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 The Case for Instance-Optimized LLMs in OLAP Databases</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bardia Mohammadi, Laurent Bindschaedler
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) can enhance analytics systems with powerful data summarization, cleaning, and semantic transformation capabilities. However, deploying LLMs at scale -- processing millions to billions of rows -- remains prohibitively expensive in computation and memory. We present IOLM-DB, a novel system that makes LLM-enhanced database queries practical through query-specific model optimization. Instead of using general-purpose LLMs, IOLM-DB generates lightweight, specialized models tailored to each query's specific needs using representative data samples. IOLM-DB reduces model footprints by up to 76% and increases throughput by up to 3.31$\times$ while maintaining accuracy through aggressive compression techniques, including quantization, sparsification, and structural pruning. We further show how our approach enables higher parallelism on existing hardware and seamlessly supports caching and batching strategies to reduce overheads. Our prototype demonstrates that leveraging LLM queries inside analytics systems is feasible at scale, opening new possibilities for future OLAP applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T13:10:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04967v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04967v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 Active Stereo in the Wild through Virtual Pattern Projection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Luca Bartolomei, Matteo Poggi, Fabio Tosi, Andrea Conti, Stefano Mattoccia
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper presents a novel general-purpose guided stereo paradigm that mimics the active stereo principle by replacing the unreliable physical pattern projector with a depth sensor. It works by projecting virtual patterns consistent with the scene geometry onto the left and right images acquired by a conventional stereo camera, using the sparse hints obtained from a depth sensor, to facilitate the visual correspondence. Purposely, any depth sensing device can be seamlessly plugged into our framework, enabling the deployment of a virtual active stereo setup in any possible environment and overcoming the severe limitations of physical pattern projection, such as the limited working range and environmental conditions. Exhaustive experiments on indoor and outdoor datasets featuring both long and close range, including those providing raw, unfiltered depth hints from off-the-shelf depth sensors, highlight the effectiveness of our approach in notably boosting the robustness and accuracy of algorithms and deep stereo without any code modification and even without re-training. Additionally, we assess the performance of our strategy on active stereo evaluation datasets with conventional pattern projection. Indeed, in all these scenarios, our virtual pattern projection paradigm achieves state-of-the-art performance. The source code is available at: https://github.com/bartn8/vppstereo.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T13:07:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.04345v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.04345v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code
  Generation Evaluation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chenchen Zhang, Yuhang Li, Can Xu, Jiaheng Liu, Ao Liu, Shihui Hu, Dengpeng Wu, Guanhua Huang, Kejiao Li, Qi Yi, Ruibin Xiong, Haotian Zhu, Yuanxing Zhang, Yuhao Jiang, Yue Zhang, Zenan Xu, Bohui Zhai, Guoxiang He, Hebin Li, Jie Zhao, Le Zhang, Lingyun Tan, Pengyu Guo, Xianshu Pang, Yang Ruan, Zhifeng Zhang, Zhonghu Wang, Ziyan Xu, Zuopu Yin, Wiggin Zhou, Chayse Zhou, Fengzong Lian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The generative capabilities of Large Language Models (LLMs) are rapidly expanding from static code to dynamic, interactive visual artifacts. This progress is bottlenecked by a critical evaluation gap: established benchmarks focus on algorithmic correctness and are blind to the visual fidelity and interactive integrity that define modern user experiences. To bridge this gap, we introduce ArtifactsBench, a new benchmark and paradigm for the automated, multimodal evaluation of visual code generation. Our framework programmatically renders each generated artifact and captures its dynamic behavior through temporal screenshots. This visual evidence, alongside the source code, is then assessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a fine-grained, per-task checklist to ensure holistic and reproducible scoring. We construct a new benchmark of 1,825 diverse tasks and evaluate over 30 leading LLMs. Our automated evaluation achieves a striking 94.4% ranking consistency with WebDev Arena, the gold-standard for human preference in web development, and over 90% pairwise agreement with human experts. This establishes ArtifactsBench as the first framework to reliably automate the assessment of human-perceived quality at scale. Our analysis provides a high-resolution map of the current SOTA, revealing that generalist models often outperform domain-specific ones. We open-source ArtifactsBench, including the benchmark, evaluation harness, and baseline results at https://artifactsbenchmark.github.io/, to provide the community with a scalable and accurate tool to accelerate the development of user-centric generative models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T12:53:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04952v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04952v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 SIGIR 2025 -- LiveRAG Challenge Report</h2>
                <div class="authors">
                    <strong>Authors:</strong> David Carmel, Simone Filice, Guy Horowitz, Yoelle Maarek, Oren Somekh, Ran Tavory, Mehdi Ghissassi, Edo Liberty, Roy Miara
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The LiveRAG Challenge at SIGIR 2025, held between March and May 2025, provided a competitive platform for advancing Retrieval-Augmented Generation (RAG) technologies. Participants from academia and industry were invited to develop a RAG-based question-answering system using a fixed corpus (Fineweb-10BT) and a common open-source LLM (Falcon3-10B-Instruct). The goal was to facilitate challenging comparisons of retrieval and prompting strategies. During the Live Challenge Day, 70 teams from 27 different countries provided answers and supportive information to 500 unseen questions within a strict two-hour time window. Evaluation was conducted in two stages: first an automated LLM-as-a-judge approach was used to compute correctness and faithfulness score, then a manual review of top ranked submissions was conducted. The finalists were announced on June 12, 2025, with prizes awarded during the LiveRAG Workshop at SIGIR 2025 in Padua, Italy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-08T06:37:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.IR</span><span>H.3.3</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04942v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04942v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 LIFT: Automating Symbolic Execution Optimization with Large Language
  Models for AI Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruoxi Wang, Kun Li, Minghui Xu, Yue Zhang, Kaidi Xu, Chunchi Liu, Yinhao Xiao, Xiuzhen Cheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Dynamic Symbolic Execution (DSE) is a key technique in program analysis, widely used in software testing, vulnerability discovery, and formal verification. In distributed AI systems, DSE plays a crucial role in identifying hard-to-detect bugs, especially those arising from complex network communication patterns. However, traditional approaches to symbolic execution are often hindered by scalability issues and inefficiencies, particularly in large-scale systems. This paper introduces LIFT (Large-language-model Integrated Functional-equivalent-IR Transformation), a novel framework that leverages Large Language Models (LLMs) to automate the optimization of Intermediate Representations (IRs) in symbolic execution. LIFT addresses the challenges of symbolic execution by providing a scalable, context-sensitive solution for IR transformation. The framework consists of two phases: IR Analysis and Optimization, where LLMs optimize time-intensive IR blocks, and Symbolic Execution and Validation, which includes benchmarking and semantic verification to ensure correctness and generalizability. Experiments on real-world binaries demonstrated significant performance improvements, including a 53.5\% reduction in execution time for bigtest and a 10.24\% reduction for random, along with reductions in IR statements, PUT instructions, and temporary variables. These results demonstrate that LLMs simplify IRs while maintaining functional correctness, enhancing symbolic execution in distributed AI systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T12:26:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04931v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04931v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 LaCoOT: Layer Collapse through Optimal Transport</h2>
                <div class="authors">
                    <strong>Authors:</strong> Victor QuÃ©tu, Nour Hezbri, Enzo Tartaglione
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Although deep neural networks are well-known for their outstanding performance in tackling complex tasks, their hunger for computational resources remains a significant hurdle, posing energy-consumption issues and restricting their deployment on resource-constrained devices, preventing their widespread adoption. In this paper, we present an optimal transport-based method to reduce the depth of over-parametrized deep neural networks, alleviating their computational burden. More specifically, we propose a new regularization strategy based on the Max-Sliced Wasserstein distance to minimize the distance between the intermediate feature distributions in the neural network. We show that minimizing this distance enables the complete removal of intermediate layers in the network, achieving better performance/depth trade-off compared to existing techniques. We assess the effectiveness of our method on traditional image classification setups and extend it to generative image models. Our code is available at https://github.com/VGCQ/LaCoOT.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T12:01:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.08933v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.08933v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 Do LLMs Understand the Safety of Their Inputs? Training-Free Moderation
  via Latent Prototypes</h2>
                <div class="authors">
                    <strong>Authors:</strong> Maciej ChrabÄszcz, Filip Szatkowski, Bartosz WÃ³jcik, Jan DubiÅski, Tomasz TrzciÅski, Sebastian Cygert
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the rise of LLMs, ensuring model safety and alignment has become a critical concern. While modern instruction-finetuned LLMs incorporate alignment during training, they still frequently require moderation tools to prevent unsafe behavior. The most common approach to moderation are guard models that flag unsafe inputs. However, guards require costly training and are typically limited to fixed-size, pre-trained options, making them difficult to adapt to evolving risks and resource constraints. We hypothesize that instruction-finetuned LLMs already encode safety-relevant information internally and explore training-free safety assessment methods that work with off-the-shelf models. We show that simple prompting allows models to recognize harmful inputs they would otherwise mishandle. We also demonstrate that safe and unsafe prompts are distinctly separable in the models' latent space. Building on this, we introduce the Latent Prototype Moderator (LPM), a training-free moderation method that uses Mahalanobis distance in latent space to assess input safety. LPM is a lightweight, customizable add-on that generalizes across model families and sizes. Our method matches or exceeds state-of-the-art guard models across multiple safety benchmarks, offering a practical and flexible solution for scalable LLM moderation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T11:43:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.16174v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.16174v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 MARBLE: A Multi-Agent Rule-Based LLM Reasoning Engine for Accident
  Severity Prediction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kaleem Ullah Qasim, Jiashu Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Accident severity prediction plays a critical role in transportation safety systems but is a persistently difficult task due to incomplete data, strong feature dependencies, and severe class imbalance in which rare but high-severity cases are underrepresented and hard to detect. Existing methods often rely on monolithic models or black box prompting, which struggle to scale in noisy, real-world settings and offer limited interpretability. To address these challenges, we propose MARBLE a multiagent rule based LLM engine that decomposes the severity prediction task across a team of specialized reasoning agents, including an interchangeable ML-backed agent. Each agent focuses on a semantic subset of features (e.g., spatial, environmental, temporal), enabling scoped reasoning and modular prompting without the risk of prompt saturation. Predictions are coordinated through either rule-based or LLM-guided consensus mechanisms that account for class rarity and confidence dynamics. The system retains structured traces of agent-level reasoning and coordination outcomes, supporting in-depth interpretability and post-hoc performance diagnostics. Across both UK and US datasets, MARBLE consistently outperforms traditional machine learning classifiers and state-of-the-art (SOTA) prompt-based reasoning methods including Chain-of-Thought (CoT), Least-to-Most (L2M), and Tree-of-Thought (ToT) achieving nearly 90% accuracy where others plateau below 48%. This performance redefines the practical ceiling for accident severity classification under real world noise and extreme class imbalance. Our results position MARBLE as a generalizable and interpretable framework for reasoning under uncertainty in safety-critical applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T11:27:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span><span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04893v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04893v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement
  Learning Framework</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chen Wang, Lai Wei, Yanzhi Zhang, Chenyang Shao, Zedong Dan, Weiran Huang, Yue Wang, Yuzhi Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in reinforcement learning (RL) have significantly enhanced the reasoning capabilities of large language models (LLMs). Group Relative Policy Optimization (GRPO), an efficient variant of PPO that lowers RL's computational cost, still faces limited exploration, low sample efficiency and instability, constraining its performance on complex reasoning tasks. To address these limitations, we introduce EFRame, an Exploration-Filter-Replay framework that systematically augments GRPO along three critical dimensions. EFRame performs additional rollouts to explore high-quality trajectories, applies online filtering to eliminate low-quality samples that introduce noise and variance, and leverages experience replay to repeatedly exploit rare but informative samples. EFRame establishes a complete and stable learning cycle, guiding the model through a structured transition from exploration to convergence. Our experiments across a variety of reasoning benchmarks demonstrate that EFRame not only improves the robustness and efficiency of training, but also enables access to deeper reasoning capabilities that remain unattainable under vanilla GRPO. Furthermore, EFRame not only enables fine-grained categorization of training samples for deeper insight into their contributions, but also introduces an efficient and precise mechanism for entropy control, which is critical for balancing exploration and convergence in RL training. Our code is available at https://github.com/597358816/EFRame.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T11:27:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.22200v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.22200v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen
  Visual Unicode Representations</h2>
                <div class="authors">
                    <strong>Authors:</strong> A. Bochkov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Understanding the locus of semantic representation in large language models (LLMs) is crucial for interpretability and architectural innovation. The dominant paradigm posits that trainable input embeddings serve as foundational "meaning vectors." This paper challenges that view. We construct Transformer models where the embedding layer is entirely frozen, with vectors derived not from data, but from the visual structure of Unicode glyphs. These non-semantic, precomputed visual embeddings are fixed throughout training. Our method is compatible with any tokenizer, including a novel Unicode-centric tokenizer we introduce to ensure universal text coverage. Despite the absence of trainable, semantically initialized embeddings, our models converge, generate coherent text, and, critically, outperform architecturally identical models with trainable embeddings on the MMLU reasoning benchmark. We attribute this to "representational interference" in conventional models, where the embedding layer is burdened with learning both structural and semantic features. Our results indicate that high-level semantics are not inherent to input embeddings but are an emergent property of the Transformer's compositional architecture and data scale. This reframes the role of embeddings from meaning containers to structural primitives. We release all code and models to foster further research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T11:17:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04886v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04886v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 Building Open-Retrieval Conversational Question Answering Systems by
  Generating Synthetic Data and Decontextualizing User Questions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Christos Vlachos, Nikolaos Stylianou, Alexandra Fiotaki, Spiros Methenitis, Elisavet Palogiannidi, Themos Stafylakis, Ion Androutsopoulos
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We consider open-retrieval conversational question answering (OR-CONVQA), an extension of question answering where system responses need to be (i) aware of dialog history and (ii) grounded in documents (or document fragments) retrieved per question. Domain-specific OR-CONVQA training datasets are crucial for real-world applications, but hard to obtain. We propose a pipeline that capitalizes on the abundance of plain text documents in organizations (e.g., product documentation) to automatically produce realistic OR-CONVQA dialogs with annotations. Similarly to real-world humanannotated OR-CONVQA datasets, we generate in-dialog question-answer pairs, self-contained (decontextualized, e.g., no referring expressions) versions of user questions, and propositions (sentences expressing prominent information from the documents) the system responses are grounded in. We show how the synthetic dialogs can be used to train efficient question rewriters that decontextualize user questions, allowing existing dialog-unaware retrievers to be utilized. The retrieved information and the decontextualized question are then passed on to an LLM that generates the system's response.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T11:16:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04884v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04884v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 HGNet: High-Order Spatial Awareness Hypergraph and Multi-Scale Context
  Attention Network for Colorectal Polyp Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaofang Liu, Lingling Sun, Xuqing Zhang, Yuannong Ye, Bin zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Colorectal cancer (CRC) is closely linked to the malignant transformation of colorectal polyps, making early detection essential. However, current models struggle with detecting small lesions, accurately localizing boundaries, and providing interpretable decisions. To address these issues, we propose HGNet, which integrates High-Order Spatial Awareness Hypergraph and Multi-Scale Context Attention. Key innovations include: (1) an Efficient Multi-Scale Context Attention (EMCA) module to enhance lesion feature representation and boundary modeling; (2) the deployment of a spatial hypergraph convolution module before the detection head to capture higher-order spatial relationships between nodes; (3) the application of transfer learning to address the scarcity of medical image data; and (4) Eigen Class Activation Map (Eigen-CAM) for decision visualization. Experimental results show that HGNet achieves 94% accuracy, 90.6% recall, and 90% mAP@0.5, significantly improving small lesion differentiation and clinical interpretability. The source code will be made publicly available upon publication of this paper.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T11:09:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04880v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04880v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 DoPI: Doctor-like Proactive Interrogation LLM for Traditional Chinese
  Medicine</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zewen Sun, Ruoxiang Huang, Jiahe Feng, Rundong Kong, Yuqian Wang, Hengyu Liu, Ziqi Gong, Yuyuan Qin, Yingxue Wang, Yu Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Enhancing interrogation capabilities in Traditional Chinese Medicine (TCM) diagnosis through multi-turn dialogues and knowledge graphs presents a significant challenge for modern AI systems. Current large language models (LLMs), despite their advancements, exhibit notable limitations in medical applications, particularly in conducting effective multi-turn dialogues and proactive questioning. These shortcomings hinder their practical application and effectiveness in simulating real-world diagnostic scenarios. To address these limitations, we propose DoPI, a novel LLM system specifically designed for the TCM domain. The DoPI system introduces a collaborative architecture comprising a guidance model and an expert model. The guidance model conducts multi-turn dialogues with patients and dynamically generates questions based on a knowledge graph to efficiently extract critical symptom information. Simultaneously, the expert model leverages deep TCM expertise to provide final diagnoses and treatment plans. Furthermore, this study constructs a multi-turn doctor-patient dialogue dataset to simulate realistic consultation scenarios and proposes a novel evaluation methodology that does not rely on manually collected real-world consultation data. Experimental results show that the DoPI system achieves an accuracy rate of 84.68 percent in interrogation outcomes, significantly enhancing the model's communication ability during diagnosis while maintaining professional expertise.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T11:04:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04877v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04877v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 Handling out-of-order input arrival in CEP engines on the edge combining
  optimistic, pessimistic and lazy evaluation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Styliani Kyrama, Anastasios Gounaris
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In Complex Event Processing, handling out-of-order, late, and duplicate events is critical for real-time analytics, especially on resource-constrained devices that process heterogeneous data from multiple sources. We present LimeCEP, a hybrid CEP approach that combines lazy evaluation, buffering, and speculative processing to efficiently handle data inconsistencies while supporting multi-pattern detection under relaxed semantics. LimeCEP integrates Kafka for efficient message ordering, retention, and duplicate elimination, and offers configurable strategies to trade off between accuracy, latency, and resource consumption. Compared to state-of-the-art systems like SASE and FlinkCEP, LimeCEP achieves up to six orders of magnitude lower latency, with up to 10 times lower memory usage and 6 times lower CPU utilization, while maintaining near-perfect precision and recall under high-disorder input streams, making it well-suited for non-cloud deployments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T10:55:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.01461v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.01461v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 Supporting Software Formal Verification with Large Language Models: An
  Experimental Study</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weiqi Wang, Marie Farrell, Lucas C. Cordeiro, Liping Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Formal methods have been employed for requirements verification for a long time. However, it is difficult to automatically derive properties from natural language requirements. SpecVerify addresses this challenge by integrating large language models (LLMs) with formal verification tools, providing a more flexible mechanism for expressing requirements. This framework combines Claude 3.5 Sonnet with the ESBMC verifier to form an automated workflow. Evaluated on nine cyber-physical systems from Lockheed Martin, SpecVerify achieves 46.5% verification accuracy, comparable to NASA's CoCoSim, but with lower false positives. Our framework formulates assertions that extend beyond the expressive power of LTL and identifies falsifiable cases that are missed by more traditional methods. Counterexample analysis reveals CoCoSim's limitations stemming from model connection errors and numerical approximation issues. While SpecVerify advances verification automation, our comparative study of Claude, ChatGPT, and Llama shows that high-quality requirements documentation and human monitoring remain critical, as models occasionally misinterpret specifications. Our results demonstrate that LLMs can significantly reduce the barriers to formal verification, while highlighting the continued importance of human-machine collaboration in achieving optimal results.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T10:30:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04857v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04857v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 NotebookLM: An LLM with RAG for active learning and collaborative
  tutoring</h2>
                <div class="authors">
                    <strong>Authors:</strong> Eugenio Tufino
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This study explores NotebookLM, a Google Gemini powered AI platform that integrates Retrieval Augmented Generation (RAG), as a collaborative physics tutor, an area of research that is developing quickly. In our implementation, NotebookLM was configured as an AI physics collaborative tutor to support students in solving conceptually oriented physics problems using a collaborative, Socratic approach. When deployed as a collaborative tutor, the system restricts student interaction to a chat only interface, promoting controlled and guided engagement. By grounding its responses in teacher provided source documents, NotebookLM helps mitigate one of the major shortcomings of standard large language models--hallucinations--thereby ensuring more traceable and reliable answers. Our experiments demonstrate NotebookLM's potential as a low cost, easily implemented RAG based tool for personalized and traceable AI assisted physics learning in diverse educational settings. Furthermore, NotebookLM also functions as a valuable study tool for both teachers and students by generating targeted questions, study guides, and supplementary materials that enhance both classroom instruction and independent research. While limitations remain, particularly regarding legal restrictions, the current text only mode of interaction, and the intrinsic reliability challenges of statistical models, this work presents a promising example of a grounded AI application in physics education.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T10:27:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.ed-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.09720v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.09720v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 $\textit{Grahak-Nyay:}$ Consumer Grievance Redressal through Large
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shrey Ganatra, Swapnil Bhattacharyya, Harshvivek Kashid, Spandan Anaokar, Shruti Nair, Reshma Sekhar, Siddharth Manohar, Rahul Hemrajani, Pushpak Bhattacharyya
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Access to consumer grievance redressal in India is often hindered by procedural complexity, legal jargon, and jurisdictional challenges. To address this, we present $\textbf{Grahak-Nyay}$ (Justice-to-Consumers), a chatbot that streamlines the process using open-source Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG). Grahak-Nyay simplifies legal complexities through a concise and up-to-date knowledge base. We introduce three novel datasets: $\textit{GeneralQA}$ (general consumer law), $\textit{SectoralQA}$ (sector-specific knowledge) and $\textit{SyntheticQA}$ (for RAG evaluation), along with $\textit{NyayChat}$, a dataset of 300 annotated chatbot conversations. We also introduce $\textit{Judgments}$ data sourced from Indian Consumer Courts to aid the chatbot in decision making and to enhance user trust. We also propose $\textbf{HAB}$ metrics ($\textbf{Helpfulness, Accuracy, Brevity}$) to evaluate chatbot performance. Legal domain experts validated Grahak-Nyay's effectiveness. Code and datasets will be released.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T10:26:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04854v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04854v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 Dialogue-Based Multi-Dimensional Relationship Extraction from Novels</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuchen Yan, Hanjie Zhao, Senbin Zhu, Hongde Liu, Zhihong Zhang, Yuxiang Jia
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Relation extraction is a crucial task in natural language processing, with broad applications in knowledge graph construction and literary analysis. However, the complex context and implicit expressions in novel texts pose significant challenges for automatic character relationship extraction. This study focuses on relation extraction in the novel domain and proposes a method based on Large Language Models (LLMs). By incorporating relationship dimension separation, dialogue data construction, and contextual learning strategies, the proposed method enhances extraction performance. Leveraging dialogue structure information, it improves the model's ability to understand implicit relationships and demonstrates strong adaptability in complex contexts. Additionally, we construct a high-quality Chinese novel relation extraction dataset to address the lack of labeled resources and support future research. Experimental results show that our method outperforms traditional baselines across multiple evaluation metrics and successfully facilitates the automated construction of character relationship networks in novels.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T10:20:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04852v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04852v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 Efficient SAR Vessel Detection for FPGA-Based On-Satellite Sensing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Colin Laganier, Liam Fletcher, Elim Kwan, Richard Walters, Victoria Nockles
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Rapid analysis of satellite data is vital for many remote sensing applications, from disaster response to environmental monitoring, but is becoming harder to achieve with the increasing volumes of data generated by modern satellites. On-satellite machine learning (ML) offers a potential solution, by reducing latency associated with transmission of these large data volumes to ground stations, but state-of-the-art models are often too large or power-hungry for satellite deployment. Vessel detection using Synthetic Aperture Radar (SAR) is a critical time-sensitive task for maritime security that exemplifies this challenge. SAR vessel detection has previously been demonstrated only by ML models that either are too large for satellite deployment, have not been developed for sufficiently low-power hardware, or have only been developed and tested on small SAR datasets that do not sufficiently represent the real-world task. Here we address this issue by developing and deploying a new efficient and highly performant SAR vessel detection model, using a customised YOLOv8 architecture specifically optimized for FPGA-based processing within common satellite power constraints (<10W). We train and evaluate our model on the largest and most diverse open SAR vessel dataset, xView3-SAR, and deploy it on a Kria KV260 MPSoC. We show that our FPGA-based model has detection and classification performance only ~2% and 3% lower than values from state-of-the-art GPU-based models, despite being two to three orders of magnitude smaller in size. This work demonstrates small yet highly performant ML models for time-critical SAR analysis, paving the way for more autonomous, responsive, and scalable Earth observation systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T10:03:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04842v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04842v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 Spec-TOD: A Specialized Instruction-Tuned LLM Framework for Efficient
  Task-Oriented Dialogue Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Quang-Vinh Nguyen, Quang-Chieu Nguyen, Hoang Pham, Khac-Hoai Nam Bui
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Task-oriented dialogue (TOD) systems facilitate goal-driven interactions between users and machines. While recent advances in deep learning have improved the performance, TOD systems often struggle in low-resource scenarios with limited labeled data. To address this challenge, we propose Spec-TOD, a novel framework designed to train an end-to-end TOD system with limited data. Spec-TOD introduces two main innovations: (i) a novel specialized end-to-end TOD framework that incorporates explicit task instructions for instruction-tuned large language models (LLMs), and (ii) an efficient training strategy that leverages lightweight, specialized LLMs to achieve strong performance with minimal supervision. Experiments on the MultiWOZ dataset, a widely used TOD benchmark, demonstrate that Spec-TOD achieves competitive results while significantly reducing the need for labeled data. These findings highlight the potential of the proposed framework in advancing efficient and effective TOD systems in low-resource settings.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T10:03:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04841v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04841v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 RewardAnything: Generalizable Principle-Following Reward Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhuohao Yu, Jiali Zeng, Weizheng Gu, Yidong Wang, Jindong Wang, Fandong Meng, Jie Zhou, Yue Zhang, Shikun Zhang, Wei Ye
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reward Models, essential for guiding Large Language Model optimization, are typically trained on fixed preference datasets, resulting in rigid alignment to single, implicit preference distributions. This prevents adaptation to diverse real-world needs-from conciseness in one task to detailed explanations in another. The standard practice of collecting task-specific preference data and retraining reward models is resource-intensive, often producing biased rewards, and limits practical application. We introduce generalizable, principle-following reward models. We propose that RMs should understand and adhere to dynamically provided natural language specifications of reward principles, similar to instruction-following in LLMs. To measure this capability, we develop RABench, a comprehensive benchmark for RMs focusing on generalization across diverse principles. Evaluations on RABench reveal poor generalization of current RMs. As a solution, we present RewardAnything, a novel RM designed and trained to explicitly follow natural language principles. We achieve SotA performance with RewardAnything in traditional RM benchmark simply by specifying a well-defined principle, and results on RABench show we excel in adapting to novel principles without retraining. Furthermore, RewardAnything integrates seamlessly with existing RLHF methods and we show by a case study on how to automatically and efficiently align LLMs with only natural language principles.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T09:53:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.03637v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.03637v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 Harnessing Pairwise Ranking Prompting Through Sample-Efficient Ranking
  Distillation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junru Wu, Le Yan, Zhen Qin, Honglei Zhuang, Paul Suganthan G. C., Tianqi Liu, Zhe Dong, Xuanhui Wang, Harrie Oosterhuis
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While Pairwise Ranking Prompting (PRP) with Large Language Models (LLMs) is one of the most effective zero-shot document ranking methods, it has a quadratic computational complexity with respect to the number of documents to be ranked, as it requires an enumeration over all possible document pairs. Consequently, the outstanding ranking performance of PRP has remained unreachable for most real-world ranking applications.   In this work, we propose to harness the effectiveness of PRP through pairwise distillation. Specifically, we distill a pointwise student ranker from pairwise teacher labels generated by PRP, resulting in an efficient student model that retains the performance of PRP with substantially lower computational costs. Furthermore, we find that the distillation process can be made sample-efficient: with only 2% of pairs, we are able to obtain the same performance as using all pairs for teacher labels. Thus, our novel approach provides a solution to harness the ranking performance of PRP without incurring high computational costs during both distillation and serving.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T09:38:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04820v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04820v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 Enabling Security on the Edge: A CHERI Compartmentalized Network Stack</h2>
                <div class="authors">
                    <strong>Authors:</strong> Donato Ferraro, Andrea Bastoni, Alexander Zuepke, Andrea Marongiu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The widespread deployment of embedded systems in critical infrastructures, interconnected edge devices like autonomous drones, and smart industrial systems requires robust security measures. Compromised systems increase the risks of operational failures, data breaches, and -- in safety-critical environments -- potential physical harm to people. Despite these risks, current security measures are often insufficient to fully address the attack surfaces of embedded devices. CHERI provides strong security from the hardware level by enabling fine-grained compartmentalization and memory protection, which can reduce the attack surface and improve the reliability of such devices. In this work, we explore the potential of CHERI to compartmentalize one of the most critical and targeted components of interconnected systems: their network stack. Our case study examines the trade-offs of isolating applications, TCP/IP libraries, and network drivers on a CheriBSD system deployed on the Arm Morello platform. Our results suggest that CHERI has the potential to enhance security while maintaining performance in embedded-like environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T09:37:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.ET</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04818v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04818v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 Application and Evaluation of Large Language Models for Forecasting the
  Impact of Traffic Incidents</h2>
                <div class="authors">
                    <strong>Authors:</strong> George Jagadeesh, Srikrishna Iyer, Michal Polanowski, Kai Xin Thia
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This study examines the feasibility of applying large language models (LLMs) for forecasting the impact of traffic incidents on the traffic flow. The use of LLMs for this task has several advantages over existing machine learning-based solutions such as not requiring a large training dataset and the ability to utilize free-text incident logs. We propose a fully LLM-based solution that predicts the incident impact using a combination of traffic features and LLM-extracted incident features. A key ingredient of this solution is an effective method of selecting examples for the LLM's in-context learning. We evaluate the performance of three advanced LLMs and two state-of-the-art machine learning models on a real traffic incident dataset. The results show that the best-performing LLM matches the accuracy of the most accurate machine learning model, despite the former not having been trained on this prediction task. The findings indicate that LLMs are a practically viable option for traffic incident impact prediction.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T09:22:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04803v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04803v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 Training-Free Query Optimization via LLM-Based Plan Similarity</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nikita Vasilenko, Alexander Demin, Vladimir Boorlakov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language model (LLM) embeddings offer a promising new avenue for database query optimization. In this paper, we explore how pre-trained execution plan embeddings can guide SQL query execution without the need for additional model training. We introduce LLM-PM (LLM-based Plan Mapping), a framework that embeds the default execution plan of a query, finds its k nearest neighbors among previously executed plans, and recommends database hintsets based on neighborhood voting. A lightweight consistency check validates the selected hint, while a fallback mechanism searches the full hint space when needed. Evaluated on the JOB-CEB benchmark using OpenGauss, LLM-PM achieves an average speed-up of 21% query latency reduction. This work highlights the potential of LLM-powered embeddings to deliver practical improvements in query performance and opens new directions for training-free, embedding-based optimizer guidance systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T09:14:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.05853v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.05853v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 Qwen vs. Gemma Integration with Whisper: A Comparative Study in
  Multilingual SpeechLLM Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tuan Nguyen, Long-Vu Hoang, Huy-Dat Tran
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper presents our system for the MLC-SLM Challenge 2025, focusing on multilingual speech recognition and language modeling with large language models (LLMs). Our approach combines a fine-tuned Whisper-large-v3 encoder with efficient projector architectures and various decoder configurations. We employ a three-stage training methodology that progressively optimizes the encoder, projector, and LLM components. Our system achieves competitive performance with a private test average WER/CER result of 16.63% using the Gemma3-12B and 18.6% using the Qwen2.5-7B as decoder-only language model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T09:09:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.SD</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.13596v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.13596v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 FIDESlib: A Fully-Fledged Open-Source FHE Library for Efficient CKKS on
  GPUs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Carlos AgullÃ³-Domingo, Ãscar Vera-LÃ³pez, Seyda Guzelhan, Lohit Daksha, Aymane El Jerari, Kaustubh Shivdikar, Rashmi Agrawal, David Kaeli, Ajay Joshi, JosÃ© L. AbellÃ¡n
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Word-wise Fully Homomorphic Encryption (FHE) schemes, such as CKKS, are gaining significant traction due to their ability to provide post-quantum-resistant, privacy-preserving approximate computing; an especially desirable feature in Machine-Learning-as-a-Service (MLaaS) cloud-computing paradigms. OpenFHE is a leading CPU-based FHE library with robust CKKS operations, but its server-side performance is not yet sufficient for practical cloud deployment. As GPU computing becomes more common in data centers, many FHE libraries are adding GPU support. However, integrating an efficient GPU backend into OpenFHE is challenging. While OpenFHE uses a Hardware Abstraction Layer (HAL), its flexible architecture sacrifices performance due to the abstraction layers required for multi-scheme and multi-backend compatibility. In this work, we introduce FIDESlib, the first open-source server-side CKKS GPU library that is fully interoperable with well-established client-side OpenFHE operations. Unlike other existing open-source GPU libraries, FIDESlib provides the first implementation featuring heavily optimized GPU kernels for all CKKS primitives, including bootstrapping. Our library also integrates robust benchmarking and testing, ensuring it remains adaptable to further optimization. Furthermore, its software architecture is designed to support extensions to a multi-GPU backend for enhanced acceleration. Our experiments across various GPU systems and the leading open-source CKKS library to date, Phantom, show that FIDESlib offers superior performance and scalability. For bootstrapping, FIDESlib achieves no less than 70x speedup over the AVX-optimized OpenFHE implementation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T08:51:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04775v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04775v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and
  Interpretable Spiking Neural Model</h2>
                <div class="authors">
                    <strong>Authors:</strong> Changwei Wu, Yifei Chen, Yuxin Du, Jinying Zong, Jie Dong, Mingxuan Liu, Yong Peng, Jin Fan, Feiwei Qin, Changmiao Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Early diagnosis of Alzheimer's Disease (AD), especially at the mild cognitive impairment (MCI) stage, is vital yet hindered by subjective assessments and the high cost of multimodal imaging modalities. Although deep learning methods offer automated alternatives, their energy inefficiency and computational demands limit real-world deployment, particularly in resource-constrained settings. As a brain-inspired paradigm, spiking neural networks (SNNs) are inherently well-suited for modeling the sparse, event-driven patterns of neural degeneration in AD, offering a promising foundation for interpretable and low-power medical diagnostics. However, existing SNNs often suffer from weak expressiveness and unstable training, which restrict their effectiveness in complex medical tasks. To address these limitations, we propose FasterSNN, a hybrid neural architecture that integrates biologically inspired LIF neurons with region-adaptive convolution and multi-scale spiking attention. This design enables sparse, efficient processing of 3D MRI while preserving diagnostic accuracy. Experiments on benchmark datasets demonstrate that FasterSNN achieves competitive performance with substantially improved efficiency and stability, supporting its potential for practical AD screening. Our source code is available at https://github.com/wuchangw/FasterSNN.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T08:47:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.09695v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.09695v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 FurniMAS: Language-Guided Furniture Decoration using Multi-Agent System</h2>
                <div class="authors">
                    <strong>Authors:</strong> Toan Nguyen, Tri Le, Quang Nguyen, Anh Nguyen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Furniture decoration is an important task in various industrial applications. However, achieving a high-quality decorative result is often time-consuming and requires specialized artistic expertise. To tackle these challenges, we explore how multi-agent systems can assist in automating the decoration process. We propose FurniMAS, a multi-agent system for automatic furniture decoration. Specifically, given a human prompt and a household furniture item such as a working desk or a TV stand, our system suggests relevant assets with appropriate styles and materials, and arranges them on the item, ensuring the decorative result meets functionality, aesthetic, and ambiance preferences. FurniMAS assembles a hybrid team of LLM-based and non-LLM agents, each fulfilling distinct roles in a typical decoration project. These agents collaborate through communication, logical reasoning, and validation to transform the requirements into the final outcome. Extensive experiments demonstrate that our FurniMAS significantly outperforms other baselines in generating high-quality 3D decor.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T08:45:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04770v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04770v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 ABench-Physics: Benchmarking Physical Reasoning in LLMs via
  High-Difficulty and Dynamic Physics Problems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yiming Zhang, Yingfan Ma, Yanmei Gu, Zhengkai Yang, Yihong Zhuang, Feng Wang, Zenan Huang, Yuanyuan Wang, Chao Huang, Bowen Song, Cheng Lin, Junbo Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have shown impressive performance in domains such as mathematics and programming, yet their capabilities in physics remain underexplored and poorly understood. Physics poses unique challenges that demand not only precise computation but also deep conceptual understanding and physical modeling skills. Existing benchmarks often fall short due to limited difficulty, multiple-choice formats, and static evaluation settings that fail to capture physical modeling ability. In this paper, we introduce ABench-Physics, a novel benchmark designed to rigorously evaluate LLMs' physical reasoning and generalization capabilities. ABench-Physics consists of two components: Phy_A, a static set of 400 graduate- or Olympiad-level problems; and Phy_B, a dynamic subset of 100 problems equipped with an automatic variation engine to test model robustness across changing conditions. All questions require precise numerical answers, with strict formatting and tolerance constraints. Our evaluation of several state-of-the-art LLMs reveals substantial performance gaps, highlighting persistent limitations in physical reasoning, especially in generalization to dynamic variants. ABench-Physics provides a challenging and diagnostic framework for advancing scientific reasoning in LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T08:43:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04766v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04766v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 CoSteer: Collaborative Decoding-Time Personalization via Local Delta
  Steering</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hang Lv, Sheng Liang, Hao Wang, Hongchao Gu, Yaxiong Wu, Wei Guo, Defu Lian, Yong Liu, Enhong Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Personalized text generation has become crucial for adapting language models to diverse and evolving users' personal context across cultural, temporal, and contextual dimensions. While existing methods often rely on centralized fine-tuning or static preference alignment, they struggle to achieve real-time adaptation under resource constraints inherent to personal devices. This limitation creates a dilemma: large cloud-based models lack access to localized user-specific information, while small on-device models cannot match the generation quality of their cloud counterparts. To address this dichotomy, we present CoSteer, a novel collaborative framework that enables decoding-time personalization through localized delta steering. Our key insight lies in leveraging the logits difference between personal context-aware and -agnostic outputs from local small models as steering signals for cloud-based LLMs. Specifically, we formulate token-level optimization as an online learning problem, where local delta vectors dynamically adjust the remote LLM's logits within the on-device environment. This approach preserves privacy by transmitting only the final steered tokens rather than raw data or intermediate vectors, while maintaining cloud-based LLMs' general capabilities without fine-tuning. Through comprehensive experiments on various personalized generation tasks, we demonstrate that CoSteer effectively assists LLMs in generating personalized content by leveraging locally stored user profiles and histories, ensuring privacy preservation through on-device data processing while maintaining acceptable computational overhead.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T08:32:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04756v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04756v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 Large Language Models for Network Intrusion Detection Systems:
  Foundations, Implementations, and Future Directions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shuo Yang, Xinran Zheng, Xinchen Zhang, Jinfeng Xu, Jinze Li, Donglin Xie, Weicai Long, Edith C. H. Ngai
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have revolutionized various fields with their exceptional capabilities in understanding, processing, and generating human-like text. This paper investigates the potential of LLMs in advancing Network Intrusion Detection Systems (NIDS), analyzing current challenges, methodologies, and future opportunities. It begins by establishing a foundational understanding of NIDS and LLMs, exploring the enabling technologies that bridge the gap between intelligent and cognitive systems in AI-driven NIDS. While Intelligent NIDS leverage machine learning and deep learning to detect threats based on learned patterns, they often lack contextual awareness and explainability. In contrast, Cognitive NIDS integrate LLMs to process both structured and unstructured security data, enabling deeper contextual reasoning, explainable decision-making, and automated response for intrusion behaviors. Practical implementations are then detailed, highlighting LLMs as processors, detectors, and explainers within a comprehensive AI-driven NIDS pipeline. Furthermore, the concept of an LLM-centered Controller is proposed, emphasizing its potential to coordinate intrusion detection workflows, optimizing tool collaboration and system performance. Finally, this paper identifies critical challenges and opportunities, aiming to foster innovation in developing reliable, adaptive, and explainable NIDS. By presenting the transformative potential of LLMs, this paper seeks to inspire advancement in next-generation network security systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T08:28:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span><span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04752v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04752v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 LLMs as Architects and Critics for Multi-Source Opinion Summarization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anuj Attri, Arnav Attri, Pushpak Bhattacharyya, Suman Banerjee, Amey Patil, Muthusamy Chelliah, Nikesh Garera
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multi-source Opinion Summarization (M-OS) extends beyond traditional opinion summarization by incorporating additional sources of product metadata such as descriptions, key features, specifications, and ratings, alongside reviews. This integration results in comprehensive summaries that capture both subjective opinions and objective product attributes essential for informed decision-making. While Large Language Models (LLMs) have shown significant success in various Natural Language Processing (NLP) tasks, their potential in M-OS remains largely unexplored. Additionally, the lack of evaluation datasets for this task has impeded further advancements. To bridge this gap, we introduce M-OS-EVAL, a benchmark dataset for evaluating multi-source opinion summaries across 7 key dimensions: fluency, coherence, relevance, faithfulness, aspect coverage, sentiment consistency, specificity. Our results demonstrate that M-OS significantly enhances user engagement, as evidenced by a user study in which, on average, 87% of participants preferred M-OS over opinion summaries. Our experiments demonstrate that factually enriched summaries enhance user engagement. Notably, M-OS-PROMPTS exhibit stronger alignment with human judgment, achieving an average Spearman correlation of \r{ho} = 0.74, which surpasses the performance of previous methodologies.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T08:27:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>I.2.7; H.3.1; I.2.6</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04751v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04751v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 LLM-based Question-Answer Framework for Sensor-driven HVAC System
  Interaction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sungmin Lee, Minju Kang, Joonhee Lee, Seungyong Lee, Dongju Kim, Jingi Hong, Jun Shin, Pei Zhang, JeongGil Ko
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Question-answering (QA) interfaces powered by large language models (LLMs) present a promising direction for improving interactivity with HVAC system insights, particularly for non-expert users. However, enabling accurate, real-time, and context-aware interactions with HVAC systems introduces unique challenges, including the integration of frequently updated sensor data, domain-specific knowledge grounding, and coherent multi-stage reasoning. In this paper, we present JARVIS, a two-stage LLM-based QA framework tailored for sensor data-driven HVAC system interaction. JARVIS employs an Expert-LLM to translate high-level user queries into structured execution instructions, and an Agent that performs SQL-based data retrieval, statistical processing, and final response generation. To address HVAC-specific challenges, JARVIS integrates (1) an adaptive context injection strategy for efficient HVAC and deployment-specific information integration, (2) a parameterized SQL builder and executor to improve data access reliability, and (3) a bottom-up planning scheme to ensure consistency across multi-stage response generation. We evaluate JARVIS using real-world data collected from a commercial HVAC system and a ground truth QA dataset curated by HVAC experts to demonstrate its effectiveness in delivering accurate and interpretable responses across diverse queries. Results show that JARVIS consistently outperforms baseline and ablation variants in both automated and user-centered assessments, achieving high response quality and accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T08:19:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04748v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04748v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 A Tale of Two Scripts: Transliteration and Post-Correction for
  Judeo-Arabic</h2>
                <div class="authors">
                    <strong>Authors:</strong> Juan Moreno Gonzalez, Bashar Alhafni, Nizar Habash
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Judeo-Arabic refers to Arabic variants historically spoken by Jewish communities across the Arab world, primarily during the Middle Ages. Unlike standard Arabic, it is written in Hebrew script by Jewish writers and for Jewish audiences. Transliterating Judeo-Arabic into Arabic script is challenging due to ambiguous letter mappings, inconsistent orthographic conventions, and frequent code-switching into Hebrew and Aramaic. In this paper, we introduce a two-step approach to automatically transliterate Judeo-Arabic into Arabic script: simple character-level mapping followed by post-correction to address grammatical and orthographic errors. We also present the first benchmark evaluation of LLMs on this task. Finally, we show that transliteration enables Arabic NLP tools to perform morphosyntactic tagging and machine translation, which would have not been feasible on the original texts.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T08:19:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04746v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04746v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 Activation Steering for Chain-of-Thought Compression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Seyedarmin Azizi, Erfan Baghaei Potraghloo, Massoud Pedram
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) excel at complex reasoning when they include intermediate steps, known as "chains of thought" (CoTs). However, these rationales are often overly verbose, even for simple problems, leading to wasted context, increased latency, and higher energy consumption. We observe that verbose, English-heavy CoTs and concise, math-centric CoTs occupy distinct regions in the model's residual-stream activation space. By extracting and injecting a "steering vector" to transition between these modes, we can reliably shift generation toward more concise reasoning, effectively compressing CoTs without retraining. We formalize this approach as Activation-Steered Compression (ASC), an inference-time technique that shortens reasoning traces by directly modifying hidden representations. In addition, we provide a theoretical analysis of the impact of ASC on the output distribution, derived from a closed-form KL-divergence-bounded constraint to regulate steering strength. Using only 100 paired verbose and concise examples, ASC achieves up to 67.43% reduction in CoT length on MATH500 and GSM8K datasets, while maintaining accuracy across 7B, 8B, and 32B parameter models. As a training-free method, ASC introduces negligible runtime overhead and, on MATH500, delivers an average 2.73x speedup in end-to-end reasoning wall-clock time on an 8B model. This makes ASC a practical and efficient tool for streamlining the deployment of reasoning-capable LLMs in latency- or cost-sensitive settings. The code is available at: https://github.com/ArminAzizi98/ASC
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-08T02:54:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04742v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04742v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 ChipSeek-R1: Generating Human-Surpassing RTL with LLM via Hierarchical
  Reward-Driven Reinforcement Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhirong Chen, Kaiyan Chang, Zhuolin Li, Xinyang He, Chujie Chen, Cangyuan Li, Mengdi Wang, Haobo Xu, Yinhe Han, Ying Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) show significant potential for automating Register-Transfer Level (RTL) code generation. However, current approaches face a critical challenge: they can not simultaneously optimize for functional correctness and hardware quality (Power, Performance, Area - PPA). Methods based on supervised fine-tuning often generate functionally correct but PPA-suboptimal code, lacking mechanisms to learn optimization principles. In contrast, post-processing techniques that attempt to improve PPA metrics after generation are often inefficient because they operate externally without updating the LLM's parameters, thus failing to enhance the model's intrinsic design capabilities.   To bridge this gap, we introduce ChipSeek-R1, a hierarchical reward-driven reinforcement learning framework to train LLMs to generate RTL code that achieves both functional correctness and optimized PPA metrics. ChipSeek-R1 employs a hierarchical reward system, which incorporates direct feedback on syntax, functional correctness (from simulators) and PPA metrics (from synthesis tools) during reinforcement learning. This enables the model to learn complex hardware design trade-offs via trial-and-error, generating RTL code that is both functionally correct and PPA-optimized. Evaluating ChipSeek-R1 on standard benchmarks (VerilogEval, RTLLM), we achieve state-of-the-art results in functional correctness. Notably, on the RTLLM benchmark, ChipSeek-R1 generated 27 RTL designs surpassing the PPA metrics of the original human-written code. Our findings demonstrate the effectiveness of integrating toolchain feedback into LLM training and highlight the potential for reinforcement learning to enable automated generation of human-surpassing RTL code. We open-source our code in anonymous github.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T08:08:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.AR</span><span>cs.PL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04736v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04736v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 "This Suits You the Best": Query Focused Comparative Explainable
  Summarization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Arnav Attri, Anuj Attri, Pushpak Bhattacharyya, Suman Banerjee, Amey Patil, Muthusamy Chelliah, Nikesh Garera
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Product recommendations inherently involve comparisons, yet traditional opinion summarization often fails to provide holistic comparative insights. We propose the novel task of generating Query-Focused Comparative Explainable Summaries (QF-CES) using Multi-Source Opinion Summarization (M-OS). To address the lack of query-focused recommendation datasets, we introduce MS-Q2P, comprising 7,500 queries mapped to 22,500 recommended products with metadata. We leverage Large Language Models (LLMs) to generate tabular comparative summaries with query-specific explanations. Our approach is personalized, privacy-preserving, recommendation engine-agnostic, and category-agnostic. M-OS as an intermediate step reduces inference latency approximately by 40% compared to the direct input approach (DIA), which processes raw data directly. We evaluate open-source and proprietary LLMs for generating and assessing QF-CES. Extensive evaluations using QF-CES-PROMPT across 5 dimensions (clarity, faithfulness, informativeness, format adherence, and query relevance) showed an average Spearman correlation of 0.74 with human judgments, indicating its potential for QF-CES evaluation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T07:58:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.IR</span><span>H.3.1; I.2.7; H.1.2</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04733v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04733v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for
  Video-LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shaojie Zhang, Jiahui Yang, Jianqin Yin, Zhenbo Luo, Jian Luan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multimodal Large Language Models (MLLMs) have demonstrated significant success in visual understanding tasks. However, challenges persist in adapting these models for video comprehension due to the large volume of data and temporal complexity. Existing Video-LLMs using uniform frame sampling often struggle to capture the query-related crucial spatiotemporal clues of videos effectively. In this paper, we introduce Q-Frame, a novel approach for adaptive frame selection and multi-resolution scaling tailored to the video's content and the specific query. Q-Frame employs a training-free, plug-and-play strategy generated by a text-image matching network like CLIP, utilizing the Gumbel-Max trick for efficient frame selection. Q-Frame allows Video-LLMs to process more frames without exceeding computational limits, thereby preserving critical temporal and spatial information. We demonstrate Q-Frame's effectiveness through extensive experiments on benchmark datasets, including MLVU, LongVideoBench, and Video-MME, illustrating its superiority over existing methods and its applicability across various video understanding tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T07:52:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.22139v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.22139v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 Text Detoxification: Data Efficiency, Semantic Preservation and Model
  Generalization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jing Yu, Yibo Zhao, Jiapeng Zhu, Wenming Shao, Bo Pang, Zhao Zhang, Xiang Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The widespread dissemination of toxic content on social media poses a serious threat to both online environments and public discourse, highlighting the urgent need for detoxification methods that effectively remove toxicity while preserving the original semantics. However, existing approaches often struggle to simultaneously achieve strong detoxification performance, semantic preservation, and robustness to out-of-distribution data. Moreover, they typically rely on costly, manually annotated parallel corpora while showing poor data efficiency. To address these challenges, we propose a two-stage training framework that jointly optimizes for data efficiency, semantic preservation, and model generalization. We first perform supervised fine-tuning on a small set of high-quality, filtered parallel data to establish a strong initialization. Then, we leverage unlabeled toxic inputs and a custom-designed reward model to train the LLM using Group Relative Policy Optimization. Experimental results demonstrate that our method effectively mitigates the trade-offs faced by previous work, achieving state-of-the-art performance with improved generalization and significantly reduced dependence on annotated data. Our code is available at: https://github.com/allacnobug/Detoxification-of-Text.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T07:48:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.01050v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.01050v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 Who's the Mole? Modeling and Detecting Intention-Hiding Malicious Agents
  in LLM-Based Multi-Agent Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yizhe Xie, Congcong Zhu, Xinyue Zhang, Minghao Wang, Chi Liu, Minglu Zhu, Tianqing Zhu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multi-agent systems powered by Large Language Models (LLM-MAS) demonstrate remarkable capabilities in collaborative problem-solving. While LLM-MAS exhibit strong collaborative abilities, the security risks in their communication and coordination remain underexplored. We bridge this gap by systematically investigating intention-hiding threats in LLM-MAS, and design four representative attack paradigms that subtly disrupt task completion while maintaining high concealment. These attacks are evaluated in centralized, decentralized, and layered communication structures. Experiments conducted on six benchmark datasets, including MMLU, MMLU-Pro, HumanEval, GSM8K, arithmetic, and biographies, demonstrate that they exhibit strong disruptive capabilities. To identify these threats, we propose a psychology-based detection framework AgentXposed, which combines the HEXACO personality model with the Reid Technique, using progressive questionnaire inquiries and behavior-based monitoring. Experiments conducted on six types of attacks show that our detection framework effectively identifies all types of malicious behaviors. The detection rate for our intention-hiding attacks is slightly lower than that of the two baselines, Incorrect Fact Injection and Dark Traits Injection, demonstrating the effectiveness of intention concealment. Our findings reveal the structural and behavioral risks posed by intention-hiding attacks and offer valuable insights into securing LLM-based multi-agent systems through psychological perspectives, which contributes to a deeper understanding of multi-agent safety. The code and data are available at https://anonymous.4open.science/r/AgentXposed-F814.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T07:34:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MA</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04724v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04724v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 LOOM-Scope: a comprehensive and efficient LOng-cOntext Model evaluation
  framework</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zecheng Tang, Haitian Wang, Quantong Qiu, Baibei Ji, Ruoxi Sun, Keyan Zhou, Juntao Li, Min Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Long-context processing has become a fundamental capability for large language models~(LLMs). To assess model's long-context performance, numerous long-context evaluation benchmarks have been proposed. However, variations in evaluation settings across these benchmarks lead to inconsistent results, making it difficult to draw reliable comparisons. Besides, the high computational cost of long-context evaluation poses a significant barrier for the community to conduct comprehensive assessments of long-context models. In this paper, we propose LOOM-Scope, a comprehensive and efficient framework for long-context evaluation. LOOM-Scope standardizes evaluation settings across diverse benchmarks, supports deployment of efficient long-context inference acceleration methods, and introduces a holistic yet lightweight benchmark suite to evaluate models comprehensively. Homepage: https://loomscope.github.io
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T07:33:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04723v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04723v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 MAIN: Mutual Alignment Is Necessary for instruction tuning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fanyi Yang, Jianfeng Liu, Xin Zhang, Haoyu Liu, Xixin Cao, Yuefeng Zhan, Hao Sun, Weiwei Deng, Feng Sun, Qi Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Instruction tuning has empowered large language models (LLMs) to achieve remarkable performance, yet its success heavily depends on the availability of large-scale, high-quality instruction-response pairs. To meet this demand, various methods have been developed to synthesize data at scale. However, current methods for scaling up data generation often overlook a crucial aspect: the alignment between instructions and responses. We hypothesize that the quality of instruction-response pairs is determined not by the individual quality of each component, but by the degree of mutual alignment. To address this, we propose a Mutual Alignment Framework (MAIN) which enforces coherence between instructions and responses through mutual constraints. We demonstrate that MAIN generalizes well across model architectures and sizes, achieving state-of-the-art performance on LLaMA, Mistral, and Qwen models across diverse benchmarks. This work underscores the critical role of instruction-response alignment in enabling generalizable and high-quality instruction tuning for LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T07:19:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.12913v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.12913v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 Why We Feel What We Feel: Joint Detection of Emotions and Their Opinion
  Triggers in E-commerce</h2>
                <div class="authors">
                    <strong>Authors:</strong> Arnav Attri, Anuj Attri, Pushpak Bhattacharyya, Suman Banerjee, Amey Patil, Muthusamy Chelliah, Nikesh Garera
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Customer reviews on e-commerce platforms capture critical affective signals that drive purchasing decisions. However, no existing research has explored the joint task of emotion detection and explanatory span identification in e-commerce reviews - a crucial gap in understanding what triggers customer emotional responses. To bridge this gap, we propose a novel joint task unifying Emotion detection and Opinion Trigger extraction (EOT), which explicitly models the relationship between causal text spans (opinion triggers) and affective dimensions (emotion categories) grounded in Plutchik's theory of 8 primary emotions. In the absence of labeled data, we introduce EOT-X, a human-annotated collection of 2,400 reviews with fine-grained emotions and opinion triggers. We evaluate 23 Large Language Models (LLMs) and present EOT-DETECT, a structured prompting framework with systematic reasoning and self-reflection. Our framework surpasses zero-shot and chain-of-thought techniques, across e-commerce domains.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T06:59:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>I.2.7; H.3.1; I.2.6</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04708v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04708v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 UrbanMind: Towards Urban General Intelligence via Tool-Enhanced
  Retrieval-Augmented Generation and Multilevel Optimization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kai Yang, Zelin Zhu, Chengtao Jian, Hui Ma, Shengjie Zhao, Xiaozhou Ye, Ye Ouyang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Urban general intelligence (UGI) refers to the capacity of AI systems to autonomously perceive, reason, and act within dynamic and complex urban environments. In this paper, we introduce UrbanMind, a tool-enhanced retrieval-augmented generation (RAG) framework designed to facilitate UGI. Central to UrbanMind is a novel architecture based on Continual Retrieval-Augmented MoE-based LLM (C-RAG-LLM), which dynamically incorporates domain-specific knowledge and evolving urban data to support long-term adaptability. The architecture of C-RAG-LLM aligns naturally with a multilevel optimization framework, where different layers are treated as interdependent sub-problems. Each layer has distinct objectives and can be optimized either independently or jointly through a hierarchical learning process. The framework is highly flexible, supporting both end-to-end training and partial layer-wise optimization based on resource or deployment constraints. To remain adaptive under data drift, it is further integrated with an incremental corpus updating mechanism. Evaluations on real-world urban tasks of a variety of complexity verify the effectiveness of the proposed framework. This work presents a promising step toward the realization of general-purpose LLM agents in future urban environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T06:57:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04706v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04706v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 XiYan-SQL: A Novel Multi-Generator Framework For Text-to-SQL</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yifu Liu, Yin Zhu, Yingqi Gao, Zhiling Luo, Xiaoxia Li, Xiaorong Shi, Yuntao Hong, Jinyang Gao, Yu Li, Bolin Ding, Jingren Zhou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> To leverage the advantages of LLM in addressing challenges in the Text-to-SQL task, we present XiYan-SQL, an innovative framework effectively generating and utilizing multiple SQL candidates. It consists of three components: 1) a Schema Filter module filtering and obtaining multiple relevant schemas; 2) a multi-generator ensemble approach generating multiple highquality and diverse SQL queries; 3) a selection model with a candidate reorganization strategy implemented to obtain the optimal SQL query. Specifically, for the multi-generator ensemble, we employ a multi-task fine-tuning strategy to enhance the capabilities of SQL generation models for the intrinsic alignment between SQL and text, and construct multiple generation models with distinct generation styles by fine-tuning across different SQL formats. The experimental results and comprehensive analysis demonstrate the effectiveness and robustness of our framework. Overall, XiYan-SQL achieves a new SOTA performance of 75.63% on the notable BIRD benchmark, surpassing all previous methods. It also attains SOTA performance on the Spider test set with an accuracy of 89.65%.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T06:50:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04701v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04701v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 Balancing Act: Prioritization Strategies for LLM-Designed Restless
  Bandit Rewards</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shresth Verma, Niclas Boehmer, Lingkai Kong, Milind Tambe
                </div>
                <div class="summary">
                    <strong>Summary:</strong> LLMs are increasingly used to design reward functions based on human preferences in Reinforcement Learning (RL). We focus on LLM-designed rewards for Restless Multi-Armed Bandits, a framework for allocating limited resources among agents. In applications such as public health, this approach empowers grassroots health workers to tailor automated allocation decisions to community needs. In the presence of multiple agents, altering the reward function based on human preferences can impact subpopulations very differently, leading to complex tradeoffs and a multi-objective resource allocation problem. We are the first to present a principled method termed Social Choice Language Model for dealing with these tradeoffs for LLM-designed rewards for multiagent planners in general and restless bandits in particular. The novel part of our model is a transparent and configurable selection component, called an adjudicator, external to the LLM that controls complex tradeoffs via a user-selected social welfare function. Our experiments demonstrate that our model reliably selects more effective, aligned, and balanced reward functions compared to purely LLM-based approaches.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T06:48:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.12112v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.12112v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 Domain Adaptation of VLM for Soccer Video Understanding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tiancheng Jiang, Henry Wang, Md Sirajus Salekin, Parmida Atighehchian, Shinan Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vision Language Models (VLMs) have demonstrated strong performance in multi-modal tasks by effectively aligning visual and textual representations. However, most video understanding VLM research has been domain-agnostic, leaving the understanding of their transfer learning capability to specialized domains under-explored. In this work, we address this by exploring the adaptability of open-source VLMs to specific domains, and focusing on soccer as an initial case study. Our approach uses large-scale soccer datasets and LLM to create instruction-following data, and use them to iteratively fine-tune the general-domain VLM in a curriculum learning fashion (first teaching the model key soccer concepts to then question answering tasks). The final adapted model, trained using a curated dataset of 20k video clips, exhibits significant improvement in soccer-specific tasks compared to the base model, with a 37.5% relative improvement for the visual question-answering task and an accuracy improvement from 11.8% to 63.5% for the downstream soccer action classification task.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T06:34:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.13860v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.13860v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 Performance Evaluation of General Purpose Large Language Models for
  Basic Linear Algebra Subprograms Code Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Daichi Mukunoki, Shun-ichiro Hayashi, Tetsuya Hoshino, Takahiro Katagiri
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Generative AI technology based on Large Language Models (LLM) has been developed and applied to assist or automatically generate program codes. In this paper, we evaluate the capability of existing general LLMs for Basic Linear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs provided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model, and o4-mini, one of the o-series of Reasoning models. Both have been released in April 2025. For the routines from level-1 to 3 BLAS, we tried to generate (1) C code without optimization from routine name only, (2) C code with basic performance optimizations (thread parallelization, SIMD vectorization, and cache blocking) from routine name only, and (3) C code with basic performance optimizations based on Fortran reference code. As a result, we found that correct code can be generated in many cases even when only routine name are given. We also confirmed that thread parallelization with OpenMP, SIMD vectorization, and cache blocking can be implemented to some extent, and that the code is faster than the reference code.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T06:33:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.DC</span><span>cs.MS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04697v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04697v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 AKEGEN: A LLM-based Tabular Corpus Generator for Evaluating Dataset
  Discovery in Data Lakes</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhenwei Dai, Chuan Lei, Asterios Katsifodimos, Xiao Qin, Christos Faloutsos, Huzefa Rangwala
                </div>
                <div class="summary">
                    <strong>Summary:</strong> How to generate a large, realistic set of tables along with joinability relationships, to stress-test dataset discovery methods? Dataset discovery methods aim to automatically identify related data assets in a data lake. The development and evaluation of such solutions for customers from a wide range of business domains, relies on diverse, high quality and domain-specific tabular benchmarks. Large language models (LLMs) are trained on a wide variety of text data, which can provide a strong foundation of general and domain-specific knowledge. In this paper, we ask the question -- \textit{can we leverage LLMs to generate a tabular benchmark adequate for evaluating the dataset discovery solutions?} In particular, we focus on the task of finding joinable tables which is the cornerstone of virtually every dataset discovery method. Current corpora for evaluating dataset discovery methods are mainly based on subsets of open data, and they suffer from three important issues: $i)$ they focus on very common and generic data types (e.g., address, id, name, etc.); $ii)$ they do not contain human-annotated column pairs; instead, practitioners synthesize ground truth using table splits (e.g., horizontal for table union search and vertical ones for joinability) and $iii)$ they do not focus on semantic column relationships.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T06:08:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04687v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04687v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 Fully Automatic Neural Network Reduction for Formal Verification</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tobias Ladner, Matthias Althoff
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Formal verification of neural networks is essential before their deployment in safety-critical applications. However, existing methods for formally verifying neural networks are not yet scalable enough to handle practical problems under strict time constraints. We address this challenge by introducing a fully automatic and sound reduction of neural networks using reachability analysis. The soundness ensures that the verification of the reduced network entails the verification of the original network. Our sound reduction approach is applicable to neural networks with any type of element-wise activation function, such as ReLU, sigmoid, and tanh. The network reduction is computed on the fly while simultaneously verifying the original network and its specification. All parameters are automatically tuned to minimize the network size without compromising verifiability. We further show the applicability of our approach to convolutional neural networks by explicitly exploiting similar neighboring pixels. Our evaluation shows that our approach reduces large neural networks to a fraction of the original number of neurons and thus shortens the verification time to a similar degree.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T06:06:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2305.01932v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2305.01932v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 Trojan Horse Prompting: Jailbreaking Conversational Multimodal Models by
  Forging Assistant Message</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wei Duan, Li Qian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rise of conversational interfaces has greatly enhanced LLM usability by leveraging dialogue history for sophisticated reasoning. However, this reliance introduces an unexplored attack surface. This paper introduces Trojan Horse Prompting, a novel jailbreak technique. Adversaries bypass safety mechanisms by forging the model's own past utterances within the conversational history provided to its API. A malicious payload is injected into a model-attributed message, followed by a benign user prompt to trigger harmful content generation. This vulnerability stems from Asymmetric Safety Alignment: models are extensively trained to refuse harmful user requests but lack comparable skepticism towards their own purported conversational history. This implicit trust in its "past" creates a high-impact vulnerability. Experimental validation on Google's Gemini-2.0-flash-preview-image-generation shows Trojan Horse Prompting achieves a significantly higher Attack Success Rate (ASR) than established user-turn jailbreaking methods. These findings reveal a fundamental flaw in modern conversational AI security, necessitating a paradigm shift from input-level filtering to robust, protocol-level validation of conversational context integrity.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T05:35:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04673v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04673v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and
  Dual Evaluation Metrics for Factuality and Coverage</h2>
                <div class="authors">
                    <strong>Authors:</strong> Saehyung Lee, Seunghyun Yoon, Trung Bui, Jing Shi, Sungroh Yoon
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multimodal large language models (MLLMs) excel at generating highly detailed captions but often produce hallucinations. Our analysis reveals that existing hallucination detection methods struggle with detailed captions. We attribute this to the increasing reliance of MLLMs on their generated text, rather than the input image, as the sequence length grows. To address this issue, we propose a multiagent approach that leverages LLM-MLLM collaboration to correct given captions. Additionally, we introduce an evaluation framework and a benchmark dataset to facilitate the systematic analysis of detailed captions. Our experiments demonstrate that our proposed evaluation method better aligns with human judgments of factuality than existing metrics and that existing approaches to improve the MLLM factuality may fall short in hyper-detailed image captioning tasks. In contrast, our proposed method significantly enhances the factual accuracy of captions, even improving those generated by GPT-4V. Finally, we highlight a limitation of VQA-centric benchmarking by demonstrating that an MLLM's performance on VQA benchmarks may not correlate with its ability to generate detailed image captions. Our code and data are available at https://github.com/adobe-research/CapMAS.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T05:27:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.15484v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.15484v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 DANCE: Resource-Efficient Neural Architecture Search with Data-Aware and
  Continuous Adaptation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Maolin Wang, Tianshuo Wei, Sheng Zhang, Ruocheng Guo, Wanyu Wang, Shanshan Ye, Lixin Zou, Xuetao Wei, Xiangyu Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Neural Architecture Search (NAS) has emerged as a powerful approach for automating neural network design. However, existing NAS methods face critical limitations in real-world deployments: architectures lack adaptability across scenarios, each deployment context requires costly separate searches, and performance consistency across diverse platforms remains challenging. We propose DANCE (Dynamic Architectures with Neural Continuous Evolution), which reformulates architecture search as a continuous evolution problem through learning distributions over architectural components. DANCE introduces three key innovations: a continuous architecture distribution enabling smooth adaptation, a unified architecture space with learned selection gates for efficient sampling, and a multi-stage training strategy for effective deployment optimization. Extensive experiments across five datasets demonstrate DANCE's effectiveness. Our method consistently outperforms state-of-the-art NAS approaches in terms of accuracy while significantly reducing search costs. Under varying computational constraints, DANCE maintains robust performance while smoothly adapting architectures to different hardware requirements. The code and appendix can be found at https://github.com/Applied-Machine-Learning-Lab/DANCE.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T05:22:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.04671v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.04671v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless
  Handwritten STEM Grading</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yoonseok Yang, Minjune Kim, Marlon Rondinelli, Keren Shao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Grading handwritten, open-ended responses remains a major bottleneck in large university STEM courses. We introduce Pensieve (https://www.pensieve.co), an AI-assisted grading platform that leverages large language models (LLMs) to transcribe and evaluate student work, providing instructors with rubric-aligned scores, transcriptions, and confidence ratings. Unlike prior tools that focus narrowly on specific tasks like transcription or rubric generation, Pensieve supports the entire grading pipeline-from scanned student submissions to final feedback-within a human-in-the-loop interface.   Pensieve has been deployed in real-world courses at over 20 institutions and has graded more than 300,000 student responses. We present system details and empirical results across four core STEM disciplines: Computer Science, Mathematics, Physics, and Chemistry. Our findings show that Pensieve reduces grading time by an average of 65%, while maintaining a 95.4% agreement rate with instructor-assigned grades for high-confidence predictions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-07T05:10:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span><span>cs.HC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.01431v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.01431v2' target='_blank'>pdf</a>
                </div>
            </div>
            
        </div>
    </body>
    </html>
    