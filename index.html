
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>ArXiv Results</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                margin: 0;
                padding: 0;
                display: flex;
                justify-content: center;
                align-items: center;
                min-height: 100vh;
                background-color: #f9f9f9;
            }
            .container {
                width: 17cm;
                padding: 2cm;
                background-color: #fff;
                box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            }
            h1, h2 {
                color: #333;
            }
            .paper {
                border: 1px solid #ccc;
                padding: 1em;
                margin-bottom: 1em;
                border-radius: 8px;
                box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            }
            .authors, .summary, .updated, .tags, .links {
                margin: 0.5em 0;
            }
            .tags span {
                background-color: #f0f0f0;
                border-radius: 4px;
                padding: 0.2em 0.5em;
                margin-right: 0.5em;
            }
            .links a {
                margin-right: 1em;
                text-decoration: none;
                color: #1a73e8;
            }
            .links a:hover {
                text-decoration: underline;
            }
        </style>
    </head>
    <body>
        <div class="container">
    <h1>Keyword: kv cache</h1>

            <div class="paper">
                <h2>#1 Deep Multiple Quantization Network on Long Behavior Sequence for
  Click-Through Rate Prediction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhuoxing Wei, Qi Liu, Qingchen Xie
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In Click-Through Rate (CTR) prediction, the long behavior sequence, comprising the user's long period of historical interactions with items has a vital influence on assessing the user's interest in the candidate item. Existing approaches strike efficiency and effectiveness through a two-stage paradigm: first retrieving hundreds of candidate-related items and then extracting interest intensity vector through target attention. However, we argue that the discrepancy in target attention's relevance distribution between the retrieved items and the full long behavior sequence inevitably leads to a performance decline. To alleviate the discrepancy, we propose the Deep Multiple Quantization Network (DMQN) to process long behavior sequence end-to-end through compressing the long behavior sequence. Firstly, the entire spectrum of long behavior sequence will be quantized into multiple codeword sequences based on multiple independent codebooks. Hierarchical Sequential Transduction Unit is incorporated to facilitate the interaction of reduced codeword sequences. Then, attention between the candidate and multiple codeword sequences will output the interest vector. To enable online serving, intermediate representations of the codeword sequences are cached, significantly reducing latency. Our extensive experiments on both industrial and public datasets confirm the effectiveness and efficiency of DMQN. The A/B test in our advertising system shows that DMQN improves CTR by 3.5% and RPM by 2.0%.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:58:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3726302.3730177' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.20865v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20865v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yang Xiang, Fernando García-Redondo, Arvind Sharma, Van Dai Nguyen, Andrea Fantini, Philippe Matagne, Siddharth Rao, Subhali Subhechha, Lynn Verschueren, Mohammed Aftab Baig, Marie Garcia Bardon, Geert Hellings
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This work explores the cross-node scaling potential of SOT-MRAM for last-level caches (LLCs) under heterogeneous system scaling paradigm. We perform extensive Design-Technology Co-Optimization (DTCO) exercises to evaluate the bitcell footprint for different cell configurations at a representative 7 nm technology and to assess their implications on read and write power-performance. We crucially identify the MTJ routing struggle in conventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary bitcell area scaling challenge and propose to use BEOL read selectors (BEOL RSs) that enable (10 -- 40) % bitcell area reduction and eventually match sub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet the required SOT switching current, provided the magnetic free layer properties be engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This is particularly to attribute to their (i) more available Si fins for write transistor and (ii) lower bitline resistance at reduced cell width. We nevertheless underscore the read tradeoff associated with BEOL RSs, with the low-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the imperfectly rectifying diode selectors suffering (2.5 -- 5)$\times$ energy cost relative to 2T1R. This article thus highlights the realistic prospects and hurdles of BEOL RSs towards holistic power-performance-area scaling of SOT-MRAM.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T08:49:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.ET</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18250v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18250v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport
  Equation Solver for Fast Scatter Correction in Multi-Spectral CT</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guoxi Zhu, Li Zhang, Zhiqiang Chen, Hewei Gao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> X-ray scatter has been a serious concern in computed tomography (CT), leading to image artifacts and distortion of CT values. The linear Boltzmann transport equation (LBTE) is recognized as a fast and accurate approach for scatter estimation. However, for multi-spectral CT, it is cumbersome to compute multiple scattering components for different spectra separately when applying LBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum Decomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute X-ray scatter distributions from CT acquisitions at two or more different spectra simultaneously, in a unified framework with no sacrifice in accuracy and nearly no increase in computation in theory. First, a matrixed-spectrum solver of LBTE is obtained by introducing an additional label dimension to expand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a principle of selection of basis using the QR decomposition, along with the above solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter correction method can be established for multi-spectral CT. We validate the effectiveness and accuracy of our method by comparing it with the Monte Carlo method, including the computational time. We also evaluate the scatter correction performance using two different phantoms for fast-kV switching based dual-energy CT, and using an elliptical phantom in a numerical simulation for kV-modulation enabled CT scans, validating that our proposed method can significantly reduce the computational cost at multiple spectra and effectively reduce scatter artifact in reconstructed CT images.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T08:05:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.med-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20524v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20524v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content
  Caching in Emerging Mega-Constellations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haoyang Shi, Xing Zhang, Sitong Li, Minghang Li, Xinming Lu, Shaoxiang Xu, Guoquan Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Significant latency in global content delivery primarily arises from insufficient terrestrial infrastructure. Deploying space-based content delivery networks within emerging mega-constellations provides an effective means to bridge the digital divide. However, space-based caching faces constraints from physical-layer dynamics, including dynamic topologies, time-varying inter-satellite link conditions, and limited onboard energy. In addition, existing mechanisms often lack fine-grained content categorization and global optimization. This paper proposes MegaCacheX, a cost-effective hierarchical framework for collaborative content distribution that achieves "Earth-independence" by providing cloud services directly from space. Specifically, data centers in Sun-synchronous orbit act as primary content sources, while caching nodes in mega-constellations and ground stations collaboratively form a distributed edge layer. MegaCacheX optimizes caching strategies by integrating content popularity, regional user distribution, and satellite trajectory predictions. Multi-tier caching nodes serve as service anchors, enabling seamless content delivery with low latency. A prototype implemented on a microservices-based, containerized testbed demonstrates that MegaCacheX reduces global content access latency by about 36% compared to baseline approaches, while maintaining cost efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T05:22:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SY</span><span>cs.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20433v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20433v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 Breaking Diffusion with Cache: Exploiting Approximate Caches in
  Diffusion Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Desen Sun, Shuncheng Jie, Sihang Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion models are a powerful class of generative models that produce content, such as images, from user prompts, but they are computationally intensive. To mitigate this cost, recent academic and industry work has adopted approximate caching, which reuses intermediate states from similar prompts in a cache. While efficient, this optimization introduces new security risks by breaking isolation among users. This work aims to comprehensively assess new security vulnerabilities arising from approximate caching. First, we demonstrate a remote covert channel established with the cache, where a sender injects prompts with special keywords into the cache and a receiver can recover that even after days, to exchange information. Second, we introduce a prompt stealing attack using the cache, where an attacker can recover existing cached prompts based on cache hit prompts. Finally, we introduce a poisoning attack that embeds the attacker's logos into the previously stolen prompt, to render them in future user prompts that hit the cache. These attacks are all performed remotely through the serving system, which indicates severe security vulnerabilities in approximate caching.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T04:46:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20424v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20424v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full
  Context-Aware Linear Attention</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhongpan Tang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Transformer architecture has become a cornerstone of modern artificial intelligence, but its core self-attention mechanism suffers from a complexity bottleneck that scales quadratically with sequence length, severely limiting its application in long-sequence tasks. To address this challenge, existing linear attention methods typically sacrifice model performance by relying on data-agnostic kernel approximations or restrictive context selection. This paper returns to the first principles of connectionism, starting from the topological structure of information flow, to introduce a novel linear attention architecture-\textbf{TLinFormer}. By reconfiguring neuron connection patterns, TLinFormer achieves strict linear complexity while computing exact attention scores and ensuring information flow remains aware of the full historical context. This design aims to bridge the performance gap prevalent between existing efficient attention methods and standard attention. Through a series of experiments, we systematically evaluate the performance of TLinFormer against a standard Transformer baseline on long-sequence inference tasks. The results demonstrate that TLinFormer exhibits overwhelming advantages in key metrics such as \textbf{inference latency}, \textbf{KV cache efficiency}, \textbf{memory footprint}, and \textbf{overall speedup}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T04:10:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20407v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20407v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 ASVD: Activation-aware Singular Value Decomposition for Compressing
  Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhihang Yuan, Yuzhang Shang, Yue Song, Dawei Yang, Qiang Wu, Yan Yan, Guangyu Sun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper, we introduce a new post-training compression paradigm for Large Language Models (LLMs) to facilitate their wider adoption. We delve into LLM weight low-rank decomposition, and find that the challenges of this task stem from (1) the distribution variance in the LLM activations and (2) the sensitivity difference among various kinds of layers. To address these issues, we propose a training-free approach called Activation-aware Singular Value Decomposition (ASVD). Specifically, ASVD manages activation outliers by transforming the weight matrix based on the activation distribution. This transformation allows the outliers in the activation matrix to be absorbed into the transformed weight matrix, thereby enhancing decomposition accuracy. Additionally, we propose an efficient iterative calibration process to optimize layer-specific decomposition by addressing the varying sensitivity of different LLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the success of the low-rank decomposition of projection matrices in the self-attention module, we further introduce ASVD to compress the KV cache. By reducing the channel dimension of KV activations, memory requirements for KV cache can be largely reduced. ASVD can further achieve 50% KV cache reductions without performance drop in a training-free manner.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T03:57:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2312.05821v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2312.05821v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 Climber: Toward Efficient Scaling Laws for Large Recommendation Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Songpei Xu, Shijia Wang, Da Guo, Xianwen Guo, Qiang Xiao, Bin Huang, Guanlin Wu, Chuanjiang Luo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformer-based generative models have achieved remarkable success across domains with various scaling law manifestations. However, our extensive experiments reveal persistent challenges when applying Transformer to recommendation systems: (1) Transformer scaling is not ideal with increased computational resources, due to structural incompatibilities with recommendation-specific features such as multi-source data heterogeneity; (2) critical online inference latency constraints (tens of milliseconds) that intensify with longer user behavior sequences and growing computational demands. We propose Climber, an efficient recommendation framework comprising two synergistic components: the model architecture for efficient scaling and the co-designed acceleration techniques. Our proposed model adopts two core innovations: (1) multi-scale sequence extraction that achieves a time complexity reduction by a constant factor, enabling more efficient scaling with sequence length; (2) dynamic temperature modulation adapting attention distributions to the multi-scenario and multi-behavior patterns. Complemented by acceleration techniques, Climber achieves a 5.15$\times$ throughput gain without performance degradation by adopting a "single user, multiple item" batched processing and memory-efficient Key-Value caching. Comprehensive offline experiments on multiple datasets validate that Climber exhibits a more ideal scaling curve. To our knowledge, this is the first publicly documented framework where controlled model scaling drives continuous online metric growth (12.19\% overall lift) without prohibitive resource costs. Climber has been successfully deployed on Netease Cloud Music, one of China's largest music streaming platforms, serving tens of millions of users daily.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T01:40:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3746252.3761561' target='_blank'>doi</a><a href='http://arxiv.org/abs/2502.09888v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.09888v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource
  Allocation and Markov Decision Process in Named Data Networking (NDN)</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fatemeh Roshanzadeh, Hamid Barati, Ali Barati
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Named Data Networking (NDN) represents a transformative shift in network architecture, prioritizing content names over host addresses to enhance data dissemination. Efficient queue and resource management are critical to NDN performance, especially under dynamic and high-traffic conditions. This paper introduces DRR-MDPF, a novel hybrid strategy that integrates the Markov Decision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR) algorithm. MDPF enables routers to intelligently predict optimal forwarding decisions based on key metrics such as bandwidth, delay, and the number of unsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation among competing data flows. The proposed method models each router as a learning agent capable of adjusting its strategies through continuous feedback and probabilistic updates. Simulation results using ndnSIM demonstrate that DRR-MDPF significantly outperforms state-of-the-art strategies including SAF, RFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest Satisfaction Rate (ISR), packet drop rate, content retrieval time, and load balancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and heavy traffic, offering enhanced adaptability and lower computational complexity due to its single-path routing design. Furthermore, its multi-metric decision-making capability enables more accurate interface selection, leading to optimized network performance. Overall, DRR-MDPF serves as an intelligent, adaptive, and scalable queue management solution for NDN, effectively addressing core challenges such as resource allocation, congestion control, and route optimization in dynamic networking environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T21:05:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20272v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20272v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 SpeedMalloc: Improving Multi-threaded Applications via a Lightweight
  Core for Memory Allocation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruihao Li, Qinzhe Wu, Krishna Kavi, Gayatri Mehta, Jonathan C. Beard, Neeraja J. Yadwadkar, Lizy K. John
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Memory allocation, though constituting only a small portion of the executed code, can have a "butterfly effect" on overall program performance, leading to significant and far-reaching impacts. Despite accounting for just approximately 5% of total instructions, memory allocation can result in up to a 2.7x performance variation depending on the allocator used. This effect arises from the complexity of memory allocation in modern multi-threaded multi-core systems, where allocator metadata becomes intertwined with user data, leading to cache pollution or increased cross-thread synchronization overhead. Offloading memory allocators to accelerators, e.g., Mallacc and Memento, is a potential direction to improve the allocator performance and mitigate cache pollution. However, these accelerators currently have limited support for multi-threaded applications, and synchronization between cores and accelerators remains a significant challenge.   We present SpeedMalloc, using a lightweight support-core to process memory allocation tasks in multi-threaded applications. The support-core is a lightweight programmable processor with efficient cross-core data synchronization and houses all allocator metadata in its own caches. This design minimizes cache conflicts with user data and eliminates the need for cross-core metadata synchronization. In addition, using a general-purpose core instead of domain-specific accelerators makes SpeedMalloc capable of adopting new allocator designs. We compare SpeedMalloc with state-of-the-art software and hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and Memento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on multithreaded workloads over these five allocators, respectively.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T20:18:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20253v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20253v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 Apple Intelligence Foundation Language Models: Tech Report 2025</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ethan Li, Anders Boesen Lindbo Larsen, Chen Zhang, Xiyou Zhou, Jun Qin, Dian Ang Yap, Narendran Raghavan, Xuankai Chang, Margit Bowler, Eray Yildiz, John Peebles, Hannah Gillis Coleman, Matteo Ronchi, Peter Gray, Keen You, Anthony Spalvieri-Kruse, Ruoming Pang, Reed Li, Yuli Yang, Emad Soroush, Zhiyun Lu, Crystal Xiao, Rong Situ, Jordan Huffaker, David Griffiths, Zaid Ahmed, Peng Zhang, Daniel Parilla, Asaf Liberman, Jennifer Mallalieu, Parsa Mazaheri, Qibin Chen, Manjot Bilkhu, Aonan Zhang, Eric Wang, Dave Nelson, Michael FitzMaurice, Thomas Voice, Jeremy Liu, Josh Shaffer, Shiwen Zhao, Prasanth Yadla, Farzin Rasteh, Pengsheng Guo, Arsalan Farooq, Jeremy Snow, Stephen Murphy, Tao Lei, Minsik Cho, George Horrell, Sam Dodge, Lindsay Hislop, Sumeet Singh, Alex Dombrowski, Aiswarya Raghavan, Sasha Sirovica, Mandana Saebi, Faye Lao, Max Lam, TJ Lu, Zhaoyang Xu, Karanjeet Singh, Marc Kirchner, David Mizrahi, Rajat Arora, Haotian Zhang, Henry Mason, Lawrence Zhou, Yi Hua, Ankur Jain, Felix Bai, Joseph Astrauskas, Floris Weers, Josh Gardner, Mira Chiang, Yi Zhang, Pulkit Agrawal, Tony Sun, Quentin Keunebroek, Matthew Hopkins, Bugu Wu, Tao Jia, Chen Chen, Xingyu Zhou, Nanzhu Wang, Peng Liu, Ruixuan Hou, Rene Rauch, Yuan Gao, Afshin Dehghan, Jonathan Janke, Zirui Wang, Cha Chen, Xiaoyi Ren, Feng Nan, Josh Elman, Dong Yin, Yusuf Goren, Jeff Lai, Yiran Fei, Syd Evans, Muyang Yu, Guoli Yin, Yi Qin, Erin Feldman, Isha Garg, Aparna Rajamani, Karla Vega, Walker Cheng, TJ Collins, Hans Han, Raul Rea Menacho, Simon Yeung, Sophy Lee, Phani Mutyala, Ying-Chang Cheng, Zhe Gan, Sprite Chu, Justin Lazarow, Alessandro Pappalardo, Federico Scozzafava, Jing Lu, Erik Daxberger, Laurent Duchesne, Jen Liu, David Güera, Stefano Ligas, Mary Beth Kery, Brent Ramerth, Ciro Sannino, Marcin Eichner, Haoshuo Huang, Rui Qian, Moritz Schwarzer-Becker, David Riazati, Mingfei Gao, Bailin Wang, Jack Cackler, Yang Lu, Ransen Niu, John Dennison, Guillaume Klein, Jeffrey Bigham, Deepak Gopinath, Navid Shiee, Darren Botten, Guillaume Tartavel, Alex Guillen Garcia, Sam Xu, Victoria MönchJuan Haladjian, Zi-Yi Dou, Matthias Paulik, Adolfo Lopez Mendez, Zhen Li, Hong-You Chen, Chao Jia, Dhaval Doshi, Zhengdong Zhang, Raunak Manjani, Aaron Franklin, Zhile Ren, David Chen, Artsiom Peshko, Nandhitha Raghuram, Hans Hao, Jiulong Shan, Kavya Nerella, Ramsey Tantawi, Vivek Kumar, Saiwen Wang, Brycen Wershing, Bhuwan Dhingra, Dhruti Shah, Ob Adaranijo, Xin Zheng, Tait Madsen, Hadas Kotek, Chang Liu, Yin Xia, Hanli Li, Suma Jayaram, Yanchao Sun, Ahmed Fakhry, Vasileios Saveris, Dustin Withers, Yanghao Li, Alp Aygar, Andres Romero Mier Y Teran, Kaiwei Huang, Mark Lee, Xiujun Li, Yuhong Li, Tyler Johnson, Jay Tang, Joseph Yitan Cheng, Futang Peng, Andrew Walkingshaw, Lucas Guibert, Abhishek Sharma, Cheng Shen, Piotr Maj, Yasutaka Tanaka, You-Cyuan Jhang, Vivian Ma, Tommi Vehvilainen, Kelvin Zou, Jeff Nichols, Matthew Lei, David Qiu, Yihao Qian, Gokul Santhanam, Wentao Wu, Yena Han, Dominik Moritz, Haijing Fu, Mingze Xu, Vivek Rathod, Jian Liu, Louis D'hauwe, Qin Ba, Haitian Sun, Haoran Yan, Philipp Dufter, Anh Nguyen, Yihao Feng, Emma Wang, Keyu He, Rahul Nair, Sanskruti Shah, Jiarui Lu, Patrick Sonnenberg, Jeremy Warner, Yuanzhi Li, Bowen Pan, Ziyi Zhong, Joe Zhou, Sam Davarnia, Olli Saarikivi, Irina Belousova, Rachel Burger, Shang-Chen Wu, Di Feng, Bas Straathof, James Chou, Yuanyang Zhang, Marco Zuliani, Eduardo Jimenez, Abhishek Sundararajan, Xianzhi Du, Chang Lan, Nilesh Shahdadpuri, Peter Grasch, Sergiu Sima, Josh Newnham, Varsha Paidi, Jianyu Wang, Kaelen Haag, Alex Braunstein, Daniele Molinari, Richard Wei, Brenda Yang, Nicholas Lusskin, Joanna Arreaza-Taylor, Meng Cao, Nicholas Seidl, Simon Wang, Jiaming Hu, Yiping Ma, Mengyu Li, Kieran Liu, Hang Su, Sachin Ravi, Chong Wang, Xin Wang, Kevin Smith, Haoxuan You, Binazir Karimzadeh, Rui Li, Jinhao Lei, Wei Fang, Alec Doane, Sam Wiseman, Ismael Fernandez, Jane Li, Andrew Hansen, Javier Movellan, Christopher Neubauer, Hanzhi Zhou, Chris Chaney, Nazir Kamaldin, Valentin Wolf, Fernando Bermúdez-Medina, Joris Pelemans, Peter Fu, Howard Xing, Xiang Kong, Wayne Shan, Gabriel Jacoby-Cooper, Dongcai Shen, Tom Gunter, Guillaume Seguin, Fangping Shi, Shiyu Li, Yang Xu, Areeba Kamal, Dan Masi, Saptarshi Guha, Qi Zhu, Jenna Thibodeau, Changyuan Zhang, Rebecca Callahan, Charles Maalouf, Wilson Tsao, Boyue Li, Qingqing Cao, Naomy Sabo, Cheng Leong, Yi Wang, Anupama Mann Anupama, Colorado Reed, Kenneth Jung, Zhifeng Chen, Mohana Prasad Sathya Moorthy, Yifei He, Erik Hornberger, Devi Krishna, Senyu Tong, Michael, Lee, David Haldimann, Yang Zhao, Bowen Zhang, Chang Gao, Chris Bartels, Sushma Rao, Nathalie Tran, Simon Lehnerer, Co Giang, Patrick Dong, Junting Pan, Biyao Wang, Dongxu Li, Mehrdad Farajtabar, Dongseong Hwang, Grace Duanmu, Eshan Verma, Sujeeth Reddy, Qi Shan, Hongbin Gao, Nan Du, Pragnya Sridhar, Forrest Huang, Yingbo Wang, Nikhil Bhendawade, Diane Zhu, Sai Aitharaju, Fred Hohman, Lauren Gardiner, Chung-Cheng Chiu, Yinfei Yang, Alper Kokmen, Frank Chu, Ke Ye, Kaan Elgin, Oron Levy, John Park, Donald Zhang, Eldon Schoop, Nina Wenzel, Michael Booker, Hyunjik Kim, Chinguun Erdenebileg, Nan Dun, Eric Liang Yang, Priyal Chhatrapati, Vishaal Mahtani, Haiming Gang, Kohen Chia, Deepa Seshadri, Donghan Yu, Yan Meng, Kelsey Peterson, Zhen Yang, Yongqiang Wang, Carina Peng, Doug Kang, Anuva Agarwal, Albert Antony, Juan Lao Tebar, Albin Madappally Jose, Regan Poston, Andy De Wang, Gerard Casamayor, Elmira Amirloo, Violet Yao, Wojciech Kryscinski, Kun Duan, Lezhi L
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce two multilingual, multimodal foundation language models that power Apple Intelligence features across Apple devices and services: i a 3B-parameter on-device model optimized for Apple silicon through architectural innovations such as KV-cache sharing and 2-bit quantization-aware training; and ii a scalable server model built on a novel Parallel-Track Mixture-of-Experts PT-MoE transformer that combines track parallelism, mixture-of-experts sparse computation, and interleaved global-local attention to deliver high quality with competitive cost on Apple's Private Cloud Compute platform. Both models are trained on large-scale multilingual and multimodal datasets sourced via responsible web crawling, licensed corpora, and high-quality synthetic data, then further refined with supervised fine-tuning and reinforcement learning on a new asynchronous platform. The resulting models support several additional languages while understanding images and executing tool calls. In public benchmarks and human evaluations, both the server model and the on-device model match or surpass comparably sized open baselines.   A new Swift-centric Foundation Models framework exposes guided generation, constrained tool calling, and LoRA adapter fine-tuning, allowing developers to integrate these capabilities with a few lines of code. The latest advancements in Apple Intelligence models are grounded in our Responsible AI approach with safeguards like content filtering and locale-specific evaluation, as well as our commitment to protecting our users' privacy with innovations like Private Cloud Compute.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T16:34:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.13575v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.13575v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 Re-thinking Memory-Bound Limitations in CGRAs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiangfeng Liu, Zhe Jiang, Anzhen Zhu, Xiaomeng Han, Mingsong Lyu, Qingxu Deng, Nan Guan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators commonly employed to boost performance in workloads with iterative structures. Existing research typically focuses on compiler or architecture optimizations aimed at improving CGRA performance, energy efficiency, flexibility, and area utilization, under the idealistic assumption that kernels can access all data from Scratchpad Memory (SPM). However, certain complex workloads-particularly in fields like graph analytics, irregular database operations, and specialized forms of high-performance computing (e.g., unstructured mesh simulations)-exhibit irregular memory access patterns that hinder CGRA utilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To address this challenge, we conduct a thorough analysis of the underlying causes of performance degradation, then propose a redesigned memory subsystem and refine the memory model. With both microarchitectural and theoretical optimization, our solution can effectively manage irregular memory accesses through CGRA-specific runahead execution mechanism and cache reconfiguration techniques. Our results demonstrate that we can achieve performance comparable to the original SPM-only system while requiring only 1.27% of the storage size. The runahead execution mechanism achieves an average 3.04x speedup (up to 6.91x), with cache reconfiguration technique providing an additional 6.02% improvement, significantly enhancing CGRA performance for irregular memory access patterns.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T12:13:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>B.3.0; B.6.0</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3760386' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.09570v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.09570v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 Spotlight Attention: Towards Efficient LLM Generation via Non-linear
  Hashing-based KV Cache Retrieval</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wenhao Li, Yuxin Zhang, Gen Luo, Haiyuan Wan, Ziyang Gong, Fei Chao, Rongrong Ji
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reducing the key-value (KV) cache burden in Large Language Models (LLMs) significantly accelerates inference. Dynamically selecting critical KV caches during decoding helps maintain performance. Existing methods use random linear hashing to identify important tokens, but this approach is inefficient due to the orthogonal distribution of queries and keys within two narrow cones in LLMs. We introduce Spotlight Attention, a novel method that employs non-linear hashing functions to optimize the embedding distribution of queries and keys, enhancing coding efficiency and robustness. We also developed a lightweight, stable training framework using a Bradley-Terry ranking-based loss, enabling optimization of the non-linear hashing module on GPUs with 16GB memory in 8 hours. Experimental results show that Spotlight Attention drastically improves retrieval precision while shortening the length of the hash code at least 5$\times$ compared to traditional linear hashing. Finally, we exploit the computational advantages of bitwise operations by implementing specialized CUDA kernels, achieving hashing retrieval for 512K tokens in under 100$\mu$s on a single A100 GPU, with end-to-end throughput up to 3$\times$ higher than vanilla decoding.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T10:11:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19740v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19740v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed
  Criticality Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Diogo Costa, Jose Martins, Sandro Pinto
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate heterogeneous computing platforms, combining general-purpose processors with specialized accelerators such as AI engines, GPUs, and high-speed networking interfaces. This heterogeneity introduces challenges, as these accelerators and DMA-capable devices act as independent bus masters, directly accessing memory. Consequently, ensuring both security and timing predictability in such environments becomes critical. To address these concerns, the Input-Output Memory Management Unit (IOMMU) plays a key role in mediating and regulating memory access, preventing unauthorized transactions while enforcing isolation and access control policies. While prior work has explored IOMMU-related side-channel vulnerabilities from a security standpoint, its role in performance interference remains largely unexplored. Moreover, many of the same architectural properties that enable side-channel leakage, such as shared TLBs, caching effects, and translation overheads, can also introduce timing unpredictability. In this work, we analyze the contention effects within IOMMU structures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how their shared nature introduce unpredictable delays. Our findings reveal that IOMMU-induced interference primarily affects small memory transactions, where translation overheads significantly impact execution time. Additionally, we hypothesize that contention effects arising from IOTLBs exhibit similar behavior across architectures due to shared caching principles, such as prefetching and hierarchical TLB structures. Notably, our experiments show that IOMMU interference can delay DMA transactions by up to 1.79x for lower-size transfers on the Arm SMMUv2 implementation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T08:30:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19670v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19670v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 FiRST: Finetuning Router-Selective Transformers for Input-Adaptive
  Latency Reduction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Akriti Jain, Saransh Sharma, Koyel Mukherjee, Soumyabrata Pal
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Auto-regressive Large Language Models (LLMs) demonstrate remarkable performance across different domains such as vision and language processing. However, due to sequential processing through a stack of transformer layers, autoregressive decoding faces significant computation/latency challenges, particularly in resource-constrained environments like mobile and edge devices. Existing approaches in literature that aim to improve latency via skipping layers have two distinct flavors - 1) Early exit, and 2) Input-agnostic heuristics where tokens exit at pre-determined layers irrespective of input sequence. Both the above strategies have limitations - the former cannot be applied to handle KV Caching necessary for speed-ups in modern framework and the latter does not capture the variation in layer importance across tasks or more generally, across input sequences. To address both limitations, we propose FiRST, an algorithm that reduces inference latency by using layer-specific routers to select a subset of transformer layers adaptively for each input sequence - the prompt (during the prefill stage) decides which layers will be skipped during decoding. FiRST preserves compatibility with KV caching enabling faster inference while being quality-aware. FiRST is model-agnostic and can be easily enabled on any pre-trained LLM. Our approach reveals that input adaptivity is critical - indeed, different task-specific middle layers play a crucial role in evolving hidden representations depending on tasks. Extensive experiments show that FiRST significantly reduces latency while outperforming other layer selection strategies in quality metics. It retains competitive performance to base model (without layer skipping) and in some cases, even improves upon it. FiRST is thus a promising and efficient solution for LLM deployment in low-resource environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-27T04:58:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.12513v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.12513v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D
  Space</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lin Li, Zehuan Huang, Haoran Feng, Gengxiong Zhuang, Rui Chen, Chunchao Guo, Lu Sheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> 3D local editing of specified regions is crucial for game industry and robot interaction. Recent methods typically edit rendered multi-view images and then reconstruct 3D models, but they face challenges in precisely preserving unedited regions and overall coherence. Inspired by structured 3D generative models, we propose VoxHammer, a novel training-free approach that performs precise and coherent editing in 3D latent space. Given a 3D model, VoxHammer first predicts its inversion trajectory and obtains its inverted latents and key-value tokens at each timestep. Subsequently, in the denoising and editing phase, we replace the denoising features of preserved regions with the corresponding inverted latents and cached key-value tokens. By retaining these contextual features, this approach ensures consistent reconstruction of preserved areas and coherent integration of edited parts. To evaluate the consistency of preserved regions, we constructed Edit3D-Bench, a human-annotated dataset comprising hundreds of samples, each with carefully labeled 3D editing regions. Experiments demonstrate that VoxHammer significantly outperforms existing methods in terms of both 3D consistency of preserved regions and overall quality. Our method holds promise for synthesizing high-quality edited paired data, thereby laying the data foundation for in-context 3D generation. See our project page at https://huanngzh.github.io/VoxHammer-Page/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-26T17:59:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19247v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19247v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 Enabling MoE on the Edge via Importance-Driven Expert Scheduling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guoying Zhu, Meng Li, Haipeng Dai, Xuechen Liu, Weijun Wang, Keran Li, Jun xiao, Ligeng Chen, Wei Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Mixture of Experts (MoE) architecture has emerged as a key technique for scaling Large Language Models by activating only a subset of experts per query. Deploying MoE on consumer-grade edge hardware, however, is constrained by limited device memory, making dynamic expert offloading essential. Unlike prior work that treats offloading purely as a scheduling problem, we leverage expert importance to guide decisions, substituting low-importance activated experts with functionally similar ones already cached in GPU memory, thereby preserving accuracy. As a result, this design reduces memory usage and data transfer, while largely eliminating PCIe overhead. In addition, we introduce a scheduling policy that maximizes the reuse ratio of GPU-cached experts, further boosting efficiency. Extensive evaluations show that our approach delivers 48% lower decoding latency with over 60% expert cache hit rate, while maintaining nearly lossless accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-26T12:32:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18983v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18983v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 Rethinking Caching for LLM Serving Systems: Beyond Traditional
  Heuristics</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jungwoo Kim, Minsang Kim, Jaeheon Lee, Chanwoo Moon, Heejin Kim, Taeho Hwang, Woosuk Chung, Yeseong Kim, Sungjin Lee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Serving Large Language Models (LLMs) at scale requires meeting strict Service Level Objectives (SLOs) under severe computational and memory constraints. Nevertheless, traditional caching strategies fall short: exact-matching and prefix caches neglect query semantics, while state-of-the-art semantic caches remain confined to traditional intuitions, offering little conceptual departure. Building on this, we present SISO, a semantic caching system that redefines efficiency for LLM serving. SISO introduces centroid-based caching to maximize coverage with minimal memory, locality-aware replacement to preserve high-value entries, and dynamic thresholding to balance accuracy and latency under varying workloads. Across diverse datasets, SISO delivers up to 1.71$\times$ higher hit ratios and consistently stronger SLO attainment compared to state-of-the-art systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-26T07:09:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18736v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18736v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 Physical Autoregressive Model for Robotic Manipulation without Action
  Pretraining</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zijian Song, Sihan Qin, Tianshui Chen, Liang Lin, Guangrun Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The scarcity of manipulation data has motivated the use of pretrained large models from other modalities in robotics. In this work, we build upon autoregressive video generation models to propose a Physical Autoregressive Model (PAR), where physical tokens combine frames and actions to represent the joint evolution of the robot and its environment. PAR leverages the world knowledge embedded in video pretraining to understand physical dynamics without requiring action pretraining, enabling accurate video prediction and consistent action trajectories. It also adopts a DiT-based de-tokenizer to model frames and actions as continuous tokens, mitigating quantization errors and facilitating mutual enhancement. Furthermore, we incorporate a causal mask with inverse kinematics, parallel training, and the KV-cache mechanism to further improve performance and efficiency. Experiments on the ManiSkill benchmark show that PAR achieves a 100\% success rate on the PushCube task, matches the performance of action-pretrained baselines on other tasks, and accurately predicts future videos with tightly aligned action trajectories. These findings underscore a promising direction for robotic manipulation by transferring world knowledge from autoregressive video pretraining. The project page is here: https://hcplab-sysu.github.io/PhysicalAutoregressiveModel/
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-26T03:23:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.09822v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.09822v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 Krul: Efficient State Restoration for Multi-turn Conversations with
  Dynamic Cross-layer KV Sharing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junyi Wen, Junyuan Liang, Zicong Hong, Wuhui Chen, Ting Cai, Zibin Zheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Efficient state restoration in multi-turn conversations with large language models (LLMs) remains a critical challenge, primarily due to the overhead of recomputing or loading full key-value (KV) caches for all historical tokens. To address this, existing approaches compress KV caches across adjacent layers with highly similar attention patterns. However, these methods often apply a fixed compression scheme across all conversations, selecting the same layer pairs for compression without considering conversation-specific attention dynamics. This static strategy overlooks variability in attention pattern similarity across different conversations, which can lead to noticeable accuracy degradation.   We present Krul, a multi-turn LLM inference system that enables accurate and efficient KV cache restoration. Krul dynamically selects compression strategies based on attention similarity across layer pairs and uses a recomputation-loading pipeline to restore the KV cache. It introduces three key innovations: 1) a preemptive compression strategy selector to preserve critical context for future conversation turns and selects a customized strategy for the conversation; 2) a token-wise heterogeneous attention similarity estimator to mitigate the attention similarity computation and storage overhead during model generation; 3) a bubble-free restoration scheduler to reduce potential bubbles brought by the imbalance of recomputing and loading stream due to compressed KV caches. Empirical evaluations on real-world tasks demonstrate that Krul achieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x reduction in KV cache storage compared to state-of-the-art methods without compromising generation quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-26T01:55:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.08045v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.08045v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhuoping Yang, Jinming Zhuang, Xingzhen Chen, Alex K. Jones, Peipei Zhou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> GPUs are critical for compute-intensive applications, yet emerging workloads such as recommender systems, graph analytics, and data analytics often exceed GPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as external memory, and the GPU-centric approach enables GPU threads to directly issue NVMe requests, further avoiding CPU intervention. However, current GPU-centric approaches adopt synchronous I/O, forcing threads to stall during long communication delays.   We propose AGILE, a lightweight asynchronous GPU-centric I/O library that eliminates deadlock risks and integrates a flexible HBM-based software cache. AGILE overlaps computation and I/O, improving performance by up to 1.88$\times$ across workloads with diverse computation-to-communication ratios. Compared to BaM on DLRM, AGILE achieves up to 1.75$\times$ speedup through efficient design and overlapping; on graph applications, AGILE reduces software cache overhead by up to 3.12$\times$ and NVMe I/O overhead by up to 2.85$\times$; AGILE also lowers per-thread register usage by up to 1.32$\times$.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-26T01:45:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.19365v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.19365v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 Strata: Hierarchical Context Caching for Long Context Language Model
  Serving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhiqiang Xie, Ziyi Xu, Mark Zhao, Yuwei An, Vikram Sharma Mailthody, Scott Mahlke, Michael Garland, Christos Kozyrakis
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) with expanding context windows face significant performance hurdles. While caching key-value (KV) states is critical for avoiding redundant computation, the storage footprint of long-context caches quickly exceeds GPU memory capacity, forcing production systems to adopt hierarchical caching across memory hierarchies. However, transferring large cached contexts back to the GPU introduces severe performance bottlenecks: fragmented I/O from paged layouts prevents full bandwidth utilization, and existing schedulers fail to account for cache-loading delays, leaving systems loading-bound rather than compute-bound. We present Strata, a hierarchical context caching framework designed for efficient long context LLM serving. Strata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling GPU and CPU memory layouts and employs cache-aware request scheduling to balance compute with I/O latency and overlapping unavoidable stalls with complementary tasks. Built on SGLang and deployed in production, Strata achieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache and 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without degrading short-context performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-26T00:09:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18572v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18572v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 Real-time 3D Visualization of Radiance Fields on Light Field Displays</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jonghyun Kim, Cheng Sun, Michael Stengel, Matthew Chan, Andrew Russell, Jaehyun Jung, Wil Braithwaite, Shalini De Mello, David Luebke
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Radiance fields have revolutionized photo-realistic 3D scene visualization by enabling high-fidelity reconstruction of complex environments, making them an ideal match for light field displays. However, integrating these technologies presents significant computational challenges, as light field displays require multiple high-resolution renderings from slightly shifted viewpoints, while radiance fields rely on computationally intensive volume rendering. In this paper, we propose a unified and efficient framework for real-time radiance field rendering on light field displays. Our method supports a wide range of radiance field representations, including NeRFs, 3D Gaussian Splatting, and Sparse Voxels, within a shared architecture based on a single-pass plane sweeping strategy and caching of shared, non-directional components. The framework generalizes across different scene formats without retraining, and avoids redundant computation across views. We further demonstrate a real-time interactive application on a Looking Glass display, achieving 200+ FPS at 512p across 45 views, enabling seamless, immersive 3D interaction. On standard benchmarks, our method achieves up to 22x speedup compared to independently rendering each view, while preserving image quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T22:21:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.GR</span><span>eess.IV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18540v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18540v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 DiskJoin: Large-scale Vector Similarity Join with SSD</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yanqi Chen, Xiao Yan, Alexandra Meliou, Eric Lo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Similarity join--a widely used operation in data science--finds all pairs of items that have distance smaller than a threshold. Prior work has explored distributed computation methods to scale similarity join to large data volumes but these methods require a cluster deployment, and efficiency suffers from expensive inter-machine communication. On the other hand, disk-based solutions are more cost-effective by using a single machine and storing the large dataset on high-performance external storage, such as NVMe SSDs, but in these methods the disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin, the first disk-based similarity join algorithm that can process billion-scale vector datasets efficiently on a single machine. DiskJoin improves disk I/O by tailoring the data access patterns to avoid repetitive accesses and read amplification. It also uses main memory as a dynamic cache and carefully manages cache eviction to improve cache hit rate and reduce disk retrieval time. For further acceleration, we adopt a probabilistic pruning technique that can effectively prune a large number of vector pairs from computation. Our evaluation on real-world, large-scale datasets shows that DiskJoin significantly outperforms alternatives, achieving speedups from 50x to 1000x.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T21:07:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18494v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18494v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 MARM: Unlocking the Future of Recommendation Systems through Memory
  Augmentation and Scalable Complexity</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiao Lv, Jiangxia Cao, Shijie Guan, Xiaoyou Zhou, Zhiguang Qi, Yaqiang Zang, Ming Li, Ben Wang, Kun Gai, Guorui Zhou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Scaling-law has guided the language model designing for past years, however, it is worth noting that the scaling laws of NLP cannot be directly applied to RecSys due to the following reasons: (1) The amount of training samples and model parameters is typically not the bottleneck for the model. Our recommendation system can generate over 50 billion user samples daily, and such a massive amount of training data can easily allow our model parameters to exceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the stability and robustness of the recommendation system, it is essential to control computational complexity FLOPs carefully. Considering the above differences with LLM, we can draw a conclusion that: for a RecSys model, compared to model parameters, the computational complexity FLOPs is a more expensive factor that requires careful control. In this paper, we propose our milestone work, MARM (Memory Augmented Recommendation Model), which explores a new cache scaling-laws successfully.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T15:48:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span><span>N/A</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.09425v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.09425v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 ILRe: Intermediate Layer Retrieval for Context Compression in Causal
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Manlai Liang, Mandi Liu, Jiangzhou Ji, Huaijun Li, Haobo Yang, Yaohan He, Jinlong Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have demonstrated success across many benchmarks. However, they still exhibit limitations in long-context scenarios, primarily due to their short effective context length, quadratic computational complexity, and high memory overhead when processing lengthy inputs. To mitigate these issues, we introduce a novel context compression pipeline, called Intermediate Layer Retrieval (ILRe), which determines one intermediate decoder layer offline, encodes context by streaming chunked prefill only up to that layer, and recalls tokens by the attention scores between the input query and full key cache in that specified layer. In particular, we propose a multi-pooling kernels allocating strategy in the token recalling process to maintain the completeness of semantics. Our approach not only reduces the prefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance comparable to or better than the full context in the long context scenarios. Without additional post training or operator development, ILRe can process a single $1M$ tokens request in less than half a minute (speedup $\approx 180\times$) and scores RULER-$1M$ benchmark of $\approx 79.8$ with model Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T10:59:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17892v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17892v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 SuperGen: An Efficient Ultra-high-resolution Video Generation System
  with Sketching and Tiling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fanjiang Ye, Zepeng Zhao, Yi Mu, Jucheng Shen, Renjie Li, Kaijian Wang, Desen Sun, Saurabh Agarwal, Myungjin Lee, Triston Cao, Aditya Akella, Arvind Krishnamurthy, T. S. Eugene Ng, Zhengzhong Tu, Yuke Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion models have recently achieved remarkable success in generative tasks (e.g., image and video generation), and the demand for high-quality content (e.g., 2K/4K videos) is rapidly increasing across various domains. However, generating ultra-high-resolution videos on existing standard-resolution (e.g., 720p) platforms remains challenging due to the excessive re-training requirements and prohibitively high computational and memory costs. To this end, we introduce SuperGen, an efficient tile-based framework for ultra-high-resolution video generation. SuperGen features a novel training-free algorithmic innovation with tiling to successfully support a wide range of resolutions without additional training efforts while significantly reducing both memory footprint and computational complexity. Moreover, SuperGen incorporates a tile-tailored, adaptive, region-aware caching strategy that accelerates video generation by exploiting redundancy across denoising steps and spatial regions. SuperGen also integrates cache-guided, communication-minimized tile parallelism for enhanced throughput and minimized latency. Evaluations demonstrate that SuperGen harvests the maximum performance gains while achieving high output quality across various benchmarks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T07:49:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17756v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17756v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 OmniCache: A Trajectory-Oriented Global Perspective on Training-Free
  Cache Reuse for Diffusion Transformer Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Huanpeng Chu, Wei Wu, Guanyu Fen, Yutao Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion models have emerged as a powerful paradigm for generative tasks such as image synthesis and video generation, with Transformer architectures further enhancing performance. However, the high computational cost of diffusion Transformers-stemming from a large number of sampling steps and complex per-step computations-presents significant challenges for real-time deployment. In this paper, we introduce OmniCache, a training-free acceleration method that exploits the global redundancy inherent in the denoising process. Unlike existing methods that determine caching strategies based on inter-step similarities and tend to prioritize reusing later sampling steps, our approach originates from the sampling perspective of DIT models. We systematically analyze the model's sampling trajectories and strategically distribute cache reuse across the entire sampling process. This global perspective enables more effective utilization of cached computations throughout the diffusion trajectory, rather than concentrating reuse within limited segments of the sampling procedure. In addition, during cache reuse, we dynamically estimate the corresponding noise and filter it out to reduce its impact on the sampling direction. Extensive experiments demonstrate that our approach accelerates the sampling process while maintaining competitive generative quality, offering a promising and practical solution for efficient deployment of diffusion-based generative models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T03:07:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.16212v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.16212v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters
  at Scale</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ge Shi, Hanieh Sadri, Qian Wang, Yu Zhang, Ying Xiong, Yong Zhang, Zhenan Fan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large language models to enhance their task-specific performance by selectively tuning the top-activated experts for the task. Serving these fine-tuned models at scale is challenging: deploying merged models in isolation is prohibitively resource-hungry, while existing multi-adapter serving systems with LoRA-style additive updates are incompatible with ESFT's expert-oriented paradigm. We present ExpertWeave, a system that serves multiple ESFT adapters concurrently over a single shared MoE base model, drastically reducing the memory footprint and improving resource utilization. To seamlessly integrate into existing inference pipelines for MoE models with non-intrusive modifications and minimal latency overhead, ExpertWeave introduces a virtual-memory-assisted expert weight manager that co-locates base-model and adapter experts without incurring memory overhead from fragmentation, and a fused kernel for batched rerouting to enable lightweight redirection of tokens to the appropriate experts at runtime. Our evaluations show that ExpertWeave can simultaneously serve multiple adapters of a 16B MoE model on a single accelerator where the baseline runs out of memory, or provides up to 94x more KV cache capacity and achieves up to 18% higher throughput while using comparable resources, all without compromising model accuracy. ExpertWeave maintains low overhead even when scaling to 20 adapters, with a 4-11% latency increase compared with serving the base model alone. Source code will be released soon.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T03:05:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17624v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17624v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated
  Prefill and Decode Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaojuan Tang, Fanxu Meng, Pingzhi Tang, Yuxuan Wang, Di Yin, Xing Sun, Muhan Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses key-value states into a low-rank latent vector, caching only this vector to reduce memory. In tensor parallelism (TP), however, attention heads are computed across multiple devices, and each device must load the full cache, eroding the advantage of MLA over Grouped Query Attention (GQA). We propose Tensor-Parallel Latent Attention (TPLA): a scheme that partitions both the latent representation and each head's input dimension across devices, performs attention independently per shard, and then combines results with an all-reduce. TPLA preserves the benefits of a compressed KV cache while unlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in TPLA still leverages the full latent representation, maintaining stronger representational capacity. TPLA is drop-in compatible with models pre-trained using MLA: it supports MLA-style prefilling and enables efficient tensor-parallel decoding without retraining. Applying simple orthogonal transforms -- e.g., the Hadamard transform or PCA -- before TP slicing further mitigates cross-shard interference, yielding minimal accuracy degradation. By reducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x and 1.93x speedups, respectively, at a 32K-token context length while maintaining performance on commonsense and LongBench benchmarks. TPLA can be implemented with FlashAttention-3, enabling practical end-to-end acceleration.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T02:24:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15881v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15881v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD
  NPUs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aadesh Deshmukh, Venkata Yaswanth Raparti, Samuel Hsu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformer-based deep learning models are increasingly deployed on energy, and DRAM bandwidth constrained devices such as laptops and gaming consoles, which presents significant challenges in meeting the latency requirements of the models. The industry is turning to neural processing units (NPUs) for superior performance-per-watt (perf/watt); however, efficiently mapping dynamic attention layers to the NPUs remains a challenging task. For optimizing perf/watt, AMD XDNA NPUs employ software managed caches and share system memory with host. This requires substantial engineering effort to unlock efficient tiling, buffer allocation, and data movement to extract the maximum efficiency from the device. This paper introduces Zen-Attention, a framework that optimizes DRAM bandwidth utilization in the attention layer of models by systematically exploring the complex design space of layer folding, tiling, and data-movement on the interconnect, and the tensor layouts to come up with an optimal solution. Our evaluation includes comparative analysis of end-to-end model latency and specific attention latency in each model. We demonstrate how the framework enhances mapping capabilities by varying input dimensions, which require padding and masking in the attention block. For representative transformer models, the Zen-Attention Framework achieves up to 4x improvement in the latency of the attention block and up to 32% improvement in end-to-end network latency compared to the baseline Unfolded- approaches.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T01:33:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17593v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17593v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 RT-Cache: Training-Free Retrieval for Real-Time Manipulation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Owen Kwon, Abraham George, Alison Bartsch, Amir Barati Farimani
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Real robots are expected to repeat the same behavior in new environments with very little new data, yet modern controllers either incur heavy per-step inference or require deployment-time fine-tuning. We propose RT-Cache, a training-free retrieval-as-control pipeline that caches diverse image action trajectories in a unified vector memory and, at test time, embeds the current frame to retrieve and replay multi-step snippets, replacing per-step model calls. A hierarchical search keeps lookups sub-second at million scale, shifting cost from compute to storage and enabling real-time control on modest GPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher success and lower completion time than strong retrieval baselines (approximately x2 higher success and ~30% faster in our settings), and a single-episode anchoring study shows immediate adaptation to a more complex, contact-rich task without fine-tuning. RT-Cache turns experience into an append-only memory, offering a simple, scalable path to few-shot deployment today and a foundation for multimodal keys and optional integration with high-level policies. Project page: https://rt-cache.github.io/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-25T00:15:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.AI</span><span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.09040v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.09040v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 PRISM: Efficient Long-Range Reasoning With Short-Context LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dulhan Jayalath, James Bradley Wendt, Nicholas Monath, Sandeep Tata, Beliz Gunel
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Long-range tasks demand reasoning over long inputs. However, existing solutions are limited, e.g., long-context models require large compute budgets, parameter-efficient fine-tuning (PEFT) needs training data, and retrieval-augmented generation (RAG) entails complex task-specific designs. Though in-context approaches overcome many of these issues, methods with short-context LLMs are inefficient, trading context for processing more tokens. We introduce PRISM, a highly token-efficient in-context method based on structured schemas that outperforms baselines on diverse tasks with 4x shorter contexts. This approach produces concise outputs and efficiently leverages key-value (KV) caches to reduce costs by up to 54%. PRISM scales down to tiny contexts without increasing costs or sacrificing quality, and generalizes to new tasks with minimal effort by generating schemas from task descriptions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-24T22:09:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.18914v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.18914v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 Evaluating Compiler Optimization Impacts on zkVM Performance</h2>
                <div class="authors">
                    <strong>Authors:</strong> Thomas Gassmann, Stefanos Chaliasos, Thodoris Sotiropoulos, Zhendong Su
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Zero-knowledge proofs (ZKPs) are the cornerstone of programmable cryptography. They enable (1) privacy-preserving and verifiable computation across blockchains, and (2) an expanding range of off-chain applications such as credential schemes. Zero-knowledge virtual machines (zkVMs) lower the barrier by turning ZKPs into a drop-in backend for standard compilation pipelines. This lets developers write proof-generating programs in conventional languages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits. However, these VMs inherit compiler infrastructures tuned for traditional architectures rather than for proof systems. In particular, standard compiler optimizations assume features that are absent in zkVMs, including cache locality, branch prediction, or instruction-level parallelism. Therefore, their impact on proof generation is questionable.   We present the first systematic study of the impact of compiler optimizations on zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an unoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero and SP1). While standard LLVM optimization levels do improve zkVM performance (over 40\%), their impact is far smaller than on traditional CPUs, since their decisions rely on hardware features rather than proof constraints. Guided by a fine-grained pass-level analysis, we~\emph{slightly} refine a small set of LLVM passes to be zkVM-aware, improving zkVM execution time by up to 45\% (average +4.6\% on RISC Zero, +1\% on SP1) and achieving consistent proving-time gains. Our work highlights the potential of compiler-level optimizations for zkVM performance and opens new direction for zkVM-specific passes, backends, and superoptimizers.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-24T20:51:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17518v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17518v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 Practical Insertion-Only Convex Hull</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ivor van der Hoog, Henrik Reinstädtler, Eva Rotenberg
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Convex hull data structures are fundamental in computational geometry. We study insertion-only data structures, supporting various containment and intersection queries. When $P$ is sorted by $x$- or $y$-coordinate, convex hulls can be constructed in linear time using classical algorithms such as Graham scan. We investigate a variety of methods tailored to the insertion-only setting. We explore a broad selection of trade-offs involving robustness, memory access patterns, and space usage, providing an extensive evaluation of both existing and novel techniques. Logarithmic-time methods rely on pointer-based tree structures, which suffer in practice due to poor memory locality. Motivated by this, we develop a vector-based solution inspired by Overmars' logarithmic method. Our structure has worse asymptotic bounds, supporting queries in $O(\log^2 n)$ time, but stores data in $O(\log n)$ contiguous vectors, greatly improving cache performance.   Through empirical evaluation on real-world and synthetic data sets, we uncover surprising trends. Let $h$ denote the size of the convex hull. We show that a na\"ive $O(h)$ insertion-only algorithm based on Graham scan consistently outperforms both theoretical and practical state-of-the-art methods under realistic workloads, even on data sets with rather large convex hulls. While tree-based methods with $O(\log h)$ update times offer solid theoretical guarantees, they are never optimal in practice. In contrast, our vector-based logarithmic method, despite its theoretically inferior bounds, is highly competitive across all tested scenarios. It is optimal whenever the convex hull becomes large.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-24T19:28:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17496v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17496v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 TreePO: Bridging the Gap of Policy Optimization and Efficacy and
  Inference Efficiency with Heuristic Tree-based Modeling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yizhi Li, Qingshui Gu, Zhoufutu Wen, Ziniu Li, Tianshun Xing, Shuyue Guo, Tianyu Zheng, Xin Zhou, Xingwei Qu, Wangchunshu Zhou, Zheng Zhang, Wei Shen, Qian Liu, Chenghua Lin, Jian Yang, Ge Zhang, Wenhao Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in aligning large language models via reinforcement learning have achieved remarkable gains in solving complex reasoning problems, but at the cost of expensive on-policy rollouts and limited exploration of diverse reasoning paths. In this work, we introduce TreePO, involving a self-guided rollout algorithm that views sequence generation as a tree-structured searching process. Composed of dynamic tree sampling policy and fixed-length segment decoding, TreePO leverages local uncertainty to warrant additional branches. By amortizing computation across common prefixes and pruning low-value paths early, TreePO essentially reduces the per-update compute burden while preserving or enhancing exploration diversity. Key contributions include: (1) a segment-wise sampling algorithm that alleviates the KV cache burden through contiguous segments and spawns new branches along with an early-stop mechanism; (2) a tree-based segment-level advantage estimation that considers both global and local proximal policy optimization. and (3) analysis on the effectiveness of probability and quality-driven dynamic divergence and fallback strategy. We empirically validate the performance gain of TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours from 22\% up to 43\% of the sampling design for the trained models, meanwhile showing up to 40\% reduction at trajectory-level and 35\% at token-level sampling compute for the existing models. While offering a free lunch of inference efficiency, TreePO reveals a practical path toward scaling RL-based post-training with fewer samples and less compute. Home page locates at https://m-a-p.ai/TreePO.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-24T16:52:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17445v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17445v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 TinySR: Pruning Diffusion for Real-World Image Super-Resolution</h2>
                <div class="authors">
                    <strong>Authors:</strong> Linwei Dong, Qingnan Fan, Yuhang Yu, Qi Zhang, Jinwei Chen, Yawei Luo, Changqing Zou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Real-world image super-resolution (Real-ISR) focuses on recovering high-quality images from low-resolution inputs that suffer from complex degradations like noise, blur, and compression. Recently, diffusion models (DMs) have shown great potential in this area by leveraging strong generative priors to restore fine details. However, their iterative denoising process incurs high computational overhead, posing challenges for real-time applications. Although one-step distillation methods, such as OSEDiff and TSD-SR, offer faster inference, they remain fundamentally constrained by their large, over-parameterized model architectures. In this work, we present TinySR, a compact yet effective diffusion model specifically designed for Real-ISR that achieves real-time performance while maintaining perceptual quality. We introduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy to facilitate more effective decision-making in depth pruning. We achieve VAE compression through channel pruning, attention removal and lightweight SepConv. We eliminate time- and prompt-related modules and perform pre-caching techniques to further speed up the model. TinySR significantly reduces computational cost and model size, achieving up to 5.68x speedup and 83% parameter reduction compared to its teacher TSD-SR, while still providing high quality results.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-24T16:17:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17434v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17434v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 DiCache: Let Diffusion Model Determine Its Own Cache</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiazi Bu, Pengyang Ling, Yujie Zhou, Yibin Wang, Yuhang Zang, Tong Wu, Dahua Lin, Jiaqi Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent years have witnessed the rapid development of acceleration techniques for diffusion models, especially caching-based acceleration methods. These studies seek to answer two fundamental questions: "When to cache" and "How to use cache", typically relying on predefined empirical laws or dataset-level priors to determine the timing of caching and utilizing handcrafted rules for leveraging multi-step caches. However, given the highly dynamic nature of the diffusion process, they often exhibit limited generalizability and fail on outlier samples. In this paper, a strong correlation is revealed between the variation patterns of the shallow-layer feature differences in the diffusion model and those of final model outputs. Moreover, we have observed that the features from different model layers form similar trajectories. Based on these observations, we present DiCache, a novel training-free adaptive caching strategy for accelerating diffusion models at runtime, answering both when and how to cache within a unified framework. Specifically, DiCache is composed of two principal components: (1) Online Probe Profiling Scheme leverages a shallow-layer online probe to obtain a stable prior for the caching error in real time, enabling the model to autonomously determine caching schedules. (2) Dynamic Cache Trajectory Alignment combines multi-step caches based on shallow-layer probe feature trajectory to better approximate the current feature, facilitating higher visual quality. Extensive experiments validate DiCache's capability in achieving higher efficiency and improved visual fidelity over state-of-the-art methods on various leading diffusion models including WAN 2.1, HunyuanVideo for video generation, and Flux for image generation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-24T13:30:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17356v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17356v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained
  Elastic Long-Context LLM Serving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bingyang Wu, Zili Zhang, Yinmin Zhong, Guanzhe Huang, Yibo Zhu, Xuanzhe Liu, Xin Jin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Prefix caching is crucial to accelerate multi-turn interactions and requests with shared prefixes. At the cluster level, existing prefix caching systems are tightly coupled with request scheduling to optimize cache efficiency and computation performance together, leading to load imbalance, data redundancy, and memory fragmentation of caching systems across instances. To address these issues, memory pooling is promising to shield the scheduler from the underlying cache management so that it can focus on the computation optimization. However, because existing prefix caching systems only transfer increasingly longer prefix caches between instances, they cannot achieve low-latency memory pooling.   To address these problems, we propose a unified segment-level prefix cache pool, TokenLake. It uses a declarative cache interface to expose requests' query tensors, prefix caches, and cache-aware operations to TokenLake for efficient pooling. Powered by this abstraction, TokenLake can manage prefix cache at the segment level with a heavy-hitter-aware load balancing algorithm to achieve better cache load balance, deduplication, and defragmentation. TokenLake also transparently minimizes the communication volume of query tensors and new caches. Based on TokenLake, the scheduler can schedule requests elastically by using existing techniques without considering prefix cache management. Evaluations on real-world workloads show that TokenLake can improve throughput by up to 2.6$\times$ and 2.0$\times$ and boost hit rate by 2.0$\times$ and 2.1$\times$, compared to state-of-the-art cache-aware routing and cache-centric PD-disaggregation solutions, respectively.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-24T05:45:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17219v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17219v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 DPad: Efficient Diffusion Language Models with Suffix Dropout</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xinhua Chen, Sitao Huang, Cong Guo, Chiyue Wei, Yintao He, Jianyi Zhang, Hai "Helen" Li, Yiran Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion-based Large Language Models (dLLMs) parallelize text generation by framing decoding as a denoising process, but suffer from high computational overhead since they predict all future suffix tokens at each step while retaining only a small fraction. We propose Diffusion Scratchpad (DPad), a training-free method that restricts attention to a small set of nearby suffix tokens, preserving fidelity while eliminating redundancy. DPad integrates two strategies: (i) a sliding window, which maintains a fixed-length suffix window, and (ii) distance-decay dropout, which deterministically removes distant suffix tokens before attention computation. This simple design is compatible with existing optimizations such as prefix caching and can be implemented with only a few lines of code. Comprehensive evaluations across multiple benchmarks on LLaDA-1.5 and Dream models demonstrate that DPad delivers up to $\mathbf{61.4\times}$ speedup over vanilla dLLMs while maintaining comparable accuracy, highlighting its potential for efficient and scalable long-sequence inference. Our code is available at https://github.com/Crys-Chen/DPad.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-23T20:28:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.14148v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.14148v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nishant Gavhane, Arush Mehrotra, Rohit Chawla, Peter Proenca
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The deployment of large-scale Mixture-of-Experts (MoE) models on edge devices presents significant challenges due to memory constraints. While MoE architectures enable efficient utilization of computational resources by activating only a subset of experts per inference, they require careful memory management to operate efficiently in resource-constrained environments. Traditional heuristic-based expert caching strategies such as MoE-Infinity struggle to maintain high cache hit rates as models parameters scale. In this work, we introduce MoE-Beyond, a learning-based expert activation predictor trained to predict expert activations during autoregressive decoding. By framing the task as a multi-label sequence prediction problem, we train a lightweight transformer model on 66 million expert activation traces extracted from LDJnr-Puffin dataset [5] using DeepSeek-V2-Chat-Lite MoE. Our predictor generalizes effectively across unseen prompts from WebGLM-QA dataset [6], achieving 97.5% accuracy and an 86.6% F1-score. Simulation results show that MoE-Beyond improves GPU cache hit rate from 17% to 72% when only 10% of experts fit in GPU cache, outperforming heuristic baselines.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-23T20:28:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17137v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17137v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 VQL: An End-to-End Context-Aware Vector Quantization Attention for
  Ultra-Long User Behavior Modeling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kaiyuan Li, Yongxiang Tang, Yanhua Cheng, Yong Bai, Yanxiang Zeng, Chao Wang, Xialong Liu, Peng Jiang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In large-scale recommender systems, ultra-long user behavior sequences encode rich signals of evolving interests. Extending sequence length generally improves accuracy, but directly modeling such sequences in production is infeasible due to latency and memory constraints. Existing solutions fall into two categories: (1) top-k retrieval, which truncates the sequence and may discard most attention mass when L >> k; and (2) encoder-based compression, which preserves coverage but often over-compresses and fails to incorporate key context such as temporal gaps or target-aware signals. Neither class achieves a good balance of low-loss compression, context awareness, and efficiency.   We propose VQL, a context-aware Vector Quantization Attention framework for ultra-long behavior modeling, with three innovations. (1) Key-only quantization: only attention keys are quantized, while values remain intact; we prove that softmax normalization yields an error bound independent of sequence length, and a codebook loss directly supervises quantization quality. This also enables L-free inference via offline caches. (2) Multi-scale quantization: attention heads are partitioned into groups, each with its own small codebook, which reduces quantization error while keeping cache size fixed. (3) Efficient context injection: static features (e.g., item category, modality) are directly integrated, and relative position is modeled via a separable temporal kernel. All context is injected without enlarging the codebook, so cached representations remain query-independent.   Experiments on three large-scale datasets (KuaiRand-1K, KuaiRec, TMALL) show that VQL consistently outperforms strong baselines, achieving higher accuracy while reducing inference latency, establishing a new state of the art in balancing accuracy and efficiency for ultra-long sequence recommendation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-23T19:58:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17125v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17125v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 Learned Structure in CARTRIDGES: Keys as Shareable Routers in
  Self-Studied Representations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Maurizio Diaz
                </div>
                <div class="summary">
                    <strong>Summary:</strong> A bottleneck for long-context LLM inference is the linearly growing KV cache. Recent work has proposed CARTRIDGES, an approach which leverages offline compute to train a much smaller KV cache than is typically required for a full document (up to 40x less memory usage at inference time). In this paper, we present the first mechanistic exploration of the learned CARTRIDGE key-value cache structure. In particular, we propose that (1) CARTRIDGE keys act as stable, shareable retrieval routers for the compressed corpora and (2) most of the learned compression occurs within the CARTRIDGE value vectors. We present empirical evidence of our routing theory across tasks, model families, and model sizes; for example, we can ablate the learned CARTRIDGE key vectors between tasks with little performance loss. Finally, we propose a slight improvement in initialization called Sampled Chunk Initialization (SCI). We suggest that SCI can lead to faster CARTRIDGE convergence than previously demonstrated in the literature. Our findings lay the groundwork for broader empirical study of CARTRIDGE training optimization which may be crucial for further scaling.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-23T14:20:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17032v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17032v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 HiCache: Training-free Acceleration of Diffusion Models via Hermite
  Polynomial-based Feature Caching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Liang Feng, Shikang Zheng, Jiacheng Liu, Yuqi Lin, Qinming Zhou, Peiliang Cai, Xinyu Wang, Junjie Chen, Chang Zou, Yue Ma, Linfeng Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion models have achieved remarkable success in content generation but suffer from prohibitive computational costs due to iterative sampling. While recent feature caching methods tend to accelerate inference through temporal extrapolation, these methods still suffer from server quality loss due to the failure in modeling the complex dynamics of feature evolution. To solve this problem, this paper presents HiCache, a training-free acceleration framework that fundamentally improves feature prediction by aligning mathematical tools with empirical properties. Our key insight is that feature derivative approximations in Diffusion Transformers exhibit multivariate Gaussian characteristics, motivating the use of Hermite polynomials-the potentially theoretically optimal basis for Gaussian-correlated processes. Besides, We further introduce a dual-scaling mechanism that ensures numerical stability while preserving predictive accuracy. Extensive experiments demonstrate HiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding baseline quality, maintaining strong performance across text-to-image, video generation, and super-resolution tasks. Core implementation is provided in the appendix, with complete code to be released upon acceptance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-23T10:35:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.16984v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.16984v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 Enhancing Memory Efficiency in Large Language Model Training Through
  Chronos-aware Pipeline Parallelism</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xinyuan Lin, Chenlu Li, Zongle Huang, Chunyu Wang, Bo Xiao, Huazhong Yang, Shishi Duan, Yongpan Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Larger model sizes and longer sequence lengths have empowered the Large Language Model (LLM) to achieve outstanding performance across various domains. However, this progress brings significant storage capacity challenges for LLM pretraining. High Bandwidth Memory (HBM) is expensive and requires more advanced packaging technologies for capacity expansion, creating an urgent need for memory-efficient scheduling strategies. Yet, prior pipeline parallelism schedules have primarily focused on reducing bubble overhead, often neglecting memory efficiency and lacking compatibility with other memory-efficient strategies. Consequently, these methods struggle to meet the storage demands of storage capacity for next-generation LLM. This work presents ChronosPipe, a Chronos-aware pipeline parallelism for memory-efficient LLM pretraining. The core insight of ChronosPipe is to treat HBM as a fast but small 'cache,' optimizing and exploiting temporal locality within LLM pretraining to enhance HBM utilization. ChronosPipe introduces a pipeline scheduling strategy, Chronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal locality of activations. Additionally, it leverages Chronos-Recomp and Chronos-Offload to efficiently harness the intrinsic temporal locality of activations and weights in Deep Neural Networks. Experiment results show that ChronosPipe can expand the trainable model size by 2.4x while maintaining comparable throughput, achieving 1.5x better than the 1F1B strategy combined with recomputation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-23T08:40:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.03182v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.03182v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long
  Sequences</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jungyoub Cha, Hyunjong Kim, Sungzoon Cho
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Speculative decoding is a widely adopted technique for accelerating inference in large language models (LLMs), but its performance degrades on long inputs due to increased attention cost and reduced draft accuracy. We introduce SpecExtend, a drop-in enhancement that improves the performance of speculative decoding on long sequences without any additional training. First, SpecExtend integrates efficient attention mechanisms such as FlashAttention and Hybrid Tree Attention into both the draft and target models. To improve draft accuracy and speed on long inputs without retraining, we propose Cross-model Retrieval, a novel KV cache eviction strategy that uses the target model's attention scores to dynamically select relevant context for the draft model. Extensive evaluations on three long-context understanding datasets show that SpecExtend accelerates standard tree-based speculative decoding by up to 2.22x for inputs up to 16K tokens, providing an effective solution for speculative decoding of long sequences. Our code is available at https://github.com/jycha98/SpecExtend .
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-22T08:45:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span><span>I.2.7; C.4</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.20776v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.20776v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion
  Transformers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shikang Zheng, Liang Feng, Xinyu Wang, Qinming Zhou, Peiliang Cai, Chang Zou, Jiacheng Liu, Yuqi Lin, Junjie Chen, Yue Ma, Linfeng Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion Transformers (DiTs) have demonstrated exceptional performance in high-fidelity image and video generation. To reduce their substantial computational costs, feature caching techniques have been proposed to accelerate inference by reusing hidden representations from previous timesteps. However, current methods often struggle to maintain generation quality at high acceleration ratios, where prediction errors increase sharply due to the inherent instability of long-step forecasting. In this work, we adopt an ordinary differential equation (ODE) perspective on the hidden-feature sequence, modeling layer representations along the trajectory as a feature-ODE. We attribute the degradation of existing caching strategies to their inability to robustly integrate historical features under large skipping intervals. To address this, we propose FoCa (Forecast-then-Calibrate), which treats feature caching as a feature-ODE solving problem. Extensive experiments on image synthesis, video generation, and super-resolution tasks demonstrate the effectiveness of FoCa, especially under aggressive acceleration. Without additional training, FoCa achieves near-lossless speedups of 5.50 times on FLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high quality with a 4.53 times speedup on DiT.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-22T08:34:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.16211v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.16211v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 Joint Cache Placement and Routing in Satellite-Terrestrial Edge
  Computing Network: A GNN-Enabled DRL Approach</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuhao Zheng, Ting You, Kejia Peng, Chang Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this letter, we investigate the problem of joint content caching and routing in satellite-terrestrial edge computing networks (STECNs) to improve caching service for geographically distributed users. To handle the challenges arising from dynamic low Earth orbit (LEO) satellite topologies and heterogeneous content demands, we propose a learning-based framework that integrates graph neural networks (GNNs) with deep reinforcement learning (DRL). The satellite network is represented as a dynamic graph, where GNNs are embedded within the DRL agent to capture spatial and topological dependencies and support routing-aware decision-making. The caching strategy is optimized by formulating the problem as a Markov decision process (MDP) and applying soft actor-critic (SAC) algorithm. Simulation results demonstrate that our approach significantly improves the delivery success rate and reduces communication traffic cost.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-22T07:57:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.16184v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.16184v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yixuan Wang, Haoyu Qiao, Lujun Li, Qingfu Zhu, Wanxiang Che
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) confront significant memory challenges due to the escalating KV cache with increasing sequence length. As a crucial technique, existing cross-layer KV cache sharing methods either necessitate modified model architectures with subsequent pre-training or incur significant performance degradation at high compression rates. To mitigate these challenges, we propose CommonKV, a training-free method for cross-layer KV cache compression through adjacent parameters sharing. Inspired by the high similarity observed in cross-layer hidden states, we utilize Singular Value Decomposition (SVD) to achieve weight sharing across adjacent parameters, resulting in a more easily mergeable latent KV cache. Furthermore, we also introduce an adaptive budget allocation strategy. It dynamically assigns compression budgets based on cosine similarity, ensuring that dissimilar caches are not over-compressed. Experiments across multiple backbone models and benchmarks including LongBench and Ruler demonstrate that the proposed method consistently outperforms existing low-rank and cross-layer approaches at various compression ratios. Moreover, we find that the benefits of CommonKV are orthogonal to other quantization and eviction methods. By integrating these approaches, we can ultimately achieve a 98\% compression ratio without significant performance loss.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-22T06:55:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.16134v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.16134v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 Lightweight and Fast Real-time Image Enhancement via Decomposition of
  the Spatial-aware Lookup Tables</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wontae Kim, Keuntek Lee, Nam Ik Cho
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The image enhancement methods based on 3D lookup tables (3D LUTs) efficiently reduce both model size and runtime by interpolating pre-calculated values at the vertices. However, the 3D LUT methods have a limitation due to their lack of spatial information, as they convert color values on a point-by-point basis. Although spatial-aware 3D LUT methods address this limitation, they introduce additional modules that require a substantial number of parameters, leading to increased runtime as image resolution increases. To address this issue, we propose a method for generating image-adaptive LUTs by focusing on the redundant parts of the tables. Our efficient framework decomposes a 3D LUT into a linear sum of low-dimensional LUTs and employs singular value decomposition (SVD). Furthermore, we enhance the modules for spatial feature fusion to be more cache-efficient. Extensive experimental results demonstrate that our model effectively decreases both the number of parameters and runtime while maintaining spatial awareness and performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-22T06:28:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.IV</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.16121v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.16121v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 Dynamic Optimization of Storage Systems Using Reinforcement Learning
  Techniques</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chiyu Cheng, Chang Zhou, Yang Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The exponential growth of data-intensive applications has placed unprecedented demands on modern storage systems, necessitating dynamic and efficient optimization strategies. Traditional heuristics employed for storage performance optimization often fail to adapt to the variability and complexity of contemporary workloads, leading to significant performance bottlenecks and resource inefficiencies. To address these challenges, this paper introduces RL-Storage, a novel reinforcement learning (RL)-based framework designed to dynamically optimize storage system configurations. RL-Storage leverages deep Q-learning algorithms to continuously learn from real-time I/O patterns and predict optimal storage parameters, such as cache size, queue depths, and readahead settings[1].This work underscores the transformative potential of reinforcement learning techniques in addressing the dynamic nature of modern storage systems. By autonomously adapting to workload variations in real time, RL-Storage provides a robust and scalable solution for optimizing storage performance, paving the way for next-generation intelligent storage infrastructures.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-22T03:36:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.OS</span><span>cs.DC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.00068v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.00068v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based
  Side-Channel Attacks on Fully Associative Randomized Caches</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chris Cao, Gururaj Saileshwar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent work presented at USENIX Security 2025 (SEC'25) claims that occupancy-based attacks can recover AES keys from the MIRAGE randomized cache. In this paper, we examine these claims and find that they arise from a modeling flaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of MIRAGE uses a constant seed to initialize the random number generator used for global evictions in MIRAGE, causing every AES encryption they trace to evict the same deterministic sequence of cache lines. This artificially creates a highly repeatable timing pattern that is not representative of a realistic implementation of MIRAGE, where eviction sequences vary randomly between encryptions. When we instead randomize the eviction seed for each run, reflecting realistic operation, the correlation between AES T-table accesses and attacker runtimes disappears, and the attack fails. These findings show that the reported leakage is an artifact of incorrect modeling, and not an actual vulnerability in MIRAGE.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T22:45:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10431v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10431v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of
  Vision-Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xinyu Chen, Haotian Zhai, Can Zhang, Xiupeng Shi, Ruirui Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In zero-shot setting, test-time adaptation adjusts pre-trained models using unlabeled data from the test phase to enhance performance on unknown test distributions. Existing cache-enhanced TTA methods rely on a low-entropy criterion to select samples for prototype construction, assuming intra-class compactness. However, low-entropy samples may be unreliable under distribution shifts, and the resulting prototypes may not ensure compact intra-class distributions. This study identifies a positive correlation between cache-enhanced performance and intra-class compactness. Based on this observation, we propose a Multi-Cache enhanced Prototype-based Test-Time Adaptation (MCP) featuring three caches: an entropy cache for initializing prototype representations with low-entropy samples, an align cache for integrating visual and textual information to achieve compact intra-class distributions, and a negative cache for prediction calibration using high-entropy samples. We further developed MCP++, a framework incorporating cross-modal prototype alignment and residual learning, introducing prototype residual fine-tuning. Comparative and ablation experiments across 15 downstream tasks demonstrate that the proposed method and framework achieve state-of-the-art generalization performance. Project Page available at: https://zhaihaotian.github.io/MCP-ICCV25/
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T20:13:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.01225v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.01225v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO
  Serving and Fast Scaling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zahra Yousefijamarani, Xinglu Wang, Qian Wang, Morgan Lindsay Heisler, Taha Shabani, Niloofar Gholipour, Parham Yassini, Hong Chang, Kan Chen, Qiantao Zhang, Xiaolong Bai, Jiannan Wang, Ying Xiong, Yong Zhang, Zhenan Fan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modern large language model (LLM) serving systems face challenges from highly variable requests with diverse lengths, priorities, and stage-specific service-level objectives (SLOs). Meeting these requires real-time scheduling, rapid and cost-effective scaling, and support for both collocated and disaggregated Prefill/Decode (P/D) architectures.   We present \textbf{HyperFlexis}, a unified LLM serving system that integrates algorithmic and system-level innovations to jointly optimize scheduling and scaling under multiple SLOs. It features a multi-SLO-aware scheduler that leverages budget estimation and request prioritization to ensure proactive SLO compliance for both new and ongoing requests. The system supports prefill- and decode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV cache transfers. It also enables cost-effective scaling decisions, prefill-decode instance linking during scaling, and rapid P/D role transitions. To accelerate scaling and reduce cold-start latency, a device-to-device (D2D) weight transfer mechanism is proposed that lowers weight loading overhead by up to \textbf{19.39$\times$}. These optimizations allow the system to achieve up to \textbf{4.44$\times$} higher SLO attainment, \textbf{65.82\%} lower request latency, and cost parity with state-of-the-art baselines. The code will be released soon.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T18:40:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15919v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15919v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 StreamMem: Query-Agnostic KV Cache Memory for Streaming Video
  Understanding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yanlai Yang, Zhuokai Zhao, Satya Narayan Shukla, Aashu Singh, Shlok Kumar Mishra, Lizhu Zhang, Mengye Ren
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multimodal large language models (MLLMs) have made significant progress in visual-language reasoning, but their ability to efficiently handle long videos remains limited. Despite recent advances in long-context MLLMs, storing and attending to the key-value (KV) cache for long visual contexts incurs substantial memory and computational overhead. Existing visual compression methods require either encoding the entire visual context before compression or having access to the questions in advance, which is impractical for long video understanding and multi-turn conversational settings. In this work, we propose StreamMem, a query-agnostic KV cache memory mechanism for streaming video understanding. Specifically, StreamMem encodes new video frames in a streaming manner, compressing the KV cache using attention scores between visual tokens and generic query tokens, while maintaining a fixed-size KV memory to enable efficient question answering (QA) in memory-constrained, long-video scenarios. Evaluation on three long video understanding and two streaming video question answering benchmarks shows that StreamMem achieves state-of-the-art performance in query-agnostic KV cache compression and is competitive with query-aware compression approaches.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T16:56:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15717v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15717v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 GoVector: An I/O-Efficient Caching Strategy for High-Dimensional Vector
  Nearest Neighbor Search</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yijie Zhou, Shengyuan Lin, Shufeng Gong, Song Yu, Shuhao Fan, Yanfeng Zhang, Ge Yu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Graph-based high-dimensional vector indices have become a mainstream solution for large-scale approximate nearest neighbor search (ANNS). However, their substantial memory footprint often requires storage on secondary devices, where frequent on-demand loading of graph and vector data leads to I/O becoming the dominant bottleneck, accounting for over 90\% of query latency. Existing static caching strategies mitigate this issue only in the initial navigation phase by preloading entry points and multi-hop neighbors, but they fail in the second phase where query-dependent nodes must be dynamically accessed to achieve high recall. We propose GoVector, an I/O-efficient caching strategy tailored for disk-based graph indices. GoVector combines (1) a static cache that stores entry points and frequently accessed neighbors, and (2) a dynamic cache that adaptively captures nodes with high spatial locality during the second search phase. To further align storage layout with similarity-driven search patterns, GoVector reorders nodes on disk so that similar vectors are colocated on the same or adjacent pages, thereby improving locality and reducing I/O overhead. Extensive experiments on multiple public datasets show that GoVector achieves substantial performance improvements. At 90% recall, it reduces I/O operations by 46% on average, increases query throughput by 1.73x, and lowers query latency by 42% compared to state-of-the-art disk-based graph indexing systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T16:21:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15694v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15694v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 CausalMesh: A Formally Verified Causal Cache for Stateful Serverless
  Computing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haoran Zhang, Zihao Zhang, Shuai Mu, Sebastian Angel, Vincent Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Stateful serverless workflows consist of multiple serverless functions that access state on a remote database. Developers sometimes add a cache layer between the serverless runtime and the database to improve I/O latency. However, in a serverless environment, functions in the same workflow may be scheduled to different nodes with different caches, which can cause non-intuitive anomalies. This paper presents CausalMesh, a novel approach to causally consistent caching in environments where a computation may migrate from one machine to another, such as in serverless computing. CausalMesh is the first cache system that supports coordination-free and abort-free read/write operations and read transactions when clients roam among multiple servers. CausalMesh also supports read-write transactional causal consistency in the presence of client roaming, but at the cost of abort-freedom.   We have formally verified CausalMesh's protocol in Dafny, and our experimental evaluation shows that CausalMesh has lower latency and higher throughput than existing proposals
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T15:25:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.14778/3704965.3704969' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.15647v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15647v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 GATEBLEED: Exploiting On-Core Accelerator Power Gating for High
  Performance & Stealthy Attacks on AI</h2>
                <div class="authors">
                    <strong>Authors:</strong> Joshua Kalyanapu, Farshad Dizani, Darsh Asher, Azam Ghanbari, Rosario Cammarota, Aydin Aysu, Samira Mirbagher Ajorpaz
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As power consumption from AI training and inference continues to increase, AI accelerators are being integrated directly into the CPU. Intel's Advanced Matrix Extensions (AMX) is one such example, debuting on the 4th generation Intel Xeon Scalable CPU. We discover a timing side and covert channel, GATEBLEED, caused by the aggressive power gating utilized to keep the CPU within operating limits. We show that the GATEBLEED side channel is a threat to AI privacy as many ML models such as transformers and CNNs make critical computationally-heavy decisions based on private values like confidence thresholds and routing logits. Timing delays from selective powering down of AMX components mean that each matrix multiplication is a potential leakage point when executed on the AMX accelerator. Our research identifies over a dozen potential gadgets across popular ML libraries (HuggingFace, PyTorch, TensorFlow, etc.), revealing that they can leak sensitive and private information. GATEBLEED poses a risk for local and remote timing inference, even under previous protective measures. GATEBLEED can be used as a high performance, stealthy remote covert channel and a generic magnifier for timing transmission channels, capable of bypassing traditional cache defenses to leak arbitrary memory addresses and evading state of the art microarchitectural attack detectors under realistic network conditions and system configurations in which previous attacks fail. We implement an end-to-end microarchitectural inference attack on a transformer model optimized with Intel AMX, achieving a membership inference accuracy of 81% and a precision of 0.89. In a CNN-based or transformer-based mixture-of-experts model optimized with Intel AMX, we leak expert choice with 100% accuracy. To our knowledge, this is the first side-channel attack on AI privacy that exploits hardware optimizations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T14:58:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.17033v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.17033v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 Efficient Mixed-Precision Large Language Model Inference with TurboMind</h2>
                <div class="authors">
                    <strong>Authors:</strong> Li Zhang, Youhe Jiang, Guoliang He, Xin Chen, Han Lv, Qian Yao, Fangcheng Fu, Kai Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Mixed-precision inference techniques reduce the memory and computational demands of Large Language Models (LLMs) by applying hybrid precision formats to model weights, activations, and KV caches. This work introduces mixed-precision LLM inference techniques that encompass (i) systematic memory and compute optimization across hierarchical storage and tensor core architectures, and (ii) comprehensive end-to-end mixed-precision optimization across diverse precision formats and hardware configurations. Our approach features two novel mixed-precision pipelines designed for optimal hardware utilization: a General Matrix Multiply (GEMM) pipeline that optimizes matrix operations through offline weight packing and online acceleration, and an attention pipeline that enables efficient attention computation with arbitrary Query, Key, and Value precision combinations. The key implementation of the pipelines includes (i) hardware-aware weight packing for automatic format optimization, (ii) adaptive head alignment for efficient attention computation, (iii) instruction-level parallelism for memory hierarchy exploitation, and (iv) KV memory loading pipeline for enhanced inference efficiency. We conduct comprehensive evaluations across 16 popular LLMs and 4 representative GPU architectures. Results demonstrate that our approach achieves up to 61% lower serving latency (30% on average) and up to 156% higher throughput (58% on average) in mixed-precision workloads compared to existing mixed-precision frameworks, establishing consistent performance improvements across all tested configurations and hardware types. This work is integrated into TurboMind, a high-performance inference engine of the LMDeploy project, which is open-sourced and publicly available at https://github.com/InternLM/lmdeploy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T14:24:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15601v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15601v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 Time-Optimal Directed q-Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Felix Windisch, Florian Unger
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Directed q-analysis is a recent extension of q-analysis, an established method for extracting structure from networks, to directed graphs. Until recently, a lack of efficient algorithms heavily restricted the application of this technique: Previous approaches scale with the square of the input size, which is also the maximal size of the output, rendering such approaches worst-case optimal. In practice, output sizes of relevant networks are usually far from the worst case, a fact that could be exploited by an (efficient) output-sensitive algorithm. We develop such an algorithm and formally describe it in detail. The key insight, obtained by carefully studying various approaches to directed q-analysis and how they relate to each other, is that inverting the order of computation leads to significant complexity gains. Targeted precomputation and caching tactics further reduce the introduced overhead, enough to achieve (under mild assumptions) a time complexity that is linear in output size. The resulting algorithm for performing directed q-analysis is shown to be time-optimal.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T13:57:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15583v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15583v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 QVecOpt: An Efficient Storage and Computing Opti-mization Framework for
  Large-scale Quantum State Simulation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mingyang Yu, Haorui Yang, Donglin Wang, Desheng Kong, Ji Du, Yulong Fu, Jing Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In response to the challenges in large-scale quantum state simulation on classical computing platforms, including memory limits, frequent disk I/O, and high computational complexity, this study builds upon a previously proposed hierarchical storage-based quantum simulation system and introduces an optimization framework, the Quantum Vector Optimization Framework (QVecOpt). QVecOpt integrates four strategies: amplitude pairing, cache optimization, block storage optimization, and parallel optimization. These collectively enhance state vector storage and computational scheduling. The amplitude pairing mechanism locates relevant amplitude pairs via bitwise XOR, reducing traversal complexity of single-qubit gates from $O(2^n)$ to $O(1)$. Cache optimization pre-allocates buffers and loads only required data, cutting disk I/O. Block storage optimization partitions the state vector for on-demand loading and local updates, reducing redundant access. Parallel optimization distributes the state vector across nodes for collaborative computation, achieving near-linear speedup. Complexity analysis shows that, compared with hierarchical storage simulation, the method reduces state vector traversals for single-qubit gates from $2^n$ to 1, removing the main bottleneck. It also lowers computational and I/O complexity from $O(2^n)$ to $O(2^n/C)$ and $O(2^n/B)$. In simulations of 16-29 qubits, efficiency improves nearly tenfold, breaking the memory bottleneck of existing tools and enabling high-bit quantum circuit simulations beyond traditional methods. This work provides an efficient, scalable solution for classical simulation of large-scale quantum computation with significant academic and practical value.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T13:24:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.ET</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15545v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15545v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lukas Höllein, Aljaž Božič, Michael Zollhöfer, Matthias Nießner
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present 3DGS-LM, a new method that accelerates the reconstruction of 3D Gaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored Levenberg-Marquardt (LM). Existing methods reduce the optimization time by decreasing the number of Gaussians or by improving the implementation of the differentiable rasterizer. However, they still rely on the ADAM optimizer to fit Gaussian parameters of a scene in thousands of iterations, which can take up to an hour. To this end, we change the optimizer to LM that runs in conjunction with the 3DGS differentiable rasterizer. For efficient GPU parallization, we propose a caching data structure for intermediate gradients that allows us to efficiently calculate Jacobian-vector products in custom CUDA kernels. In every LM iteration, we calculate update directions from multiple image subsets using these kernels and combine them in a weighted mean. Overall, our method is 20% faster than the original 3DGS while obtaining the same reconstruction quality. Our optimization is also agnostic to other methods that acclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T12:52:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.12892v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.12892v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 TrimCaching: Parameter-sharing Edge Caching for AI Model Downloading</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guanqiao Qu, Zheng Lin, Qian Chen, Jian Li, Fangming Liu, Xianhao Chen, Kaibin Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Next-generation mobile networks are expected to facilitate fast AI model downloading to end users. By caching models on edge servers, mobile networks can deliver models to end users with low latency, resulting in a paradigm of edge model caching. In this paper, we develop a novel model placement framework, called parameter-sharing model caching (TrimCaching). TrimCaching exploits the key observation that a wide range of AI models, such as convolutional neural networks or large language models, can share a significant proportion of parameter blocks containing reusable knowledge, thereby improving storage efficiency. To this end, we formulate a parameter-sharing model placement problem to maximize the cache hit ratio in multi-edge wireless networks by balancing the fundamental tradeoff between storage efficiency and service latency. We show that the formulated problem is a submodular maximization problem with submodular constraints, for which no polynomial-time approximation algorithm exists. To tackle this challenge, we study an important special case, where a small fixed number of parameter blocks are shared across models, which often holds in practice. In such a case, a polynomial-time algorithm with a $\left(1-\epsilon\right)/2$-approximation guarantee is developed. Subsequently, we address the original problem for the general case by developing a greedy algorithm. Simulation results demonstrate that the proposed TrimCaching framework significantly improves the cache hit ratio compared with state-of-the-art content caching without exploiting shared parameters in AI models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T11:43:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.14204v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.14204v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 Gorgeous: Revisiting the Data Layout for Disk-Resident High-Dimensional
  Vector Search</h2>
                <div class="authors">
                    <strong>Authors:</strong> Peiqi Yin, Xiao Yan, Qihui Zhou, Hui Li, Xiaolu Li, Lin Zhang, Meiling Wang, Xin Yao, James Cheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Similarity-based vector search underpins many important applications, but a key challenge is processing massive vector datasets (e.g., in TBs). To reduce costs, some systems utilize SSDs as the primary data storage. They employ a proximity graph, which connects similar vectors to form a graph and is the state-of-the-art index for vector search. However, these systems are hindered by sub-optimal data layouts that fail to effectively utilize valuable memory space to reduce disk access and suffer from poor locality for accessing disk-resident data. Through extensive profiling and analysis, we found that the structure of the proximity graph index is accessed more frequently than the vectors themselves, yet existing systems do not distinguish between the two. To address this problem, we design the Gorgeous system with the principle of prioritizing graph structure over vectors. Specifically, Gorgeous features a memory cache that keeps the adjacency lists of graph nodes to improve cache hits and a disk block format that explicitly stores neighbors' adjacency lists along with a vector to enhance data locality. Experimental results show that Gorgeous consistently outperforms two state-of-the-art disk-based systems for vector search, boosting average query throughput by over 60% and reducing query latency by over 35%.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T06:26:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15290v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15290v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache
  Channel Pruning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Huanxuan Liao, Yixing Xu, Shizhu He, Guanchen Li, Xuanwu Yin, Dong Li, Emad Barsoum, Jun Zhao, Kang Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Long-context inference in large language models (LLMs) is increasingly constrained by the KV cache bottleneck: memory usage grows linearly with sequence length, while attention computation scales quadratically. Existing approaches address this issue by compressing the KV cache along the temporal axis through strategies such as token eviction or merging to reduce memory and computational overhead. However, these methods often neglect fine-grained importance variations across feature dimensions (i.e., the channel axis), thereby limiting their ability to effectively balance efficiency and model accuracy. In reality, we observe that channel saliency varies dramatically across both queries and positions: certain feature channels carry near-zero information for a given query, while others spike in relevance. To address this oversight, we propose SPARK, a training-free plug-and-play method that applies unstructured sparsity by pruning KV at the channel level, while dynamically restoring the pruned entries during attention score computation. Notably, our approach is orthogonal to existing KV compression and quantization techniques, making it compatible for integration with them to achieve further acceleration. By reducing channel-level redundancy, SPARK enables processing of longer sequences within the same memory budget. For sequences of equal length, SPARK not only preserves or improves model accuracy but also reduces KV cache storage by over 30% compared to eviction-based methods. Furthermore, even with an aggressive pruning ratio of 80%, SPARK maintains performance with less degradation than 5% compared to the baseline eviction method, demonstrating its robustness and effectiveness. Our code will be available at https://github.com/Xnhyacinth/SparK.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-21T03:48:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15212v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15212v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in
  Mixture-of-Experts LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruyi Ding, Tianhong Xu, Xinyi Shen, Aidong Adam Ding, Yunsi Fei
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The transformer architecture has become a cornerstone of modern AI, fueling remarkable progress across applications in natural language processing, computer vision, and multimodal learning. As these models continue to scale explosively for performance, implementation efficiency remains a critical challenge. Mixture of Experts (MoE) architectures, selectively activating specialized subnetworks (experts), offer a unique balance between model accuracy and computational cost. However, the adaptive routing in MoE architectures, where input tokens are dynamically directed to specialized experts based on their semantic meaning inadvertently opens up a new attack surface for privacy breaches. These input-dependent activation patterns leave distinctive temporal and spatial traces in hardware execution, which adversaries could exploit to deduce sensitive user data. In this work, we propose MoEcho, discovering a side channel analysis based attack surface that compromises user privacy on MoE based systems. Specifically, in MoEcho, we introduce four novel architectural side channels on different computing platforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and Performance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting these vulnerabilities, we propose four attacks that effectively breach user privacy in large language models (LLMs) and vision language models (VLMs) based on MoE architectures: Prompt Inference Attack, Response Reconstruction Attack, Visual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first runtime architecture level security analysis of the popular MoE structure common in modern transformers, highlighting a serious security and privacy threat and calling for effective and timely safeguards when harnessing MoE based models for developing efficient large scale AI services.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-20T20:02:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15036v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15036v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 Rethinking the Potential of Layer Freezing for Efficient DNN Training</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chence Yang, Ci Zhang, Lei Lu, Qitao Tan, Sheng Li, Ao Li, Xulong Tang, Shaoyi Huang, Jinzhen Wang, Guoming Li, Jundong Li, Xiaoming Zhai, Jin Lu, Geng Yuan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the growing size of deep neural networks and datasets, the computational costs of training have significantly increased. The layer-freezing technique has recently attracted great attention as a promising method to effectively reduce the cost of network training. However, in traditional layer-freezing methods, frozen layers are still required for forward propagation to generate feature maps for unfrozen layers, limiting the reduction of computation costs. To overcome this, prior works proposed a hypothetical solution, which caches feature maps from frozen layers as a new dataset, allowing later layers to train directly on stored feature maps. While this approach appears to be straightforward, it presents several major challenges that are severely overlooked by prior literature, such as how to effectively apply augmentations to feature maps and the substantial storage overhead introduced. If these overlooked challenges are not addressed, the performance of the caching method will be severely impacted and even make it infeasible. This paper is the first to comprehensively explore these challenges and provides a systematic solution. To improve training accuracy, we propose \textit{similarity-aware channel augmentation}, which caches channels with high augmentation sensitivity with a minimum additional storage cost. To mitigate storage overhead, we incorporate lossy data compression into layer freezing and design a \textit{progressive compression} strategy, which increases compression rates as more layers are frozen, effectively reducing storage costs. Finally, our solution achieves significant reductions in training cost while maintaining model accuracy, with a minor time overhead. Additionally, we conduct a comprehensive evaluation of freezing and compression strategies, providing insights into optimizing their application for efficient DNN training.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-20T19:54:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15033v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15033v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 Lossless Compression of Neural Network Components: Weights, Checkpoints,
  and K/V Caches in Low-Precision Formats</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anat Heilper, Doron Singer
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As deep learning models grow and deployment becomes more widespread, reducing the storage and transmission costs of neural network weights has become increasingly important. While prior work such as ZipNN has shown that lossless compression methods - particularly those based on Huffman encoding floating-point exponents can significantly reduce model sizes, these techniques have primarily been applied to higher-precision formats such as FP32 and BF16. In this work, we extend the ZipNN approach to lower-precision floating-point formats, specifically FP8 and FP4, which are gaining popularity for efficient inference. We design a compression method that separates and compresses the exponent and mantissa components independently using entropy coding. Our evaluation shows compression ratios up to 62% for BF16 and 83% for FP8. We also investigate the compressibility of key-value (K/V) cache tensors used in large language models (LLMs), finding that they, too, exhibit compressible patterns, enabling memory savings during deployment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-20T12:46:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.NE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19263v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19263v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 Diverse Negative Sampling for Implicit Collaborative Filtering</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yueqing Xuan, Kacper Sokol, Mark Sanderson, Jeffrey Chan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Implicit collaborative filtering recommenders are usually trained to learn user positive preferences. Negative sampling, which selects informative negative items to form negative training data, plays a crucial role in this process. Since items are often clustered in the latent space, existing negative sampling strategies normally oversample negative items from the dense regions. This leads to homogeneous negative data and limited model expressiveness. In this paper, we propose Diverse Negative Sampling (DivNS), a novel approach that explicitly accounts for diversity in negative training data during the negative sampling process. DivNS first finds hard negative items with large preference scores and constructs user-specific caches that store unused but highly informative negative samples. Then, its diversity-augmented sampler selects a diverse subset of negative items from the cache while ensuring dissimilarity from the user's hard negatives. Finally, a synthetic negatives generator combines the selected diverse negatives with hard negatives to form more effective training data. The resulting synthetic negatives are both informative and diverse, enabling recommenders to learn a broader item space and improve their generalisability. Extensive experiments on four public datasets demonstrate the effectiveness of DivNS in improving recommendation quality while maintaining computational efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-20T06:48:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.14468v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.14468v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 You Only Evaluate Once: A Tree-based Rerank Method at Meituan</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shuli Wang, Yinqiu Huang, Changhao Li, Yuan Zhou, Yonggang Liu, Yongqiang Zhang, Yinhua Zhu, Haitao Wang, Xingxing Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reranking plays a crucial role in modern recommender systems by capturing the mutual influences within the list. Due to the inherent challenges of combinatorial search spaces, most methods adopt a two-stage search paradigm: a simple General Search Unit (GSU) efficiently reduces the candidate space, and an Exact Search Unit (ESU) effectively selects the optimal sequence. These methods essentially involve making trade-offs between effectiveness and efficiency, while suffering from a severe \textbf{inconsistency problem}, that is, the GSU often misses high-value lists from ESU. To address this problem, we propose YOLOR, a one-stage reranking method that removes the GSU while retaining only the ESU. Specifically, YOLOR includes: (1) a Tree-based Context Extraction Module (TCEM) that hierarchically aggregates multi-scale contextual features to achieve "list-level effectiveness", and (2) a Context Cache Module (CCM) that enables efficient feature reuse across candidate permutations to achieve "permutation-level efficiency". Extensive experiments across public and industry datasets validate YOLOR's performance, and we have successfully deployed YOLOR on the Meituan food delivery platform.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-20T04:36:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3746252.3761539' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.14420v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.14420v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 H2EAL: Hybrid-Bonding Architecture with Hybrid Sparse Attention for
  Efficient Long-Context LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zizhuo Fu, Xiaotian Guo, Wenxuan Zeng, Shuzhang Zhong, Yadong Zhang, Peiyu Chen, Runsheng Wang, Le Ye, Meng Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have demonstrated remarkable proficiency in a wide range of natural language processing applications. However, the high energy and latency overhead induced by the KV cache limits the edge deployment, especially for long contexts. Emerging hybrid bonding (HB) technology has been proposed as a promising alternative to conventional near-memory processing (NMP) architectures, offering improved bandwidth efficiency and lower power consumption while exhibiting characteristics of distributed memory. In this paper, we propose H2EAL, a hybrid bonding-based accelerator with sparse attention algorithm-hardware co-design for efficient LLM inference at the edge. At the algorithm level, we propose a hybrid sparse attention scheme with static and dynamic sparsity for different heads to fully leverage the sparsity with high accuracy. At the hardware level, we co-design the hardware to support hybrid sparse attention and propose memory-compute co-placement to address the distributed memory bottleneck. Since different attention heads exhibit different sparse patterns and the attention structure often mismatches the HB architecture, we further develop a load-balancing scheduler with parallel tiled attention to address workload imbalance and optimize the mapping strategy. Extensive experiments demonstrate H2EAL achieves 5.20~48.21x speedup and 6.22~73.48x energy efficiency improvement over baseline HB implementation, with a negligible average accuracy drop of 0.87% on multiple benchmarks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-20T03:42:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.16653v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.16653v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 Scavenger+: Revisiting Space-Time Tradeoffs in Key-Value Separated
  LSM-trees</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jianshun Zhang, Fang Wang, Jiaxin Ou, Yi Wang, Ming Zhao, Sheng Qiu, Junxun Huang, Baoquan Li, Peng Fang, Dan Feng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Key-Value Stores (KVS) based on log-structured merge-trees (LSM-trees) are widely used in storage systems but face significant challenges, such as high write amplification caused by compaction. KV-separated LSM-trees address write amplification but introduce significant space amplification, a critical concern in cost-sensitive scenarios. Garbage collection (GC) can reduce space amplification, but existing strategies are often inefficient and fail to account for workload characteristics. Moreover, current key-value (KV) separated LSM-trees overlook the space amplification caused by the index LSM-tree. In this paper, we systematically analyze the sources of space amplification in KV-separated LSM-trees and propose Scavenger+, which achieves a better performance-space trade-off. Scavenger+ introduces (1) an I/O-efficient garbage collection scheme to reduce I/O overhead, (2) a space-aware compaction strategy based on compensated size to mitigate index-induced space amplification, and (3) a dynamic GC scheduler that adapts to system load to make better use of CPU and storage resources. Extensive experiments demonstrate that Scavenger+ significantly improves write performance and reduces space amplification compared to state-of-the-art KV-separated LSM-trees, including BlobDB, Titan, and TerarkDB.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-19T15:26:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/TC.2025.3587513' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.13935v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.13935v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 Scavenger: Better Space-Time Trade-Offs for Key-Value Separated
  LSM-trees</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jianshun Zhang, Fang Wang, Sheng Qiu, Yi Wang, Jiaxin Ou, Junxun Huang, Baoquan Li, Peng Fang, Dan Feng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Key-Value Stores (KVS) implemented with log-structured merge-tree (LSM-tree) have gained widespread acceptance in storage systems. Nonetheless, a significant challenge arises in the form of high write amplification due to the compaction process. While KV-separated LSM-trees successfully tackle this issue, they also bring about substantial space amplification problems, a concern that cannot be overlooked in cost-sensitive scenarios. Garbage collection (GC) holds significant promise for space amplification reduction, yet existing GC strategies often fall short in optimization performance, lacking thorough consideration of workload characteristics. Additionally, current KV-separated LSM-trees also ignore the adverse effect of the space amplification in the index LSM-tree. In this paper, we systematically analyze the sources of space amplification of KV-separated LSM-trees and introduce Scavenger, which achieves a better trade-off between performance and space amplification. Scavenger initially proposes an I/O-efficient garbage collection scheme to reduce I/O overhead and incorporates a space-aware compaction strategy based on compensated size to minimize the space amplification of index LSM-trees. Extensive experiments show that Scavenger significantly improves write performance and achieves lower space amplification than other KV-separated LSM-trees (including BlobDB, Titan, and TerarkDB).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-19T15:08:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/ICDE60146.2024.00312' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.13909v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.13909v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 Tight Inter-Core Cache Contention Analysis for WCET Estimation on
  Multicore Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shuai Zhao, Jieyu Jiang, Shenlin Cai, Yaowei Liang, Chen Jie, Yinjie Fang, Wei Zhang, Guoquan Zhang, Yaoyao Gu, Xiang Xiao, Wei Qin, Xiangzhen Ouyang, Wanli Chang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> WCET (Worst-Case Execution Time) estimation on multicore architecture is particularly challenging mainly due to the complex accesses over cache shared by multiple cores. Existing analysis identifies possible contentions between parallel tasks by leveraging the partial order of the tasks or their program regions. Unfortunately, they overestimate the number of cache misses caused by a remote block access without considering the actual cache state and the number of accesses. This paper reports a new analysis for inter-core cache contention. Based on the order of program regions in a task, we first identify memory references that could be affected if a remote access occurs in a region. Afterwards, a fine-grained contention analysis is constructed that computes the number of cache misses based on the access quantity of local and remote blocks. We demonstrate that the overall inter-core cache interference of a task can be obtained via dynamic programming. Experiments show that compared to existing methods, the proposed analysis reduces inter-core cache interference and WCET estimations by 52.31% and 8.94% on average, without significantly increasing computation overhead.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-19T14:30:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.13863v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.13863v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 Zobrist Hash-based Duplicate Detection in Symbolic Regression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bogdan Burlacu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Symbolic regression encompasses a family of search algorithms that aim to discover the best fitting function for a set of data without requiring an a priori specification of the model structure. The most successful and commonly used technique for symbolic regression is Genetic Programming (GP), an evolutionary search method that evolves a population of mathematical expressions through the mechanism of natural selection. In this work we analyze the efficiency of the evolutionary search in GP and show that many points in the search space are re-visited and re-evaluated multiple times by the algorithm, leading to wasted computational effort. We address this issue by introducing a caching mechanism based on the Zobrist hash, a type of hashing frequently used in abstract board games for the efficient construction and subsequent update of transposition tables. We implement our caching approach using the open-source framework Operon and demonstrate its performance on a selection of real-world regression problems, where we observe up to 34\% speedups without any detrimental effects on search quality. The hashing approach represents a straightforward way to improve runtime performance while also offering some interesting possibilities for adjusting search strategy based on cached information.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-19T14:18:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.13859v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.13859v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 INDS: Incremental Named Data Streaming for Real-Time Point Cloud Video</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruonan Chai, Yixiang Zhu, Xinjiao Li, Jiawei Li, Zili Meng, Dirk Kutscher
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Real-time streaming of point cloud video, characterized by massive data volumes and high sensitivity to packet loss, remains a key challenge for immersive applications under dynamic network conditions. While connection-oriented protocols such as TCP and more modern alternatives like QUIC alleviate some transport-layer inefficiencies, including head-of-line blocking, they still retain a coarse-grained, segment-based delivery model and a centralized control loop that limit fine-grained adaptation and effective caching. We introduce INDS (Incremental Named Data Streaming), an adaptive streaming framework based on Information-Centric Networking (ICN) that rethinks delivery for hierarchical, layered media. INDS leverages the Octree structure of point cloud video and expressive content naming to support progressive, partial retrieval of enhancement layers based on consumer bandwidth and decoding capability. By combining time-windows with Group-of-Frames (GoF), INDS's naming scheme supports fine-grained in-network caching and facilitates efficient multi-user data reuse. INDS can be deployed as an overlay, remaining compatible with QUIC-based transport infrastructure as well as future Media-over-QUIC (MoQ) architectures, without requiring changes to underlying IP networks. Our prototype implementation shows up to 80% lower delay, 15-50% higher throughput, and 20-30% increased cache hit rates compared to state-of-the-art DASH-style systems. Together, these results establish INDS as a scalable, cache-friendly solution for real-time point cloud streaming under variable and lossy conditions, while its compatibility with MoQ overlays further positions it as a practical, forward-compatible architecture for emerging immersive media systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-19T11:54:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MM</span><span>C.2.1; C.2.4; H.5.1</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.13756v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.13756v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint
  Caching and Resource-Aware Graph Partitioning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xianfeng Song, Yi Zou, Zheng Shi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Graph Neural Networks (GNNs) have shown remarkable capabilities in processing graph-structured data prevalent in various real-world applications. However, the scalability of full-batch GNN training becomes severely limited by high communication overhead and load imbalance in distributed environments. In this paper, we present CaPGNN, a novel framework for efficient parallel full-batch GNN training on single-server with multi-GPU, designed specifically to reduce redundant inter-GPU communication and balance computational workloads. We propose a joint adaptive caching algorithm that leverages both CPU and GPU memory to significantly reduce the repetitive transmission of vertex features across partitions. Additionally, we introduce a resource-aware graph partitioning algorithm that adjusts subgraph sizes dynamically according to the heterogeneous computational and communication capacities of GPUs. Extensive experiments on large-scale benchmark datasets demonstrate that CaPGNN effectively reduces communication costs by up to 96% and accelerates GNN training by up to 12.7 times compared to state-of-the-art approaches. Our results highlight the potential of adaptive caching and resource-aware partitioning to facilitate scalable, efficient, and practical deployment of full-batch GNN training in distributed computing environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-19T10:21:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.13716v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.13716v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units
  with Precision Recovery</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weicheng Xue, Baisong Xu, Kai Yang, Yongxiang Liu, Dengdeng Fan, Pengxiang Xu, Yonghong Tian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Low-precision matrix engines, such as FP16 cube, offer high throughput but lack support for full-precision computation. In this work, we propose SGEMM-cube, a high-performance algorithm for emulating FP32 general matrix-matrix multiplication (GEMM) using only FP16 computation units on a representative AI accelerator. The method decomposes each FP32 operand into two FP16 values and compensates for numerical errors through a tunable scaling strategy. A detailed analysis of numerical errors, including underflow conditions and precision loss, guides the selection of scaling parameters to preserve up to 22 bits of mantissa accuracy. We further investigate the effect of computation order on accuracy and demonstrate that a term-wise accumulation scheme improves numerical stability over conventional FP32 GEMM in low-exponent regimes. Finally, a cache-aware blocking strategy and double-buffered pipeline are introduced to overlap memory transfers with computation, enabling SGEMM-cube to achieve up to 77\% of the theoretical FP32-equivalent peak performance on Ascend 910A NPU lacking native FP32 support. Extensive numerical experiments confirm that our method not only recovers the accuracy of native FP32 GEMM but also exhibits superior numerical stability under certain conditions, due to its structured and error-aware computation order.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-19T09:13:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.23387v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.23387v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale
  Architectures</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anders Johansson, Evan Weinberg, Christian R. Trott, Megan J. McCarthy, Stan G. Moore
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Since its inception in 1995, LAMMPS has grown to be a world-class molecular dynamics code, with thousands of users, over one million lines of code, and multi-scale simulation capabilities. We discuss how LAMMPS has adapted to the modern heterogeneous computing landscape by integrating the Kokkos performance portability library into the existing C++ code. We investigate performance portability of simple pairwise, many-body reactive, and machine-learned force-field interatomic potentials. We present results on GPUs across different vendors and generations, and analyze performance trends, probing FLOPS throughput, memory bandwidths, cache capabilities, and thread-atomic operation performance. Finally, we demonstrate strong scaling on all current US exascale machines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the three potentials.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-19T05:27:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.PF</span><span>physics.comp-ph</span><span>C.1.4; C.2.4; C.4; D.1.3; D.3.4; E.1; I.6; I.6.8; J.2</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.13523v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.13523v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated
  Diffusion Transformers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wongi Jeong, Kyungryeol Lee, Hoigi Seo, Se Young Chun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion transformers have emerged as an alternative to U-net-based diffusion models for high-fidelity image and video generation, offering superior scalability. However, their heavy computation remains a major obstacle to real-world deployment. Existing acceleration methods primarily exploit the temporal dimension such as reusing cached features across diffusion timesteps. Here, we propose Region-Adaptive Latent Upsampling (RALU), a training-free framework that accelerates inference along spatial dimension. RALU performs mixed-resolution sampling across three stages: 1) low-resolution denoising latent diffusion to efficiently capture global semantic structure, 2) region-adaptive upsampling on specific regions prone to artifacts at full-resolution, and 3) all latent upsampling at full-resolution for detail refinement. To stabilize generations across resolution transitions, we leverage noise-timestep rescheduling to adapt the noise level across varying resolutions. Our method significantly reduces computation while preserving image quality by achieving up to 7.0$\times$ speed-up on FLUX and 3.0$\times$ on Stable Diffusion 3 with minimal degradation. Furthermore, RALU is complementary to existing temporal accelerations such as caching methods, thus can be seamlessly integrated to further reduce inference latency without compromising generation quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-19T03:13:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>eess.IV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.08422v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.08422v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A
  first-principles DFT+$U$+$V$ study</h2>
                <div class="authors">
                    <strong>Authors:</strong> Indukuru Ramesh Reddy, Sayandeep Ghosh, Bongjae Kim, Chang-Jong Kang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Nonlocal Coulomb interactions play a crucial role in stabilizing distinct electronic phases in kagome materials. In this work, we systematically investigate the effects of on-site ($U$) and inter-site ($V$) Coulomb interactions on the electronic structure and stability of charge-density-wave (CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory (DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and stability of CDW phases, whereas $U$ suppresses these phases, highlighting a fundamental competition between local and nonlocal Coulomb interactions. By directly comparing our theoretical results with angle-resolved photoemission spectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that accurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings establish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable insights into the correlated electronic states in kagome metals and serving as a foundation for future explorations of correlation-driven phenomena in related materials.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-19T01:38:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.str-el</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.17995v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.17995v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data
  Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ayoub Ben Chaliah, Hela Dellagi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present Datarus-R1-14B, a 14 B-parameter open-weights language model fine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and graduate-level problem solver. Datarus is trained not on isolated question-answer pairs but on full analytical trajectories including reasoning steps, code execution, error traces, self-corrections, and final conclusions, all captured in a ReAct-style notebook format spanning finance, medicine, numerical analysis, and other quantitative domains. Our training pipeline combines (i) a trajectory-centric synthetic data generator that yielded 144 000 tagged notebook episodes, (ii) a dual-reward framework blending a lightweight tag-based structural signal with a Hierarchical Reward Model (HRM) that scores both single-step soundness and end-to-end coherence, and (iii) a memory-optimized implementation of Group Relative Policy Optimization (GRPO) featuring KV-cache reuse, sequential generation, and reference-model sharding. A cosine curriculum smoothly shifts emphasis from structural fidelity to semantic depth, reducing the format collapse and verbosity that often plague RL-aligned LLMs. A central design choice in Datarus is it dual reasoning interface. In agentic mode the model produces ReAct-tagged steps that invoke Python tools to execute real code; in reflection mode it outputs compact Chain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On demanding postgraduate-level problems, Datarus exhibits an "AHA-moment" pattern: it sketches hypotheses, revises them once or twice, and converges avoiding the circular, token-inflating loops common to contemporary systems. Across standard public benchmarks Datarus surpasses similar size models and even reaches the level of larger reasoning models such as QwQ-32B achieving up to 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting 18-49% fewer tokens per solution.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-18T21:58:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.13382v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.13382v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical
  Manufacturing Scale-Up</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sakhinana Sagar Srinivas, Shivam Gupta, Venkataramana Runkana
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in generative AI have accelerated the discovery of novel chemicals and materials. However, scaling these discoveries to industrial production remains a major bottleneck due to the synthesis gap -- the need to develop entirely new manufacturing processes. This challenge requires detailed engineering blueprints: PFDs for equipment layouts and material/energy flows, and PIDs for process plant operations. Current AI systems cannot yet reliably generate these critical engineering schematics, creating a fundamental obstacle to manufacturing scale-up of novel discoveries. We present a closed-loop, physics-aware framework for automated generation of industrially viable PFDs and PIDs. The framework integrates three key components: (1) domain-specialized small language models (SLMs) trained for auto-generation of PFDs and PIDs, (2) a hierarchical knowledge graph containing process flow and instrumentation descriptions for 1,020+ chemicals for Graph Retrieval-Augmented Generation (GRAG), and (3) an open-source chemical process simulator for modeling, simulation, optimization, and analysis of novel chemical processes. The SLMs are trained through a multi-stage pipeline on synthetic datasets, with process simulator-in-the-loop validation ensuring feasibility. To enhance computational efficiency, the framework implements structural pruning (width and depth) guided by importance heuristics to reduce language model size while preserving accuracy, followed by advanced inference optimizations including FlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization, and Test-Time Inference Scaling. Experimental results demonstrate that our framework generates simulator-validated process descriptions with high fidelity.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-18T16:52:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.24584v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.24584v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning
  Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruikang Liu, Yuxuan Sun, Manyi Zhang, Haoli Bai, Xianzhi Yu, Tiezheng Yu, Chun Yuan, Lu Hou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in reasoning language models have demonstrated remarkable performance in complex tasks, but their extended chain-of-thought reasoning process increases inference overhead. While quantization has been widely adopted to reduce the inference cost of large language models, its impact on reasoning models remains understudied. In this paper, we conduct the first systematic study on quantized reasoning models, evaluating the open-sourced DeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B parameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache, and activation quantization using state-of-the-art algorithms at varying bit-widths, with extensive evaluation across mathematical (AIME, MATH-500), scientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our findings reveal that while lossless quantization can be achieved with W8A8 or W4A16 quantization, lower bit-widths introduce significant accuracy risks. We further identify model size, model origin, and task difficulty as critical determinants of performance. Contrary to expectations, quantized models do not exhibit increased output lengths. In addition, strategically scaling the model sizes or reasoning steps can effectively enhance the performance. All quantized models and codes are open-sourced in https://github.com/ruikangliu/Quantized-Reasoning-Models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-18T16:06:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.04823v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.04823v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 Some optimization possibilities in data plane programming</h2>
                <div class="authors">
                    <strong>Authors:</strong> Altangerel Gereltsetseg, Tejfel Máté
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Software-defined networking (SDN) technology aims to create a highly flexible network by decoupling control plane and the data plane and programming them independently. There has been a lot of research on improving and optimizing the control plane, and data plane programming is a relatively new concept, so study on it is one of the hot topics for researchers. At the 2019 Dagstuhl Seminar, well-known scientists on computer networking discussed challenges and problems in the field of data plane programming that need to be addressed over the next 10 years. Based on this seminar issues and papers review, we suggested some possible solutions which are for optimizing data plane to improve packet processing performance and link utilization. The suggestions include (i) enriching data plane language with asynchronous external function, (ii) compression based on payload size, (iii) in-network caching for fast packet processing, and (iv) offloading external functions to an additional thread, virtual machine (VM) or server, etc. In addition, we implemented some of these in the P4 data plane language to illustrate the practicality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-18T09:41:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.12767v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.12767v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 Dissecting CPU-GPU Unified Physical Memory on AMD MI300A APUs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jacob Wahlgren, Gabin Schieffer, Ruimin Shi, Edgar A. León, Roger Pearce, Maya Gokhale, Ivy Peng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Discrete GPUs are a cornerstone of HPC and data center systems, requiring management of separate CPU and GPU memory spaces. Unified Virtual Memory (UVM) has been proposed to ease the burden of memory management; however, at a high cost in performance. The recent introduction of AMD's MI300A Accelerated Processing Units (APUs)--as deployed in the El Capitan supercomputer--enables HPC systems featuring integrated CPU and GPU with Unified Physical Memory (UPM) for the first time. This work presents the first comprehensive characterization of the UPM architecture on MI300A. We first analyze the UPM system properties, including memory latency, bandwidth, and coherence overhead. We then assess the efficiency of the system software in memory allocation, page fault handling, TLB management, and Infinity Cache utilization. We propose a set of porting strategies for transforming applications for the UPM architecture and evaluate six applications on the MI300A APU. Our results show that applications on UPM using the unified memory model can match or outperform those in the explicitly managed model--while reducing memory costs by up to 44%.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-18T09:06:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.12743v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.12743v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuanxin Wei, Lansong Diao, Bujiao Chen, Shenggan Cheng, Zhengping Qian, Wenyuan Yu, Nong Xiao, Wei Lin, Jiangsu Du
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Leveraging the Transformer architecture and the diffusion process, video DiT models have emerged as a dominant approach for high-quality video generation. However, their multi-step iterative denoising process incurs high computational cost and inference latency. Caching, a widely adopted optimization method in DiT models, leverages the redundancy in the diffusion process to skip computations in different granularities (e.g., step, cfg, block). Nevertheless, existing caching methods are limited to single-granularity strategies, struggling to balance generation quality and inference speed in a flexible manner. In this work, we propose MixCache, a training-free caching-based framework for efficient video DiT inference. It first distinguishes the interference and boundary between different caching strategies, and then introduces a context-aware cache triggering strategy to determine when caching should be enabled, along with an adaptive hybrid cache decision strategy for dynamically selecting the optimal caching granularity. Extensive experiments on diverse models demonstrate that, MixCache can significantly accelerate video generation (e.g., 1.94$\times$ speedup on Wan 14B, 1.97$\times$ speedup on HunyuanVideo) while delivering both superior generation quality and inference efficiency compared to baseline methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-18T07:49:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.GR</span><span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.12691v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.12691v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for
  NGINX</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aayush Gupta, Arpit Bhayani
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Web proxies such as NGINX commonly rely on least-recently-used (LRU) eviction, which is size agnostic and can thrash under periodic bursts and mixed object sizes. We introduce Cold-RL, a learned eviction policy for NGINX that replaces LRU's forced-expire path with a dueling Deep Q-Network served by an ONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL samples the K least-recently-used objects, extracts six lightweight features (age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT), and requests a bitmask of victims; a hard timeout of 500 microseconds triggers immediate fallback to native LRU. Policies are trained offline by replaying NGINX access logs through a cache simulator with a simple reward: a retained object earns one point if it is hit again before TTL expiry. We compare against LRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial workloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538, a 146 percent improvement over the best classical baseline; at 100 MB, from 0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods (about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th percentile eviction latency within budget. To our knowledge, this is the first reinforcement learning eviction policy integrated into NGINX with strict SLOs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-17T20:01:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.DB</span><span>cs.NI</span><span>C.2.4; C.4; D.4.2; I.2.6</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.12485v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.12485v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 Accelerating LLM Inference via Dynamic KV Cache Placement in
  Heterogeneous Memory System</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yunhua Fang, Rui Xie, Asad Ul Haq, Linsen Ma, Kaoutar El Maghraoui, Naigang Wang, Meng Wang, Liu Liu, Tong Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Model (LLM) inference is increasingly constrained by memory bandwidth, with frequent access to the key-value (KV) cache dominating data movement. While attention sparsity reduces some memory traffic, the relevance of past tokens varies over time, requiring the full KV cache to remain accessible and sustaining pressure on both bandwidth and capacity. With advances in interconnects such as NVLink and LPDDR5X, modern AI hardware now integrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making heterogeneous memory systems a practical solution. This work investigates dynamic KV cache placement across such systems to maximize aggregated bandwidth utilization under capacity constraints. Rather than proposing a specific scheduling policy, we formulate the placement problem mathematically and derive a theoretical upper bound, revealing substantial headroom for runtime optimization. To our knowledge, this is the first formal treatment of dynamic KV cache scheduling in heterogeneous memory systems for LLM inference.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-17T19:07:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>cs.AI</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.13231v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.13231v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 ZigzagAttention: Efficient Long-Context Inference with Exclusive
  Retrieval and Streaming Heads</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhuorui Liu, Chen Zhang, Dawei Song
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the rapid development of large language models (LLMs), handling long context has become one of the vital abilities in LLMs. Such long-context ability is accompanied by difficulties in deployment, especially due to the increased consumption of KV cache. There is certain work aiming to optimize the memory footprint of KV cache, inspired by the observation that attention heads can be categorized into retrieval heads that are of great significance and streaming heads that are of less significance. Typically, identifying the streaming heads and and waiving the KV cache in the streaming heads would largely reduce the overhead without hurting the performance that much. However, since employing both retrieval and streaming heads in one layer decomposes one large round of attention computation into two small ones, it may unexpectedly bring extra latency on accessing and indexing tensors. Based on this intuition, we impose an important improvement to the identification process of retrieval and streaming heads, in which we design a criterion that enforces exclusively retrieval or streaming heads gathered in one unique layer. In this way, we further eliminate the extra latency and only incur negligible performance degradation. Our method named \textsc{ZigzagAttention} is competitive among considered baselines owing to reduced latency and comparable performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-17T15:48:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.12407v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.12407v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 Enhancement of the energy storage and electrocaloric effect performances
  in 0.4 BCZT 0.6 BSTSn medium entropy ceramic prepared by sol gel method</h2>
                <div class="authors">
                    <strong>Authors:</strong> S. Khardazi, Z. Gargar, A. Lyubchyk, O. Zakir, D. Mezzane, M. Amjoud, A. Alimoussa, Z. Kutnjak
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Based on the traditional polycrystalline ferroelectric Ba0.85Ca0.15Zr0.10Ti0.90O3, the 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6 Ba0.9Sr0.1Ti0.9Sn0.1O3 medium entropy material with good energy storage and electrocaloric effect performances is designed and synthesized by the solgel method. The structural, dielectric, energy storage and electrocaloric effect properties of the prepared sample were studied. The findings demonstrate that the 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6 Ba0.9Sr0.1Ti0.9Sn0.1O3 ceramic simultaneously has a significant recoverable energy storage density of 255.4 mJ/cm3, an efficiency of 67.9%, a large ECE temperature change of 1.36 K, and a high ECE responsivity of 0.453 K.mm/kV under a low electric field of 30 kV/cm. Moreover, excellent temperature stability of Wrec (less than 10%) was achieved in the investigated sample 0.4BCZT 0.6BSTSn.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-17T13:05:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.mtrl-sci</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1016/j.jssc.2025.125547' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.12357v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.12357v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based
  Token Eviction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Raghavv Goel, Junyoung Park, Mukul Gagrani, Dalton Jones, Matthew Morse, Harper Langston, Mingu Lee, Chris Lott
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While long context support of large language models has extended their abilities, it also incurs challenges in memory and compute which becomes crucial bottlenecks in resource-restricted devices. Token eviction, a widely adopted post-training methodology designed to alleviate the bottlenecks by evicting less important tokens from the cache, typically uses attention scores as proxy metrics for token importance. However, one major limitation of attention score as a token-wise importance metrics is that it lacks the information about contribution of tokens to the attention output. In this paper, we propose a simple eviction criterion based on the contribution of cached tokens to attention outputs. Our method, CAOTE, optimizes for eviction error due to token eviction, by seamlessly integrating attention scores and value vectors. This is the first method which uses value tokens on top of attention-based eviction scores in closed-form. Additionally, CAOTE can act as a meta-heuristic method with flexible usage with any token eviction method. We show that CAOTE, when combined with the state-of-the-art attention score-based methods, always improves accuracies on the downstream task, indicating the importance of leveraging information from values during token eviction process.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-16T23:41:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.14051v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.14051v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 Memory-Augmented Transformers: A Systematic Review from Neuroscience
  Principles to Enhanced Model Architectures</h2>
                <div class="authors">
                    <strong>Authors:</strong> Parsa Omidi, Xingshuai Huang, Axel Laborieux, Bahareh Nikpour, Tianyu Shi, Armaghan Eshaghi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Memory is fundamental to intelligence, enabling learning, reasoning, and adaptability across biological and artificial systems. While Transformer architectures excel at sequence modeling, they face critical limitations in long-range context retention, continual learning, and knowledge integration. This review presents a unified framework bridging neuroscience principles, including dynamic multi-timescale memory, selective attention, and consolidation, with engineering advances in Memory-Augmented Transformers. We organize recent progress through three taxonomic dimensions: functional objectives (context extension, reasoning, knowledge integration, adaptation), memory representations (parameter-encoded, state-based, explicit, hybrid), and integration mechanisms (attention fusion, gated control, associative retrieval). Our analysis of core memory operations (reading, writing, forgetting, and capacity management) reveals a shift from static caches toward adaptive, test-time learning systems. We identify persistent challenges in scalability and interference, alongside emerging solutions including hierarchical buffering and surprise-gated updates. This synthesis provides a roadmap toward cognitively-inspired, lifelong-learning Transformer architectures.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-16T03:17:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10824v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10824v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 KV-Auditor: Auditing Local Differential Privacy for Correlated Key-Value
  Estimation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jingnan Xu, Leixia Wang, Xiaofeng Meng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> To protect privacy for data-collection-based services, local differential privacy (LDP) is widely adopted due to its rigorous theoretical bound on privacy loss. However, mistakes in complex theoretical analysis or subtle implementation errors may undermine its practical guarantee. To address this, auditing is crucial to confirm that LDP protocols truly protect user data. However, existing auditing methods, though, mainly target machine learning and federated learning tasks based on centralized differentially privacy (DP), with limited attention to LDP. Moreover, the few studies on LDP auditing focus solely on simple frequency estimation task for discrete data, leaving correlated key-value data - which requires both discrete frequency estimation for keys and continuous mean estimation for values - unexplored.   To bridge this gap, we propose KV-Auditor, a framework for auditing LDP-based key-value estimation mechanisms by estimating their empirical privacy lower bounds. Rather than traditional LDP auditing methods that relies on binary output predictions, KV-Auditor estimates this lower bound by analyzing unbounded output distributions, supporting continuous data. Specifically, we classify state-of-the-art LDP key-value mechanisms into interactive and non-interactive types. For non-interactive mechanisms, we propose horizontal KV-Auditor for small domains with sufficient samples and vertical KV-Auditor for large domains with limited samples. For interactive mechanisms, we design a segmentation strategy to capture incremental privacy leakage across iterations. Finally, we perform extensive experiments to validate the effectiveness of our approach, offering insights for optimizing LDP-based key-value estimators.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-15T14:17:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.11495v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.11495v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for
  Vision-Language-Action Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chenghao Liu, Jiachen Zhang, Chengxuan Li, Zhimu Zhou, Shixin Wu, Songfang Huang, Huiling Duan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vision-Language-Action (VLA) models process visual inputs independently at each timestep, discarding valuable temporal information inherent in robotic manipulation tasks. This frame-by-frame processing makes models vulnerable to visual noise while ignoring the substantial coherence between consecutive frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a training-free approach that intelligently integrates historical and current visual representations to enhance VLA inference quality. Our method employs dual-dimension detection combining efficient grayscale pixel difference analysis with attention-based semantic relevance assessment, enabling selective temporal token fusion through hard fusion strategies and keyframe anchoring to prevent error accumulation. Comprehensive experiments across LIBERO, SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0 percentage points average on LIBERO (72.4\% vs 68.4\% baseline), cross-environment validation on SimplerEnv (4.8\% relative improvement), and 8.7\% relative improvement on real robot tasks. Our approach proves model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably, TTF reveals that selective Query matrix reuse in attention mechanisms enhances rather than compromises performance, suggesting promising directions for direct KQV matrix reuse strategies that achieve computational acceleration while improving task success rates.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-15T12:03:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.LG</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19257v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19257v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless
  Edge-Device Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rui Bao, Nan Xue, Yaping Sun, Zhiyong Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The integration of wireless communications and Large Language Models (LLMs) is poised to unlock ubiquitous intelligent services, yet deploying them in wireless edge-device collaborative environments presents a critical trade-off between inference quality and end-to-end latency. A fundamental mismatch exists between task complexity and resource allocation: offloading simple queries invites prohibitive latency, while on-device models lack the capacity for demanding computations. To address this challenge, we propose a dynamic, quality-latency aware routing framework that orchestrates inference between a lightweight model on the mobile device and a powerful model on the edge server. Our framework employs two distinct cost models: for single-turn queries, it fuses a BERT-predicted semantic score with communication and computation overheads; for multi-turn dialogues, it further quantifies context-aware costs arising from model switching and KV-cache management. While maintaining full inference quality, extensive experiments demonstrate that our framework cuts average response latency by 5-15% and reduces large model invocations by 10-20% against competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-15T07:55:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>cs.AI</span><span>cs.LG</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.11291v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.11291v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mukund Choudhary, KV Aditya Srivatsa, Gaurja Aeron, Antara Raaghavi Bhattacharya, Dang Khoa Dang Dinh, Ikhlasul Akmal Hanif, Daria Kotova, Ekaterina Kochmar, Monojit Choudhury
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have demonstrated potential in reasoning tasks, but their performance on linguistics puzzles remains consistently poor. These puzzles, often derived from Linguistics Olympiad (LO) contests, provide a minimal contamination environment to assess LLMs' linguistic reasoning abilities across low-resource languages. This work analyses LLMs' performance on 629 problems across 41 low-resource languages by labelling each with linguistically informed features to unveil weaknesses. Our analyses show that LLMs struggle with puzzles involving higher morphological complexity and perform better on puzzles involving linguistic features that are also found in English. We also show that splitting words into morphemes as a pre-processing step improves solvability, indicating a need for more informed and language-specific tokenisers. These findings thus offer insights into some challenges in linguistic reasoning and modelling of low-resource languages.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-15T06:53:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.11260v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.11260v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal
  Parallelism</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zedong Liu, Shenggan Cheng, Guangming Tan, Yang You, Dingwen Tao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multimodal large language models (MLLMs) extend LLMs to handle images, videos, and audio by incorporating feature extractors and projection modules. However, these additional components -- combined with complex inference pipelines and heterogeneous workloads -- introduce significant inference overhead. Therefore, efficiently serving MLLMs remains a major challenge. Current tightly coupled serving architectures struggle to distinguish between mixed request types or adapt parallelism strategies to different inference stages, leading to increased time-to-first-token (TTFT) latency and poor resource utilization. To address this, we introduce Elastic Multimodal Parallelism (EMP), a new serving paradigm that elastically adapts to resource heterogeneity across request types and inference stages. Building upon EMP, we develop ElasticMM, an MLLM serving system that (1) separates requests into independent modality groups with dynamic resource allocation via a modality-aware load balancer; (2) decouples inference stages and enables parallelism adjustment and adaptive scaling via elastic partition scheduling; and (3) improves inference efficiency through unified multimodal prefix caching and non-blocking encoding. Experiments on diverse real-world datasets show that ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by up to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level objectives (SLOs).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-15T04:27:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.10069v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.10069v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 A Survey on Diffusion Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tianyi Li, Mingda Chen, Bowei Guo, Zhiqiang Shen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:47:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10875v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10875v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guangda Liu, Chengwei Li, Zhenyu Ning, Minyi Guo, Jieru Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have been widely deployed with rapidly expanding context windows to support increasingly demanding applications. However, long contexts pose significant deployment challenges, primarily due to the KV cache whose size grows proportionally with context length. While KV cache compression methods are proposed to address this issue, KV dropping methods incur considerable accuracy loss, and KV retrieval methods suffer from significant efficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization framework to enhance KV retrieval efficiency while preserving accuracy. On the algorithm side, FreeKV introduces speculative retrieval to shift the KV selection and recall processes out of the critical path, combined with fine-grained correction to ensure accuracy. On the system side, FreeKV employs hybrid KV layouts across CPU and GPU memory to eliminate fragmented data transfers, and leverages double-buffered streamed recall to further improve efficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy across various scenarios and models, delivering up to 13$\times$ speedup compared to SOTA KV retrieval methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T16:12:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.13109v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.13109v2' target='_blank'>pdf</a>
                </div>
            </div>
            <h1>Keyword: LLM Inference</h1>

            <div class="paper">
                <h2>#1 Bitune: Leveraging Bidirectional Attention to Improve Decoder-Only LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dawid J. Kopiczko, Tijmen Blankevoort, Yuki M. Asano
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Decoder-only large language models typically rely solely on masked causal attention, which limits their expressiveness by restricting information flow to one direction. We propose Bitune, a method that enhances pretrained decoder-only LLMs by incorporating bidirectional attention into prompt processing. We evaluate Bitune in instruction-tuning and question-answering settings, showing significant improvements in performance on commonsense reasoning, arithmetic, and language understanding tasks. Furthermore, extensive ablation studies validate the role of each component of the method, and demonstrate that Bitune is compatible with various parameter-efficient finetuning techniques and full model finetuning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:59:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2405.14862v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.14862v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn
  Dialogue with Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Adam Coscia, Shunan Guo, Eunyee Koh, Alex Endert
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As multi-turn dialogues with large language models (LLMs) grow longer and more complex, how can users better evaluate and review progress on their conversational goals? We present OnGoal, an LLM chat interface that helps users better manage goal progress. OnGoal provides real-time feedback on goal alignment through LLM-assisted evaluation, explanations for evaluation results with examples, and overviews of goal progression over time, enabling users to navigate complex dialogues more effectively. Through a study with 20 participants on a writing task, we evaluate OnGoal against a baseline chat interface without goal tracking. Using OnGoal, participants spent less time and effort to achieve their goals while exploring new prompting strategies to overcome miscommunication, suggesting tracking and visualizing goals can enhance engagement and resilience in LLM dialogues. Our findings inspired design implications for future LLM chat interfaces that improve goal communication, reduce cognitive load, enhance interactivity, and enable feedback to improve LLM performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:58:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3746059.3747746' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.21061v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.21061v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 Enabling Equitable Access to Trustworthy Financial Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> William Jurayj, Nils Holzenberger, Benjamin Van Durme
                </div>
                <div class="summary">
                    <strong>Summary:</strong> According to the United States Internal Revenue Service, ''the average American spends $\$270$ and 13 hours filing their taxes''. Even beyond the U.S., tax filing requires complex reasoning, combining application of overlapping rules with numerical calculations. Because errors can incur costly penalties, any automated system must deliver high accuracy and auditability, making modern large language models (LLMs) poorly suited for this task. We propose an approach that integrates LLMs with a symbolic solver to calculate tax obligations. We evaluate variants of this system on the challenging StAtutory Reasoning Assessment (SARA) dataset, and include a novel method for estimating the cost of deploying such a system based on real-world penalties for tax errors. We further show how combining up-front translation of plain-text rules into formal logic programs, combined with intelligently retrieved exemplars for formal case representations, can dramatically improve performance on this task and reduce costs to well below real-world averages. Our results demonstrate the promise and economic feasibility of neuro-symbolic architectures for increasing equitable access to reliable tax assistance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:55:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.CY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.21051v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.21051v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 Estimating Machine Translation Difficulty</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lorenzo Proietti, Stefano Perrella, Vilém Zouhar, Roberto Navigli, Tom Kocmi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Machine translation quality has steadily improved over the years, achieving near-perfect translations in recent benchmarks. These high-quality outputs make it difficult to distinguish between state-of-the-art models and to identify areas for future improvement. In this context, automatically identifying texts where machine translation systems struggle holds promise for developing more discriminative evaluations and guiding future research.   In this work, we address this gap by formalizing the task of translation difficulty estimation, defining a text's difficulty based on the expected quality of its translations. We introduce a new metric to evaluate difficulty estimators and use it to assess both baselines and novel approaches. Finally, we demonstrate the practical utility of difficulty estimators by using them to construct more challenging benchmarks for machine translation. Our results show that dedicated models outperform both heuristic-based methods and LLM-as-a-judge approaches, with Sentinel-src achieving the best performance. Thus, we release two improved models for difficulty estimation, Sentinel-src-24 and Sentinel-src-25, which can be used to scan large collections of texts and select those most likely to challenge contemporary machine translation systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:54:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10175v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10175v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 CogVLA: Cognition-Aligned Vision-Language-Action Model via
  Instruction-Driven Routing & Sparsification</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wei Li, Renshan Zhang, Rui Shao, Jie He, Liqiang Nie
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent Vision-Language-Action (VLA) models built on pre-trained Vision-Language Models (VLMs) require extensive post-training, resulting in high computational overhead that limits scalability and deployment.We propose CogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages instruction-driven routing and sparsification to improve both efficiency and performance. CogVLA draws inspiration from human multimodal coordination and introduces a 3-stage progressive architecture. 1) Encoder-FiLM based Aggregation Routing (EFA-Routing) injects instruction information into the vision encoder to selectively aggregate and compress dual-stream visual tokens, forming a instruction-aware latent representation. 2) Building upon this compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing) introduces action intent into the language model by pruning instruction-irrelevant visually grounded tokens, thereby achieving token-level sparsity. 3) To ensure that compressed perception inputs can still support accurate and coherent action generation, we introduce V-L-A Coupled Attention (CAtten), which combines causal vision-language attention with bidirectional action parallel decoding. Extensive experiments on the LIBERO benchmark and real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art performance with success rates of 97.4% and 70.0%, respectively, while reducing training costs by 2.5-fold and decreasing inference latency by 2.8-fold compared to OpenVLA. CogVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/CogVLA.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:50:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.21046v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.21046v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 MMG-Vid: Maximizing Marginal Gains at Segment-level and Token-level for
  Efficient Video LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junpeng Ma, Qizhe Zhang, Ming Lu, Zhibin Wang, Qiang Zhou, Jun Song, Shanghang Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Video Large Language Models (VLLMs) excel in video understanding, but their excessive visual tokens pose a significant computational challenge for real-world applications. Current methods aim to enhance inference efficiency by visual token pruning. However, they do not consider the dynamic characteristics and temporal dependencies of video frames, as they perceive video understanding as a multi-frame task. To address these challenges, we propose MMG-Vid, a novel training-free visual token pruning framework that removes redundancy by Maximizing Marginal Gains at both segment-level and token-level. Specifically, we first divide the video into segments based on frame similarity, and then dynamically allocate the token budget for each segment to maximize the marginal gain of each segment. Subsequently, we propose a temporal-guided DPC algorithm that jointly models inter-frame uniqueness and intra-frame diversity, thereby maximizing the marginal gain of each token. By combining both stages, MMG-Vid can maximize the utilization of the limited token budget, significantly improving efficiency while maintaining strong performance. Extensive experiments demonstrate that MMG-Vid can maintain over 99.5% of the original performance, while effectively reducing 75% visual tokens and accelerating the prefilling stage by 3.9x on LLaVA-OneVision-7B. Code will be released soon.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:50:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.21044v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.21044v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 From Promise to Practical Reality: Transforming Diffusion MRI Analysis
  with Fast Deep Learning Enhancement</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xinyi Wang, Michael Barnett, Frederique Boonstra, Yael Barnett, Mariano Cabezas, Arkiev D'Souza, Matthew C. Kiernan, Kain Kyle, Meng Law, Lynette Masters, Zihao Tang, Stephen Tisch, Sicong Tu, Anneke Van Der Walt, Dongang Wang, Fernando Calamante, Weidong Cai, Chenyu Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Fiber orientation distribution (FOD) is an advanced diffusion MRI modeling technique that represents complex white matter fiber configurations, and a key step for subsequent brain tractography and connectome analysis. Its reliability and accuracy, however, heavily rely on the quality of the MRI acquisition and the subsequent estimation of the FODs at each voxel. Generating reliable FODs from widely available clinical protocols with single-shell and low-angular-resolution acquisitions remains challenging but could potentially be addressed with recent advances in deep learning-based enhancement techniques. Despite advancements, existing methods have predominantly been assessed on healthy subjects, which have proved to be a major hurdle for their clinical adoption. In this work, we validate a newly optimized enhancement framework, FastFOD-Net, across healthy controls and six neurological disorders. This accelerated end-to-end deep learning framework enhancing FODs with superior performance and delivering training/inference efficiency for clinical use ($60\times$ faster comparing to its predecessor). With the most comprehensive clinical evaluation to date, our work demonstrates the potential of FastFOD-Net in accelerating clinical neuroscience research, empowering diffusion MRI analysis for disease differentiation, improving interpretability in connectome applications, and reducing measurement errors to lower sample size requirements. Critically, this work will facilitate the more widespread adoption of, and build clinical trust in, deep learning based methods for diffusion MRI enhancement. Specifically, FastFOD-Net enables robust analysis of real-world, clinical diffusion MRI data, comparable to that achievable with high-quality research acquisitions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:44:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10950v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10950v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 Reusing Computation in Text-to-Image Diffusion for Efficient Generation
  of Image Sets</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dale Decatur, Thibault Groueix, Wang Yifan, Rana Hanocka, Vladimir Kim, Matheus Gadelha
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Text-to-image diffusion models enable high-quality image generation but are computationally expensive. While prior work optimizes per-inference efficiency, we explore an orthogonal approach: reducing redundancy across correlated prompts. Our method leverages the coarse-to-fine nature of diffusion models, where early denoising steps capture shared structures among similar prompts. We propose a training-free approach that clusters prompts based on semantic similarity and shares computation in early diffusion steps. Experiments show that for models trained conditioned on image embeddings, our approach significantly reduces compute cost while improving image quality. By leveraging UnClip's text-to-image prior, we enhance diffusion step allocation for greater efficiency. Our method seamlessly integrates with existing pipelines, scales with prompt sets, and reduces the environmental and financial burden of large-scale text-to-image generation. Project page: https://ddecatur.github.io/hierarchical-diffusion/
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:35:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.21032v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.21032v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 Constraint Learning in Multi-Agent Dynamic Games from Demonstrations of
  Local Nash Interactions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhouyu Zhang, Chih-Yuan Chiu, Glen Chou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present an inverse dynamic game-based algorithm to learn parametric constraints from a given dataset of local generalized Nash equilibrium interactions between multiple agents. Specifically, we introduce mixed-integer linear programs (MILP) encoding the Karush-Kuhn-Tucker (KKT) conditions of the interacting agents, which recover constraints consistent with the Nash stationarity of the interaction demonstrations. We establish theoretical guarantees that our method learns inner approximations of the true safe and unsafe sets, as well as limitations of constraint learnability from demonstrations of Nash equilibrium interactions. We also use the interaction constraints recovered by our method to design motion plans that robustly satisfy the underlying constraints. Across simulations and hardware experiments, our methods proved capable of inferring constraints and designing interactive motion plans for various classes of constraints, both convex and non-convex, from interaction demonstrations of agents with nonlinear dynamics.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:30:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19945v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19945v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 The Ramon Llull's Thinking Machine for Automated Ideation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xinran Zhao, Boyuan Zheng, Chenglei Si, Haofei Yu, Ken Liu, Runlong Zhou, Ruochen Li, Tong Chen, Xiang Li, Yiming Zhang, Tongshuang Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for generating knowledge through symbolic recombination - as a conceptual foundation for building a modern Llull's thinking machine for research ideation. Our approach defines three compositional axes: Theme (e.g., efficiency, adaptivity), Domain (e.g., question answering, machine translation), and Method (e.g., adversarial training, linear attention). These elements represent high-level abstractions common in scientific work - motivations, problem settings, and technical approaches - and serve as building blocks for LLM-driven exploration. We mine elements from human experts or conference papers and show that prompting LLMs with curated combinations produces research ideas that are diverse, relevant, and grounded in current literature. This modern thinking machine offers a lightweight, interpretable tool for augmenting scientific creativity and suggests a path toward collaborative ideation between humans and AI.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:29:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19200v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19200v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 Pivotal inference for linear predictions in stationary processes</h2>
                <div class="authors">
                    <strong>Authors:</strong> Holger Dette, Sebastian Kühnert
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper we develop pivotal inference for the final (FPE) and relative final prediction error (RFPE) of linear forecasts in stationary processes. Our approach is based on a novel self-normalizing technique and avoids the estimation of the asymptotic variances of the empirical autocovariances. We provide pivotal confidence intervals for the (R)FPE, develop estimates for the minimal order of a linear prediction that is required to obtain a prespecified forecasting accuracy and also propose (pivotal) statistical tests for the hypotheses that the (R)FPE exceeds a given threshold. Additionally, we provide new (pivotal) inference tools for the partial autocorrelation, which do not require the assumption of an autoregressive process.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:28:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.ST</span><span>stat.ME</span><span>stat.TH</span><span>62M10, 62M20</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.21025v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.21025v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 An Agile Method for Implementing Retrieval Augmented Generation Tools in
  Industrial SMEs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mathieu Bourdin, Anas Neumann, Thomas Paviot, Robert Pellerin, Samir Lamouri
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Retrieval-Augmented Generation (RAG) has emerged as a powerful solution to mitigate the limitations of Large Language Models (LLMs), such as hallucinations and outdated knowledge. However, deploying RAG-based tools in Small and Medium Enterprises (SMEs) remains a challenge due to their limited resources and lack of expertise in natural language processing (NLP). This paper introduces EASI-RAG, Enterprise Application Support for Industrial RAG, a structured, agile method designed to facilitate the deployment of RAG systems in industrial SME contexts. EASI-RAG is based on method engineering principles and comprises well-defined roles, activities, and techniques. The method was validated through a real-world case study in an environmental testing laboratory, where a RAG tool was implemented to answer operators queries using data extracted from operational procedures. The system was deployed in under a month by a team with no prior RAG experience and was later iteratively improved based on user feedback. Results demonstrate that EASI-RAG supports fast implementation, high user adoption, delivers accurate answers, and enhances the reliability of underlying data. This work highlights the potential of RAG deployment in industrial SMEs. Future works include the need for generalization across diverse use cases and further integration with fine-tuned models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:27:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.21024v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.21024v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 Inference-Time Alignment Control for Diffusion Models with Reinforcement
  Learning Guidance</h2>
                <div class="authors">
                    <strong>Authors:</strong> Luozhijie Jin, Zijie Qiu, Jie Liu, Zijie Diao, Lifeng Qiao, Ning Ding, Alex Lamb, Xipeng Qiu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Denoising-based generative models, particularly diffusion and flow matching algorithms, have achieved remarkable success. However, aligning their output distributions with complex downstream objectives, such as human preferences, compositional accuracy, or data compressibility, remains challenging. While reinforcement learning (RL) fine-tuning methods, inspired by advances in RL from human feedback (RLHF) for large language models, have been adapted to these generative frameworks, current RL approaches are suboptimal for diffusion models and offer limited flexibility in controlling alignment strength after fine-tuning. In this work, we reinterpret RL fine-tuning for diffusion models through the lens of stochastic differential equations and implicit reward conditioning. We introduce Reinforcement Learning Guidance (RLG), an inference-time method that adapts Classifier-Free Guidance (CFG) by combining the outputs of the base and RL fine-tuned models via a geometric average. Our theoretical analysis shows that RLG's guidance scale is mathematically equivalent to adjusting the KL-regularization coefficient in standard RL objectives, enabling dynamic control over the alignment-quality trade-off without further training. Extensive experiments demonstrate that RLG consistently improves the performance of RL fine-tuned models across various architectures, RL algorithms, and downstream tasks, including human preferences, compositional control, compressibility, and text rendering. Furthermore, RLG supports both interpolation and extrapolation, thereby offering unprecedented flexibility in controlling generative alignment. Our approach provides a practical and theoretically sound solution for enhancing and controlling diffusion model alignment at inference. The source code for RLG is publicly available at the Github: https://github.com/jinluo12345/Reinforcement-learning-guidance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:18:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.21016v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.21016v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 Expert Routing with Synthetic Data for Continual Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yewon Byun, Sanket Vaibhav Mehta, Saurabh Garg, Emma Strubell, Michael Oberst, Bryan Wilder, Zachary C. Lipton
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In many real-world settings, regulations and economic incentives permit the sharing of models but not data across institutional boundaries. In such scenarios, practitioners might hope to adapt models to new domains, without losing performance on previous domains (so-called catastrophic forgetting). While any single model may struggle to achieve this goal, learning an ensemble of domain-specific experts offers the potential to adapt more closely to each individual institution. However, a core challenge in this context is determining which expert to deploy at test time. In this paper, we propose Generate to Discriminate (G2D), a domain-incremental continual learning method that leverages synthetic data to train a domain-discriminator that routes samples at inference time to the appropriate expert. Surprisingly, we find that leveraging synthetic data in this capacity is more effective than using the samples to \textit{directly} train the downstream classifier (the more common approach to leveraging synthetic data in the lifelong learning literature). We observe that G2D outperforms competitive domain-incremental learning methods on tasks in both vision and language modalities, providing a new perspective on the use of synthetic data in the lifelong learning literature.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:13:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.17009v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.17009v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 ChainReaction! Structured Approach with Causal Chains as Intermediate
  Representations for Improved and Explainable Causal Video Question Answering</h2>
                <div class="authors">
                    <strong>Authors:</strong> Paritosh Parmar, Eric Peh, Basura Fernando
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Existing Causal-Why Video Question Answering (VideoQA) models often struggle with higher-order reasoning, relying on opaque, monolithic pipelines that entangle video understanding, causal inference, and answer generation. These black-box approaches offer limited interpretability and tend to depend on shallow heuristics. We propose a novel, modular framework that explicitly decouples causal reasoning from answer generation, introducing natural language causal chains as interpretable intermediate representations. Inspired by human cognitive models, these structured cause-effect sequences bridge low-level video content with high-level causal reasoning, enabling transparent and logically coherent inference. Our two-stage architecture comprises a Causal Chain Extractor (CCE) that generates causal chains from video-question pairs, and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in these chains. To address the lack of annotated reasoning traces, we introduce a scalable method for generating high-quality causal chains from existing datasets using large language models. We also propose CauCo, a new evaluation metric for causality-oriented captioning. Experiments on three large-scale benchmarks demonstrate that our approach not only outperforms state-of-the-art models, but also yields substantial gains in explainability, user trust, and generalization -- positioning the CCE as a reusable causal reasoning engine across diverse domains. Project page: https://paritoshparmar.github.io/chainreaction/
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:10:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.CL</span><span>cs.HC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.21010v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.21010v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 Rapid Mismatch Estimation via Neural Network Informed Variational
  Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mateusz Jaszczuk, Nadia Figueroa
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With robots increasingly operating in human-centric environments, ensuring soft and safe physical interactions, whether with humans, surroundings, or other machines, is essential. While compliant hardware can facilitate such interactions, this work focuses on impedance controllers that allow torque-controlled robots to safely and passively respond to contact while accurately executing tasks. From inverse dynamics to quadratic programming-based controllers, the effectiveness of these methods relies on accurate dynamics models of the robot and the object it manipulates. Any model mismatch results in task failures and unsafe behaviors. Thus, we introduce Rapid Mismatch Estimation (RME), an adaptive, controller-agnostic, probabilistic framework that estimates end-effector dynamics mismatches online, without relying on external force-torque sensors. From the robot's proprioceptive feedback, a Neural Network Model Mismatch Estimator generates a prior for a Variational Inference solver, which rapidly converges to the unknown parameters while quantifying uncertainty. With a real 7-DoF manipulator driven by a state-of-the-art passive impedance controller, RME adapts to sudden changes in mass and center of mass at the end-effector in $\sim400$ ms, in static and dynamic settings. We demonstrate RME in a collaborative scenario where a human attaches an unknown basket to the robot's end-effector and dynamically adds/removes heavy items, showcasing fast and safe adaptation to changing dynamics during physical interaction without any external sensory system.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:09:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.21007v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.21007v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 Practical indistinguishability in a gene regulatory network inference
  problem, a case study</h2>
                <div class="authors">
                    <strong>Authors:</strong> Cody E. FitzGerald, Shelley Reich, Victor Agaba, Arjun Mathur, Michael S. Werner, Niall M. Mangan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Computationally inferring mechanistic insights from typical biological data is a challenging pursuit. Even the highest-quality experimental data come with challenges. There are always sources of noise, a limit to how often we can measure the system, and we can rarely measure all the relevant states that participate in the underlying complexity. There are usually sources of uncertainty in model development, which give rise to multiple competing model structures. To underscore the need for further analysis of structural uncertainty in modeling, we use a meta-analysis across six journals covering mathematical biology and show that a huge number of models for biological systems are developed each year, but model selection and comparison across model structures appear to be less common. We walk through a case study involving inference of regulatory network structure involved in a developmental decision in the nematode, \textit{Pristonchus pacificus}. We use real biological data and compare across 13,824 models--each corresponding to a different regulatory network structure, to determine which regulatory features are supported by the data across three experimental conditions. We find that the best-fitting models for each experimental condition share a combination of features and identify a regulatory network that is common across the model sets for each condition. This model can describe the data across the experimental conditions we considered and exhibits a high degree of positive regulation and interconnectivity between the key regulators, \textit{eud-1}, $textit{sult-1}, and \textit{nhr-40}. While the biological results are specific to the molecular biology of development in \textit{Pristonchus pacificus}, the general modeling framework and underlying challenges we faced doing this analysis are widespread across biology, chemistry, physics, and many other scientific disciplines.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:08:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>q-bio.MN</span><span>q-bio.QM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.21006v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.21006v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 Lethe: Purifying Backdoored Large Language Models with Knowledge
  Dilution</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chen Chen, Yuchen Sun, Jiaxin Gao, Xueluan Gong, Qian Wang, Ziyao Wang, Yongsen Zheng, Kwok-Yan Lam
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have seen significant advancements, achieving superior performance in various Natural Language Processing (NLP) tasks. However, they remain vulnerable to backdoor attacks, where models behave normally for standard queries but generate harmful responses or unintended output when specific triggers are activated. Existing backdoor defenses either lack comprehensiveness, focusing on narrow trigger settings, detection-only mechanisms, and limited domains, or fail to withstand advanced scenarios like model-editing-based, multi-trigger, and triggerless attacks. In this paper, we present LETHE, a novel method to eliminate backdoor behaviors from LLMs through knowledge dilution using both internal and external mechanisms. Internally, LETHE leverages a lightweight dataset to train a clean model, which is then merged with the backdoored model to neutralize malicious behaviors by diluting the backdoor impact within the model's parametric memory. Externally, LETHE incorporates benign and semantically relevant evidence into the prompt to distract LLM's attention from backdoor features. Experimental results on classification and generation domains across 5 widely used LLMs demonstrate that LETHE outperforms 8 state-of-the-art defense baselines against 8 backdoor attacks. LETHE reduces the attack success rate of advanced backdoor attacks by up to 98% while maintaining model utility. Furthermore, LETHE has proven to be cost-efficient and robust against adaptive backdoor attacks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:05:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.21004v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.21004v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic
  Support in Addiction Recovery</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junda Wang, Zonghai Yao, Zhichao Yang, Lingxi Li, Junhui Qian, Hong Yu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Substance use disorders (SUDs) affect over 36 million people worldwide, yet few receive effective care due to stigma, motivational barriers, and limited personalized support. Although large language models (LLMs) show promise for mental-health assistance, most systems lack tight integration with clinically validated strategies, reducing effectiveness in addiction recovery. We present ChatThero, a multi-agent conversational framework that couples dynamic patient modeling with context-sensitive therapeutic dialogue and adaptive persuasive strategies grounded in cognitive behavioral therapy (CBT) and motivational interviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy, Medium, and Hard resistance levels, and train ChatThero with a two-stage pipeline comprising supervised fine-tuning (SFT) followed by direct preference optimization (DPO). In evaluation, ChatThero yields a 41.5\% average gain in patient motivation, a 0.49\% increase in treatment confidence, and resolves hard cases with 26\% fewer turns than GPT-4o, and both automated and human clinical assessments rate it higher in empathy, responsiveness, and behavioral realism. The framework supports rigorous, privacy-preserving study of therapeutic conversation and provides a robust, replicable basis for research and clinical translation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T16:57:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20996v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20996v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 FFHFlow: Diverse and Uncertainty-Aware Dexterous Grasp Generation via
  Flow Variational Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qian Feng, Jianxiang Feng, Zhaopeng Chen, Rudolph Triebel, Alois Knoll
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Synthesizing diverse, uncertainty-aware grasps for multi-fingered hands from partial observations remains a critical challenge in robot learning. Prior generative methods struggle to model the intricate grasp distribution of dexterous hands and often fail to reason about shape uncertainty inherent in partial point clouds, leading to unreliable or overly conservative grasps. We propose FFHFlow, a flow-based variational framework that generates diverse, robust multi-finger grasps while explicitly quantifying perceptual uncertainty in the partial point clouds. Our approach leverages a normalizing flow-based deep latent variable model to learn a hierarchical grasp manifold, overcoming the mode collapse and rigid prior limitations of conditional Variational Autoencoders (cVAEs). By exploiting the invertibility and exact likelihoods of flows, FFHFlow introspects shape uncertainty in partial observations and identifies novel object structures, enabling risk-aware grasp synthesis. To further enhance reliability, we integrate a discriminative grasp evaluator with the flow likelihoods, formulating an uncertainty-aware ranking strategy that prioritizes grasps robust to shape ambiguity. Extensive experiments in simulation and real-world setups demonstrate that FFHFlow outperforms state-of-the-art baselines (including diffusion models) in grasp diversity and success rate, while achieving run-time efficient sampling. We also showcase its practical value in cluttered and confined environments, where diversity-driven sampling excels by mitigating collisions (Project Page: https://sites.google.com/view/ffhflow/home/).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T16:44:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.15161v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.15161v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 Dynamic Context Compression for Efficient RAG</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shuyu Guo, Zhaochun Ren
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but incurs significant inference costs due to lengthy retrieved contexts. While context compression mitigates this issue, existing methods apply fixed compression rates, over-compressing simple queries or under-compressing complex ones. We propose Adaptive Context Compression for RAG (ACC-RAG), a framework that dynamically adjusts compression rates based on input complexity, optimizing inference efficiency without sacrificing accuracy. ACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with a context selector to retain minimal sufficient information, akin to human skimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms fixed-rate methods and matches/unlocks over 4 times faster inference versus standard RAG while maintaining or improving accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T16:42:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.22931v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.22931v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 Program Semantic Inequivalence Game with Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Antonio Valerio Miceli-Barone, Vaishak Belle, Ali Payani
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) can achieve strong performance on everyday coding tasks, but they can fail on complex tasks that require non-trivial reasoning about program semantics. Finding training examples to teach LLMs to solve these tasks can be challenging.   In this work, we explore a method to synthetically generate code reasoning training data based on a semantic inequivalence game SInQ: a generator agent creates program variants that are semantically distinct, derived from a dataset of real-world programming tasks, while an evaluator agent has to identify input examples that cause the original programs and the generated variants to diverge in their behaviour, with the agents training each other semi-adversarially. We prove that this setup enables theoretically unlimited improvement through self-play in the limit of infinite computational resources.   We evaluated our approach on multiple code generation and understanding benchmarks, including cross-language vulnerability detection (Lu et al., 2021), where our method improves vulnerability detection in C/C++ code despite being trained exclusively on Python code, and the challenging Python builtin identifier swap benchmark (Miceli-Barone et al., 2023), showing that whereas modern LLMs still struggle with this benchmark, our approach yields substantial improvements.   We release the code needed to replicate the experiments, as well as the generated synthetic data, which can be used to fine-tune LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T16:38:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.PL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.03818v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.03818v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 Efficient Neuro-Symbolic Learning of Constraints and Objective</h2>
                <div class="authors">
                    <strong>Authors:</strong> Marianne Defresne, Romain Gambardella, Sophie Barbe, Thomas Schiex
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the ongoing quest for hybridizing discrete reasoning with neural nets, there is an increasing interest in neural architectures that can learn how to solve discrete reasoning or optimization problems from natural inputs, a task that Large Language Models seem to struggle with.   Objectives: We introduce a differentiable neuro-symbolic architecture and a loss function dedicated to learning how to solve NP-hard reasoning problems.   Methods: Our new probabilistic loss allows for learning both the constraints and the objective, thus delivering a complete model that can be scrutinized and completed with side constraints. By pushing the combinatorial solver out of the training loop, our architecture also offers scalable training while exact inference gives access to maximum accuracy.   Results: We empirically show that it can efficiently learn how to solve NP-hard reasoning problems from natural inputs. On three variants of the Sudoku benchmark -- symbolic, visual, and many-solution --, our approach requires a fraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut task, it optimizes the regret better than a Decision-Focused-Learning regret-dedicated loss. Finally, it efficiently learns the energy optimization formulation of the large real-world problem of designing proteins.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T16:33:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.LO</span><span>cs.SC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20978v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20978v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 ConfLogger: Enhance Systems' Configuration Diagnosability through
  Configuration Logging</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shiwen Shan, Yintong Huo, Yuxin Su, Zhining Wang, Dan Li, Zibin Zheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modern configurable systems offer customization via intricate configuration spaces, yet such flexibility introduces pervasive configuration-related issues such as misconfigurations and latent softwarebugs. Existing diagnosability supports focus on post-failure analysis of software behavior to identify configuration issues, but none of these approaches look into whether the software clue sufficient failure information for diagnosis. To fill in the blank, we propose the idea of configuration logging to enhance existing logging practices at the source code level. We develop ConfLogger, the first tool that unifies configuration-aware static taint analysis with LLM-based log generation to enhance software configuration diagnosability. Specifically, our method 1) identifies configuration-sensitive code segments by tracing configuration-related data flow in the whole project, and 2) generates diagnostic log statements by analyzing configuration code contexts. Evaluation results on eight popular software systems demonstrate the effectiveness of ConfLogger to enhance configuration diagnosability. Specifically, ConfLogger-enhanced logs successfully aid a log-based misconfiguration diagnosis tool to achieve 100% accuracy on error localization in 30 silent misconfiguration scenarios, with 80% directly resolvable through explicit configuration information exposed. In addition, ConfLogger achieves 74% coverage of existing logging points, outperforming baseline LLM-based loggers by 12% and 30%. It also gains 8.6% higher in precision, 79.3% higher in recall, and 26.2% higher in F1 compared to the state-of-the-art baseline in terms of variable logging while also augmenting diagnostic value. A controlled user study on 22 cases further validated its utility, speeding up diagnostic time by 1.25x and improving troubleshooting accuracy by 251.4%.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T16:31:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3744916.3764570' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.20977v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20977v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue
  Agents</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tianjian Liu, Fanqi Wan, Jiajian Guo, Xiaojun Quan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Proactive dialogue has emerged as a critical and challenging research problem in advancing large language models (LLMs). Existing works predominantly focus on domain-specific or task-oriented scenarios, which leads to fragmented evaluations and limits the comprehensive exploration of models' proactive conversation abilities. In this work, we propose ProactiveEval, a unified framework designed for evaluating proactive dialogue capabilities of LLMs. This framework decomposes proactive dialogue into target planning and dialogue guidance, establishing evaluation metrics across various domains. Moreover, it also enables the automatic generation of diverse and challenging evaluation data. Based on the proposed framework, we develop 328 evaluation environments spanning 6 distinct domains. Through experiments with 22 different types of LLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional performance on target planning and dialogue guidance tasks, respectively. Finally, we investigate how reasoning capabilities influence proactive behaviors and discuss their implications for future model development.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T16:26:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20973v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20973v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 Cosmo-Learn: code for learning cosmology using different methods and
  mock data</h2>
                <div class="authors">
                    <strong>Authors:</strong> Reginald Christian Bernardo, Daniela Grandón, Jackson Levi Said, Víctor H. Cárdenas, Gene Carlo Belinario, Reinabelle Reyes
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present cosmo_learn, an open-source python-based software package designed to simulate cosmological data and perform data-driven inference using a range of modern statistical and machine learning techniques. Motivated by the growing complexity of cosmological models and the emergence of observational tensions, cosmo_learn provides a standardized and flexible framework for benchmarking cosmological inference methods. The package supports realistic noise modeling for key observables in the late Universe, including cosmic chronometers, supernovae Ia, baryon acoustic oscillations, redshift space distortions, and gravitational wave bright sirens. We demonstrate the internal consistency of the simulated data with the input cosmology via residuals and parameter recovery using a fiducial $w$CDM model. Built-in learning and inference modules include traditional Markov Chain Monte Carlo, as well as more recent approaches such as genetic algorithms, Gaussian processes, Bayesian ridge regression, and artificial neural networks. These methods are implemented in a modular and extensible architecture designed to facilitate comparisons across inference strategies in a common pipeline. By providing a flexible and transparent simulation and learning environment, cosmo_learn supports both educational and research efforts at the intersection of cosmology, statistics, and machine learning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T16:25:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.CO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20971v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20971v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 DrivingGaussian++: Towards Realistic Reconstruction and Editable
  Simulation for Surrounding Dynamic Driving Scenes</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yajiao Xiong, Xiaoyu Zhou, Yongtao Wan, Deqing Sun, Ming-Hsuan Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present DrivingGaussian++, an efficient and effective framework for realistic reconstructing and controllable editing of surrounding dynamic autonomous driving scenes. DrivingGaussian++ models the static background using incremental 3D Gaussians and reconstructs moving objects with a composite dynamic Gaussian graph, ensuring accurate positions and occlusions. By integrating a LiDAR prior, it achieves detailed and consistent scene reconstruction, outperforming existing methods in dynamic scene reconstruction and photorealistic surround-view synthesis. DrivingGaussian++ supports training-free controllable editing for dynamic driving scenes, including texture modification, weather simulation, and object manipulation, leveraging multi-view images and depth priors. By integrating large language models (LLMs) and controllable editing, our method can automatically generate dynamic object motion trajectories and enhance their realism during the optimization process. DrivingGaussian++ demonstrates consistent and realistic editing results and generates dynamic multi-view driving scenarios, while significantly enhancing scene diversity. More results and code can be found at the project site: https://xiong-creator.github.io/DrivingGaussian_plus.github.io
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T16:22:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20965v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20965v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 SaRoHead: Detecting Satire in a Multi-Domain Romanian News Headline
  Dataset</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mihnea-Alexandru Vîrlan, Răzvan-Alexandru Smădu, Dumitru-Clementin Cercel, Florin Pop, Mihaela-Claudia Cercel
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The primary goal of a news headline is to summarize an event in as few words as possible. Depending on the media outlet, a headline can serve as a means to objectively deliver a summary or improve its visibility. For the latter, specific publications may employ stylistic approaches that incorporate the use of sarcasm, irony, and exaggeration, key elements of a satirical approach. As such, even the headline must reflect the tone of the satirical main content. Current approaches for the Romanian language tend to detect the non-conventional tone (i.e., satire and clickbait) of the news content by combining both the main article and the headline. Because we consider a headline to be merely a brief summary of the main article, we investigate in this paper the presence of satirical tone in headlines alone, testing multiple baselines ranging from standard machine learning algorithms to deep learning models. Our experiments show that Bidirectional Transformer models outperform both standard machine-learning approaches and Large Language Models (LLMs), particularly when the meta-learning Reptile approach is employed.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T16:22:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.07612v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.07612v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 Distinct weak asymmetric interactions shape human brain functions as
  probability fluxes</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yoshiaki Horiike, Shin Fujishiro
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The functional computation of the human brain arises from the collective behaviour of the underlying neural network. The emerging technology enables the recording of population activity in neurons, and the theory of neural networks is expected to explain and extract functional computations from the data. Thermodynamically, a large proportion of the whole-body energy is consumed by the brain, and functional computation of the human brain seems to involve high energy consumption. The human brain, however, does not increase its energy consumption with its function, and most of its energy consumption is not involved in specific brain function: how can the human brain perform its wide repertoire of functional computations without drastically changing its energy consumption? Here, we present a mechanism to perform functional computation by subtle modification of the interaction network among the brain regions. We first show that, by analyzing the data of spontaneous and task-induced whole-cerebral-cortex activity, the probability fluxes, which are the microscopic irreversible measure of state transitions, exhibit unique patterns depending on the task being performed, indicating that the human brain function is a distinct sequence of the brain state transitions. We then fit the parameters of Ising spin systems with asymmetric interactions, where we reveal that the symmetric interactions among the brain regions are strong and task-independent, but the antisymmetric interactions are subtle and task-dependent, and the inferred model reproduces most of the observed probability flux patterns. Our results indicate that the human brain performs its functional computation by subtly modifying the antisymmetric interaction among the brain regions, which might be possible with a small amount of energy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T16:19:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.bio-ph</span><span>cond-mat.dis-nn</span><span>cond-mat.stat-mech</span><span>physics.data-an</span><span>q-bio.NC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20961v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20961v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 Nonparametric Inference for Noise Covariance Kernels in Parabolic SPDEs
  using Space-Time Infill-Asymptotics</h2>
                <div class="authors">
                    <strong>Authors:</strong> Andreas Petersson, Dennis Schroers
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We develop an asymptotic limit theory for nonparametric estimation of the noise covariance kernel in linear parabolic stochastic partial differential equations (SPDEs) with additive colored noise, using space-time infill asymptotics. The method employs discretized infinite-dimensional realized covariations and requires only mild regularity assumptions on the kernel to ensure consistent estimation and asymptotic normality of the estimator. On this basis, we construct omnibus goodness-of-fit tests for the noise covariance that are independent of the SPDE's differential operator. Our framework accommodates a variety of spatial sampling schemes and allows for reliable inference even when spatial resolution is coarser than temporal resolution.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T16:09:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.ST</span><span>stat.TH</span><span>60H15, 62M20, 62G05, 62G10, 35R60</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20947v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20947v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 STARE at the Structure: Steering ICL Exemplar Selection with Structural
  Alignment</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiaqian Li, Qisheng Hu, Jing Li, Wenya Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In-Context Learning (ICL) has become a powerful paradigm that enables LLMs to perform a wide range of tasks without task-specific fine-tuning. However, the effectiveness of ICL heavily depends on the quality of exemplar selection. In particular, for structured prediction tasks such as semantic parsing, existing ICL selection strategies often overlook structural alignment, leading to suboptimal performance and poor generalization. To address this issue, we propose a novel two-stage exemplar selection strategy that achieves a strong balance between efficiency, generalizability, and performance. First, we fine-tune a BERT-based retriever using structure-aware supervision, guiding it to select exemplars that are both semantically relevant and structurally aligned. Then, we enhance the retriever with a plug-in module, which amplifies syntactically meaningful information in the hidden representations. This plug-in is model-agnostic, requires minimal overhead, and can be seamlessly integrated into existing pipelines. Experiments on four benchmarks spanning three semantic parsing tasks demonstrate that our method consistently outperforms existing baselines with multiple recent LLMs as inference-time models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T16:04:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20944v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20944v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 AI Reasoning Models for Problem Solving in Physics</h2>
                <div class="authors">
                    <strong>Authors:</strong> Amir Bralin, N. Sanjay Rebello
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reasoning models are the new generation of Large Language Models (LLMs) capable of complex problem solving. Their reliability in solving introductory physics problems was tested by evaluating a sample of n = 5 solutions generated by one such model -- OpenAI's o3-mini -- per each problem from 20 chapters of a standard undergraduate textbook. In total, N = 408 problems were given to the model and N x n = 2,040 generated solutions examined. The model successfully solved 94% of the problems posed, excelling at the beginning topics in mechanics but struggling with the later ones such as waves and thermodynamics.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T16:02:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.ed-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20941v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20941v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 How Can Input Reformulation Improve Tool Usage Accuracy in a Complex
  Dynamic Environment? A Study on $τ$-bench</h2>
                <div class="authors">
                    <strong>Authors:</strong> Venkatesh Mishra, Amir Saeidi, Satyam Raj, Mutsumi Nakamura, Jayanth Srinivasa, Gaowen Liu, Ali Payani, Chitta Baral
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in reasoning and planning capabilities of large language models (LLMs) have enabled their potential as autonomous agents capable of tool use in dynamic environments. However, in multi-turn conversational environments like $\tau$-bench, these agents often struggle with consistent reasoning, adherence to domain-specific policies, and extracting correct information over a long horizon of tool-calls and conversation. To capture and mitigate these failures, we conduct a comprehensive manual analysis of the common errors occurring in the conversation trajectories. We then experiment with reformulations of inputs to the tool-calling agent for improvement in agent decision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA) framework, which automatically reformulates user queries augmented with relevant domain rules and tool suggestions for the tool-calling agent to focus on. The results show that IRMA significantly outperforms ReAct, Function Calling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in overall pass^5 scores. These findings highlight the superior reliability and consistency of IRMA compared to other methods in dynamic environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:57:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20931v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20931v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 Chain-of-Alpha: Unleashing the Power of Large Language Models for Alpha
  Mining in Quantitative Trading</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lang Cao, Zekun Xi, Long Liao, Ziwei Yang, Zheng Cao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Alpha factor mining is a fundamental task in quantitative trading, aimed at discovering interpretable signals that can predict asset returns beyond systematic market risk. While traditional methods rely on manual formula design or heuristic search with machine learning, recent advances have leveraged Large Language Models (LLMs) for automated factor discovery. However, existing LLM-based alpha mining approaches remain limited in terms of automation, generality, and efficiency. In this paper, we propose Chain-of-Alpha, a novel, simple, yet effective and efficient LLM-based framework for fully automated formulaic alpha mining. Our method features a dual-chain architecture, consisting of a Factor Generation Chain and a Factor Optimization Chain, which iteratively generate, evaluate, and refine candidate alpha factors using only market data, while leveraging backtest feedback and prior optimization knowledge. The two chains work synergistically to enable high-quality alpha discovery without human intervention and offer strong scalability. Extensive experiments on real-world A-share benchmarks demonstrate that Chain-of-Alpha outperforms existing baselines across multiple metrics, presenting a promising direction for LLM-driven quantitative research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:52:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.06312v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.06312v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 Palm distributions of superposed point processes for statistical
  inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mario Beraha, Federico Camerlenghi, Lorenzo Ghilotti
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Palm distributions play a central role in the study of point processes and their associated summary statistics. In this paper, we characterize the Palm distributions of the superposition of two independent point processes, establishing a simple mixture representation depending on the point processes' Palm distributions and moment measures. We explore two statistical applications enabled by our main result. First, we consider minimum contrast estimation for superposed point processes based on Ripley's $K$ function. Second, we focus on the class of shot-noise Cox processes and obtain a tractable expression for the Janossy density which leads to maximum likelihood estimation via a novel expectation-maximization algorithm. Both approaches are validated through numerical simulations. Extensions to the superposition of multiple point processes, and higher-order Palm distributions, are also discussed.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:52:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.ST</span><span>math.PR</span><span>stat.TH</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20924v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20924v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics</h2>
                <div class="authors">
                    <strong>Authors:</strong> Carter Blum, Katja Filippova, Ann Yuan, Asma Ghandeharioun, Julian Zimmert, Fred Zhang, Jessica Hoffmann, Tal Linzen, Martin Wattenberg, Lucas Dixon, Mor Geva
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) struggle with cross-lingual knowledge transfer: they hallucinate when asked in one language about facts expressed in a different language during training. This work introduces a controlled setting to study the causes and dynamics of this phenomenon by training small Transformer models from scratch on synthetic multilingual datasets. We identify a learning phase wherein a model develops either separate or unified representations of the same facts across languages, and show that unification is essential for cross-lingual transfer. We also show that the degree of unification depends on mutual information between facts and training data language, and on how easy it is to extract that language. Based on these insights, we develop methods to modulate the level of cross-lingual transfer by manipulating data distribution and tokenization, and we introduce metrics and visualizations to formally characterize their effects on unification. Our work shows how controlled settings can shed light on pre-training dynamics and suggests new directions for improving cross-lingual transfer in LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:51:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.11017v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.11017v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 Static Factorisation of Probabilistic Programs With User-Labelled Sample
  Statements and While Loops</h2>
                <div class="authors">
                    <strong>Authors:</strong> Markus Böck, Jürgen Cito
                </div>
                <div class="summary">
                    <strong>Summary:</strong> It is commonly known that any Bayesian network can be implemented as a probabilistic program, but the reverse direction is not so clear. In this work, we address the open question to what extent a probabilistic program with user-labelled sample statements and while loops - features found in languages like Gen, Turing, and Pyro - can be represented graphically. To this end, we extend existing operational semantics to support these language features. By translating a program to its control-flow graph, we define a sound static analysis that approximates the dependency structure of the random variables in the program. As a result, we obtain a static factorisation of the implicitly defined program density, which is equivalent to the known Bayesian network factorisation for programs without loops and constant labels, but constitutes a novel graphical representation for programs that define an unbounded number of random variables via loops or dynamic labels. We further develop a sound program slicing technique to leverage this structure to statically enable three well-known optimisations for the considered program class: we reduce the variance of gradient estimates in variational inference and we speed up both single-site Metropolis Hastings and sequential Monte Carlo. These optimisations are proven correct and empirically shown to match or outperform existing techniques.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:51:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20922v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20922v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 SageLM: A Multi-aspect and Explainable Large Language Model for Speech
  Judgement</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuan Ge, Junxiang Zhang, Xiaoqian Liu, Bei Li, Xiangnan Ma, Chenglong Wang, Kaiyang Ye, Yangfan Du, Linfeng Zhang, Yuxin Huang, Tong Xiao, Zhengtao Yu, JingBo Zhu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Speech-to-Speech (S2S) Large Language Models (LLMs) are foundational to natural human-computer interaction, enabling end-to-end spoken dialogue systems. However, evaluating these models remains a fundamental challenge. We propose \texttt{SageLM}, an end-to-end, multi-aspect, and explainable speech LLM for comprehensive S2S LLMs evaluation. First, unlike cascaded approaches that disregard acoustic features, SageLM jointly assesses both semantic and acoustic dimensions. Second, it leverages rationale-based supervision to enhance explainability and guide model learning, achieving superior alignment with evaluation outcomes compared to rule-based reinforcement learning methods. Third, we introduce \textit{SpeechFeedback}, a synthetic preference dataset, and employ a two-stage training paradigm to mitigate the scarcity of speech preference data. Trained on both semantic and acoustic dimensions, SageLM achieves an 82.79\% agreement rate with human evaluators, outperforming cascaded and SLM-based baselines by at least 7.42\% and 26.20\%, respectively.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:47:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20916v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20916v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 Research Challenges in Relational Database Management Systems for LLM
  Queries</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kerem Akillioglu, Anurag Chakraborty, Sairaj Voruganti, M. Tamer Özsu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have become essential for applications such as text summarization, sentiment analysis, and automated question-answering. Recently, LLMs have also been integrated into relational database management systems to enhance querying and support advanced data processing. Companies such as Amazon, Databricks, Google, and Snowflake offer LLM invocation directly within SQL, denoted as LLM queries, to boost data insights. However, open-source solutions currently have limited functionality and poor performance. In this work, we present an early exploration of two open-source systems and one enterprise platform, using five representative queries to expose functional, performance, and scalability limits in today's SQL-invoked LLM integrations. We identify three main issues: enforcing structured outputs, optimizing resource utilization, and improving query planning. We implemented initial solutions and observed improvements in accommodating LLM powered SQL queries. These early gains demonstrate that tighter integration of LLM+DBMS is the key to scalable and efficient processing of LLM queries.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:41:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20912v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20912v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 Quantum Verifiable Rewards for Post-Training Qiskit Code Assistant</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nicolas Dupuis, Adarsh Tiwari, Youssef Mroueh, David Kremer, Ismael Faro, Juan Cruz-Benito
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Qiskit is an open-source quantum computing framework that allows users to design, simulate, and run quantum circuits on real quantum hardware. We explore post-training techniques for LLMs to assist in writing Qiskit code. We introduce quantum verification as an effective method for ensuring code quality and executability on quantum hardware. To support this, we developed a synthetic data pipeline that generates quantum problem-unit test pairs and used it to create preference data for aligning LLMs with DPO. Additionally, we trained models using GRPO, leveraging quantum-verifiable rewards provided by the quantum hardware. Our best-performing model, combining DPO and GRPO, surpasses the strongest open-source baselines on the challenging Qiskit-HumanEval-hard benchmark.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:37:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>quant-ph</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20907v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20907v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 Inferring processes within dynamic forest models using hybrid modeling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Maximilian Pichler, Yannek Käber
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modeling forest dynamics under novel climatic conditions requires a careful balance between process-based understanding and empirical flexibility. Dynamic Vegetation Models (DVM) represent ecological processes mechanistically, but their performance is prone to misspecified assumptions about functional forms. Inferring the structure of these processes and their functional forms correctly from data remains a major challenge because current approaches, such as plug-in estimators, have proven ineffective. We introduce Forest Informed Neural Networks (FINN), a hybrid modeling approach that combines a forest gap model with deep neural networks (DNN). FINN replaces processes with DNNs, which are then calibrated alongside the other mechanistic components in one unified step. In a case study on the Barro Colorado Island 50-ha plot we demonstrate that replacing the growth process with a DNN improves predictive performance and succession trajectories compared to a mechanistic version of FINN. Furthermore, we discovered that the DNN learned an ecologically plausible, improved functional form of the growth process, which we extracted from the DNN using explainable AI. In conclusion, our new hybrid modeling approach offers a versatile opportunity to infer forest dynamics from data and to improve forecasts of ecosystem trajectories under unprecedented environmental change.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:37:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>q-bio.QM</span><span>cs.LG</span><span>stat.ML</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.01228v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.01228v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 Clustering of DESI galaxies split by thermal Sunyaev-Zeldovich effect</h2>
                <div class="authors">
                    <strong>Authors:</strong> M. Rashkovetskyi, D. J. Eisenstein, J. Aguilar, S. Ahlen, A. Anand, D. Bianchi, D. Brooks, F. J. Castander, T. Claybaugh, A. Cuceu, K. S. Dawson, A. de la Macorra, Arjun Dey, P. Doel, S. Ferraro, A. Font-Ribera, J. E. Forero-Romero, E. Gaztañaga, G. Gutierrez, H. K. Herrera-Alcantar, K. Honscheid, C. Howlett, M. Ishak, R. Joyce, R. Kehoe, T. Kisner, A. Kremin, O. Lahav, A. Lambert, M. Landriau, M. Manera, R. Miquel, E. Mueller, S. Nadathur, N. Palanque-Delabrouille, W. J. Percival, F. Prada, I. Pérez-Ràfols, A. J. Ross, G. Rossi, E. Sanchez, D. Schlegel, M. Schubnell, J. Silber, D. Sprayberry, G. Tarlé, B. A. Weaver, R. Zhou, H. Zou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The thermal Sunyaev-Zeldovich (tSZ) effect is associated with galaxy clusters - extremely large and dense structures tracing the dark matter with a higher bias than isolated galaxies. We propose to use the tSZ data to separate galaxies from redshift surveys into distinct subpopulations corresponding to different densities and biases independently of the redshift survey systematics. Leveraging the information from different environments, as in density-split and density-marked clustering, is known to tighten the constraints on cosmological parameters, like $\Omega_m$, $\sigma_8$ and neutrino mass. We use data from the Dark Energy Spectroscopic Instrument (DESI) and the Atacama Cosmology Telescope (ACT) in their region of overlap to demonstrate informative tSZ splitting of Luminous Red Galaxies (LRGs). We discover a significant increase in the large-scale clustering of DESI LRGs corresponding to detections starting from 1-2 sigma in the ACT DR6 + Planck tSZ Compton-$y$ map, below the cluster candidate threshold (4 sigma). We also find that such galaxies have higher line-of-sight coordinate (and velocity) dispersions and a higher number of close neighbors than both the full sample and near-zero tSZ regions. We produce simple simulations of tSZ maps that are intrinsically consistent with galaxy catalogs and do not include systematic effects, and find a similar pattern of large-scale clustering enhancement with tSZ effect significance. Moreover, we observe that this relative bias pattern remains largely unchanged with variations in the galaxy-halo connection model in our simulations. This is promising for future cosmological inference from tSZ-split clustering with semi-analytical models. Thus, we demonstrate that valuable cosmological information is present in the lower signal-to-noise regions of the thermal Sunyaev-Zeldovich map, extending far beyond the individual cluster candidates.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:36:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.CO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20904v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20904v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 Improving Quantization with Post-Training Model Expansion</h2>
                <div class="authors">
                    <strong>Authors:</strong> Giuseppe Franco, Pablo Monteagudo-Lago, Ian Colbert, Nicholas Fraser, Michaela Blott
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The size of a model has been a strong predictor of its quality, as well as its cost. As such, the trade-off between model cost and quality has been well-studied. Post-training optimizations like quantization and pruning have typically focused on reducing the overall volume of pre-trained models to reduce inference costs while maintaining model quality. However, recent advancements have introduced optimization techniques that, interestingly, expand models post-training, increasing model size to improve quality when reducing volume. For instance, to enable 4-bit weight and activation quantization, incoherence processing often necessitates inserting online Hadamard rotations in the compute graph, and preserving highly sensitive weights often calls for additional higher precision computations. However, if application requirements cannot be met, the prevailing solution is to relax quantization constraints. In contrast, we demonstrate post-training model expansion is a viable strategy to improve model quality within a quantization co-design space, and provide theoretical justification. We show it is possible to progressively and selectively expand the size of a pre-trained large language model (LLM) to improve model quality without end-to-end retraining. In particular, when quantizing the weights and activations to 4 bits for Llama3 1B, we reduce the gap to full-precision perplexity by an average of 9% relative to both QuaRot and SpinQuant with only 5% more parameters, which is still a 3.8% reduction in volume relative to a BF16 reference model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:33:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.17513v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.17513v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 Language-Enhanced Mobile Manipulation for Efficient Object Search in
  Indoor Environments</h2>
                <div class="authors">
                    <strong>Authors:</strong> Liding Zhang, Zeqi Li, Kuanqi Cai, Qian Huang, Zhenshan Bing, Alois Knoll
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Enabling robots to efficiently search for and identify objects in complex, unstructured environments is critical for diverse applications ranging from household assistance to industrial automation. However, traditional scene representations typically capture only static semantics and lack interpretable contextual reasoning, limiting their ability to guide object search in completely unfamiliar settings. To address this challenge, we propose a language-enhanced hierarchical navigation framework that tightly integrates semantic perception and spatial reasoning. Our method, Goal-Oriented Dynamically Heuristic-Guided Hierarchical Search (GODHS), leverages large language models (LLMs) to infer scene semantics and guide the search process through a multi-level decision hierarchy. Reliability in reasoning is achieved through the use of structured prompts and logical constraints applied at each stage of the hierarchy. For the specific challenges of mobile manipulation, we introduce a heuristic-based motion planner that combines polar angle sorting with distance prioritization to efficiently generate exploration paths. Comprehensive evaluations in Isaac Sim demonstrate the feasibility of our framework, showing that GODHS can locate target objects with higher search efficiency compared to conventional, non-semantic search strategies. Website and Video are available at: https://drapandiger.github.io/GODHS
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:27:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20899v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20899v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 HAS-GPU: Efficient Hybrid Auto-scaling with Fine-grained GPU Allocation
  for SLO-aware Serverless Inferences</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jianfeng Gu, Puxuan Wang, Isaac David Nunez Araya, Kai Huang, Michael Gerndt
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Serverless Computing (FaaS) has become a popular paradigm for deep learning inference due to the ease of deployment and pay-per-use benefits. However, current serverless inference platforms encounter the coarse-grained and static GPU resource allocation problems during scaling, which leads to high costs and Service Level Objective (SLO) violations in fluctuating workloads. Meanwhile, current platforms only support horizontal scaling for GPU inferences, thus the cold start problem further exacerbates the problems. In this paper, we propose HAS-GPU, an efficient Hybrid Auto-scaling Serverless architecture with fine-grained GPU allocation for deep learning inferences. HAS-GPU proposes an agile scheduler capable of allocating GPU Streaming Multiprocessor (SM) partitions and time quotas with arbitrary granularity and enables significant vertical quota scalability at runtime. To resolve performance uncertainty introduced by massive fine-grained resource configuration spaces, we propose the Resource-aware Performance Predictor (RaPP). Furthermore, we present an adaptive hybrid auto-scaling algorithm with both horizontal and vertical scaling to ensure inference SLOs and minimize GPU costs. The experiments demonstrated that compared to the mainstream serverless inference platform, HAS-GPU reduces function costs by an average of 10.8x with better SLO guarantees. Compared to state-of-the-art spatio-temporal GPU sharing serverless framework, HAS-GPU reduces function SLO violation by 4.8x and cost by 1.72x on average.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:23:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1007/978-3-031-99854-6_11' target='_blank'>doi</a><a href='http://arxiv.org/abs/2505.01968v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.01968v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 The Uneven Impact of Post-Training Quantization in Machine Translation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Benjamin Marie, Atsushi Fujita
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Quantization is essential for deploying large language models (LLMs) on resource-constrained hardware, but its implications for multilingual tasks remain underexplored. We conduct the first large-scale evaluation of post-training quantization (PTQ) on machine translation across 55 languages using five LLMs ranging from 1.7B to 70B parameters. Our analysis reveals that while 4-bit quantization often preserves translation quality for high-resource languages and large models, significant degradation occurs for low-resource and typologically diverse languages, particularly in 2-bit settings. We compare four quantization techniques (AWQ, BitsAndBytes, GGUF, and AutoRound), showing that algorithm choice and model size jointly determine robustness. GGUF variants provide the most consistent performance, even at 2-bit precision. Additionally, we quantify the interactions between quantization, decoding hyperparameters, and calibration languages, finding that language-matched calibration offers benefits primarily in low-bit scenarios. Our findings offer actionable insights for deploying multilingual LLMs for machine translation under quantization constraints, especially in low-resource settings.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:22:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20893v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20893v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 PromptSleuth: Detecting Prompt Injection via Semantic Intent Invariance</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mengxiao Wang, Yuxuan Zhang, Guofei Gu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are increasingly integrated into real-world applications, from virtual assistants to autonomous agents. However, their flexibility also introduces new attack vectors-particularly Prompt Injection (PI), where adversaries manipulate model behavior through crafted inputs. As attackers continuously evolve with paraphrased, obfuscated, and even multi-task injection strategies, existing benchmarks are no longer sufficient to capture the full spectrum of emerging threats.   To address this gap, we construct a new benchmark that systematically extends prior efforts. Our benchmark subsumes the two widely-used existing ones while introducing new manipulation techniques and multi-task scenarios, thereby providing a more comprehensive evaluation setting. We find that existing defenses, though effective on their original benchmarks, show clear weaknesses under our benchmark, underscoring the need for more robust solutions. Our key insight is that while attack forms may vary, the adversary's intent-injecting an unauthorized task-remains invariant. Building on this observation, we propose PromptSleuth, a semantic-oriented defense framework that detects prompt injection by reasoning over task-level intent rather than surface features. Evaluated across state-of-the-art benchmarks, PromptSleuth consistently outperforms existing defense while maintaining comparable runtime and cost efficiency. These results demonstrate that intent-based semantic reasoning offers a robust, efficient, and generalizable strategy for defending LLMs against evolving prompt injection threats.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:19:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20890v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20890v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 A Highly Clean Recipe Dataset with Ingredient States Annotation for
  State Probing Task</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mashiro Toyooka, Kiyoharu Aizawa, Yoko Yamakata
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are trained on a vast amount of procedural texts, but they do not directly observe real-world phenomena. In the context of cooking recipes, this poses a challenge, as intermediate states of ingredients are often omitted, making it difficult for models to track ingredient states and understand recipes accurately. In this paper, we apply state probing, a method for evaluating a language model's understanding of the world, to the domain of cooking. We propose a new task and dataset for evaluating how well LLMs can recognize intermediate ingredient states during cooking procedures. We first construct a new Japanese recipe dataset with clear and accurate annotations of ingredient state changes, collected from well-structured and controlled recipe texts. Using this dataset, we design three novel tasks to evaluate whether LLMs can track ingredient state transitions and identify ingredients present at intermediate steps. Our experiments with widely used LLMs, such as Llama3.1-70B and Qwen2.5-72B, show that learning ingredient state knowledge improves their understanding of cooking processes, achieving performance comparable to commercial LLMs. The dataset are publicly available at: https://huggingface.co/datasets/mashi6n/nhkrecipe-100-anno-1
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:15:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MM</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.17232v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.17232v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 MSRS: Evaluating Multi-Source Retrieval-Augmented Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rohan Phanse, Yijie Zhou, Kejian Shi, Wencai Zhang, Yixin Liu, Yilun Zhao, Arman Cohan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Retrieval-augmented systems are typically evaluated in settings where information required to answer the query can be found within a single source or the answer is short-form or factoid-based. However, many real-world applications demand the ability to integrate and summarize information scattered across multiple sources, where no single source is sufficient to respond to the user's question. In such settings, the retrieval component of a RAG pipeline must recognize a variety of relevance signals, and the generation component must connect and synthesize information across multiple sources. We present a scalable framework for constructing evaluation benchmarks that challenge RAG systems to integrate information across distinct sources and generate long-form responses. Using our framework, we build two new benchmarks on Multi-Source Retrieval and Synthesis: MSRS-Story and MSRS-Meet, representing narrative synthesis and summarization tasks, respectively, that require retrieval from large collections. Our extensive experiments with various RAG pipelines -- including sparse and dense retrievers combined with frontier LLMs -- reveal that generation quality is highly dependent on retrieval effectiveness, which varies greatly by task. While multi-source synthesis proves challenging even in an oracle retrieval setting, we find that reasoning models significantly outperform standard LLMs at this distinct step.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:59:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20867v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20867v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 Publish to Perish: Prompt Injection Attacks on LLM-Assisted Peer Review</h2>
                <div class="authors">
                    <strong>Authors:</strong> Matteo Gioele Collu, Umberto Salviati, Roberto Confalonieri, Mauro Conti, Giovanni Apruzzese
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are increasingly being integrated into the scientific peer-review process, raising new questions about their reliability and resilience to manipulation. In this work, we investigate the potential for hidden prompt injection attacks, where authors embed adversarial text within a paper's PDF to influence the LLM-generated review. We begin by formalising three distinct threat models that envision attackers with different motivations -- not all of which implying malicious intent. For each threat model, we design adversarial prompts that remain invisible to human readers yet can steer an LLM's output toward the author's desired outcome. Using a user study with domain scholars, we derive four representative reviewing prompts used to elicit peer reviews from LLMs. We then evaluate the robustness of our adversarial prompts across (i) different reviewing prompts, (ii) different commercial LLM-based systems, and (iii) different peer-reviewed papers. Our results show that adversarial prompts can reliably mislead the LLM, sometimes in ways that adversely affect a "honest-but-lazy" reviewer. Finally, we propose and empirically assess methods to reduce detectability of adversarial prompts under automated content checks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:57:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20863v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20863v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 Uniform Quasi ML based inference for the panel AR(1) model</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hugo Kruiniger
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper proposes new inference methods for panel AR models with arbitrary initial conditions and heteroskedasticity and possibly additional regressors that are robust to the strength of identification. Specifically, we consider several Maximum Likelihood based methods of constructing tests and confidence sets (CSs) and show that (Quasi) LM tests and CSs that use the expected Hessian rather than the observed Hessian of the log-likelihood have correct asymptotic size (in a uniform sense). We derive the power envelope of a Fixed Effects version of such a LM test for hypotheses involving the autoregressive parameter when the average information matrix is estimated by a centered OPG estimator and the model is only second-order identified, and show that it coincides with the maximal attainable power curve in the worst case setting. We also study the empirical size and power properties of these (Quasi) LM tests and CSs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:52:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>econ.EM</span><span>62F03, 62F05</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20855v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20855v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 JADES: A Universal Framework for Jailbreak Assessment via
  Decompositional Scoring</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junjie Chu, Mingjie Li, Ziqing Yang, Ye Leng, Chenhao Lin, Chao Shen, Michael Backes, Yun Shen, Yang Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Accurately determining whether a jailbreak attempt has succeeded is a fundamental yet unresolved challenge. Existing evaluation methods rely on misaligned proxy indicators or naive holistic judgments. They frequently misinterpret model responses, leading to inconsistent and subjective assessments that misalign with human perception. To address this gap, we introduce JADES (Jailbreak Assessment via Decompositional Scoring), a universal jailbreak evaluation framework. Its key mechanism is to automatically decompose an input harmful question into a set of weighted sub-questions, score each sub-answer, and weight-aggregate the sub-scores into a final decision. JADES also incorporates an optional fact-checking module to strengthen the detection of hallucinations in jailbreak responses. We validate JADES on JailbreakQR, a newly introduced benchmark proposed in this work, consisting of 400 pairs of jailbreak prompts and responses, each meticulously annotated by humans. In a binary setting (success/failure), JADES achieves 98.5% agreement with human evaluators, outperforming strong baselines by over 9%. Re-evaluating five popular attacks on four LLMs reveals substantial overestimation (e.g., LAA's attack success rate on GPT-3.5-Turbo drops from 93% to 69%). Our results show that JADES could deliver accurate, consistent, and interpretable evaluations, providing a reliable basis for measuring future jailbreak attacks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:40:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20848v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20848v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 Multilingual Contextualization of Large Language Models for
  Document-Level Machine Translation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Miguel Moura Ramos, Patrick Fernandes, Sweta Agrawal, André F. T. Martins
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have demonstrated strong performance in sentence-level machine translation, but scaling to document-level translation remains challenging, particularly in modeling long-range dependencies and discourse phenomena across sentences and paragraphs. In this work, we propose a method to improve LLM-based long-document translation through targeted fine-tuning on high-quality document-level data, which we curate and introduce as DocBlocks. Our approach supports multiple translation paradigms, including direct document-to-document and chunk-level translation, by integrating instructions both with and without surrounding context. This enables models to better capture cross-sentence dependencies while maintaining strong sentence-level translation performance. Experimental results show that incorporating multiple translation paradigms improves document-level translation quality and inference speed compared to prompting and agent-based methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:32:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.12140v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.12140v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 Learning Primitive Embodied World Models: Towards Scalable Robotic
  Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qiao Sun, Liujia Yang, Wei Tang, Wei Huang, Kaixin Xu, Yongchao Chen, Mingyu Liu, Jiange Yang, Haoyi Zhu, Yating Wang, Tong He, Yilun Chen, Xili Dai, Nanyang Ye, Qinying Gu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While video-generation-based embodied world models have gained increasing attention, their reliance on large-scale embodied interaction data remains a key bottleneck. The scarcity, difficulty of collection, and high dimensionality of embodied data fundamentally limit the alignment granularity between language and actions and exacerbate the challenge of long-horizon video generation--hindering generative models from achieving a "GPT moment" in the embodied domain. There is a naive observation: the diversity of embodied data far exceeds the relatively small space of possible primitive motions. Based on this insight, we propose a novel paradigm for world modeling--Primitive Embodied World Models (PEWM). By restricting video generation to fixed short horizons, our approach 1) enables fine-grained alignment between linguistic concepts and visual representations of robotic actions, 2) reduces learning complexity, 3) improves data efficiency in embodied data collection, and 4) decreases inference latency. By equipping with a modular Vision-Language Model (VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further enables flexible closed-loop control and supports compositional generalization of primitive-level policies over extended, complex tasks. Our framework leverages the spatiotemporal vision priors in video models and the semantic awareness of VLMs to bridge the gap between fine-grained physical interaction and high-level reasoning, paving the way toward scalable, interpretable, and general-purpose embodied intelligence.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:31:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.AI</span><span>cs.MM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20840v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20840v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 GDLLM: A Global Distance-aware Modeling Approach Based on Large Language
  Models for Event Temporal Relation Extraction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jie Zhao, Wanting Ning, Yuxiao Fei, Yubo Feng, Lishuang Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In Natural Language Processing(NLP), Event Temporal Relation Extraction (ETRE) is to recognize the temporal relations of two events. Prior studies have noted the importance of language models for ETRE. However, the restricted pre-trained knowledge of Small Language Models(SLMs) limits their capability to handle minority class relations in imbalanced classification datasets. For Large Language Models(LLMs), researchers adopt manually designed prompts or instructions, which may introduce extra noise, leading to interference with the model's judgment of the long-distance dependencies between events. To address these issues, we propose GDLLM, a Global Distance-aware modeling approach based on LLMs. We first present a distance-aware graph structure utilizing Graph Attention Network(GAT) to assist the LLMs in capturing long-distance dependency features. Additionally, we design a temporal feature learning paradigm based on soft inference to augment the identification of relations with a short-distance proximity band, which supplements the probabilistic information generated by LLMs into the multi-head attention mechanism. Since the global feature can be captured effectively, our framework substantially enhances the performance of minority relation classes and improves the overall learning ability. Experiments on two publicly available datasets, TB-Dense and MATRES, demonstrate that our approach achieves state-of-the-art (SOTA) performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:23:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20828v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20828v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 ASiM: Modeling and Analyzing Inference Accuracy of SRAM-Based Analog CiM
  Circuits</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wenlun Zhang, Shimpei Ando, Yung-Chin Chen, Kentaro Yoshioka
                </div>
                <div class="summary">
                    <strong>Summary:</strong> SRAM-based Analog Compute-in-Memory (ACiM) demonstrates promising energy efficiency for deep neural network (DNN) processing. Nevertheless, efforts to optimize efficiency frequently compromise accuracy, and this trade-off remains insufficiently studied due to the difficulty of performing full-system validation. Specifically, existing simulation tools rarely target SRAM-based ACiM and exhibit inconsistent accuracy predictions, highlighting the need for a standardized, SRAM CiM circuit-aware evaluation methodology. This paper presents ASiM, a simulation framework for evaluating inference accuracy in SRAM-based ACiM systems. ASiM captures critical effects in SRAM based analog compute in memory systems, such as ADC quantization, bit parallel encoding, and analog noise, which must be modeled with high fidelity due to their distinct behavior in charge domain architectures compared to other memory technologies. ASiM supports a wide range of modern DNN workloads, including CNN and Transformer-based models such as ViT, and scales to large-scale tasks like ImageNet classification. Our results indicate that bit-parallel encoding can improve energy efficiency with only modest accuracy degradation; however, even 1 LSB of analog noise can significantly impair inference performance, particularly in complex tasks such as ImageNet. To address this, we explore hybrid analog-digital execution and majority voting schemes, both of which enhance robustness without negating energy savings. ASiM bridges the gap between hardware design and inference performance, offering actionable insights for energy-efficient, high-accuracy ACiM deployment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:17:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.11022v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.11022v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 cMALC-D: Contextual Multi-Agent LLM-Guided Curriculum Learning with
  Diversity-Based Context Blending</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anirudh Satheesh, Keenan Powell, Hua Wei
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Many multi-agent reinforcement learning (MARL) algorithms are trained in fixed simulation environments, making them brittle when deployed in real-world scenarios with more complex and uncertain conditions. Contextual MARL (cMARL) addresses this by parameterizing environments with context variables and training a context-agnostic policy that performs well across all environment configurations. Existing cMARL methods attempt to use curriculum learning to help train and evaluate context-agnostic policies, but they often rely on unreliable proxy signals, such as value estimates or generalized advantage estimates that are noisy and unstable in multi-agent settings due to inter-agent dynamics and partial observability. To address these issues, we propose Contextual Multi-Agent LLM-Guided Curriculum Learning with Diversity-Based Context Blending (cMALC-D), a framework that uses Large Language Models (LLMs) to generate semantically meaningful curricula and provide a more robust evaluation signal. To prevent mode collapse and encourage exploration, we introduce a novel diversity-based context blending mechanism that creates new training scenarios by combining features from prior contexts. Experiments in traffic signal control domains demonstrate that cMALC-D significantly improves both generalization and sample efficiency compared to existing curriculum learning baselines. We provide code at https://github.com/DaRL-LibSignal/cMALC-D.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:16:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20818v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20818v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 A Graph-Based Test-Harness for LLM Evaluation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jessica Lundin, Guillaume Chabot-Couture
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present a first known prototype of a dynamic, systematic benchmark of medical guidelines for 400+ questions, with 3.3+ trillion possible combinations, covering 100\% of guideline relationships. We transformed the WHO IMCI handbook into a directed graph with 200+ nodes (conditions, symptoms, treatments, follow-ups, severities) and 300+ edges, then used graph traversal to generate questions that incorporated age-specific scenarios and contextual distractors to ensure clinical relevance. Our graph-based approach enables systematic evaluation across clinical tasks (45-67\% accuracy), and we find models excel at symptom recognition but struggle with triaging severity, treatment protocols and follow-up care, demonstrating how customized benchmarks can identify specific capability gaps that general-domain evaluations miss. Beyond evaluation, this dynamic MCQA methodology enhances LLM post-training (supervised finetuning, GRPO, DPO), where correct answers provide high-reward samples without expensive human annotation. The graph-based approach successfully addresses the coverage limitations of manually curated benchmarks. This methodology is a step toward scalable, contamination-resistant solution for creating comprehensive benchmarks that can be dynamically generated, including when the guidelines are updated. Code and datasets are available at https://github.com/jessicalundin/graph_testing_harness
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:10:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20810v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20810v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 The Epistemic Support-Point Filter (ESPF): A Bounded Possibilistic
  Framework for Ordinal State Estimation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Moriba Jah, Van Haslett
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Traditional state estimation methods rely on probabilistic assumptions that often collapse epistemic uncertainty into scalar beliefs, risking overconfidence in sparse or adversarial sensing environments. We introduce the Epistemic Support-Point Filter (ESPF), a novel non-Bayesian filtering framework fully grounded in possibility theory and epistemic humility. ESPF redefines the evolution of belief over state space using compatibility-weighted support updates, surprisalaware pruning, and adaptive dispersion via sparse grid quadrature. Unlike conventional filters, ESPF does not seek a posterior distribution, but rather maintains a structured region of plausibility or non-rejection, updated using ordinal logic rather than integration. For multi-model inference, we employ the Choquet integral to fuse competing hypotheses based on a dynamic epistemic capacity function, generalizing classical winner-take-all strategies. The result is an inference engine capable of dynamically contracting or expanding belief support in direct response to information structure, without requiring prior statistical calibration. This work presents a foundational shift in how inference, evidence, and ignorance are reconciled, supporting robust estimation where priors are unavailable, misleading, or epistemically unjustified.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:08:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>cs.CE</span><span>cs.SY</span><span>eess.SY</span><span>math.DS</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20806v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20806v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 Steering Towards Fairness: Mitigating Political Bias in LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Afrozah Nadeem, Mark Dras, Usman Naseem
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in large language models (LLMs) have enabled their widespread use across diverse real-world applications. However, concerns remain about their tendency to encode and reproduce ideological biases along political and economic dimensions. In this paper, we employ a framework for probing and mitigating such biases in decoder-based LLMs through analysis of internal model representations. Grounded in the Political Compass Test (PCT), this method uses contrastive pairs to extract and compare hidden layer activations from models like Mistral and DeepSeek. We introduce a comprehensive activation extraction pipeline capable of layer-wise analysis across multiple ideological axes, revealing meaningful disparities linked to political framing. Our results show that decoder LLMs systematically encode representational bias across layers, which can be leveraged for effective steering vector-based mitigation. This work provides new insights into how political bias is encoded in LLMs and offers a principled approach to debiasing beyond surface-level output interventions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:07:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.08846v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.08846v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 Exploring Machine Learning and Language Models for Multimodal Depression
  Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Javier Si Zhao Hong, Timothy Zoe Delaya, Sherwyn Chan Yin Kit, Pai Chet Ng, Xiaoxiao Miao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper presents our approach to the first Multimodal Personality-Aware Depression Detection Challenge, focusing on multimodal depression detection using machine learning and deep learning models. We explore and compare the performance of XGBoost, transformer-based architectures, and large language models (LLMs) on audio, video, and text features. Our results highlight the strengths and limitations of each type of model in capturing depression-related signals across modalities, offering insights into effective multimodal representation strategies for mental health prediction.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:07:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.SD</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20805v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20805v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 Evolution favours positively biased reasoning in sequential interactions
  with high future gains</h2>
                <div class="authors">
                    <strong>Authors:</strong> Marco Saponara, Elias Fernandez Domingos, Jorge M. Pacheco, Tom Lenaerts
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Empirical evidence shows that human behaviour often deviates from game-theoretical rationality. For instance, humans may hold unrealistic expectations about future outcomes. As the evolutionary roots of such biases remain unclear, we investigate here how reasoning abilities and cognitive biases co-evolve using Evolutionary Game Theory. In our model, individuals in a population deploy a variety of unbiased and biased level-k reasoning strategies to anticipate others' behaviour in sequential interactions, represented by the Incremental Centipede Game. Positively biased reasoning strategies have a systematic inference bias towards higher but uncertain rewards, while negatively biased strategies reflect the opposite tendency. We find that selection consistently favours positively biased reasoning, with rational behaviour even going extinct. This bias co-evolves with bounded rationality, as the reasoning depth remains limited in the population. Interestingly, positively biased agents may co-exist with non-reasoning agents, thus pointing to a novel equilibrium. Longer games further promote positively biased reasoning, as they can lead to higher future rewards. The biased reasoning strategies proposed in this model may reflect cognitive phenomena like wishful thinking and defensive pessimism. This work therefore supports the claim that certain cognitive biases, despite deviating from rational judgment, constitute an adaptive feature to better cope with social dilemmas.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:01:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1098/rsif.2025.0153' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.20799v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20799v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 Leveraging LLMs for Formal Software Requirements -- Challenges and
  Prospects</h2>
                <div class="authors">
                    <strong>Authors:</strong> Arshad Beg, Diarmuid O'Donoghue, Rosemary Monahan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Software correctness is ensured mathematically through formal verification, which involves the resources of generating formal requirement specifications and having an implementation that must be verified. Tools such as model-checkers and theorem provers ensure software correctness by verifying the implementation against the specification. Formal methods deployment is regularly enforced in the development of safety-critical systems e.g. aerospace, medical devices and autonomous systems. Generating these specifications from informal and ambiguous natural language requirements remains the key challenge. Our project, VERIFAI^{1}, aims to investigate automated and semi-automated approaches to bridge this gap, using techniques from Natural Language Processing (NLP), ontology-based domain modelling, artefact reuse, and large language models (LLMs). This position paper presents a preliminary synthesis of relevant literature to identify recurring challenges and prospective research directions in the generation of verifiable specifications from informal requirements.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T13:50:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>D.2.1; D.2.4; D.2.10; F.4.1; F.4.3</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.14330v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.14330v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 Causal resilience curves: A data-driven framework for quantifying the
  spatiotemporal impacts of metro service disruptions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nan Zhang, Daniel Hörcher, Prateek Bansal, Daniel J. Graham
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Urban metro systems move vast numbers of passengers with a high level of efficiency in resource use, but frequently experience disruptions that result in delays, crowding, and deterioration in passenger satisfaction and patronage. To quantify these adverse consequences, this paper presents a novel, data-driven causal inference framework to measure metro resilience by estimating both the direct and spillover effects of service disruptions on passenger demand, journey time, travel speed and on-board crowding. By integrating high-frequency smart card data into a synthetic control design, we use weighted non-disrupted days to construct unbiased counterfactuals, which resolves confounding factors and accurately captures disruption propagation across the network. The impact estimates are further translated into station-level causal resilience curves that reveal spatial heterogeneity in the temporal patterns of degradation and recovery across locations, providing metro operators with actionable insights for targeted interventions and resource allocation. A case study of the Hong Kong MTR demonstrates the framework's superiority over naive typical-day comparisons and machine-learning benchmarks in delivering unbiased resilience curves. This paper is the first to derive causal estimates of dynamic metro resilience. This practical tool can be generalised to evaluate resilience in a broad range of public transport systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T13:36:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.AP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2310.07514v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2310.07514v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 Canonical Bayesian Linear System Identification</h2>
                <div class="authors">
                    <strong>Authors:</strong> Andrey Bryutkin, Matthew E. Levine, Iñigo Urteaga, Youssef Marzouk
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Standard Bayesian approaches for linear time-invariant (LTI) system identification are hindered by parameter non-identifiability; the resulting complex, multi-modal posteriors make inference inefficient and impractical. We solve this problem by embedding canonical forms of LTI systems within the Bayesian framework. We rigorously establish that inference in these minimal parameterizations fully captures all invariant system dynamics (e.g., transfer functions, eigenvalues, predictive distributions of system outputs) while resolving identifiability. This approach unlocks the use of meaningful, structure-aware priors (e.g., enforcing stability via eigenvalues) and ensures conditions for a Bernstein--von Mises theorem -- a link between Bayesian and frequentist large-sample asymptotics that is broken in standard forms. Extensive simulations with modern MCMC methods highlight advantages over standard parameterizations: canonical forms achieve higher computational efficiency, generate interpretable and well-behaved posteriors, and provide robust uncertainty estimates, particularly from limited data.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T13:32:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ML</span><span>cs.LG</span><span>cs.SY</span><span>eess.SY</span><span>stat.CO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.11535v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.11535v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 Solar peculiar motion inferred from dipole anisotropy in redshift
  distribution of quasars appears to lie along the Galactic Centre direction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ashok K. Singal
                </div>
                <div class="summary">
                    <strong>Summary:</strong> According to the Cosmological Principle an observer stationary with respect to the comoving coordinates of the expanding universe should find the redshift distribution of distant quasars to be isotropic. However, the observed redshift distribution in a large sample of 1.3 million quasars shows a significant dipole anisotropy. A peculiar motion of the observer could introduce such a dipole anisotropy in the observed redshift distribution. However, the motion inferred therefrom turns out to be not only many times the peculiar motion estimated from the anisotropy in the Cosmic Microwave Background (CMB), but also nearly in a direction at a right angle. The Solar peculiar motion, in fact, turns out to be, quite unexpectedly, in the direction of the Galactic Centre. Such a statistically significant discrepancy in peculiar motion, derived by different methodologies, could imply a violation of the cosmological principle, a cornerstone in the foundation of the standard model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T13:26:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.CO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20769v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20769v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 Turning the Spell Around: Lightweight Alignment Amplification via
  Rank-One Safety Injection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Harethah Abu Shairah, Hasan Abed Al Kader Hammoud, George Turkiyyah, Bernard Ghanem
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Safety alignment in Large Language Models (LLMs) often involves mediating internal representations to refuse harmful requests. Recent research has demonstrated that these safety mechanisms can be bypassed by ablating or removing specific representational directions within the model. In this paper, we propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box method that amplifies a model's safety alignment by permanently steering its activations toward the refusal-mediating subspace. ROSI operates as a simple, fine-tuning-free rank-one weight modification applied to all residual stream write matrices. The required safety direction can be computed from a small set of harmful and harmless instruction pairs. We show that ROSI consistently increases safety refusal rates - as evaluated by Llama Guard 3 - while preserving the utility of the model on standard benchmarks such as MMLU, HellaSwag, and Arc. Furthermore, we show that ROSI can also re-align 'uncensored' models by amplifying their own latent safety directions, demonstrating its utility as an effective last-mile safety procedure. Our results suggest that targeted, interpretable weight steering is a cheap and potent mechanism to improve LLM safety, complementing more resource-intensive fine-tuning paradigms.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T13:22:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20766v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20766v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 Feel the Difference? A Comparative Analysis of Emotional Arcs in Real
  and LLM-Generated CBT Sessions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaoyi Wang, Jiwei Zhang, Guangtao Zhang, Honglei Guo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Synthetic therapy dialogues generated by large language models (LLMs) are increasingly used in mental health NLP to simulate counseling scenarios, train models, and supplement limited real-world data. However, it remains unclear whether these synthetic conversations capture the nuanced emotional dynamics of real therapy. In this work, we conduct the first comparative analysis of emotional arcs between real and LLM-generated Cognitive Behavioral Therapy dialogues. We adapt the Utterance Emotion Dynamics framework to analyze fine-grained affective trajectories across valence, arousal, and dominance dimensions. Our analysis spans both full dialogues and individual speaker roles (counselor and client), using real sessions transcribed from public videos and synthetic dialogues from the CACTUS dataset. We find that while synthetic dialogues are fluent and structurally coherent, they diverge from real conversations in key emotional properties: real sessions exhibit greater emotional variability,more emotion-laden language, and more authentic patterns of reactivity and regulation. Moreover, emotional arc similarity between real and synthetic speakers is low, especially for clients. These findings underscore the limitations of current LLM-generated therapy data and highlight the importance of emotional fidelity in mental health applications. We introduce RealCBT, a curated dataset of real CBT sessions, to support future research in this space.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T13:19:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20764v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20764v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and
  Efficient Open-Ended Text Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuanhao Ding, Esteban Garces Arias, Meimingwei Li, Julian Rodemann, Matthias Aßenmacher, Danlu Chen, Gaojuan Fan, Christian Heumann, Chongsheng Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Open-ended text generation faces a critical challenge: balancing coherence with diversity in LLM outputs. While contrastive search-based decoding strategies have emerged to address this trade-off, their practical utility is often limited by hyperparameter dependence and high computational costs. We introduce GUARD, a self-adaptive decoding method that effectively balances these competing objectives through a novel "Glocal" uncertainty-driven framework. GUARD combines global entropy estimates with local entropy deviations to integrate both long-term and short-term uncertainty signals. We demonstrate that our proposed global entropy formulation effectively mitigates abrupt variations in uncertainty, such as sudden overconfidence or high entropy spikes, and provides theoretical guarantees of unbiasedness and consistency. To reduce computational overhead, we incorporate a simple yet effective token-count-based penalty into GUARD. Experimental results demonstrate that GUARD achieves a good balance between text diversity and coherence, while exhibiting substantial improvements in generation speed. In a more nuanced comparison study across different dimensions of text quality, both human and LLM evaluators validated its remarkable performance. Our code is available at https://github.com/YecanLee/GUARD.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T13:14:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20757v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20757v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 Specializing General-purpose LLM Embeddings for Implicit Hate Speech
  Detection across Datasets</h2>
                <div class="authors">
                    <strong>Authors:</strong> Vassiliy Cheremetiev, Quang Long Ho Ngo, Chau Ying Kot, Alina Elena Baia, Andrea Cavallaro
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Implicit hate speech (IHS) is indirect language that conveys prejudice or hatred through subtle cues, sarcasm or coded terminology. IHS is challenging to detect as it does not include explicit derogatory or inflammatory words. To address this challenge, task-specific pipelines can be complemented with external knowledge or additional information such as context, emotions and sentiment data. In this paper, we show that, by solely fine-tuning recent general-purpose embedding models based on large language models (LLMs), such as Stella, Jasper, NV-Embed and E5, we achieve state-of-the-art performance. Experiments on multiple IHS datasets show up to 1.10 percentage points improvements for in-dataset, and up to 20.35 percentage points improvements in cross-dataset evaluation, in terms of F1-macro score.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T13:08:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3746275.3762209' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.20750v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20750v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 Mix, Align, Distil: Reliable Cross-Domain Atypical Mitosis
  Classification</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kaustubh Atey, Sameer Anand Jha, Gouranga Bala, Amit Sethi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Atypical mitotic figures (AMFs) are important histopathological markers yet remain challenging to identify consistently, particularly under domain shift stemming from scanner, stain, and acquisition differences. We present a simple training-time recipe for domain-robust AMF classification in MIDOG 2025 Task 2. The approach (i) increases feature diversity via style perturbations inserted at early and mid backbone stages, (ii) aligns attention-refined features across sites using weak domain labels (Scanner, Origin, Species, Tumor) through an auxiliary alignment loss, and (iii) stabilizes predictions by distilling from an exponential moving average (EMA) teacher with temperature-scaled KL divergence. On the organizer-run preliminary leaderboard for atypical mitosis classification, our submission attains balanced accuracy of 0.8762, sensitivity of 0.8873, specificity of 0.8651, and ROC AUC of 0.9499. The method incurs negligible inference-time overhead, relies only on coarse domain metadata, and delivers strong, balanced performance, positioning it as a competitive submission for the MIDOG 2025 challenge.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T13:04:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20745v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20745v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 From Law to Gherkin: A Human-Centred Quasi-Experiment on the Quality of
  LLM-Generated Behavioural Specifications from Food-Safety Regulations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shabnam Hassani, Mehrdad Sabetzadeh, Daniel Amyot
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Context: Laws and regulations increasingly affect software design and quality assurance, but legal texts are written in technology-neutral language. This creates challenges for engineers who must develop compliance artifacts such as requirements and acceptance criteria. Manual creation is labor-intensive, error-prone, and requires domain expertise. Advances in Generative AI (GenAI), especially Large Language Models (LLMs), offer a way to automate deriving such artifacts.   Objective: We present the first systematic human-subject study of LLMs' ability to derive behavioral specifications from legal texts using a quasi-experimental design. These specifications translate legal requirements into a developer-friendly form.   Methods: Ten participants evaluated specifications generated from food-safety regulations by Claude and Llama. Using Gherkin, a structured BDD language, 60 specifications were produced. Each participant assessed 12 across five criteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Each specification was reviewed by two participants, yielding 120 assessments.   Results: For Relevance, 75% of ratings were highest and 20% second-highest. Clarity reached 90% highest. Completeness: 75% highest, 19% second. Singularity: 82% highest, 12% second. Time Savings: 68% highest, 24% second. No lowest ratings occurred. Mann-Whitney U tests showed no significant differences across participants or models. Llama slightly outperformed Claude in Clarity, Completeness, and Time Savings, while Claude was stronger in Singularity. Feedback noted hallucinations and omissions but confirmed the utility of the specifications.   Conclusion: LLMs can generate high-quality Gherkin specifications from legal texts, reducing manual effort and providing structured artifacts useful for implementation, assurance, and test generation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T13:04:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20744v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20744v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 Rethinking Testing for LLM Applications: Characteristics, Challenges,
  and a Lightweight Interaction Protocol</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wei Ma, Yixiao Yang, Qiang Hu, Shi Ying, Zhi Jin, Bo Du, Zhenchang Xing, Tianlin Li, Junjie Shi, Yang Liu, Linxiao Jiang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Applications of Large Language Models~(LLMs) have evolved from simple text generators into complex software systems that integrate retrieval augmentation, tool invocation, and multi-turn interactions. Their inherent non-determinism, dynamism, and context dependence pose fundamental challenges for quality assurance. This paper decomposes LLM applications into a three-layer architecture: \textbf{\textit{System Shell Layer}}, \textbf{\textit{Prompt Orchestration Layer}}, and \textbf{\textit{LLM Inference Core}}. We then assess the applicability of traditional software testing methods in each layer: directly applicable at the shell layer, requiring semantic reinterpretation at the orchestration layer, and necessitating paradigm shifts at the inference core. A comparative analysis of Testing AI methods from the software engineering community and safety analysis techniques from the AI community reveals structural disconnects in testing unit abstraction, evaluation metrics, and lifecycle management. We identify four fundamental differences that underlie 6 core challenges. To address these, we propose four types of collaborative strategies (\emph{Retain}, \emph{Translate}, \emph{Integrate}, and \emph{Runtime}) and explore a closed-loop, trustworthy quality assurance framework that combines pre-deployment validation with runtime monitoring. Based on these strategies, we offer practical guidance and a protocol proposal to support the standardization and tooling of LLM application testing. We propose a protocol \textbf{\textit{Agent Interaction Communication Language}} (AICL) that is used to communicate between AI agents. AICL has the test-oriented features and is easily integrated in the current agent framework.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T13:00:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20737v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20737v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 Leveraging Semantic Triples for Private Document Generation with Local
  Differential Privacy Guarantees</h2>
                <div class="authors">
                    <strong>Authors:</strong> Stephen Meisenbacher, Maulik Chevli, Florian Matthes
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Many works at the intersection of Differential Privacy (DP) in Natural Language Processing aim to protect privacy by transforming texts under DP guarantees. This can be performed in a variety of ways, from word perturbations to full document rewriting, and most often under local DP. Here, an input text must be made indistinguishable from any other potential text, within some bound governed by the privacy parameter $\varepsilon$. Such a guarantee is quite demanding, and recent works show that privatizing texts under local DP can only be done reasonably under very high $\varepsilon$ values. Addressing this challenge, we introduce DP-ST, which leverages semantic triples for neighborhood-aware private document generation under local DP guarantees. Through the evaluation of our method, we demonstrate the effectiveness of the divide-and-conquer paradigm, particularly when limiting the DP notion (and privacy guarantees) to that of a privatization neighborhood. When combined with LLM post-processing, our method allows for coherent text generation even at lower $\varepsilon$ values, while still balancing privacy and utility. These findings highlight the importance of coherence in achieving balanced privatization outputs at reasonable $\varepsilon$ levels.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T12:59:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20736v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20736v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 Re4: Scientific Computing Agent with Rewriting, Resolution, Review and
  Revision</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ao Cheng, Lei Zhang, Guowei He
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) serve as an active and promising field of generative artificial intelligence and have demonstrated abilities to perform complex tasks in multiple domains, including mathematical and scientific reasoning. In this work, we construct a novel agent framework for solving representative problems in scientific computing. The proposed agent, incorporating a "rewriting-resolution-review-revision" logical chain via three reasoning LLMs (functioning as the Consultant, Reviewer, and Programmer, respectively), is integrated in a collaborative and interactive manner. The Consultant module endows the agent with knowledge transfer capabilities to link problems to professional domain insights, thereby rewriting problem descriptions through text augmentation. The Programmer module is responsible for generating and executing well-structured code to deliver the problem resolution. The Reviewer module equips the agent with the capacity for self-debugging and self-refinement through interactive feedback with code runtime outputs. By leveraging the end-to-end review mechanism, the executable code provided by the Programmer attains the iterative revision. A comprehensive evaluation is conducted on the performance of the proposed agent framework in solving PDEs, ill-conditioned linear systems, and data-driven physical analysis problems. Compared to single-model, this collaborative framework significantly improves the bug-free code generation rate and reduces the occurrence of non-physical solutions, thereby establishing a highly reliable framework for autonomous code generation based on natural language descriptions. The review mechanism improved the average execution success (bug-free code and non-NaN solutions) rate of the latest reasoning models. In summary, our agent framework establishes automatic code generation and review as a promising scientific computing paradigm.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T12:50:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>physics.comp-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20729v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20729v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 Upper Limits on the Isotropic Gravitational-Wave Background from the
  first part of LIGO, Virgo, and KAGRA's fourth Observing Run</h2>
                <div class="authors">
                    <strong>Authors:</strong> The LIGO Scientific Collaboration, the Virgo Collaboration, the KAGRA Collaboration, A. G. Abac, I. Abouelfettouh, F. Acernese, K. Ackley, C. Adamcewicz, S. Adhicary, D. Adhikari, N. Adhikari, R. X. Adhikari, V. K. Adkins, S. Afroz, A. Agapito, D. Agarwal, M. Agathos, N. Aggarwal, S. Aggarwal, O. D. Aguiar, I. -L. Ahrend, L. Aiello, A. Ain, P. Ajith, T. Akutsu, S. Albanesi, W. Ali, S. Al-Kershi, C. Alléné, A. Allocca, S. Al-Shammari, P. A. Altin, S. Alvarez-Lopez, W. Amar, O. Amarasinghe, A. Amato, F. Amicucci, C. Amra, A. Ananyeva, S. B. Anderson, W. G. Anderson, M. Andia, M. Ando, M. Andrés-Carcasona, T. Andrić, J. Anglin, S. Ansoldi, J. M. Antelis, S. Antier, M. Aoumi, E. Z. Appavuravther, S. Appert, S. K. Apple, K. Arai, A. Araya, M. C. Araya, M. Arca Sedda, J. S. Areeda, N. Aritomi, F. Armato, S. Armstrong, N. Arnaud, M. Arogeti, S. M. Aronson, G. Ashton, Y. Aso, L. Asprea, M. Assiduo, S. Assis de Souza Melo, S. M. Aston, P. Astone, F. Attadio, F. Aubin, K. AultONeal, G. Avallone, E. A. Avila, S. Babak, C. Badger, S. Bae, S. Bagnasco, L. Baiotti, R. Bajpai, T. Baka, A. M. Baker, K. A. Baker, T. Baker, G. Baldi, N. Baldicchi, M. Ball, G. Ballardin, S. W. Ballmer, S. Banagiri, B. Banerjee, D. Bankar, T. M. Baptiste, P. Baral, M. Baratti, J. C. Barayoga, B. C. Barish, D. Barker, N. Barman, P. Barneo, F. Barone, B. Barr, L. Barsotti, M. Barsuglia, D. Barta, A. M. Bartoletti, M. A. Barton, I. Bartos, A. Basalaev, R. Bassiri, A. Basti, M. Bawaj, P. Baxi, J. C. Bayley, A. C. Baylor, P. A. Baynard II, M. Bazzan, V. M. Bedakihale, F. Beirnaert, M. Bejger, D. Belardinelli, A. S. Bell, D. S. Bellie, L. Bellizzi, W. Benoit, I. Bentara, J. D. Bentley, M. Ben Yaala, S. Bera, F. Bergamin, B. K. Berger, S. Bernuzzi, M. Beroiz, D. Bersanetti, T. Bertheas, A. Bertolini, J. Betzwieser, D. Beveridge, G. Bevilacqua, N. Bevins, R. Bhandare, R. Bhatt, D. Bhattacharjee, S. Bhattacharyya, S. Bhaumik, V. Biancalana, A. Bianchi, I. A. Bilenko, G. Billingsley, A. Binetti, S. Bini, C. Binu, S. Biot, O. Birnholtz, S. Biscoveanu, A. Bisht, M. Bitossi, M. -A. Bizouard, S. Blaber, J. K. Blackburn, L. A. Blagg, C. D. Blair, D. G. Blair, N. Bode, N. Boettner, G. Boileau, M. Boldrini, G. N. Bolingbroke, A. Bolliand, L. D. Bonavena, R. Bondarescu, F. Bondu, E. Bonilla, M. S. Bonilla, A. Bonino, R. Bonnand, A. Borchers, S. Borhanian, V. Boschi, S. Bose, V. Bossilkov, Y. Bothra, A. Boudon, L. Bourg, G. Bouyer, M. Boyle, A. Bozzi, C. Bradaschia, P. R. Brady, A. Branch, M. Branchesi, I. Braun, T. Briant, A. Brillet, M. Brinkmann, P. Brockill, E. Brockmueller, A. F. Brooks, B. C. Brown, D. D. Brown, M. L. Brozzetti, S. Brunett, G. Bruno, R. Bruntz, J. Bryant, Y. Bu, F. Bucci, J. Buchanan, O. Bulashenko, T. Bulik, H. J. Bulten, A. Buonanno, K. Burtnyk, R. Buscicchio, D. Buskulic, C. Buy, R. L. Byer, G. S. Cabourn Davies, R. Cabrita, V. Cáceres-Barbosa, L. Cadonati, G. Cagnoli, C. Cahillane, A. Calafat, T. A. Callister, E. Calloni, S. R. Callos, G. Caneva Santoro, K. C. Cannon, H. Cao, L. A. Capistran, E. Capocasa, E. Capote, G. Capurri, G. Carapella, F. Carbognani, M. Carlassara, J. B. Carlin, T. K. Carlson, M. F. Carney, M. Carpinelli, G. Carrillo, J. J. Carter, G. Carullo, A. Casallas-Lagos, J. Casanueva Diaz, C. Casentini, S. Y. Castro-Lucas, S. Caudill, M. Cavaglià, R. Cavalieri, A. Ceja, G. Cella, P. Cerdá-Durán, E. Cesarini, N. Chabbra, W. Chaibi, A. Chakraborty, P. Chakraborty, S. Chakraborty, S. Chalathadka Subrahmanya, J. C. L. Chan, M. Chan, K. Chang, S. Chao, P. Charlton, E. Chassande-Mottin, C. Chatterjee, Debarati Chatterjee, Deep Chatterjee, M. Chaturvedi, S. Chaty, K. Chatziioannou, A. Chen, A. H. -Y. Chen, D. Chen, H. Chen, H. Y. Chen, S. Chen, Yanbei Chen, Yitian Chen, H. P. Cheng, P. Chessa, H. T. Cheung, S. Y. Cheung, F. Chiadini, G. Chiarini, A. Chiba, A. Chincarini, M. L. Chiofalo, A. Chiummo, C. Chou, S. Choudhary, N. Christensen, S. S. Y. Chua, G. Ciani, P. Ciecielag, M. Cieślar, M. Cifaldi, B. Cirok, F. Clara, J. A. Clark, T. A. Clarke, P. Clearwater, S. Clesse, F. Cleva, E. Coccia, E. Codazzo, P. -F. Cohadon, S. Colace, E. Colangeli, M. Colleoni, C. G. Collette, J. Collins, S. Colloms, A. Colombo, C. M. Compton, G. Connolly, L. Conti, T. R. Corbitt, I. Cordero-Carrión, S. Corezzi, N. J. Cornish, I. Coronado, A. Corsi, R. Cottingham, M. W. Coughlin, A. Couineaux, P. Couvares, D. M. Coward, R. Coyne, A. Cozzumbo, J. D. E. Creighton, T. D. Creighton, P. Cremonese, S. Crook, R. Crouch, J. Csizmazia, J. R. Cudell, T. J. Cullen, A. Cumming, E. Cuoco, M. Cusinato, L. V. Da Conceição, T. Dal Canton, S. Dal Pra, G. Dálya, B. D'Angelo, S. Danilishin, S. D'Antonio, K. Danzmann, K. E. Darroch, L. P. Dartez, R. Das, A. Dasgupta, V. Dattilo, A. Daumas, N. Davari, I. Dave, A. Davenport, M. Davier, T. F. Davies, D. Davis, L. Davis, M. C. Davis, P. Davis, E. J. Daw, M. Dax, J. De Bolle, M. Deenadayalan, J. Degallaix, M. De Laurentis, F. De Lillo, S. Della Torre, W. Del Pozzo, A. Demagny, F. De Marco, G. Demasi, F. De Matteis, N. Demos, T. Dent, A. Depasse, N. DePergola, R. De Pietri, R. De Rosa, C. De Rossi, M. Desai, R. DeSalvo, A. DeSimone, R. De Simone, A. Dhani, R. Diab, M. C. Díaz, M. Di Cesare, G. Dideron, T. Dietrich, L. Di Fiore, C. Di Fronzo, M. Di Giovanni, T. Di Girolamo, D. Diksha, J. Ding, S. Di Pace, I. Di Palma, D. Di Piero, F. Di Renzo, Divyajyoti, A. Dmitriev, J. P. Docherty, Z. Doctor, N. Doerksen, E. Dohmen, A. Doke, A. Domiciano De Souza, L. D'Onofrio, F. Donovan, K. L. Dooley, T. Dooney, S. Doravari, O. Dorosh, W. J. D. Doyle, M. Drago, J. C. Driggers, L. Dunn, U. Dupletsa, D. D'Urso, P. Dutta Roy, H. Duval, S. E. Dwyer, C. Eassa, M. Ebersold, T. Eckhardt, G. Eddolls, A. Effler, J. Eichholz, H. Einsle, M. Eisenmann, M. Emma, K. Endo, R. Enficiaud, L. Errico, R. Espinosa, M. C. Espitia, M. Esposito, R. C. Essick, H. Estellés, T. Etzel, M. Evans, T. Evstafyeva, B. E. Ewing, J. M. Ezquiaga, F. Fabrizi, V. Fafone, S. Fairhurst, A. M. Farah, B. Farr, W. M. Farr, G. Favaro, M. Favata, M. Fays, M. Fazio, J. Feicht, M. M. Fejer, R. Felicetti, E. Fenyvesi, J. Fernandes, T. Fernandes, D. Fernando, S. Ferraiuolo, T. A. Ferreira, F. Fidecaro, P. Figura, A. Fiori, I. Fiori, M. Fishbach, R. P. Fisher, R. Fittipaldi, V. Fiumara, R. Flaminio, S. M. Fleischer, L. S. Fleming, E. Floden, H. Fong, J. A. Font, F. Fontinele-Nunes, C. Foo, B. Fornal, K. Franceschetti, F. Frappez, S. Frasca, F. Frasconi, J. P. Freed, Z. Frei, A. Freise, O. Freitas, R. Frey, W. Frischhertz, P. Fritschel, V. V. Frolov, G. G. Fronzé, M. Fuentes-Garcia, S. Fujii, T. Fujimori, P. Fulda, M. Fyffe, B. Gadre, J. R. Gair, S. Galaudage, V. Galdi, R. Gamba, A. Gamboa, S. Gamoji, D. Ganapathy, A. Ganguly, B. Garaventa, J. García-Bellido, C. García-Quirós, J. W. Gardner, K. A. Gardner, S. Garg, J. Gargiulo, X. Garrido, A. Garron, F. Garufi, P. A. Garver, C. Gasbarra, B. Gateley, F. Gautier, V. Gayathri, T. Gayer, G. Gemme, A. Gennai, V. Gennari, J. George, R. George, O. Gerberding, L. Gergely, Archisman Ghosh, Sayantan Ghosh, Shaon Ghosh, Shrobana Ghosh, Suprovo Ghosh, Tathagata Ghosh, J. A. Giaime, K. D. Giardina, D. R. Gibson, C. Gier, S. Gkaitatzis, J. Glanzer, F. Glotin, J. Godfrey, R. V. Godley, P. Godwin, A. S. Goettel, E. Goetz, J. Golomb, S. Gomez Lopez, B. Goncharov, G. González, P. Goodarzi, S. Goode, A. W. Goodwin-Jones, M. Gosselin, R. Gouaty, D. W. Gould, K. Govorkova, A. Grado, V. Graham, A. E. Granados, M. Granata, V. Granata, S. Gras, P. Grassia, J. Graves, C. Gray, R. Gray, G. Greco, A. C. Green, L. Green, S. M. Green, S. R. Green, C. Greenberg, A. M. Gretarsson, H. K. Griffin, D. Griffith, H. L. Griggs, G. Grignani, C. Grimaud, H. Grote, S. Grunewald, D. Guerra, D. Guetta, G. M. Guidi, A. R. Guimaraes, H. K. Gulati, F. Gulminelli, H. Guo, W. Guo, Y. Guo, Anuradha Gupta, I. Gupta, N. C. Gupta, S. K. Gupta, V. Gupta, N. Gupte, J. Gurs, N. Gutierrez, N. Guttman, F. Guzman, D. Haba, M. Haberland, S. Haino, E. D. Hall, E. Z. Hamilton, G. Hammond, M. Haney, J. Hanks, C. Hanna, M. D. Hannam, O. A. Hannuksela, A. G. Hanselman, H. Hansen, J. Hanson, S. Hanumasagar, R. Harada, A. R. Hardison, S. Harikumar, K. Haris, I. Harley-Trochimczyk, T. Harmark, J. Harms, G. M. Harry, I. W. Harry, J. Hart, B. Haskell, C. J. Haster, K. Haughian, H. Hayakawa, K. Hayama, M. C. Heintze, J. Heinze, J. Heinzel, H. Heitmann, F. Hellman, A. F. Helmling-Cornell, G. Hemming, O. Henderson-Sapir, M. Hendry, I. S. Heng, M. H. Hennig, C. Henshaw, M. Heurs, A. L. Hewitt, J. Heynen, J. Heyns, S. Higginbotham, S. Hild, S. Hill, Y. Himemoto, N. Hirata, C. Hirose, D. Hofman, B. E. Hogan, N. A. Holland, I. J. Hollows, D. E. Holz, L. Honet, D. J. Horton-Bailey, J. Hough, S. Hourihane, N. T. Howard, E. J. Howell, C. G. Hoy, C. A. Hrishikesh, P. Hsi, H. -F. Hsieh, H. -Y. Hsieh, C. Hsiung, S. -H. Hsu, W. -F. Hsu, Q. Hu, H. Y. Huang, Y. Huang, Y. T. Huang, A. D. Huddart, B. Hughey, V. Hui, S. Husa, R. Huxford, L. Iampieri, G. A. Iandolo, M. Ianni, G. Iannone, J. Iascau, K. Ide, R. Iden, A. Ierardi, S. Ikeda, H. Imafuku, Y. Inoue, G. Iorio, P. Iosif, M. H. Iqbal, J. Irwin, R. Ishikawa, M. Isi, K. S. Isleif, Y. Itoh, M. Iwaya, B. R. Iyer, C. Jacquet, P. -E. Jacquet, T. Jacquot, S. J. Jadhav, S. P. Jadhav, M. Jain, T. Jain, A. L. James, K. Jani, J. Janquart, K. Janssens, N. N. Janthalur, S. Jaraba, P. Jaranowski, R. Jaume, W. Javed, A. Jennings, M. Jensen, W. Jia, J. Jiang, H. -B. Jin, G. R. Johns, N. A. Johnson, M. C. Johnston, R. Johnston, N. Johny, D. H. Jones, D. I. Jones, R. Jones, H. E. Jose, P. Joshi, S. K. Joshi, G. Joubert, J. Ju, L. Ju, K. Jung, J. Junker, V. Juste, H. B. Kabagoz, T. Kajita, I. Kaku, V. Kalogera, M. Kalomenopoulos, M. Kamiizumi, N. Kanda, S. Kandhasamy, G. Kang, N. C. Kannachel, J. B. Kanner, S. A. KantiMahanty, S. J. Kapadia, D. P. Kapasi, M. Karthikeyan, M. Kasprzack, H. Kato, T. Kato, E. Katsavounidis, W. Katzman, R. Kaushik, K. Kawabe, R. Kawamoto, D. Keitel, L. J. Kemperman, J. Kennington, F. A. Kerkow, R. Kesharwani, J. S. Key, R. Khadela, S. Khadka, S. S. Khadkikar, F. Y. Khalili, F. Khan, T. Khanam, M. Khursheed, N. M. Khusid, W. Kiendrebeogo, N. Kijbunchoo, C. Kim, J. C. Kim, K. Kim, M. H. Kim, S. Kim, Y. -M. Kim, C. Kimball, K. Kimes, M. Kinnear, J. S. Kissel, S. Klimenko, A. M. Knee, E. J. Knox, N. Knust, K. Kobayashi, S. M. Koehlenbeck, G. Koekoek, K. Kohri, K. Kokeyama, S. Koley, P. Kolitsidou, A. E. Koloniari, K. Komori, A. K. H. Kong, A. Kontos, L. M. Koponen, M. Korobko, X. Kou, A. Koushik, N. Kouvatsos, M. Kovalam, T. Koyama, D. B. Kozak, S. L. Kranzhoff, V. Kringel, N. V. Krishnendu, S. Kroker, A. Królak, K. Kruska, J. Kubisz, G. Kuehn, S. Kulkarni, A. Kulur Ramamohan, Achal Kumar, Anil Kumar, Praveen Kumar, Prayush Kumar, Rahul Kumar, Rakesh Kumar, J. Kume, K. Kuns, N. Kuntimaddi, S. Kuroyanagi, S. Kuwahara, K. Kwak, K. Kwan, S. Kwon, G. Lacaille, D. Laghi, A. H. Laity, E. Lalande, M. Lalleman, P. C. Lalremruati, M. Landry, B. B. Lane, R. N. Lang, J. Lange, R. Langgin, B. Lantz, I. La Rosa, J. Larsen, A. Lartaux-Vollard, P. D. Lasky, J. Lawrence, M. Laxen, C. Lazarte, A. Lazzarini, C. Lazzaro, P. Leaci, L. Leali, Y. K. Lecoeuche, H. M. Lee, H. W. Lee, J. Lee, K. Lee, R. -K. Lee, R. Lee, Sungho Lee, Sunjae Lee, Y. Lee, I. N. Legred, J. Lehmann, L. Lehner, M. Le Jean, A. Lemaître, M. Lenti, M. Leonardi, M. Lequime, N. Leroy, M. Lesovsky, N. Letendre, M. Lethuillier, Y. Levin, K. Leyde, A. K. Y. Li, K. L. Li, T. G. F. Li, X. Li, Y. Li, Z. Li, A. Lihos, E. T. Lin, F. Lin, L. C. -C. Lin, Y. -C. Lin, C. Lindsay, S. D. Linker, A. Liu, G. C. Liu, Jian Liu, F. Llamas Villarreal, J. Llobera-Querol, R. K. L. Lo, J. -P. Locquet, S. C. G. Loggins, M. R. Loizou, L. T. London, A. Longo, D. Lopez, M. Lopez Portilla, M. Lorenzini, A. Lorenzo-Medina, V. Loriette, M. Lormand, G. Losurdo, E. Lotti, T. P. Lott IV, J. D. Lough, H. A. Loughlin, C. O. Lousto, N. Low, N. Lu, L. Lucchesi, H. Lück, D. Lumaca, A. P. Lundgren, A. W. Lussier, R. Macas, M. MacInnis, D. M. Macleod, I. A. O. MacMillan, A. Macquet, K. Maeda, S. Maenaut, S. S. Magare, R. M. Magee, E. Maggio, R. Maggiore, M. Magnozzi, M. Mahesh, M. Maini, S. Majhi, E. Majorana, C. N. Makarem, D. Malakar, J. A. Malaquias-Reis, U. Mali, S. Maliakal, A. Malik, L. Mallick, A. -K. Malz, N. Man, M. Mancarella, V. Mandic, V. Mangano, B. Mannix, G. L. Mansell, M. Manske, M. Mantovani, M. Mapelli, C. Marinelli, F. Marion, A. S. Markosyan, A. Markowitz, E. Maros, S. Marsat, F. Martelli, I. W. Martin, R. M. Martin, B. B. Martinez, D. A. Martinez, M. Martinez, V. Martinez, A. Martini, J. C. Martins, D. V. Martynov, E. J. Marx, L. Massaro, A. Masserot, M. Masso-Reid, S. Mastrogiovanni, T. Matcovich, M. Matiushechkina, N. Mavalvala, N. Maxwell, G. McCarrol, R. McCarthy, D. E. McClelland, S. McCormick, L. McCuller, S. McEachin, C. McElhenny, G. I. McGhee, J. McGinn, K. B. M. McGowan, J. McIver, A. McLeod, I. McMahon, T. McRae, R. McTeague, D. Meacher, B. N. Meagher, R. Mechum, Q. Meijer, A. Melatos, C. S. Menoni, F. Mera, R. A. Mercer, L. Mereni, K. Merfeld, E. L. Merilh, J. R. Mérou, J. D. Merritt, M. Merzougui, C. Messick, B. Mestichelli, M. Meyer-Conde, P. M. Meyers, F. Meylahn, A. Mhaske, A. Miani, H. Miao, C. Michel, Y. Michimura, H. Middleton, D. P. Mihaylov, S. J. Miller, M. Millhouse, E. Milotti, V. Milotti, Y. Minenkov, E. M. Minihan, Ll. M. Mir, L. Mirasola, M. Miravet-Tenés, C. -A. Miritescu, A. Mishra, C. Mishra, T. Mishra, A. L. Mitchell, J. G. Mitchell, S. Mitra, V. P. Mitrofanov, K. Mitsuhashi, R. Mittleman, O. Miyakawa, S. Miyoki, A. Miyoko, G. Mo, L. Mobilia, S. R. P. Mohapatra, S. R. Mohite, M. Molina-Ruiz, M. Mondin, J. K. Monsalve, M. Montani, C. J. Moore, D. Moraru, A. More, S. More, C. Moreno, E. A. Moreno, G. Moreno, A. Moreso Serra, S. Morisaki, Y. Moriwaki, G. Morras, A. Moscatello, M. Mould, B. Mours, C. M. Mow-Lowry, L. Muccillo, F. Muciaccia, D. Mukherjee, Samanwaya Mukherjee, Soma Mukherjee, Subroto Mukherjee, Suvodip Mukherjee, N. Mukund, A. Mullavey, H. Mullock, J. Mundi, C. L. Mungioli, M. Murakoshi, P. G. Murray, D. Nabari, S. L. Nadji, A. Nagar, N. Nagarajan, K. Nakagaki, K. Nakamura, H. Nakano, M. Nakano, D. Nanadoumgar-Lacroze, D. Nandi, V. Napolano, P. Narayan, I. Nardecchia, T. Narikawa, H. Narola, L. Naticchioni, R. K. Nayak, L. Negri, A. Nela, C. Nelle, A. Nelson, T. J. N. Nelson, M. Nery, A. Neunzert, S. Ng, L. Nguyen Quynh, S. A. Nichols, A. B. Nielsen, Y. Nishino, A. Nishizawa, S. Nissanke, W. Niu, F. Nocera, J. Noller, M. Norman, C. North, J. Novak, R. Nowicki, J. F. Nuño Siles, L. K. Nuttall, K. Obayashi, J. Oberling, J. O'Dell, E. Oelker, M. Oertel, G. Oganesyan, T. O'Hanlon, M. Ohashi, F. Ohme, R. Oliveri, R. Omer, B. O'Neal, M. Onishi, K. Oohara, B. O'Reilly, M. Orselli, R. O'Shaughnessy, S. O'Shea, S. Oshino, C. Osthelder, I. Ota, D. J. Ottaway, A. Ouzriat, H. Overmier, B. J. Owen, R. Ozaki, A. E. Pace, R. Pagano, M. A. Page, A. Pai, L. Paiella, A. Pal, S. Pal, M. A. Palaia, M. Pálfi, P. P. Palma, C. Palomba, P. Palud, H. Pan, J. Pan, K. C. Pan, P. K. Panda, Shiksha Pandey, Swadha Pandey, P. T. H. Pang, F. Pannarale, K. A. Pannone, B. C. Pant, F. H. Panther, M. Panzeri, F. Paoletti, A. Paolone, A. Papadopoulos, E. E. Papalexakis, L. Papalini, G. Papigkiotis, A. Paquis, A. Parisi, B. -J. Park, J. Park, W. Parker, G. Pascale, D. Pascucci, A. Pasqualetti, R. Passaquieti, L. Passenger, D. Passuello, O. Patane, A. V. Patel, D. Pathak, A. Patra, B. Patricelli, B. G. Patterson, K. Paul, S. Paul, E. Payne, T. Pearce, M. Pedraza, A. Pele, F. E. Peña Arellano, X. Peng, Y. Peng, S. Penn, M. D. Penuliar, A. Perego, Z. Pereira, C. Périgois, G. Perna, A. Perreca, J. Perret, S. Perriès, J. W. Perry, D. Pesios, S. Peters, S. Petracca, C. Petrillo, H. P. Pfeiffer, H. Pham, K. A. Pham, K. S. Phukon, H. Phurailatpam, M. Piarulli, L. Piccari, O. J. Piccinni, M. Pichot, M. Piendibene, F. Piergiovanni, L. Pierini, G. Pierra, V. Pierro, M. Pietrzak, M. Pillas, F. Pilo, L. Pinard, I. M. Pinto, M. Pinto, B. J. Piotrzkowski, M. Pirello, M. D. Pitkin, A. Placidi, E. Placidi, M. L. Planas, W. Plastino, C. Plunkett, R. Poggiani, E. Polini, J. Pomper, L. Pompili, J. Poon, E. Porcelli, E. K. Porter, C. Posnansky, R. Poulton, J. Powell, G. S. Prabhu, M. Pracchia, B. K. Pradhan, T. Pradier, A. K. Prajapati, K. Prasai, R. Prasanna, P. Prasia, G. Pratten, G. Principe, G. A. Prodi, P. Prosperi, P. Prosposito, A. C. Providence, A. Puecher, J. Pullin, P. Puppo, M. Pürrer, H. Qi, J. Qin, G. Quéméner, V. Quetschke, L. H. Quiceno, P. J. Quinonez, N. Qutob, R. Rading, I. Rainho, S. Raja, C. Rajan, B. Rajbhandari, K. E. Ramirez, F. A. Ramis Vidal, M. Ramos Arevalo, A. Ramos-Buades, S. Ranjan, K. Ransom, P. Rapagnani, B. Ratto, A. Ravichandran, A. Ray, V. Raymond, M. Razzano, J. Read, T. Regimbau, S. Reid, C. Reissel, D. H. Reitze, A. I. Renzini, A. Renzini, B. Revenu, A. Revilla Peña, R. Reyes, L. Ricca, F. Ricci, M. Ricci, A. Ricciardone, J. Rice, J. W. Richardson, M. L. Richardson, A. Rijal, K. Riles, H. K. Riley, S. Rinaldi, J. Rittmeyer, C. Robertson, F. Robinet, M. Robinson, A. Rocchi, L. Rolland, J. G. Rollins, A. E. Romano, J. D. Romano, R. Romano, A. Romero, I. M. Romero-Shaw, J. H. Romie, S. Ronchini, T. J. Roocke, L. Rosa, T. J. Rosauer, C. A. Rose, D. Rosińska, M. P. Ross, M. Rossello-Sastre, S. Rowan, S. K. Roy, S. Roy, D. Rozza, P. Ruggi, N. Ruhama, E. Ruiz Morales, K. Ruiz-Rocha, S. Sachdev, T. Sadecki, P. Saffarieh, S. Safi-Harb, M. R. Sah, S. Saha, T. Sainrat, S. Sajith Menon, K. Sakai, Y. Sakai, M. Sakellariadou, S. Sakon, O. S. Salafia, F. Salces-Carcoba, L. Salconi, M. Saleem, F. Salemi, M. Sallé, S. U. Salunkhe, S. Salvador, A. Salvarese, A. Samajdar, A. Sanchez, E. J. Sanchez, L. E. Sanchez, N. Sanchis-Gual, J. R. Sanders, E. M. Sänger, F. Santoliquido, F. Sarandrea, T. R. Saravanan, N. Sarin, P. Sarkar, A. Sasli, P. Sassi, B. Sassolas, B. S. Sathyaprakash, R. Sato, S. Sato, Yukino Sato, Yu Sato, O. Sauter, R. L. Savage, T. Sawada, H. L. Sawant, S. Sayah, V. Scacco, D. Schaetzl, M. Scheel, A. Schiebelbein, M. G. Schiworski, P. Schmidt, S. Schmidt, R. Schnabel, M. Schneewind, R. M. S. Schofield, K. Schouteden, B. W. Schulte, B. F. Schutz, E. Schwartz, M. Scialpi, J. Scott, S. M. Scott, R. M. Sedas, T. C. Seetharamu, M. Seglar-Arroyo, Y. Sekiguchi, D. Sellers, N. Sembo, A. S. Sengupta, E. G. Seo, J. W. Seo, V. Sequino, M. Serra, A. Sevrin, T. Shaffer, U. S. Shah, M. A. Shaikh, L. Shao, A. K. Sharma, Preeti Sharma, Prianka Sharma, Ritwik Sharma, S. Sharma Chaudhary, P. Shawhan, N. S. Shcheblanov, E. Sheridan, Z. -H. Shi, M. Shikauchi, R. Shimomura, H. Shinkai, S. Shirke, D. H. Shoemaker, D. M. Shoemaker, R. W. Short, S. ShyamSundar, A. Sider, H. Siegel, D. Sigg, L. Silenzi, L. Silvestri, M. Simmonds, L. P. Singer, Amitesh Singh, Anika Singh, D. Singh, N. Singh, S. Singh, A. M. Sintes, V. Sipala, V. Skliris, B. J. J. Slagmolen, D. A. Slater, T. J. Slaven-Blair, J. Smetana, J. R. Smith, L. Smith, R. J. E. Smith, W. J. Smith, S. Soares de Albuquerque Filho, M. Soares-Santos, K. Somiya, I. Song, S. Soni, V. Sordini, F. Sorrentino, H. Sotani, F. Spada, V. Spagnuolo, A. P. Spencer, P. Spinicelli, A. K. Srivastava, F. Stachurski, C. J. Stark, D. A. Steer, N. Steinle, J. Steinlechner, S. Steinlechner, N. Stergioulas, P. Stevens, M. StPierre, M. D. Strong, A. Strunk, A. L. Stuver, M. Suchenek, S. Sudhagar, Y. Sudo, N. Sueltmann, L. Suleiman, K. D. Sullivan, J. Sun, L. Sun, S. Sunil, J. Suresh, B. J. Sutton, P. J. Sutton, K. Suzuki, M. Suzuki, B. L. Swinkels, A. Syx, M. J. Szczepańczyk, P. Szewczyk, M. Tacca, H. Tagoshi, K. Takada, H. Takahashi, R. Takahashi, A. Takamori, S. Takano, H. Takeda, K. Takeshita, I. Takimoto Schmiegelow, M. Takou-Ayaoh, C. Talbot, M. Tamaki, N. Tamanini, D. Tanabe, K. Tanaka, S. J. Tanaka, S. Tanioka, D. B. Tanner, W. Tanner, L. Tao, R. D. Tapia, E. N. Tapia San Martín, C. Taranto, A. Taruya, J. D. Tasson, J. G. Tau, D. Tellez, R. Tenorio, H. Themann, A. Theodoropoulos, M. P. Thirugnanasambandam, L. M. Thomas, M. Thomas, P. Thomas, J. E. Thompson, S. R. Thondapu, K. A. Thorne, E. Thrane, J. Tissino, A. Tiwari, Pawan Tiwari, Praveer Tiwari, S. Tiwari, V. Tiwari, M. R. Todd, M. Toffano, A. M. Toivonen, K. Toland, A. E. Tolley, T. Tomaru, V. Tommasini, T. Tomura, H. Tong, C. Tong-Yu, A. Torres-Forné, C. I. Torrie, I. Tosta e Melo, E. Tournefier, M. Trad Nery, K. Tran, A. Trapananti, R. Travaglini, F. Travasso, G. Traylor, M. Trevor, M. C. Tringali, A. Tripathee, G. Troian, A. Trovato, L. Trozzo, R. J. Trudeau, T. Tsang, S. Tsuchida, L. Tsukada, K. Turbang, M. Turconi, C. Turski, H. Ubach, N. Uchikata, T. Uchiyama, R. P. Udall, T. Uehara, K. Ueno, V. Undheim, L. E. Uronen, T. Ushiba, M. Vacatello, H. Vahlbruch, N. Vaidya, G. Vajente, A. Vajpeyi, J. Valencia, M. Valentini, S. A. Vallejo-Peña, S. Vallero, V. Valsan, M. van Dael, E. Van den Bossche, J. F. J. van den Brand, C. Van Den Broeck, M. van der Sluys, A. Van de Walle, J. van Dongen, K. Vandra, M. VanDyke, H. van Haevermaet, J. V. van Heijningen, P. Van Hove, J. Vanier, M. VanKeuren, J. Vanosky, N. van Remortel, M. Vardaro, A. F. Vargas, V. Varma, A. N. Vazquez, A. Vecchio, G. Vedovato, J. Veitch, P. J. Veitch, S. Venikoudis, R. C. Venterea, P. Verdier, M. Vereecken, D. Verkindt, B. Verma, Y. Verma, S. M. Vermeulen, F. Vetrano, A. Veutro, A. Viceré, S. Vidyant, A. D. Viets, A. Vijaykumar, A. Vilkha, N. Villanueva Espinosa, V. Villa-Ortega, E. T. Vincent, J. -Y. Vinet, S. Viret, S. Vitale, H. Vocca, D. Voigt, E. R. G. von Reis, J. S. A. von Wrangel, W. E. Vossius, L. Vujeva, S. P. Vyatchanin, J. Wack, L. E. Wade, M. Wade, K. J. Wagner, L. Wallace, E. J. Wang, H. Wang, J. Z. Wang, W. H. Wang, Y. F. Wang, G. Waratkar, J. Warner, M. Was, T. Washimi, N. Y. Washington, D. Watarai, B. Weaver, S. A. Webster, N. L. Weickhardt, M. Weinert, A. J. Weinstein, R. Weiss, L. Wen, K. Wette, J. T. Whelan, B. F. Whiting, C. Whittle, E. G. Wickens, D. Wilken, A. T. Wilkin, B. M. Williams, D. Williams, M. J. Williams, N. S. Williams, J. L. Willis, B. Willke, M. Wils, L. Wilson, C. W. Winborn, J. Winterflood, C. C. Wipf, G. Woan, J. Woehler, N. E. Wolfe, H. T. Wong, I. C. F. Wong, K. Wong, T. Wouters, J. L. Wright, M. Wright, B. Wu, C. Wu, D. S. Wu, H. Wu, K. Wu, Q. Wu, T. Y. Wu, Y. Wu, Z. Wu, E. Wuchner, D. M. Wysocki, V. A. Xu, Y. Xu, N. Yadav, H. Yamamoto, K. Yamamoto, T. S. Yamamoto, T. Yamamoto, R. Yamazaki, T. Yan, K. Z. Yang, Y. Yang, Z. Yarbrough, J. Yebana, S. -W. Yeh, A. B. Yelikar, X. Yin, J. Yokoyama, T. Yokozawa, S. Yuan, H. Yuzurihara, M. Zanolin, M. Zeeshan, T. Zelenova, J. -P. Zendri, M. Zeoli, M. Zerrad, M. Zevin, L. Zhang, N. Zhang, R. Zhang, T. Zhang, C. Zhao, Yue Zhao, Yuhang Zhao, Z. -C. Zhao, Y. Zheng, H. Zhong, H. Zhou, H. O. Zhu, Z. -H. Zhu, A. B. Zimmerman, L. Zimmermann, M. E. Zucker, J. Zweizig
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present results from the search for an isotropic gravitational-wave background using Advanced LIGO and Advanced Virgo data from O1 through O4a, the first part of the fourth observing run. This background is the accumulated signal from unresolved sources throughout cosmic history and encodes information about the merger history of compact binaries throughout the Universe, as well as exotic physics and potentially primordial processes from the early cosmos. Our cross-correlation analysis reveals no statistically significant background signal, enabling us to constrain several theoretical scenarios. For compact binary coalescences which approximately follow a 2/3 power-law spectrum, we constrain the fractional energy density to $\Omega_{\rm GW}(25{\rm Hz})\leq 2.0\times 10^{-9}$ (95% cred.), a factor of 1.7 improvement over previous results. Scale-invariant backgrounds are constrained to $\Omega_{\rm GW}(25{\rm Hz})\leq 2.8\times 10^{-9}$, representing a 2.1x sensitivity gain. We also place new limits on gravity theories predicting non-standard polarization modes and confirm that terrestrial magnetic noise sources remain below detection threshold. Combining these spectral limits with population models for GWTC-4, the latest gravitational-wave event catalog, we find our constraints remain above predicted merger backgrounds but are approaching detectability. The joint analysis combining the background limits shown here with the GWTC-4 catalog enables improved inference of the binary black hole merger rate evolution across cosmic time. Employing GWTC-4 inference results and standard modeling choices, we estimate that the total background arising from compact binary coalescences is $\Omega_{\rm CBC}(25{\rm Hz})={0.9^{+1.1}_{-0.5}\times 10^{-9}}$ at 90% confidence, where the largest contribution is due to binary black holes only, $\Omega_{\rm BBH}(25{\rm Hz})=0.8^{+1.1}_{-0.5}\times 10^{-9}$.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T12:44:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>gr-qc</span><span>astro-ph.CO</span><span>astro-ph.HE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20721v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20721v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 Multi-Lingual Implicit Discourse Relation Recognition with Multi-Label
  Hierarchical Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nelson Filipe Costa, Leila Kosseim
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper introduces the first multi-lingual and multi-label classification model for implicit discourse relation recognition (IDRR). Our model, HArch, is evaluated on the recently released DiscoGeM 2.0 corpus and leverages hierarchical dependencies between discourse senses to predict probability distributions across all three sense levels in the PDTB 3.0 framework. We compare several pre-trained encoder backbones and find that RoBERTa-HArch achieves the best performance in English, while XLM-RoBERTa-HArch performs best in the multi-lingual setting. In addition, we compare our fine-tuned models against GPT-4o and Llama-4-Maverick using few-shot prompting across all language configurations. Our results show that our fine-tuned models consistently outperform these LLMs, highlighting the advantages of task-specific fine-tuning over prompting in IDRR. Finally, we report SOTA results on the DiscoGeM 1.0 corpus, further validating the effectiveness of our hierarchical approach.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T12:30:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20712v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20712v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 HSTPROMO Internal Proper Motion Kinematics of Dwarf Spheroidal Galaxies:
  II. Velocity Anisotropy and Dark Matter Cusp Slope of Sculptor</h2>
                <div class="authors">
                    <strong>Authors:</strong> Eduardo Vitral, Roeland P. van der Marel, Sangmo Tony Sohn, Jorge Peñarrubia, Ekta Patel, Laura L. Watkins, Mattia Libralato, Kevin McKinnon, Andrea Bellini, Paul Bennet
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We analyze three epochs of HST imaging over 20 years for the Sculptor dwarf spheroidal galaxy, measuring precise proper motions for 119 stars and combining them with 1760 existing line-of-sight velocities. This catalog yields the first radially-resolved 3D velocity dispersion profiles for Sculptor. We confirm mild oblate rotation, with major-axis velocities reaching $\sim 2$ km s$^{-1}$ beyond 20.0 arcmin. Using a methodology similar to that in the first paper in this series, we solve the Jeans equations in oblate axisymmetric geometry to infer the galaxy's mass profile. Our modeling reveals a significant degeneracy due to the unknown galaxy inclination, which is overlooked under spherical symmetry assumptions. This degeneracy allows acceptable fits across a range of dark matter profiles, from cuspy to cored. While we do not directly constrain the inclination with our Jeans models, higher-order line-of-sight velocity moments provide useful additional constraints: comparisons with scalefree models from de Bruijne et al. (1996) favor highly flattened (more face-on) configurations. Adopting an inclination well consistent with these comparisons ($i = 57.1$ degrees), we find, alongside radial velocity anisotropy, a dark matter density slope of $\Gamma_{\rm dark} = 0.29^{+0.31}_{-0.41}$ within the radial extent of the 3D velocity data, ruling out a cusp with $\Gamma_{\rm dark} \leq -1$ at 99.8% confidence. This confidence increases for lower inclinations and decreases drastically for nearly edge-on configurations. The results qualitatively agree with $\Lambda$CDM, SIDM, and Fuzzy DM scenarios that predict core formation, while our specific measurements provide quantitative constraints on the prescriptions of feedback, cross sections, or particle masses required by these models, respectively.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T12:29:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.GA</span><span>astro-ph.CO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20711v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20711v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 Constraints on QCD-based equation of state of quark stars from neutron
  star maximum mass, radius, and tidal deformability observations</h2>
                <div class="authors">
                    <strong>Authors:</strong> João V. Zastrow, Jonas P. Pereira, Rafael C. R. de Lima, Jorge E. Horvath
                </div>
                <div class="summary">
                    <strong>Summary:</strong> (Abridged) Neutron stars (NSs), the densest known objects composed of matter, provide a unique laboratory to probe whether strange quark matter is the true ground state of matter. We investigate the parameter space of the equation of state of strange stars using a quantum chromodynamics (QCD)-informed model. The parameters - related to the energy density difference between quark matter and the QCD vacuum, the strength of strong interactions, and the gap parameter for color superconductivity - are sampled via quasi-random Latin hypercube sampling to ensure uniform coverage. To constrain them, we incorporate observational data on the maximum mass of NSs (from binary and merger systems), the radii of $1.4$ M$_{\odot}$ NSs (from gravitational wave and electromagnetic observations), and tidal deformabilities (from GW170817). Our results show that quark strong interactions play a key role, requiring at least a $20\%$ deviation from the free-quark limit. We also find that color superconductivity is relevant, with the gap parameter reaching up to $\sim 230$ MeV for a strange quark mass of $100$ MeV. The surface-to-vacuum energy density jump lies in the range $(1.1-2.2)$ $\rho_{\rm{sat}}$, where $\rho_{\rm{sat}} \simeq 2.7 \times 10^{14}$ g cm$^{-3}$. Observational constraints also imply that a $1.4$ M$_{\odot}$ quark star has a radius of $(10.0-12.3)$ km and tidal deformability between $270$ and $970$. These are consistent with the low mass and radius inferred for the compact object XMMU J173203.3-344518. Our results provide useful inputs for future studies on quark and hybrid stars, including their tidal properties, thermal evolution, quasi-normal modes, and ellipticities.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T12:27:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.HE</span><span>gr-qc</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.08926v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.08926v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social
  Interactions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Maojia Song, Tej Deep Pala, Weisheng Jin, Amir Zadeh, Chuan Li, Dorien Herremans, Soujanya Poria
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) are increasingly deployed in multi-agent systems (MAS) as components of collaborative intelligence, where peer interactions dynamically shape individual decision-making. Although prior work has focused on conformity bias, we extend the analysis to examine how LLMs form trust from previous impressions, resist misinformation, and integrate peer input during interaction, key factors for achieving collective intelligence under complex social dynamics. We present KAIROS, a benchmark simulating quiz contests with peer agents of varying reliability, offering fine-grained control over conditions such as expert-novice roles, noisy crowds, and adversarial peers. LLMs receive both historical interactions and current peer responses, allowing systematic investigation into how trust, peer action, and self-confidence influence decisions. As for mitigation strategies, we evaluate prompting, supervised fine-tuning, and reinforcement learning, Group Relative Policy Optimisation (GRPO), across multiple models. Our results reveal that GRPO with multi-agent context combined with outcome-based rewards and unconstrained reasoning achieves the best overall performance, but also decreases the robustness to social influence compared to Base models. The code and datasets are available at: https://github.com/declare-lab/KAIROS.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T12:18:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18321v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18321v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 LASE: Learned Adjacency Spectral Embeddings</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sofía Pérez Casulo, Marcelo Fiori, Federico Larroca, Gonzalo Mateos
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We put forth a principled design of a neural architecture to learn nodal Adjacency Spectral Embeddings (ASE) from graph inputs. By bringing to bear the gradient descent (GD) method and leveraging the principle of algorithm unrolling, we truncate and re-interpret each GD iteration as a layer in a graph neural network (GNN) that is trained to approximate the ASE. Accordingly, we call the resulting embeddings and our parametric model Learned ASE (LASE), which is interpretable, parameter efficient, robust to inputs with unobserved edges, and offers controllable complexity during inference. LASE layers combine Graph Convolutional Network (GCN) and fully-connected Graph Attention Network (GAT) modules, which is intuitively pleasing since GCN-based local aggregations alone are insufficient to express the sought graph eigenvectors. We propose several refinements to the unrolled LASE architecture (such as sparse attention in the GAT module and decoupled layerwise parameters) that offer favorable approximation error versus computation tradeoffs; even outperforming heavily-optimized eigendecomposition routines from scientific computing libraries. Because LASE is a differentiable function with respect to its parameters as well as its graph input, we can seamlessly integrate it as a trainable module within a larger (semi-)supervised graph representation learning pipeline. The resulting end-to-end system effectively learns ``discriminative ASEs'' that exhibit competitive performance in supervised link prediction and node classification tasks, outperforming a GNN even when the latter is endowed with open loop, meaning task-agnostic, precomputed spectral positional encodings.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T12:15:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>stat.ML</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.17734v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.17734v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 Token Buncher: Shielding LLMs from Harmful Reinforcement Learning
  Fine-Tuning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weitao Feng, Lixu Wang, Tianyi Wei, Jie Zhang, Chongyang Gao, Sinong Zhan, Peizhuo Lv, Wei Dong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As large language models (LLMs) continue to grow in capability, so do the risks of harmful misuse through fine-tuning. While most prior studies assume that attackers rely on supervised fine-tuning (SFT) for such misuse, we systematically demonstrate that reinforcement learning (RL) enables adversaries to more effectively break safety alignment and facilitate advanced harmful task assistance, under matched computational budgets. To counter this emerging threat, we propose TokenBuncher, the first effective defense specifically targeting RL-based harmful fine-tuning. TokenBuncher suppresses the foundation on which RL relies: model response uncertainty. By constraining uncertainty, RL-based fine-tuning can no longer exploit distinct reward signals to drive the model toward harmful behaviors. We realize this defense through entropy-as-reward RL and a Token Noiser mechanism designed to prevent the escalation of expert-domain harmful capabilities. Extensive experiments across multiple models and RL algorithms show that TokenBuncher robustly mitigates harmful RL fine-tuning while preserving benign task utility and finetunability. Our results highlight that RL-based harmful fine-tuning poses a greater systemic risk than SFT, and that TokenBuncher provides an effective and general defense.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T12:07:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20697v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20697v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 NLKI: A lightweight Natural Language Knowledge Integration Framework for
  Improving Small VLMs in Commonsense VQA Tasks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aritra Dutta, Swapnanil Mukherjee, Deepanway Ghosal, Somak Aditya
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Commonsense visual-question answering often hinges on knowledge that is missing from the image or the question. Small vision-language models (sVLMs) such as ViLT, VisualBERT and FLAVA therefore lag behind their larger generative counterparts. To study the effect of careful commonsense knowledge integration on sVLMs, we present an end-to-end framework (NLKI) that (i) retrieves natural language facts, (ii) prompts an LLM to craft natural language explanations, and (iii) feeds both signals to sVLMs respectively across two commonsense VQA datasets (CRIC, AOKVQA) and a visual-entailment dataset (e-SNLI-VE). Facts retrieved using a fine-tuned ColBERTv2 and an object information-enriched prompt yield explanations that largely cut down hallucinations, while lifting the end-to-end answer accuracy by up to 7% (across 3 datasets), making FLAVA and other models in NLKI match or exceed medium-sized VLMs such as Qwen-2 VL-2B and SmolVLM-2.5B. As these benchmarks contain 10-25% label noise, additional finetuning using noise-robust losses (such as symmetric cross entropy and generalised cross entropy) adds another 2.5% in CRIC, and 5.5% in AOKVQA. Our findings expose when LLM-based commonsense knowledge beats retrieval from commonsense knowledge bases, how noise-aware training stabilises small models in the context of external knowledge augmentation, and why parameter-efficient commonsense reasoning is now within reach for 250M models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T12:05:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19724v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19724v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 Leveraging Large Language Models for Generating Research Topic
  Ontologies: A Multi-Disciplinary Study</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tanay Aggarwal, Angelo Salatino, Francesco Osborne, Enrico Motta
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Ontologies and taxonomies of research fields are critical for managing and organising scientific knowledge, as they facilitate efficient classification, dissemination and retrieval of information. However, the creation and maintenance of such ontologies are expensive and time-consuming tasks, usually requiring the coordinated effort of multiple domain experts. Consequently, ontologies in this space often exhibit uneven coverage across different disciplines, limited inter-domain connectivity, and infrequent updating cycles. In this study, we investigate the capability of several large language models to identify semantic relationships among research topics within three academic domains: biomedicine, physics, and engineering. The models were evaluated under three distinct conditions: zero-shot prompting, chain-of-thought prompting, and fine-tuning on existing ontologies. Additionally, we assessed the cross-domain transferability of fine-tuned models by measuring their performance when trained in one domain and subsequently applied to a different one. To support this analysis, we introduce PEM-Rel-8K, a novel dataset consisting of over 8,000 relationships extracted from the most widely adopted taxonomies in the three disciplines considered in this study: MeSH, PhySH, and IEEE. Our experiments demonstrate that fine-tuning LLMs on PEM-Rel-8K yields excellent performance across all disciplines.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T11:53:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DL</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20693v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20693v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 Large sample properties of GMM estimators under second-order
  identification</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hugo Kruiniger
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Dovonon and Hall (Journal of Econometrics, 2018) proposed a limiting distribution theory for GMM estimators for a p - dimensional globally identified parameter vector {\phi} when local identification conditions fail at first-order but hold at second-order. They assumed that the first-order underidentification is due to the expected Jacobian having rank p-1 at the true value {\phi}_{0}, i.e., having a rank deficiency of one. After reparametrizing the model such that the last column of the Jacobian vanishes, they showed that the GMM estimator of the first p-1 parameters converges at rate T^{-1/2} and the GMM estimator of the remaining parameter, {\phi}_{p}, converges at rate T^{-1/4}. They also provided a limiting distribution of T^{1/4}({\phi}_{p}-{\phi}_{0,p}) subject to a (non-transparent) condition which they claimed to be not restrictive in general. However, as we show in this paper, their condition is in fact only satisfied when {\phi} is overidentified and the limiting distribution of T^{1/4}({\phi}_{p}-{\phi}_{0,p}), which is non-standard, depends on whether {\phi} is exactly identified or overidentified. In particular, the limiting distributions of the sign of T^{1/4}({\phi}_{p}-{\phi}_{0,p}) for the cases of exact and overidentification, respectively, are different and are obtained by using expansions of the GMM objective function of different orders. Unsurprisingly, we find that the limiting distribution theories of Dovonon and Hall (2018) for Indirect Inference (II) estimation under two different scenarios with second-order identification where the target function is a GMM estimator of the auxiliary parameter vector, are incomplete for similar reasons. We discuss how our results for GMM estimation can be used to complete both theories and how they can be used to obtain the limiting distributions of the II estimators in the case of exact identification under either scenario.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T11:50:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>econ.EM</span><span>math.ST</span><span>stat.TH</span><span>62E20</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2307.13475v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2307.13475v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for
  Multi-modal Sarcasm Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junjie Chen, Hang Yu, Subin Huang, Sanmin Liu, Linfeng Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Sarcasm in social media, often expressed through text-image combinations, poses challenges for sentiment analysis and intention mining. Current multi-modal sarcasm detection methods have been demonstrated to overly rely on spurious cues within the textual modality, revealing a limited ability to genuinely identify sarcasm through nuanced text-image interactions. To solve this problem, we propose InterCLIP-MEP, which introduces Interactive CLIP (InterCLIP) with an efficient training strategy to extract enriched text-image representations by embedding cross-modal information directly into each encoder. Additionally, we design a Memory-Enhanced Predictor (MEP) with a dynamic dual-channel memory that stores valuable test sample knowledge during inference, acting as a non-parametric classifier for robust sarcasm recognition. Experiments on two benchmarks demonstrate that InterCLIP-MEP achieves state-of-the-art performance, with significant accuracy and F1 score improvements on MMSD and MMSD2.0. Our code is available at https://github.com/CoderChen01/InterCLIP-MEP.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T11:35:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.16464v6' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.16464v6' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 "Humor, Art, or Misinformation?": A Multimodal Dataset for Intent-Aware
  Synthetic Image Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anastasios Skoularikis, Stefanos-Iordanis Papadopoulos, Symeon Papadopoulos, Panagiotis C. Petrantonakis
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in multimodal AI have enabled progress in detecting synthetic and out-of-context content. However, existing efforts largely overlook the intent behind AI-generated images. To fill this gap, we introduce S-HArM, a multimodal dataset for intent-aware classification, comprising 9,576 "in the wild" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art, or Misinformation. Additionally, we explore three prompting strategies (image-guided, description-guided, and multimodally-guided) to construct a large-scale synthetic training dataset with Stable Diffusion. We conduct an extensive comparative study including modality fusion, contrastive learning, reconstruction networks, attention mechanisms, and large vision-language models. Our results show that models trained on image- and multimodally-guided data generalize better to "in the wild" content, due to preserved visual context. However, overall performance remains limited, highlighting the complexity of inferring intent and the need for specialized architectures.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T11:22:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.MM</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3746275.3762215' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.20670v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20670v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 CodecBench: A Comprehensive Benchmark for Acoustic and Semantic
  Evaluation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruifan Deng, Yitian Gong, Qinghui Gao, Luozhijie Jin, Qinyuan Cheng, Zhaoye Fei, Shimin Li, Xipeng Qiu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the rise of multimodal large language models (LLMs), audio codec plays an increasingly vital role in encoding audio into discrete tokens, enabling integration of audio into text-based LLMs. Current audio codec captures two types of information: acoustic and semantic. As audio codec is applied to diverse scenarios in speech language model , it needs to model increasingly complex information and adapt to varied contexts, such as scenarios with multiple speakers, background noise, or richer paralinguistic information. However, existing codec's own evaluation has been limited by simplistic metrics and scenarios, and existing benchmarks for audio codec are not designed for complex application scenarios, which limits the assessment performance on complex datasets for acoustic and semantic capabilities. We introduce CodecBench, a comprehensive evaluation dataset to assess audio codec performance from both acoustic and semantic perspectives across four data domains. Through this benchmark, we aim to identify current limitations, highlight future research directions, and foster advances in the development of audio codec. The codes are available at https://github.com/RayYuki/CodecBench.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T11:07:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.AS</span><span>cs.SD</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20660v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20660v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 Prediction of EDS Maps from 4DSTEM Diffraction Patterns Using
  Convolutional Neural Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mridul Kumar, Yevgeny Rakita
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Understanding the relationship between atomic structure (order) and chemical composition (chemistry) is critical for advancing materials science, yet traditional spectroscopic techniques can be slow and damaging to sensitive samples. Four-dimensional scanning transmission electron microscopy (4D-STEM) captures detailed diffraction patterns across scanned regions, providing rich structural information, while energy dispersive X-ray spectroscopy (EDS) offers complementary chemical data. In this work, we develop a machine learning framework that predicts EDS spectra directly from 4D-STEM diffraction patterns, reducing beam exposure and acquisition time. A convolutional neural network (CNN) accurately infers elemental compositions, particularly for elements with strong diffraction contrast or higher concentrations, such as Oxygen and Tellurium. Both extrapolation and interpolation strategies demonstrate consistent performance, with improved predictions when additional structural context is available. Visual and cross-correlation analyses confirm the model's ability to capture global and local compositional trends. This approach establishes a data-driven pathway to non-destructive, high-throughput materials characterization.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T11:02:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.mtrl-sci</span><span>cond-mat.dis-nn</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20657v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20657v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 Compositionality in Time Series: A Proof of Concept using Symbolic
  Dynamics and Compositional Data Augmentation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Michael Hagmann, Michael Staniek, Stefan Riezler
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This work investigates whether time series of natural phenomena can be understood as being generated by sequences of latent states which are ordered in systematic and regular ways. We focus on clinical time series and ask whether clinical measurements can be interpreted as being generated by meaningful physiological states whose succession follows systematic principles. Uncovering the underlying compositional structure will allow us to create synthetic data to alleviate the notorious problem of sparse and low-resource data settings in clinical time series forecasting, and deepen our understanding of clinical data. We start by conceptualizing compositionality for time series as a property of the data generation process, and then study data-driven procedures that can reconstruct the elementary states and composition rules of this process. We evaluate the success of this methods using two empirical tests originating from a domain adaptation perspective. Both tests infer the similarity of the original time series distribution and the synthetic time series distribution from the similarity of expected risk of time series forecasting models trained and tested on original and synthesized data in specific ways. Our experimental results show that the test set performance achieved by training on compositionally synthesized data is comparable to training on original clinical time series data, and that evaluation of models on compositionally synthesized test data shows similar results to evaluating on original test data, outperforming randomization-based data augmentation. An additional downstream evaluation of the prediction task of sequential organ failure assessment (SOFA) scores shows significant performance gains when model training is entirely based on compositionally synthesized data compared to training on original data.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T11:02:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20656v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20656v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 Improving Alignment in LVLMs with Debiased Self-Judgment</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sihan Yang, Chenhang Cui, Zihao Zhao, Yiyang Zhou, Weilong Yan, Ying Wei, Huaxiu Yao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid advancements in Large Language Models (LLMs) and Large Visual-Language Models (LVLMs) have opened up new opportunities for integrating visual and linguistic modalities. However, effectively aligning these modalities remains challenging, often leading to hallucinations--where generated outputs are not grounded in the visual input--and raising safety concerns across various domains. Existing alignment methods, such as instruction tuning and preference tuning, often rely on external datasets, human annotations, or complex post-processing, which limit scalability and increase costs. To address these challenges, we propose a novel approach that generates the debiased self-judgment score, a self-evaluation metric created internally by the model without relying on external resources. This enables the model to autonomously improve alignment. Our method enhances both decoding strategies and preference tuning processes, resulting in reduced hallucinations, enhanced safety, and improved overall capability. Empirical results show that our approach significantly outperforms traditional methods, offering a more effective solution for aligning LVLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T11:01:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20655v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20655v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 Refining Text Generation for Realistic Conversational Recommendation via
  Direct Preference Optimization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Manato Tajiri, Michimasa Inaba
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Conversational Recommender Systems (CRSs) aim to elicit user preferences via natural dialogue to provide suitable item recommendations. However, current CRSs often deviate from realistic human interactions by rapidly recommending items in brief sessions. This work addresses this gap by leveraging Large Language Models (LLMs) to generate dialogue summaries from dialogue history and item recommendation information from item description. This approach enables the extraction of both explicit user statements and implicit preferences inferred from the dialogue context. We introduce a method using Direct Preference Optimization (DPO) to ensure dialogue summary and item recommendation information are rich in information crucial for effective recommendations. Experiments on two public datasets validate our method's effectiveness in fostering more natural and realistic conversational recommendation processes.Our implementation is publicly available at: https://github.com/UEC-InabaLab/Refining-LLM-Text
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T10:57:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19918v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19918v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 CyberSleuth: Autonomous Blue-Team LLM Agent for Web Attack Forensics</h2>
                <div class="authors">
                    <strong>Authors:</strong> Stefano Fumero, Kai Huang, Matteo Boffa, Danilo Giordano, Marco Mellia, Zied Ben Houidi, Dario Rossi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Model (LLM) agents are powerful tools for automating complex tasks. In cybersecurity, researchers have primarily explored their use in red-team operations such as vulnerability discovery and penetration tests. Defensive uses for incident response and forensics have received comparatively less attention and remain at an early stage. This work presents a systematic study of LLM-agent design for the forensic investigation of realistic web application attacks. We propose CyberSleuth, an autonomous agent that processes packet-level traces and application logs to identify the targeted service, the exploited vulnerability (CVE), and attack success. We evaluate the consequences of core design decisions - spanning tool integration and agent architecture - and provide interpretable guidance for practitioners. We benchmark four agent architectures and six LLM backends on 20 incident scenarios of increasing complexity, identifying CyberSleuth as the best-performing design. In a separate set of 10 incidents from 2025, CyberSleuth correctly identifies the exact CVE in 80% of cases. At last, we conduct a human study with 22 experts, which rated the reports of CyberSleuth as complete, useful, and coherent. They also expressed a slight preference for DeepSeek R1, a good news for open source LLM. To foster progress in defensive LLM research, we release both our benchmark and the CyberSleuth platform as a foundation for fair, reproducible evaluation of forensic agents.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T10:45:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20643v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20643v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 Emergent dynamics of active elastic microbeams</h2>
                <div class="authors">
                    <strong>Authors:</strong> Q. Martinet, Y. Li, A. Aubret, E. Hannezo, J. Palacci
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In equilibrium, the physical properties of matter are set by the interactions between the constituents. In contrast, the energy input of the individual components controls the behavior of synthetic or living active matter. Great progress has been made in understanding the emergent phenomena in active fluids, though their inability to resist shear forces hinders their practical use. This motivates the exploration of active solids as shape-shifting materials, yet, we lack controlled synthetic systems to devise active solids with unconventional properties. %and bridge the gap between active solids made of macroscopic robots and the complexity of biological materials. Here we build active elastic beams from dozens of active colloids and unveil complex emergent behaviors such as self-oscillations or persistent rotations. Developing tensile tests at the microscale, we show that the active beams are ultra-soft materials, with large (non-equilibrium) fluctuations. Combining experiments, theory, and stochastic inference, we show that the dynamics of the active beams can be mapped on different phase transitions which are tuned by boundary conditions. More quantitatively, we assess all relevant parameters by independent measurements or first-principles calculations, and find that our theoretical description agrees with the experimental observations. Our results demonstrate that the simple addition of activity to an elastic beam unveils novel physics and can inspire design strategies for active solids and functional microscopic machines.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T10:41:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.soft</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20642v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20642v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 GDS Agent: A Graph Algorithmic Reasoning Agent</h2>
                <div class="authors">
                    <strong>Authors:</strong> Borun Shi, Ioannis Panagiotas
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have shown remarkable multimodal information processing and reasoning ability. When equipped with tools through function calling and enhanced with retrieval-augmented techniques, compound LLM-based systems can access closed data sources and answer questions about them. However, they still struggle to process and reason over large-scale graph-structure data. We introduce the GDS (Graph Data Science) agent in this technical report. The GDS agent introduces a comprehensive set of graph algorithms as tools, together with preprocessing (retrieval) and postprocessing of algorithm results, in a model context protocol (MCP) server. The server can be used with any modern LLM out-of-the-box. GDS agent allows users to ask any question that implicitly and intrinsically requires graph algorithmic reasoning about their data, and quickly obtain accurate and grounded answers. We also introduce a new benchmark that evaluates intermediate tool calls as well as final responses. The results indicate that GDS agent is able to solve a wide spectrum of graph tasks. We also provide detailed case studies for more open-ended tasks and study scenarios where the agent struggles. Finally, we discuss the remaining challenges and the future roadmap.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T10:35:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20637v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20637v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 Schema-Guided Response Generation using Multi-Frame Dialogue State for
  Motivational Interviewing Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jie Zeng, Yukiko I. Nakano
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The primary goal of Motivational Interviewing (MI) is to help clients build their own motivation for behavioral change. To support this in dialogue systems, it is essential to guide large language models (LLMs) to generate counselor responses aligned with MI principles. By employing a schema-guided approach, this study proposes a method for updating multi-frame dialogue states and a strategy decision mechanism that dynamically determines the response focus in a manner grounded in MI principles. The proposed method was implemented in a dialogue system and evaluated through a user study. Results showed that the proposed system successfully generated MI-favorable responses and effectively encouraged the user's (client's) deliberation by asking eliciting questions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T10:34:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20635v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20635v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 Continuously Steering LLMs Sensitivity to Contextual Knowledge with
  Proxy Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yilin Wang, Heng Wang, Yuyang Bai, Minnan Luo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In Large Language Models (LLMs) generation, there exist knowledge conflicts and scenarios where parametric knowledge contradicts knowledge provided in the context. Previous works studied tuning, decoding algorithms, or locating and editing context-aware neurons to adapt LLMs to be faithful to new contextual knowledge. However, they are usually inefficient or ineffective for large models, not workable for black-box models, or unable to continuously adjust LLMs' sensitivity to the knowledge provided in the context. To mitigate these problems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a simple framework that can steer LLMs' sensitivity to contextual knowledge continuously at a lightweight cost. Specifically, we tune two small LMs (i.e. proxy models) and use the difference in their output distributions to shift the original distribution of an LLM without modifying the LLM weights. In the evaluation process, we not only design synthetic data and fine-grained metrics to measure models' sensitivity to contextual knowledge but also use a real conflict dataset to validate CSKS's practical efficacy. Extensive experiments demonstrate that our framework achieves continuous and precise control over LLMs' sensitivity to contextual knowledge, enabling both increased sensitivity and reduced sensitivity, thereby allowing LLMs to prioritize either contextual or parametric knowledge as needed flexibly. Our data and code are available at https://github.com/OliveJuiceLin/CSKS.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T10:00:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19720v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19720v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 Revisiting the Privacy Risks of Split Inference: A GAN-Based Data
  Reconstruction Attack via Progressive Feature Optimization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yixiang Qiu, Yanhan Liu, Hongyao Yu, Hao Fang, Bin Chen, Shu-Tao Xia, Ke Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The growing complexity of Deep Neural Networks (DNNs) has led to the adoption of Split Inference (SI), a collaborative paradigm that partitions computation between edge devices and the cloud to reduce latency and protect user privacy. However, recent advances in Data Reconstruction Attacks (DRAs) reveal that intermediate features exchanged in SI can be exploited to recover sensitive input data, posing significant privacy risks. Existing DRAs are typically effective only on shallow models and fail to fully leverage semantic priors, limiting their reconstruction quality and generalizability across datasets and model architectures. In this paper, we propose a novel GAN-based DRA framework with Progressive Feature Optimization (PFO), which decomposes the generator into hierarchical blocks and incrementally refines intermediate representations to enhance the semantic fidelity of reconstructed images. To stabilize the optimization and improve image realism, we introduce an L1-ball constraint during reconstruction. Extensive experiments show that our method outperforms prior attacks by a large margin, especially in high-resolution scenarios, out-of-distribution settings, and against deeper and more complex DNNs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T10:00:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20613v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20613v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 DART: Distilling Autoregressive Reasoning to Silent Thought</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nan Jiang, Ziming Wu, De-Chuan Zhan, Fuming Lai, Shaobing Lian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Chain-of-Thought (CoT) reasoning has significantly advanced Large Language Models (LLMs) in solving complex tasks. However, its autoregressive paradigm leads to significant computational overhead, hindering its deployment in latency-sensitive applications. To address this, we propose \textbf{DART} (\textbf{D}istilling \textbf{A}utoregressive \textbf{R}easoning to Silent \textbf{T}hought), a self-distillation framework that enables LLMs to replace autoregressive CoT with non-autoregressive Silent Thought (ST). Specifically, DART introduces two training pathways: the CoT pathway for traditional reasoning and the ST pathway for generating answers directly from a few ST tokens. The ST pathway utilizes a lightweight Reasoning Evolvement Module (REM) to align its hidden states with the CoT pathway, enabling the ST tokens to evolve into informative embeddings. During inference, only the ST pathway is activated, leveraging evolving ST tokens to deliver the answer directly. Extensive experimental results demonstrate that DART offers significant performance gains compared with existing non-autoregressive baselines without extra inference latency, serving as a feasible alternative for efficient reasoning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T09:45:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.11752v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.11752v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality
  Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haodong Duan, Xinyu Fang, Junming Yang, Xiangyu Zhao, Yuxuan Qiao, Mo Li, Amit Agarwal, Zhe Chen, Lin Chen, Yuan Liu, Yubo Ma, Hailong Sun, Yifan Zhang, Shiyin Lu, Tack Hwa Wong, Weiyun Wang, Peiheng Zhou, Xiaozhe Li, Chaoyou Fu, Junbo Cui, Jixuan Chen, Enxin Song, Song Mao, Shengyuan Ding, Tianhao Liang, Zicheng Zhang, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, Dahua Lin, Kai Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present VLMEvalKit: an open-source toolkit for evaluating large multi-modality models based on PyTorch. The toolkit aims to provide a user-friendly and comprehensive framework for researchers and developers to evaluate existing multi-modality models and publish reproducible evaluation results. In VLMEvalKit, we implement over 200+ different large multi-modality models, including both proprietary APIs and open-source models, as well as more than 80 different multi-modal benchmarks. By implementing a single interface, new models can be easily added to the toolkit, while the toolkit automatically handles the remaining workloads, including data preparation, distributed inference, prediction post-processing, and metric calculation. Although the toolkit is currently mainly used for evaluating large vision-language models, its design is compatible with future updates that incorporate additional modalities, such as audio and video. Based on the evaluation results obtained with the toolkit, we host OpenVLM Leaderboard, a comprehensive leaderboard to track the progress of multi-modality learning research. The toolkit is released on https://github.com/open-compass/VLMEvalKit and is actively maintained.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T09:40:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.11691v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.11691v4' target='_blank'>pdf</a>
                </div>
            </div>
            <h1>Keyword: LLM Deployment</h1>

            <div class="paper">
                <h2>#1 Learning on the Fly: Rapid Policy Adaptation via Differentiable
  Simulation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiahe Pan, Jiaxu Xing, Rudolf Reiter, Yifan Zhai, Elie Aljalbout, Davide Scaramuzza
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Learning control policies in simulation enables rapid, safe, and cost-effective development of advanced robotic capabilities. However, transferring these policies to the real world remains difficult due to the sim-to-real gap, where unmodeled dynamics and environmental disturbances can degrade policy performance. Existing approaches, such as domain randomization and Real2Sim2Real pipelines, can improve policy robustness, but either struggle under out-of-distribution conditions or require costly offline retraining. In this work, we approach these problems from a different perspective. Instead of relying on diverse training conditions before deployment, we focus on rapidly adapting the learned policy in the real world in an online fashion. To achieve this, we propose a novel online adaptive learning framework that unifies residual dynamics learning with real-time policy adaptation inside a differentiable simulation. Starting from a simple dynamics model, our framework refines the model continuously with real-world data to capture unmodeled effects and disturbances such as payload changes and wind. The refined dynamics model is embedded in a differentiable simulation framework, enabling gradient backpropagation through the dynamics and thus rapid, sample-efficient policy updates beyond the reach of classical RL methods like PPO. All components of our system are designed for rapid adaptation, enabling the policy to adjust to unseen disturbances within 5 seconds of training. We validate the approach on agile quadrotor control under various disturbances in both simulation and the real world. Our framework reduces hovering error by up to 81% compared to L1-MPC and 55% compared to DATT, while also demonstrating robustness in vision-based control without explicit state estimation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:59:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.21065v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.21065v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 Bitune: Leveraging Bidirectional Attention to Improve Decoder-Only LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dawid J. Kopiczko, Tijmen Blankevoort, Yuki M. Asano
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Decoder-only large language models typically rely solely on masked causal attention, which limits their expressiveness by restricting information flow to one direction. We propose Bitune, a method that enhances pretrained decoder-only LLMs by incorporating bidirectional attention into prompt processing. We evaluate Bitune in instruction-tuning and question-answering settings, showing significant improvements in performance on commonsense reasoning, arithmetic, and language understanding tasks. Furthermore, extensive ablation studies validate the role of each component of the method, and demonstrate that Bitune is compatible with various parameter-efficient finetuning techniques and full model finetuning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:59:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2405.14862v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.14862v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn
  Dialogue with Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Adam Coscia, Shunan Guo, Eunyee Koh, Alex Endert
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As multi-turn dialogues with large language models (LLMs) grow longer and more complex, how can users better evaluate and review progress on their conversational goals? We present OnGoal, an LLM chat interface that helps users better manage goal progress. OnGoal provides real-time feedback on goal alignment through LLM-assisted evaluation, explanations for evaluation results with examples, and overviews of goal progression over time, enabling users to navigate complex dialogues more effectively. Through a study with 20 participants on a writing task, we evaluate OnGoal against a baseline chat interface without goal tracking. Using OnGoal, participants spent less time and effort to achieve their goals while exploring new prompting strategies to overcome miscommunication, suggesting tracking and visualizing goals can enhance engagement and resilience in LLM dialogues. Our findings inspired design implications for future LLM chat interfaces that improve goal communication, reduce cognitive load, enhance interactivity, and enable feedback to improve LLM performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:58:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3746059.3747746' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.21061v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.21061v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 Enabling Equitable Access to Trustworthy Financial Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> William Jurayj, Nils Holzenberger, Benjamin Van Durme
                </div>
                <div class="summary">
                    <strong>Summary:</strong> According to the United States Internal Revenue Service, ''the average American spends $\$270$ and 13 hours filing their taxes''. Even beyond the U.S., tax filing requires complex reasoning, combining application of overlapping rules with numerical calculations. Because errors can incur costly penalties, any automated system must deliver high accuracy and auditability, making modern large language models (LLMs) poorly suited for this task. We propose an approach that integrates LLMs with a symbolic solver to calculate tax obligations. We evaluate variants of this system on the challenging StAtutory Reasoning Assessment (SARA) dataset, and include a novel method for estimating the cost of deploying such a system based on real-world penalties for tax errors. We further show how combining up-front translation of plain-text rules into formal logic programs, combined with intelligently retrieved exemplars for formal case representations, can dramatically improve performance on this task and reduce costs to well below real-world averages. Our results demonstrate the promise and economic feasibility of neuro-symbolic architectures for increasing equitable access to reliable tax assistance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:55:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.CY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.21051v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.21051v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 Estimating Machine Translation Difficulty</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lorenzo Proietti, Stefano Perrella, Vilém Zouhar, Roberto Navigli, Tom Kocmi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Machine translation quality has steadily improved over the years, achieving near-perfect translations in recent benchmarks. These high-quality outputs make it difficult to distinguish between state-of-the-art models and to identify areas for future improvement. In this context, automatically identifying texts where machine translation systems struggle holds promise for developing more discriminative evaluations and guiding future research.   In this work, we address this gap by formalizing the task of translation difficulty estimation, defining a text's difficulty based on the expected quality of its translations. We introduce a new metric to evaluate difficulty estimators and use it to assess both baselines and novel approaches. Finally, we demonstrate the practical utility of difficulty estimators by using them to construct more challenging benchmarks for machine translation. Our results show that dedicated models outperform both heuristic-based methods and LLM-as-a-judge approaches, with Sentinel-src achieving the best performance. Thus, we release two improved models for difficulty estimation, Sentinel-src-24 and Sentinel-src-25, which can be used to scan large collections of texts and select those most likely to challenge contemporary machine translation systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:54:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10175v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10175v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 Veritas: Generalizable Deepfake Detection via Pattern-Aware Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hao Tan, Jun Lan, Zichang Tan, Ajian Liu, Chuanbiao Song, Senyuan Shi, Huijia Zhu, Weiqiang Wang, Jun Wan, Zhen Lei
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Deepfake detection remains a formidable challenge due to the complex and evolving nature of fake content in real-world scenarios. However, existing academic benchmarks suffer from severe discrepancies from industrial practice, typically featuring homogeneous training sources and low-quality testing images, which hinder the practical deployments of current detectors. To mitigate this gap, we introduce HydraFake, a dataset that simulates real-world challenges with hierarchical generalization testing. Specifically, HydraFake involves diversified deepfake techniques and in-the-wild forgeries, along with rigorous training and evaluation protocol, covering unseen model architectures, emerging forgery techniques and novel data domains. Building on this resource, we propose Veritas, a multi-modal large language model (MLLM) based deepfake detector. Different from vanilla chain-of-thought (CoT), we introduce pattern-aware reasoning that involves critical reasoning patterns such as "planning" and "self-reflection" to emulate human forensic process. We further propose a two-stage training pipeline to seamlessly internalize such deepfake reasoning capacities into current MLLMs. Experiments on HydraFake dataset reveal that although previous detectors show great generalization on cross-model scenarios, they fall short on unseen forgeries and data domains. Our Veritas achieves significant gains across different OOD scenarios, and is capable of delivering transparent and faithful detection outputs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:53:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.21048v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.21048v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 CogVLA: Cognition-Aligned Vision-Language-Action Model via
  Instruction-Driven Routing & Sparsification</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wei Li, Renshan Zhang, Rui Shao, Jie He, Liqiang Nie
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent Vision-Language-Action (VLA) models built on pre-trained Vision-Language Models (VLMs) require extensive post-training, resulting in high computational overhead that limits scalability and deployment.We propose CogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages instruction-driven routing and sparsification to improve both efficiency and performance. CogVLA draws inspiration from human multimodal coordination and introduces a 3-stage progressive architecture. 1) Encoder-FiLM based Aggregation Routing (EFA-Routing) injects instruction information into the vision encoder to selectively aggregate and compress dual-stream visual tokens, forming a instruction-aware latent representation. 2) Building upon this compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing) introduces action intent into the language model by pruning instruction-irrelevant visually grounded tokens, thereby achieving token-level sparsity. 3) To ensure that compressed perception inputs can still support accurate and coherent action generation, we introduce V-L-A Coupled Attention (CAtten), which combines causal vision-language attention with bidirectional action parallel decoding. Extensive experiments on the LIBERO benchmark and real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art performance with success rates of 97.4% and 70.0%, respectively, while reducing training costs by 2.5-fold and decreasing inference latency by 2.8-fold compared to OpenVLA. CogVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/CogVLA.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:50:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.21046v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.21046v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 MMG-Vid: Maximizing Marginal Gains at Segment-level and Token-level for
  Efficient Video LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junpeng Ma, Qizhe Zhang, Ming Lu, Zhibin Wang, Qiang Zhou, Jun Song, Shanghang Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Video Large Language Models (VLLMs) excel in video understanding, but their excessive visual tokens pose a significant computational challenge for real-world applications. Current methods aim to enhance inference efficiency by visual token pruning. However, they do not consider the dynamic characteristics and temporal dependencies of video frames, as they perceive video understanding as a multi-frame task. To address these challenges, we propose MMG-Vid, a novel training-free visual token pruning framework that removes redundancy by Maximizing Marginal Gains at both segment-level and token-level. Specifically, we first divide the video into segments based on frame similarity, and then dynamically allocate the token budget for each segment to maximize the marginal gain of each segment. Subsequently, we propose a temporal-guided DPC algorithm that jointly models inter-frame uniqueness and intra-frame diversity, thereby maximizing the marginal gain of each token. By combining both stages, MMG-Vid can maximize the utilization of the limited token budget, significantly improving efficiency while maintaining strong performance. Extensive experiments demonstrate that MMG-Vid can maintain over 99.5% of the original performance, while effectively reducing 75% visual tokens and accelerating the prefilling stage by 3.9x on LLaVA-OneVision-7B. Code will be released soon.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:50:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.21044v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.21044v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 Predicting Trends in $V_{OC}$ Through Rapid, Multimodal Characterization
  of State-of-the-Art p-i-n Perovskite Devices</h2>
                <div class="authors">
                    <strong>Authors:</strong> Amy E. Louks, Brandon T. Motes, Anthony T. Troupe, Axel F. Palmstrom, Joseph J. Berry, Dane W. deQuilettes
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Perovskite photovoltaic technologies are approaching commercial deployment, yet single junction and tandem architectures both still have significant room to improve power conversion efficiency and stability. The ability to perform rapid screening of material quality after altering processing conditions is critical to accelerating the optimization and commercialization of perovskite-based technologies. Currently, researchers utilize a wide range of stand-alone metrology tools to isolate sources of power loss throughout a device stack, which can be slow and labor intensive. Here, we demonstrate the use of a multimodal metrology approach to rapidly determine the maximum achievable and predicted open circuit voltages of > 100 perovskite devices during fabrication. Acquisition of these different data are facilitated by combining them into a single integrated measurement platform. We show that these data and automated analysis can be used to rapidly understand and ultimately predict quantitative trends in open circuit voltages of state-of-the-art devices architectures. The data and automated analysis workflow presented provides a reliable approach to quickly identify absorber and charge transport layer combinations that can lead to improved open circuit voltages.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:41:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.mtrl-sci</span><span>physics.app-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.21037v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.21037v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 The Ramon Llull's Thinking Machine for Automated Ideation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xinran Zhao, Boyuan Zheng, Chenglei Si, Haofei Yu, Ken Liu, Runlong Zhou, Ruochen Li, Tong Chen, Xiang Li, Yiming Zhang, Tongshuang Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for generating knowledge through symbolic recombination - as a conceptual foundation for building a modern Llull's thinking machine for research ideation. Our approach defines three compositional axes: Theme (e.g., efficiency, adaptivity), Domain (e.g., question answering, machine translation), and Method (e.g., adversarial training, linear attention). These elements represent high-level abstractions common in scientific work - motivations, problem settings, and technical approaches - and serve as building blocks for LLM-driven exploration. We mine elements from human experts or conference papers and show that prompting LLMs with curated combinations produces research ideas that are diverse, relevant, and grounded in current literature. This modern thinking machine offers a lightweight, interpretable tool for augmenting scientific creativity and suggests a path toward collaborative ideation between humans and AI.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:29:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19200v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19200v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 An Agile Method for Implementing Retrieval Augmented Generation Tools in
  Industrial SMEs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mathieu Bourdin, Anas Neumann, Thomas Paviot, Robert Pellerin, Samir Lamouri
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Retrieval-Augmented Generation (RAG) has emerged as a powerful solution to mitigate the limitations of Large Language Models (LLMs), such as hallucinations and outdated knowledge. However, deploying RAG-based tools in Small and Medium Enterprises (SMEs) remains a challenge due to their limited resources and lack of expertise in natural language processing (NLP). This paper introduces EASI-RAG, Enterprise Application Support for Industrial RAG, a structured, agile method designed to facilitate the deployment of RAG systems in industrial SME contexts. EASI-RAG is based on method engineering principles and comprises well-defined roles, activities, and techniques. The method was validated through a real-world case study in an environmental testing laboratory, where a RAG tool was implemented to answer operators queries using data extracted from operational procedures. The system was deployed in under a month by a team with no prior RAG experience and was later iteratively improved based on user feedback. Results demonstrate that EASI-RAG supports fast implementation, high user adoption, delivers accurate answers, and enhances the reliability of underlying data. This work highlights the potential of RAG deployment in industrial SMEs. Future works include the need for generalization across diverse use cases and further integration with fine-tuned models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:27:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.21024v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.21024v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 Lethe: Purifying Backdoored Large Language Models with Knowledge
  Dilution</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chen Chen, Yuchen Sun, Jiaxin Gao, Xueluan Gong, Qian Wang, Ziyao Wang, Yongsen Zheng, Kwok-Yan Lam
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have seen significant advancements, achieving superior performance in various Natural Language Processing (NLP) tasks. However, they remain vulnerable to backdoor attacks, where models behave normally for standard queries but generate harmful responses or unintended output when specific triggers are activated. Existing backdoor defenses either lack comprehensiveness, focusing on narrow trigger settings, detection-only mechanisms, and limited domains, or fail to withstand advanced scenarios like model-editing-based, multi-trigger, and triggerless attacks. In this paper, we present LETHE, a novel method to eliminate backdoor behaviors from LLMs through knowledge dilution using both internal and external mechanisms. Internally, LETHE leverages a lightweight dataset to train a clean model, which is then merged with the backdoored model to neutralize malicious behaviors by diluting the backdoor impact within the model's parametric memory. Externally, LETHE incorporates benign and semantically relevant evidence into the prompt to distract LLM's attention from backdoor features. Experimental results on classification and generation domains across 5 widely used LLMs demonstrate that LETHE outperforms 8 state-of-the-art defense baselines against 8 backdoor attacks. LETHE reduces the attack success rate of advanced backdoor attacks by up to 98% while maintaining model utility. Furthermore, LETHE has proven to be cost-efficient and robust against adaptive backdoor attacks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:05:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.21004v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.21004v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yaniv Hassidof, Tom Jurgenson, Kiril Solovey
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Kinodynamic motion planning is concerned with computing collision-free trajectories while abiding by the robot's dynamic constraints. This critical problem is often tackled using sampling-based planners (SBPs) that explore the robot's high-dimensional state space by constructing a search tree via action propagations. Although SBPs can offer global guarantees on completeness and solution quality, their performance is often hindered by slow exploration due to uninformed action sampling. Learning-based approaches can yield significantly faster runtimes, yet they fail to generalize to out-of-distribution (OOD) scenarios and lack critical guarantees, e.g., safety, thus limiting their deployment on physical robots. We present Diffusion Tree (DiTree): a \emph{provably-generalizable} framework leveraging diffusion policies (DPs) as informed samplers to efficiently guide state-space search within SBPs. DiTree combines DP's ability to model complex distributions of expert trajectories, conditioned on local observations, with the completeness of SBPs to yield \emph{provably-safe} solutions within a few action propagation iterations for complex dynamical systems. We demonstrate DiTree's power with an implementation combining the popular RRT planner with a DP action sampler trained on a \emph{single environment}. In comprehensive evaluations on OOD scenarios, % DiTree has comparable runtimes to a standalone DP (3x faster than classical SBPs), while improving the average success rate over DP and SBPs. DiTree is on average 3x faster than classical SBPs, and outperforms all other approaches by achieving roughly 30\% higher success rate. Project webpage: https://sites.google.com/view/ditree.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T17:04:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.21001v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.21001v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic
  Support in Addiction Recovery</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junda Wang, Zonghai Yao, Zhichao Yang, Lingxi Li, Junhui Qian, Hong Yu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Substance use disorders (SUDs) affect over 36 million people worldwide, yet few receive effective care due to stigma, motivational barriers, and limited personalized support. Although large language models (LLMs) show promise for mental-health assistance, most systems lack tight integration with clinically validated strategies, reducing effectiveness in addiction recovery. We present ChatThero, a multi-agent conversational framework that couples dynamic patient modeling with context-sensitive therapeutic dialogue and adaptive persuasive strategies grounded in cognitive behavioral therapy (CBT) and motivational interviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy, Medium, and Hard resistance levels, and train ChatThero with a two-stage pipeline comprising supervised fine-tuning (SFT) followed by direct preference optimization (DPO). In evaluation, ChatThero yields a 41.5\% average gain in patient motivation, a 0.49\% increase in treatment confidence, and resolves hard cases with 26\% fewer turns than GPT-4o, and both automated and human clinical assessments rate it higher in empathy, responsiveness, and behavioral realism. The framework supports rigorous, privacy-preserving study of therapeutic conversation and provides a robust, replicable basis for research and clinical translation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T16:57:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20996v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20996v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 InterAct-Video: Reasoning-Rich Video QA for Urban Traffic</h2>
                <div class="authors">
                    <strong>Authors:</strong> Joseph Raj Vishal, Divesh Basina, Rutuja Patil, Manas Srinivas Gowda, Katha Naik, Yezhou Yang, Bharatesh Chakravarthi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Traffic monitoring is crucial for urban mobility, road safety, and intelligent transportation systems (ITS). Deep learning has advanced video-based traffic monitoring through video question answering (VideoQA) models, enabling structured insight extraction from traffic videos. However, existing VideoQA models struggle with the complexity of real-world traffic scenes, where multiple concurrent events unfold across spatiotemporal dimensions. To address these challenges, this paper introduces \textbf{InterAct VideoQA}, a curated dataset designed to benchmark and enhance VideoQA models for traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of real-world traffic footage collected from diverse intersections, segmented into 10-second video clips, with over 25,000 question-answer (QA) pairs covering spatiotemporal dynamics, vehicle interactions, incident detection, and other critical traffic attributes. State-of-the-art VideoQA models are evaluated on InterAct VideoQA, exposing challenges in reasoning over fine-grained spatiotemporal dependencies within complex traffic scenarios. Additionally, fine-tuning these models on InterAct VideoQA yields notable performance improvements, demonstrating the necessity of domain-specific datasets for VideoQA. InterAct VideoQA is publicly available as a benchmark dataset to facilitate future research in real-world deployable VideoQA models for intelligent transportation systems. GitHub Repo: https://github.com/joe-rabbit/InterAct_VideoQA
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T16:45:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.14743v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.14743v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 Dynamic Context Compression for Efficient RAG</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shuyu Guo, Zhaochun Ren
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but incurs significant inference costs due to lengthy retrieved contexts. While context compression mitigates this issue, existing methods apply fixed compression rates, over-compressing simple queries or under-compressing complex ones. We propose Adaptive Context Compression for RAG (ACC-RAG), a framework that dynamically adjusts compression rates based on input complexity, optimizing inference efficiency without sacrificing accuracy. ACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with a context selector to retain minimal sufficient information, akin to human skimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms fixed-rate methods and matches/unlocks over 4 times faster inference versus standard RAG while maintaining or improving accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T16:42:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.22931v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.22931v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 Program Semantic Inequivalence Game with Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Antonio Valerio Miceli-Barone, Vaishak Belle, Ali Payani
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) can achieve strong performance on everyday coding tasks, but they can fail on complex tasks that require non-trivial reasoning about program semantics. Finding training examples to teach LLMs to solve these tasks can be challenging.   In this work, we explore a method to synthetically generate code reasoning training data based on a semantic inequivalence game SInQ: a generator agent creates program variants that are semantically distinct, derived from a dataset of real-world programming tasks, while an evaluator agent has to identify input examples that cause the original programs and the generated variants to diverge in their behaviour, with the agents training each other semi-adversarially. We prove that this setup enables theoretically unlimited improvement through self-play in the limit of infinite computational resources.   We evaluated our approach on multiple code generation and understanding benchmarks, including cross-language vulnerability detection (Lu et al., 2021), where our method improves vulnerability detection in C/C++ code despite being trained exclusively on Python code, and the challenging Python builtin identifier swap benchmark (Miceli-Barone et al., 2023), showing that whereas modern LLMs still struggle with this benchmark, our approach yields substantial improvements.   We release the code needed to replicate the experiments, as well as the generated synthetic data, which can be used to fine-tune LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T16:38:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.PL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.03818v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.03818v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 ConfLogger: Enhance Systems' Configuration Diagnosability through
  Configuration Logging</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shiwen Shan, Yintong Huo, Yuxin Su, Zhining Wang, Dan Li, Zibin Zheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modern configurable systems offer customization via intricate configuration spaces, yet such flexibility introduces pervasive configuration-related issues such as misconfigurations and latent softwarebugs. Existing diagnosability supports focus on post-failure analysis of software behavior to identify configuration issues, but none of these approaches look into whether the software clue sufficient failure information for diagnosis. To fill in the blank, we propose the idea of configuration logging to enhance existing logging practices at the source code level. We develop ConfLogger, the first tool that unifies configuration-aware static taint analysis with LLM-based log generation to enhance software configuration diagnosability. Specifically, our method 1) identifies configuration-sensitive code segments by tracing configuration-related data flow in the whole project, and 2) generates diagnostic log statements by analyzing configuration code contexts. Evaluation results on eight popular software systems demonstrate the effectiveness of ConfLogger to enhance configuration diagnosability. Specifically, ConfLogger-enhanced logs successfully aid a log-based misconfiguration diagnosis tool to achieve 100% accuracy on error localization in 30 silent misconfiguration scenarios, with 80% directly resolvable through explicit configuration information exposed. In addition, ConfLogger achieves 74% coverage of existing logging points, outperforming baseline LLM-based loggers by 12% and 30%. It also gains 8.6% higher in precision, 79.3% higher in recall, and 26.2% higher in F1 compared to the state-of-the-art baseline in terms of variable logging while also augmenting diagnostic value. A controlled user study on 22 cases further validated its utility, speeding up diagnostic time by 1.25x and improving troubleshooting accuracy by 251.4%.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T16:31:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3744916.3764570' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.20977v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20977v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue
  Agents</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tianjian Liu, Fanqi Wan, Jiajian Guo, Xiaojun Quan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Proactive dialogue has emerged as a critical and challenging research problem in advancing large language models (LLMs). Existing works predominantly focus on domain-specific or task-oriented scenarios, which leads to fragmented evaluations and limits the comprehensive exploration of models' proactive conversation abilities. In this work, we propose ProactiveEval, a unified framework designed for evaluating proactive dialogue capabilities of LLMs. This framework decomposes proactive dialogue into target planning and dialogue guidance, establishing evaluation metrics across various domains. Moreover, it also enables the automatic generation of diverse and challenging evaluation data. Based on the proposed framework, we develop 328 evaluation environments spanning 6 distinct domains. Through experiments with 22 different types of LLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional performance on target planning and dialogue guidance tasks, respectively. Finally, we investigate how reasoning capabilities influence proactive behaviors and discuss their implications for future model development.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T16:26:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20973v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20973v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 DrivingGaussian++: Towards Realistic Reconstruction and Editable
  Simulation for Surrounding Dynamic Driving Scenes</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yajiao Xiong, Xiaoyu Zhou, Yongtao Wan, Deqing Sun, Ming-Hsuan Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present DrivingGaussian++, an efficient and effective framework for realistic reconstructing and controllable editing of surrounding dynamic autonomous driving scenes. DrivingGaussian++ models the static background using incremental 3D Gaussians and reconstructs moving objects with a composite dynamic Gaussian graph, ensuring accurate positions and occlusions. By integrating a LiDAR prior, it achieves detailed and consistent scene reconstruction, outperforming existing methods in dynamic scene reconstruction and photorealistic surround-view synthesis. DrivingGaussian++ supports training-free controllable editing for dynamic driving scenes, including texture modification, weather simulation, and object manipulation, leveraging multi-view images and depth priors. By integrating large language models (LLMs) and controllable editing, our method can automatically generate dynamic object motion trajectories and enhance their realism during the optimization process. DrivingGaussian++ demonstrates consistent and realistic editing results and generates dynamic multi-view driving scenarios, while significantly enhancing scene diversity. More results and code can be found at the project site: https://xiong-creator.github.io/DrivingGaussian_plus.github.io
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T16:22:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20965v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20965v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 SaRoHead: Detecting Satire in a Multi-Domain Romanian News Headline
  Dataset</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mihnea-Alexandru Vîrlan, Răzvan-Alexandru Smădu, Dumitru-Clementin Cercel, Florin Pop, Mihaela-Claudia Cercel
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The primary goal of a news headline is to summarize an event in as few words as possible. Depending on the media outlet, a headline can serve as a means to objectively deliver a summary or improve its visibility. For the latter, specific publications may employ stylistic approaches that incorporate the use of sarcasm, irony, and exaggeration, key elements of a satirical approach. As such, even the headline must reflect the tone of the satirical main content. Current approaches for the Romanian language tend to detect the non-conventional tone (i.e., satire and clickbait) of the news content by combining both the main article and the headline. Because we consider a headline to be merely a brief summary of the main article, we investigate in this paper the presence of satirical tone in headlines alone, testing multiple baselines ranging from standard machine learning algorithms to deep learning models. Our experiments show that Bidirectional Transformer models outperform both standard machine-learning approaches and Large Language Models (LLMs), particularly when the meta-learning Reptile approach is employed.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T16:22:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.07612v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.07612v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 Characterizing Trust Boundary Vulnerabilities in TEE Containers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weijie Liu, Hongbo Chen, Shuo Huai, Zhen Xu, Wenhao Wang, Zhi Li, Zheli Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Trusted Execution Environments (TEEs) have emerged as a cornerstone of confidential computing, garnering significant attention from both academia and industry. To enable the secure development, execution, and deployment, of applications on TEE platforms, TEE containers have been introduced as middleware solutions. These containers aim to shield applications from potentially malicious operating systems and orchestration interfaces while maintaining usability and reliability. In this paper, we analyze the isolation strategies employed by existing TEE containers to protect secure applications. To address the challenges in analyzing these interfaces, we designed an automated analyzer to precisely identify and evaluate their isolation boundaries. We observed that some TEE containers fail to achieve their intended goals due to critical design and implementation flaws, such as information leakage, rollback attacks, denial-of-service, and Iago attacks, which pose significant security risks. Drawing from our findings, we share key lessons to guide the development of more secure container solutions and discuss emerging trends in TEE containerization design.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T16:20:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20962v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20962v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 Digital Twin-Empowered Deep Reinforcement Learning for Intelligent VNF
  Migration in Edge-Core Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Faisal Ahmed, Suresh Subramaniam, Motoharu Matsuura, Hiroshi Hasegawa, Shih-Chun Lin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The growing demand for services and the rapid deployment of virtualized network functions (VNFs) pose significant challenges for achieving low-latency and energy-efficient orchestration in modern edge-core network infrastructures. To address these challenges, this study proposes a Digital Twin (DT)-empowered Deep Reinforcement Learning framework for intelligent VNF migration that jointly minimizes average end-to-end (E2E) delay and energy consumption. By formulating the VNF migration problem as a Markov Decision Process and utilizing the Advantage Actor-Critic model, the proposed framework enables adaptive and real-time migration decisions. A key innovation of the proposed framework is the integration of a DT module composed of a multi-task Variational Autoencoder and a multi-task Long Short-Term Memory network. This combination collectively simulates environment dynamics and generates high-quality synthetic experiences, significantly enhancing training efficiency and accelerating policy convergence. Simulation results demonstrate substantial performance gains, such as significant reductions in both average E2E delay and energy consumption, thereby establishing new benchmarks for intelligent VNF migration in edge-core networks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T16:18:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20957v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20957v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 STARE at the Structure: Steering ICL Exemplar Selection with Structural
  Alignment</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiaqian Li, Qisheng Hu, Jing Li, Wenya Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In-Context Learning (ICL) has become a powerful paradigm that enables LLMs to perform a wide range of tasks without task-specific fine-tuning. However, the effectiveness of ICL heavily depends on the quality of exemplar selection. In particular, for structured prediction tasks such as semantic parsing, existing ICL selection strategies often overlook structural alignment, leading to suboptimal performance and poor generalization. To address this issue, we propose a novel two-stage exemplar selection strategy that achieves a strong balance between efficiency, generalizability, and performance. First, we fine-tune a BERT-based retriever using structure-aware supervision, guiding it to select exemplars that are both semantically relevant and structurally aligned. Then, we enhance the retriever with a plug-in module, which amplifies syntactically meaningful information in the hidden representations. This plug-in is model-agnostic, requires minimal overhead, and can be seamlessly integrated into existing pipelines. Experiments on four benchmarks spanning three semantic parsing tasks demonstrate that our method consistently outperforms existing baselines with multiple recent LLMs as inference-time models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T16:04:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20944v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20944v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 AI Reasoning Models for Problem Solving in Physics</h2>
                <div class="authors">
                    <strong>Authors:</strong> Amir Bralin, N. Sanjay Rebello
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reasoning models are the new generation of Large Language Models (LLMs) capable of complex problem solving. Their reliability in solving introductory physics problems was tested by evaluating a sample of n = 5 solutions generated by one such model -- OpenAI's o3-mini -- per each problem from 20 chapters of a standard undergraduate textbook. In total, N = 408 problems were given to the model and N x n = 2,040 generated solutions examined. The model successfully solved 94% of the problems posed, excelling at the beginning topics in mechanics but struggling with the later ones such as waves and thermodynamics.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T16:02:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.ed-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20941v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20941v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 How Can Input Reformulation Improve Tool Usage Accuracy in a Complex
  Dynamic Environment? A Study on $τ$-bench</h2>
                <div class="authors">
                    <strong>Authors:</strong> Venkatesh Mishra, Amir Saeidi, Satyam Raj, Mutsumi Nakamura, Jayanth Srinivasa, Gaowen Liu, Ali Payani, Chitta Baral
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in reasoning and planning capabilities of large language models (LLMs) have enabled their potential as autonomous agents capable of tool use in dynamic environments. However, in multi-turn conversational environments like $\tau$-bench, these agents often struggle with consistent reasoning, adherence to domain-specific policies, and extracting correct information over a long horizon of tool-calls and conversation. To capture and mitigate these failures, we conduct a comprehensive manual analysis of the common errors occurring in the conversation trajectories. We then experiment with reformulations of inputs to the tool-calling agent for improvement in agent decision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA) framework, which automatically reformulates user queries augmented with relevant domain rules and tool suggestions for the tool-calling agent to focus on. The results show that IRMA significantly outperforms ReAct, Function Calling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in overall pass^5 scores. These findings highlight the superior reliability and consistency of IRMA compared to other methods in dynamic environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:57:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20931v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20931v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 Chain-of-Alpha: Unleashing the Power of Large Language Models for Alpha
  Mining in Quantitative Trading</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lang Cao, Zekun Xi, Long Liao, Ziwei Yang, Zheng Cao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Alpha factor mining is a fundamental task in quantitative trading, aimed at discovering interpretable signals that can predict asset returns beyond systematic market risk. While traditional methods rely on manual formula design or heuristic search with machine learning, recent advances have leveraged Large Language Models (LLMs) for automated factor discovery. However, existing LLM-based alpha mining approaches remain limited in terms of automation, generality, and efficiency. In this paper, we propose Chain-of-Alpha, a novel, simple, yet effective and efficient LLM-based framework for fully automated formulaic alpha mining. Our method features a dual-chain architecture, consisting of a Factor Generation Chain and a Factor Optimization Chain, which iteratively generate, evaluate, and refine candidate alpha factors using only market data, while leveraging backtest feedback and prior optimization knowledge. The two chains work synergistically to enable high-quality alpha discovery without human intervention and offer strong scalability. Extensive experiments on real-world A-share benchmarks demonstrate that Chain-of-Alpha outperforms existing baselines across multiple metrics, presenting a promising direction for LLM-driven quantitative research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:52:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.06312v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.06312v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics</h2>
                <div class="authors">
                    <strong>Authors:</strong> Carter Blum, Katja Filippova, Ann Yuan, Asma Ghandeharioun, Julian Zimmert, Fred Zhang, Jessica Hoffmann, Tal Linzen, Martin Wattenberg, Lucas Dixon, Mor Geva
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) struggle with cross-lingual knowledge transfer: they hallucinate when asked in one language about facts expressed in a different language during training. This work introduces a controlled setting to study the causes and dynamics of this phenomenon by training small Transformer models from scratch on synthetic multilingual datasets. We identify a learning phase wherein a model develops either separate or unified representations of the same facts across languages, and show that unification is essential for cross-lingual transfer. We also show that the degree of unification depends on mutual information between facts and training data language, and on how easy it is to extract that language. Based on these insights, we develop methods to modulate the level of cross-lingual transfer by manipulating data distribution and tokenization, and we introduce metrics and visualizations to formally characterize their effects on unification. Our work shows how controlled settings can shed light on pre-training dynamics and suggests new directions for improving cross-lingual transfer in LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:51:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.11017v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.11017v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 SageLM: A Multi-aspect and Explainable Large Language Model for Speech
  Judgement</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuan Ge, Junxiang Zhang, Xiaoqian Liu, Bei Li, Xiangnan Ma, Chenglong Wang, Kaiyang Ye, Yangfan Du, Linfeng Zhang, Yuxin Huang, Tong Xiao, Zhengtao Yu, JingBo Zhu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Speech-to-Speech (S2S) Large Language Models (LLMs) are foundational to natural human-computer interaction, enabling end-to-end spoken dialogue systems. However, evaluating these models remains a fundamental challenge. We propose \texttt{SageLM}, an end-to-end, multi-aspect, and explainable speech LLM for comprehensive S2S LLMs evaluation. First, unlike cascaded approaches that disregard acoustic features, SageLM jointly assesses both semantic and acoustic dimensions. Second, it leverages rationale-based supervision to enhance explainability and guide model learning, achieving superior alignment with evaluation outcomes compared to rule-based reinforcement learning methods. Third, we introduce \textit{SpeechFeedback}, a synthetic preference dataset, and employ a two-stage training paradigm to mitigate the scarcity of speech preference data. Trained on both semantic and acoustic dimensions, SageLM achieves an 82.79\% agreement rate with human evaluators, outperforming cascaded and SLM-based baselines by at least 7.42\% and 26.20\%, respectively.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:47:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20916v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20916v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 Research Challenges in Relational Database Management Systems for LLM
  Queries</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kerem Akillioglu, Anurag Chakraborty, Sairaj Voruganti, M. Tamer Özsu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have become essential for applications such as text summarization, sentiment analysis, and automated question-answering. Recently, LLMs have also been integrated into relational database management systems to enhance querying and support advanced data processing. Companies such as Amazon, Databricks, Google, and Snowflake offer LLM invocation directly within SQL, denoted as LLM queries, to boost data insights. However, open-source solutions currently have limited functionality and poor performance. In this work, we present an early exploration of two open-source systems and one enterprise platform, using five representative queries to expose functional, performance, and scalability limits in today's SQL-invoked LLM integrations. We identify three main issues: enforcing structured outputs, optimizing resource utilization, and improving query planning. We implemented initial solutions and observed improvements in accommodating LLM powered SQL queries. These early gains demonstrate that tighter integration of LLM+DBMS is the key to scalable and efficient processing of LLM queries.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:41:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20912v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20912v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 Quantum Verifiable Rewards for Post-Training Qiskit Code Assistant</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nicolas Dupuis, Adarsh Tiwari, Youssef Mroueh, David Kremer, Ismael Faro, Juan Cruz-Benito
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Qiskit is an open-source quantum computing framework that allows users to design, simulate, and run quantum circuits on real quantum hardware. We explore post-training techniques for LLMs to assist in writing Qiskit code. We introduce quantum verification as an effective method for ensuring code quality and executability on quantum hardware. To support this, we developed a synthetic data pipeline that generates quantum problem-unit test pairs and used it to create preference data for aligning LLMs with DPO. Additionally, we trained models using GRPO, leveraging quantum-verifiable rewards provided by the quantum hardware. Our best-performing model, combining DPO and GRPO, surpasses the strongest open-source baselines on the challenging Qiskit-HumanEval-hard benchmark.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:37:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>quant-ph</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20907v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20907v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 Real-Time Tracking Antenna System for Moving Targets</h2>
                <div class="authors">
                    <strong>Authors:</strong> Adham Saad, Aya Sherif Nassef, Mahmoud Mohamed Elshahed, Mohamed Ismail Ahmed
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper presents the design and implementation of a compact, cost-effective phased array antenna system. It is capable of real-time beam-steering for dynamic target-tracking applications. The system employs a 4$\times$4 rectangular microstrip patch array, utilizing advanced beamforming techniques and a Direction of Arrival (DoA) estimation algorithm. It achieves $\pm 42^{\circ}$ wide-angle scanning in both azimuth and elevation planes. The design emphasizes a balance between high angular coverage and consistent gain performance. This makes it suitable for wireless tracking, radar, and satellite communication terminals. Fabricated on Rogers 6010.2LM substrate, the system demonstrates reproducibility and scalability. All components are sourced locally to ensure practical deployment. The system is built using commercially available components, highlighting its affordability for research and prototyping purposes.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:36:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SY</span><span>cs.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20905v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20905v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 Improving Quantization with Post-Training Model Expansion</h2>
                <div class="authors">
                    <strong>Authors:</strong> Giuseppe Franco, Pablo Monteagudo-Lago, Ian Colbert, Nicholas Fraser, Michaela Blott
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The size of a model has been a strong predictor of its quality, as well as its cost. As such, the trade-off between model cost and quality has been well-studied. Post-training optimizations like quantization and pruning have typically focused on reducing the overall volume of pre-trained models to reduce inference costs while maintaining model quality. However, recent advancements have introduced optimization techniques that, interestingly, expand models post-training, increasing model size to improve quality when reducing volume. For instance, to enable 4-bit weight and activation quantization, incoherence processing often necessitates inserting online Hadamard rotations in the compute graph, and preserving highly sensitive weights often calls for additional higher precision computations. However, if application requirements cannot be met, the prevailing solution is to relax quantization constraints. In contrast, we demonstrate post-training model expansion is a viable strategy to improve model quality within a quantization co-design space, and provide theoretical justification. We show it is possible to progressively and selectively expand the size of a pre-trained large language model (LLM) to improve model quality without end-to-end retraining. In particular, when quantizing the weights and activations to 4 bits for Llama3 1B, we reduce the gap to full-precision perplexity by an average of 9% relative to both QuaRot and SpinQuant with only 5% more parameters, which is still a 3.8% reduction in volume relative to a BF16 reference model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:33:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.17513v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.17513v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 OneRec-V2 Technical Report</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guorui Zhou, Hengrui Hu, Hongtao Cheng, Huanjie Wang, Jiaxin Deng, Jinghao Zhang, Kuo Cai, Lejian Ren, Lu Ren, Liao Yu, Pengfei Zheng, Qiang Luo, Qianqian Wang, Qigen Hu, Rui Huang, Ruiming Tang, Shiyao Wang, Shujie Yang, Tao Wu, Wuchao Li, Xinchen Luo, Xingmei Wang, Yi Su, Yunfan Wu, Zexuan Cheng, Zhanyu Liu, Zixing Zhang, Bin Zhang, Boxuan Wang, Chaoyi Ma, Chengru Song, Chenhui Wang, Chenglong Chu, Di Wang, Dongxue Meng, Dunju Zang, Fan Yang, Fangyu Zhang, Feng Jiang, Fuxing Zhang, Gang Wang, Guowang Zhang, Han Li, Honghui Bao, Hongyang Cao, Jiaming Huang, Jiapeng Chen, Jiaqiang Liu, Jinghui Jia, Kun Gai, Lantao Hu, Liang Zeng, Qiang Wang, Qidong Zhou, Rongzhou Zhang, Shengzhe Wang, Shihui He, Shuang Yang, Siyang Mao, Sui Huang, Tiantian He, Tingting Gao, Wei Yuan, Xiao Liang, Xiaoxiao Xu, Xugang Liu, Yan Wang, Yang Zhou, Yi Wang, Yiwu Liu, Yue Song, Yufei Zhang, Yunfeng Zhao, Zhixin Ling, Ziming Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent breakthroughs in generative AI have transformed recommender systems through end-to-end generation. OneRec reformulates recommendation as an autoregressive generation task, achieving high Model FLOPs Utilization. While OneRec-V1 has shown significant empirical success in real-world deployment, two critical challenges hinder its scalability and performance: (1) inefficient computational allocation where 97.66% of resources are consumed by sequence encoding rather than generation, and (2) limitations in reinforcement learning relying solely on reward models.   To address these challenges, we propose OneRec-V2, featuring: (1) Lazy Decoder-Only Architecture: Eliminates encoder bottlenecks, reducing total computation by 94% and training resources by 90%, enabling successful scaling to 8B parameters. (2) Preference Alignment with Real-World User Interactions: Incorporates Duration-Aware Reward Shaping and Adaptive Ratio Clipping to better align with user preferences using real-world feedback.   Extensive A/B tests on Kuaishou demonstrate OneRec-V2's effectiveness, improving App Stay Time by 0.467%/0.741% while balancing multi-objective recommendations. This work advances generative recommendation scalability and alignment with real-world feedback, representing a step forward in the development of end-to-end recommender systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:29:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20900v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20900v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 Language-Enhanced Mobile Manipulation for Efficient Object Search in
  Indoor Environments</h2>
                <div class="authors">
                    <strong>Authors:</strong> Liding Zhang, Zeqi Li, Kuanqi Cai, Qian Huang, Zhenshan Bing, Alois Knoll
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Enabling robots to efficiently search for and identify objects in complex, unstructured environments is critical for diverse applications ranging from household assistance to industrial automation. However, traditional scene representations typically capture only static semantics and lack interpretable contextual reasoning, limiting their ability to guide object search in completely unfamiliar settings. To address this challenge, we propose a language-enhanced hierarchical navigation framework that tightly integrates semantic perception and spatial reasoning. Our method, Goal-Oriented Dynamically Heuristic-Guided Hierarchical Search (GODHS), leverages large language models (LLMs) to infer scene semantics and guide the search process through a multi-level decision hierarchy. Reliability in reasoning is achieved through the use of structured prompts and logical constraints applied at each stage of the hierarchy. For the specific challenges of mobile manipulation, we introduce a heuristic-based motion planner that combines polar angle sorting with distance prioritization to efficiently generate exploration paths. Comprehensive evaluations in Isaac Sim demonstrate the feasibility of our framework, showing that GODHS can locate target objects with higher search efficiency compared to conventional, non-semantic search strategies. Website and Video are available at: https://drapandiger.github.io/GODHS
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:27:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20899v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20899v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 HAS-GPU: Efficient Hybrid Auto-scaling with Fine-grained GPU Allocation
  for SLO-aware Serverless Inferences</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jianfeng Gu, Puxuan Wang, Isaac David Nunez Araya, Kai Huang, Michael Gerndt
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Serverless Computing (FaaS) has become a popular paradigm for deep learning inference due to the ease of deployment and pay-per-use benefits. However, current serverless inference platforms encounter the coarse-grained and static GPU resource allocation problems during scaling, which leads to high costs and Service Level Objective (SLO) violations in fluctuating workloads. Meanwhile, current platforms only support horizontal scaling for GPU inferences, thus the cold start problem further exacerbates the problems. In this paper, we propose HAS-GPU, an efficient Hybrid Auto-scaling Serverless architecture with fine-grained GPU allocation for deep learning inferences. HAS-GPU proposes an agile scheduler capable of allocating GPU Streaming Multiprocessor (SM) partitions and time quotas with arbitrary granularity and enables significant vertical quota scalability at runtime. To resolve performance uncertainty introduced by massive fine-grained resource configuration spaces, we propose the Resource-aware Performance Predictor (RaPP). Furthermore, we present an adaptive hybrid auto-scaling algorithm with both horizontal and vertical scaling to ensure inference SLOs and minimize GPU costs. The experiments demonstrated that compared to the mainstream serverless inference platform, HAS-GPU reduces function costs by an average of 10.8x with better SLO guarantees. Compared to state-of-the-art spatio-temporal GPU sharing serverless framework, HAS-GPU reduces function SLO violation by 4.8x and cost by 1.72x on average.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:23:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1007/978-3-031-99854-6_11' target='_blank'>doi</a><a href='http://arxiv.org/abs/2505.01968v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.01968v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 The Uneven Impact of Post-Training Quantization in Machine Translation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Benjamin Marie, Atsushi Fujita
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Quantization is essential for deploying large language models (LLMs) on resource-constrained hardware, but its implications for multilingual tasks remain underexplored. We conduct the first large-scale evaluation of post-training quantization (PTQ) on machine translation across 55 languages using five LLMs ranging from 1.7B to 70B parameters. Our analysis reveals that while 4-bit quantization often preserves translation quality for high-resource languages and large models, significant degradation occurs for low-resource and typologically diverse languages, particularly in 2-bit settings. We compare four quantization techniques (AWQ, BitsAndBytes, GGUF, and AutoRound), showing that algorithm choice and model size jointly determine robustness. GGUF variants provide the most consistent performance, even at 2-bit precision. Additionally, we quantify the interactions between quantization, decoding hyperparameters, and calibration languages, finding that language-matched calibration offers benefits primarily in low-bit scenarios. Our findings offer actionable insights for deploying multilingual LLMs for machine translation under quantization constraints, especially in low-resource settings.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:22:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20893v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20893v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 PromptSleuth: Detecting Prompt Injection via Semantic Intent Invariance</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mengxiao Wang, Yuxuan Zhang, Guofei Gu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are increasingly integrated into real-world applications, from virtual assistants to autonomous agents. However, their flexibility also introduces new attack vectors-particularly Prompt Injection (PI), where adversaries manipulate model behavior through crafted inputs. As attackers continuously evolve with paraphrased, obfuscated, and even multi-task injection strategies, existing benchmarks are no longer sufficient to capture the full spectrum of emerging threats.   To address this gap, we construct a new benchmark that systematically extends prior efforts. Our benchmark subsumes the two widely-used existing ones while introducing new manipulation techniques and multi-task scenarios, thereby providing a more comprehensive evaluation setting. We find that existing defenses, though effective on their original benchmarks, show clear weaknesses under our benchmark, underscoring the need for more robust solutions. Our key insight is that while attack forms may vary, the adversary's intent-injecting an unauthorized task-remains invariant. Building on this observation, we propose PromptSleuth, a semantic-oriented defense framework that detects prompt injection by reasoning over task-level intent rather than surface features. Evaluated across state-of-the-art benchmarks, PromptSleuth consistently outperforms existing defense while maintaining comparable runtime and cost efficiency. These results demonstrate that intent-based semantic reasoning offers a robust, efficient, and generalizable strategy for defending LLMs against evolving prompt injection threats.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:19:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20890v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20890v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 A Highly Clean Recipe Dataset with Ingredient States Annotation for
  State Probing Task</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mashiro Toyooka, Kiyoharu Aizawa, Yoko Yamakata
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are trained on a vast amount of procedural texts, but they do not directly observe real-world phenomena. In the context of cooking recipes, this poses a challenge, as intermediate states of ingredients are often omitted, making it difficult for models to track ingredient states and understand recipes accurately. In this paper, we apply state probing, a method for evaluating a language model's understanding of the world, to the domain of cooking. We propose a new task and dataset for evaluating how well LLMs can recognize intermediate ingredient states during cooking procedures. We first construct a new Japanese recipe dataset with clear and accurate annotations of ingredient state changes, collected from well-structured and controlled recipe texts. Using this dataset, we design three novel tasks to evaluate whether LLMs can track ingredient state transitions and identify ingredients present at intermediate steps. Our experiments with widely used LLMs, such as Llama3.1-70B and Qwen2.5-72B, show that learning ingredient state knowledge improves their understanding of cooking processes, achieving performance comparable to commercial LLMs. The dataset are publicly available at: https://huggingface.co/datasets/mashi6n/nhkrecipe-100-anno-1
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:15:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MM</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.17232v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.17232v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 Deep Learning Framework for Early Detection of Pancreatic Cancer Using
  Multi-Modal Medical Imaging Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dennis Slobodzian, Karissa Tilbury, Amir Kordijazi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Pacreatic ductal adenocarcinoma (PDAC) remains one of the most lethal forms of cancer, with a five-year survival rate below 10% primarily due to late detection. This research develops and validates a deep learning framework for early PDAC detection through analysis of dual-modality imaging: autofluorescence and second harmonic generation (SHG). We analyzed 40 unique patient samples to create a specialized neural network capable of distinguishing between normal, fibrotic, and cancerous tissue. Our methodology evaluated six distinct deep learning architectures, comparing traditional Convolutional Neural Networks (CNNs) with modern Vision Transformers (ViTs). Through systematic experimentation, we identified and overcome significant challenges in medical image analysis, including limited dataset size and class imbalance. The final optimized framework, based on a modified ResNet architecture with frozen pre-trained layers and class-weighted training, achieved over 90% accuracy in cancer detection. This represents a significant improvement over current manual analysis methods an demonstrates potential for clinical deployment. This work establishes a robust pipeline for automated PDAC detection that can augment pathologists' capabilities while providing a foundation for future expansion to other cancer types. The developed methodology also offers valuable insights for applying deep learning to limited-size medical imaging datasets, a common challenge in clinical applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T15:07:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20877v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20877v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 MSRS: Evaluating Multi-Source Retrieval-Augmented Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rohan Phanse, Yijie Zhou, Kejian Shi, Wencai Zhang, Yixin Liu, Yilun Zhao, Arman Cohan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Retrieval-augmented systems are typically evaluated in settings where information required to answer the query can be found within a single source or the answer is short-form or factoid-based. However, many real-world applications demand the ability to integrate and summarize information scattered across multiple sources, where no single source is sufficient to respond to the user's question. In such settings, the retrieval component of a RAG pipeline must recognize a variety of relevance signals, and the generation component must connect and synthesize information across multiple sources. We present a scalable framework for constructing evaluation benchmarks that challenge RAG systems to integrate information across distinct sources and generate long-form responses. Using our framework, we build two new benchmarks on Multi-Source Retrieval and Synthesis: MSRS-Story and MSRS-Meet, representing narrative synthesis and summarization tasks, respectively, that require retrieval from large collections. Our extensive experiments with various RAG pipelines -- including sparse and dense retrievers combined with frontier LLMs -- reveal that generation quality is highly dependent on retrieval effectiveness, which varies greatly by task. While multi-source synthesis proves challenging even in an oracle retrieval setting, we find that reasoning models significantly outperform standard LLMs at this distinct step.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:59:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20867v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20867v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 Breaking Barriers in Health Monitoring: Multi-Scenario Vital Sign
  Detection Using Mm-Wave MIMO FMCW Radar</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ehsan Sadeghi, Paul Havinga
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper explores the deployment of mm-wave Frequency Modulated Continuous Wave (FMCW) radar for vital sign detection across multiple scenarios. We focus on overcoming the limitations of traditional sensing methods by enhancing signal processing techniques to capture subtle physiological changes effectively. Our study introduces novel adaptations of the Prony and MUSIC algorithms tailored for real-time heart and respiration rate monitoring, significantly advancing the accuracy and reliability of non-contact vital sign monitoring using radar technologies. Notably, these algorithms demonstrate a robust ability to suppress noise and harmonic interference. For instance, the mean absolute errors (MAE) for MUSIC and Prony in heart rate detection are 1.8 and 0.81, respectively, while for respiration rate, the MAEs are 1.01 and 0.8, respectively. These results underscore the potential of FMCW radar as a reliable, non-invasive solution for continuous vital sign monitoring in healthcare settings, particularly in clinical and emergency scenarios where traditional contact-based monitoring is impractical.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:58:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20864v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20864v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 Publish to Perish: Prompt Injection Attacks on LLM-Assisted Peer Review</h2>
                <div class="authors">
                    <strong>Authors:</strong> Matteo Gioele Collu, Umberto Salviati, Roberto Confalonieri, Mauro Conti, Giovanni Apruzzese
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are increasingly being integrated into the scientific peer-review process, raising new questions about their reliability and resilience to manipulation. In this work, we investigate the potential for hidden prompt injection attacks, where authors embed adversarial text within a paper's PDF to influence the LLM-generated review. We begin by formalising three distinct threat models that envision attackers with different motivations -- not all of which implying malicious intent. For each threat model, we design adversarial prompts that remain invisible to human readers yet can steer an LLM's output toward the author's desired outcome. Using a user study with domain scholars, we derive four representative reviewing prompts used to elicit peer reviews from LLMs. We then evaluate the robustness of our adversarial prompts across (i) different reviewing prompts, (ii) different commercial LLM-based systems, and (iii) different peer-reviewed papers. Our results show that adversarial prompts can reliably mislead the LLM, sometimes in ways that adversely affect a "honest-but-lazy" reviewer. Finally, we propose and empirically assess methods to reduce detectability of adversarial prompts under automated content checks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:57:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20863v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20863v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 JADES: A Universal Framework for Jailbreak Assessment via
  Decompositional Scoring</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junjie Chu, Mingjie Li, Ziqing Yang, Ye Leng, Chenhao Lin, Chao Shen, Michael Backes, Yun Shen, Yang Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Accurately determining whether a jailbreak attempt has succeeded is a fundamental yet unresolved challenge. Existing evaluation methods rely on misaligned proxy indicators or naive holistic judgments. They frequently misinterpret model responses, leading to inconsistent and subjective assessments that misalign with human perception. To address this gap, we introduce JADES (Jailbreak Assessment via Decompositional Scoring), a universal jailbreak evaluation framework. Its key mechanism is to automatically decompose an input harmful question into a set of weighted sub-questions, score each sub-answer, and weight-aggregate the sub-scores into a final decision. JADES also incorporates an optional fact-checking module to strengthen the detection of hallucinations in jailbreak responses. We validate JADES on JailbreakQR, a newly introduced benchmark proposed in this work, consisting of 400 pairs of jailbreak prompts and responses, each meticulously annotated by humans. In a binary setting (success/failure), JADES achieves 98.5% agreement with human evaluators, outperforming strong baselines by over 9%. Re-evaluating five popular attacks on four LLMs reveals substantial overestimation (e.g., LAA's attack success rate on GPT-3.5-Turbo drops from 93% to 69%). Our results show that JADES could deliver accurate, consistent, and interpretable evaluations, providing a reliable basis for measuring future jailbreak attacks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:40:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20848v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20848v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 Multilingual Contextualization of Large Language Models for
  Document-Level Machine Translation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Miguel Moura Ramos, Patrick Fernandes, Sweta Agrawal, André F. T. Martins
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have demonstrated strong performance in sentence-level machine translation, but scaling to document-level translation remains challenging, particularly in modeling long-range dependencies and discourse phenomena across sentences and paragraphs. In this work, we propose a method to improve LLM-based long-document translation through targeted fine-tuning on high-quality document-level data, which we curate and introduce as DocBlocks. Our approach supports multiple translation paradigms, including direct document-to-document and chunk-level translation, by integrating instructions both with and without surrounding context. This enables models to better capture cross-sentence dependencies while maintaining strong sentence-level translation performance. Experimental results show that incorporating multiple translation paradigms improves document-level translation quality and inference speed compared to prompting and agent-based methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:32:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.12140v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.12140v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 GDLLM: A Global Distance-aware Modeling Approach Based on Large Language
  Models for Event Temporal Relation Extraction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jie Zhao, Wanting Ning, Yuxiao Fei, Yubo Feng, Lishuang Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In Natural Language Processing(NLP), Event Temporal Relation Extraction (ETRE) is to recognize the temporal relations of two events. Prior studies have noted the importance of language models for ETRE. However, the restricted pre-trained knowledge of Small Language Models(SLMs) limits their capability to handle minority class relations in imbalanced classification datasets. For Large Language Models(LLMs), researchers adopt manually designed prompts or instructions, which may introduce extra noise, leading to interference with the model's judgment of the long-distance dependencies between events. To address these issues, we propose GDLLM, a Global Distance-aware modeling approach based on LLMs. We first present a distance-aware graph structure utilizing Graph Attention Network(GAT) to assist the LLMs in capturing long-distance dependency features. Additionally, we design a temporal feature learning paradigm based on soft inference to augment the identification of relations with a short-distance proximity band, which supplements the probabilistic information generated by LLMs into the multi-head attention mechanism. Since the global feature can be captured effectively, our framework substantially enhances the performance of minority relation classes and improves the overall learning ability. Experiments on two publicly available datasets, TB-Dense and MATRES, demonstrate that our approach achieves state-of-the-art (SOTA) performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:23:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20828v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20828v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 ASiM: Modeling and Analyzing Inference Accuracy of SRAM-Based Analog CiM
  Circuits</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wenlun Zhang, Shimpei Ando, Yung-Chin Chen, Kentaro Yoshioka
                </div>
                <div class="summary">
                    <strong>Summary:</strong> SRAM-based Analog Compute-in-Memory (ACiM) demonstrates promising energy efficiency for deep neural network (DNN) processing. Nevertheless, efforts to optimize efficiency frequently compromise accuracy, and this trade-off remains insufficiently studied due to the difficulty of performing full-system validation. Specifically, existing simulation tools rarely target SRAM-based ACiM and exhibit inconsistent accuracy predictions, highlighting the need for a standardized, SRAM CiM circuit-aware evaluation methodology. This paper presents ASiM, a simulation framework for evaluating inference accuracy in SRAM-based ACiM systems. ASiM captures critical effects in SRAM based analog compute in memory systems, such as ADC quantization, bit parallel encoding, and analog noise, which must be modeled with high fidelity due to their distinct behavior in charge domain architectures compared to other memory technologies. ASiM supports a wide range of modern DNN workloads, including CNN and Transformer-based models such as ViT, and scales to large-scale tasks like ImageNet classification. Our results indicate that bit-parallel encoding can improve energy efficiency with only modest accuracy degradation; however, even 1 LSB of analog noise can significantly impair inference performance, particularly in complex tasks such as ImageNet. To address this, we explore hybrid analog-digital execution and majority voting schemes, both of which enhance robustness without negating energy savings. ASiM bridges the gap between hardware design and inference performance, offering actionable insights for energy-efficient, high-accuracy ACiM deployment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:17:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.11022v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.11022v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 cMALC-D: Contextual Multi-Agent LLM-Guided Curriculum Learning with
  Diversity-Based Context Blending</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anirudh Satheesh, Keenan Powell, Hua Wei
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Many multi-agent reinforcement learning (MARL) algorithms are trained in fixed simulation environments, making them brittle when deployed in real-world scenarios with more complex and uncertain conditions. Contextual MARL (cMARL) addresses this by parameterizing environments with context variables and training a context-agnostic policy that performs well across all environment configurations. Existing cMARL methods attempt to use curriculum learning to help train and evaluate context-agnostic policies, but they often rely on unreliable proxy signals, such as value estimates or generalized advantage estimates that are noisy and unstable in multi-agent settings due to inter-agent dynamics and partial observability. To address these issues, we propose Contextual Multi-Agent LLM-Guided Curriculum Learning with Diversity-Based Context Blending (cMALC-D), a framework that uses Large Language Models (LLMs) to generate semantically meaningful curricula and provide a more robust evaluation signal. To prevent mode collapse and encourage exploration, we introduce a novel diversity-based context blending mechanism that creates new training scenarios by combining features from prior contexts. Experiments in traffic signal control domains demonstrate that cMALC-D significantly improves both generalization and sample efficiency compared to existing curriculum learning baselines. We provide code at https://github.com/DaRL-LibSignal/cMALC-D.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:16:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20818v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20818v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 Lightweight Gradient Descent Optimization for Mitigating Hardware
  Imperfections in RIS Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pedro H. C. de Souza, Luiz A. M. Pereira, Faustino R. Gómez, Elsa M. Materón, Jorge Ricardo Mejía-Salazar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Ongoing discussions about the future of wireless communications are reaching a turning point as standardization activities for the sixth generation of mobile networks (6G) become more mature. New technologies must now face renewed scrutiny by the industry and academia in order to be ready for deployment in the near future. Recently, reconfigurable intelligent surfaces (RISs) gained attention as a promising solution for improving the propagation conditions of signal transmission in general. The RIS is a planar array of tunable resonant elements designed to dynamically and precisely manipulate the reflection of incident electromagnetic waves. However, the physical structure of the RIS and its components may be subject to practical limitations and imperfections. It is imperative that the hardware imperfections (HWIs) associated with the RIS be analyzed, so that it remains a feasible technology from a practical standpoint. Moreover, solutions for mitigating the HWIs must be considered, as is discussed in this work. More specifically, we introduce a gradient descent optimization for mitigating HWIs in RIS-aided wideband communication systems. Numerical results show that the proposed optimization is able to compensate for HWIs such as the phase-shift noise (PSN) and RIS surface deformations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:14:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.15544v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.15544v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 A Graph-Based Test-Harness for LLM Evaluation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jessica Lundin, Guillaume Chabot-Couture
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present a first known prototype of a dynamic, systematic benchmark of medical guidelines for 400+ questions, with 3.3+ trillion possible combinations, covering 100\% of guideline relationships. We transformed the WHO IMCI handbook into a directed graph with 200+ nodes (conditions, symptoms, treatments, follow-ups, severities) and 300+ edges, then used graph traversal to generate questions that incorporated age-specific scenarios and contextual distractors to ensure clinical relevance. Our graph-based approach enables systematic evaluation across clinical tasks (45-67\% accuracy), and we find models excel at symptom recognition but struggle with triaging severity, treatment protocols and follow-up care, demonstrating how customized benchmarks can identify specific capability gaps that general-domain evaluations miss. Beyond evaluation, this dynamic MCQA methodology enhances LLM post-training (supervised finetuning, GRPO, DPO), where correct answers provide high-reward samples without expensive human annotation. The graph-based approach successfully addresses the coverage limitations of manually curated benchmarks. This methodology is a step toward scalable, contamination-resistant solution for creating comprehensive benchmarks that can be dynamically generated, including when the guidelines are updated. Code and datasets are available at https://github.com/jessicalundin/graph_testing_harness
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:10:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20810v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20810v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 Steering Towards Fairness: Mitigating Political Bias in LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Afrozah Nadeem, Mark Dras, Usman Naseem
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in large language models (LLMs) have enabled their widespread use across diverse real-world applications. However, concerns remain about their tendency to encode and reproduce ideological biases along political and economic dimensions. In this paper, we employ a framework for probing and mitigating such biases in decoder-based LLMs through analysis of internal model representations. Grounded in the Political Compass Test (PCT), this method uses contrastive pairs to extract and compare hidden layer activations from models like Mistral and DeepSeek. We introduce a comprehensive activation extraction pipeline capable of layer-wise analysis across multiple ideological axes, revealing meaningful disparities linked to political framing. Our results show that decoder LLMs systematically encode representational bias across layers, which can be leveraged for effective steering vector-based mitigation. This work provides new insights into how political bias is encoded in LLMs and offers a principled approach to debiasing beyond surface-level output interventions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:07:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.08846v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.08846v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 Exploring Machine Learning and Language Models for Multimodal Depression
  Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Javier Si Zhao Hong, Timothy Zoe Delaya, Sherwyn Chan Yin Kit, Pai Chet Ng, Xiaoxiao Miao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper presents our approach to the first Multimodal Personality-Aware Depression Detection Challenge, focusing on multimodal depression detection using machine learning and deep learning models. We explore and compare the performance of XGBoost, transformer-based architectures, and large language models (LLMs) on audio, video, and text features. Our results highlight the strengths and limitations of each type of model in capturing depression-related signals across modalities, offering insights into effective multimodal representation strategies for mental health prediction.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:07:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.SD</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20805v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20805v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 EPICS for Small-Scale Laboratories with Python Soft IOCs</h2>
                <div class="authors">
                    <strong>Authors:</strong> James D. Maxwell
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While the Experimental Physics and Industrial Control System (EPICS) is widely used at large laboratories for slow controls and instrumentation, the deployment of a full EPICS installation can be difficult, with a steep learning curve to new users. Taking advantage of the pythonSoftIOC module, we developed an EPICS slow controls implementation for Jefferson Lab's Hall B cryotarget written entirely in Python and based on software IOCs that communicate with instruments over Ethernet. This system ran successfully, interfacing with Jefferson Lab's full EPICS network, and we offer it as an example of the capabilities of pythonSoftIOC to build lightweight, yet robust and flexible instrumentation platforms that would be easily adapted for use at a small-scale laboratory. University groups can use these examples to build complete slow controls systems, from device communication to data archiving and display, using open-source, mature EPICS tools and student-friendly Python as an alternative to expensive and proprietary systems such as LabView.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T14:01:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.ins-det</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20800v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20800v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 Leveraging LLMs for Formal Software Requirements -- Challenges and
  Prospects</h2>
                <div class="authors">
                    <strong>Authors:</strong> Arshad Beg, Diarmuid O'Donoghue, Rosemary Monahan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Software correctness is ensured mathematically through formal verification, which involves the resources of generating formal requirement specifications and having an implementation that must be verified. Tools such as model-checkers and theorem provers ensure software correctness by verifying the implementation against the specification. Formal methods deployment is regularly enforced in the development of safety-critical systems e.g. aerospace, medical devices and autonomous systems. Generating these specifications from informal and ambiguous natural language requirements remains the key challenge. Our project, VERIFAI^{1}, aims to investigate automated and semi-automated approaches to bridge this gap, using techniques from Natural Language Processing (NLP), ontology-based domain modelling, artefact reuse, and large language models (LLMs). This position paper presents a preliminary synthesis of relevant literature to identify recurring challenges and prospective research directions in the generation of verifiable specifications from informal requirements.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T13:50:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>D.2.1; D.2.4; D.2.10; F.4.1; F.4.3</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.14330v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.14330v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 Turning the Spell Around: Lightweight Alignment Amplification via
  Rank-One Safety Injection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Harethah Abu Shairah, Hasan Abed Al Kader Hammoud, George Turkiyyah, Bernard Ghanem
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Safety alignment in Large Language Models (LLMs) often involves mediating internal representations to refuse harmful requests. Recent research has demonstrated that these safety mechanisms can be bypassed by ablating or removing specific representational directions within the model. In this paper, we propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box method that amplifies a model's safety alignment by permanently steering its activations toward the refusal-mediating subspace. ROSI operates as a simple, fine-tuning-free rank-one weight modification applied to all residual stream write matrices. The required safety direction can be computed from a small set of harmful and harmless instruction pairs. We show that ROSI consistently increases safety refusal rates - as evaluated by Llama Guard 3 - while preserving the utility of the model on standard benchmarks such as MMLU, HellaSwag, and Arc. Furthermore, we show that ROSI can also re-align 'uncensored' models by amplifying their own latent safety directions, demonstrating its utility as an effective last-mile safety procedure. Our results suggest that targeted, interpretable weight steering is a cheap and potent mechanism to improve LLM safety, complementing more resource-intensive fine-tuning paradigms.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T13:22:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20766v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20766v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 Feel the Difference? A Comparative Analysis of Emotional Arcs in Real
  and LLM-Generated CBT Sessions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaoyi Wang, Jiwei Zhang, Guangtao Zhang, Honglei Guo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Synthetic therapy dialogues generated by large language models (LLMs) are increasingly used in mental health NLP to simulate counseling scenarios, train models, and supplement limited real-world data. However, it remains unclear whether these synthetic conversations capture the nuanced emotional dynamics of real therapy. In this work, we conduct the first comparative analysis of emotional arcs between real and LLM-generated Cognitive Behavioral Therapy dialogues. We adapt the Utterance Emotion Dynamics framework to analyze fine-grained affective trajectories across valence, arousal, and dominance dimensions. Our analysis spans both full dialogues and individual speaker roles (counselor and client), using real sessions transcribed from public videos and synthetic dialogues from the CACTUS dataset. We find that while synthetic dialogues are fluent and structurally coherent, they diverge from real conversations in key emotional properties: real sessions exhibit greater emotional variability,more emotion-laden language, and more authentic patterns of reactivity and regulation. Moreover, emotional arc similarity between real and synthetic speakers is low, especially for clients. These findings underscore the limitations of current LLM-generated therapy data and highlight the importance of emotional fidelity in mental health applications. We introduce RealCBT, a curated dataset of real CBT sessions, to support future research in this space.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T13:19:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20764v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20764v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 Occlusion Robustness of CLIP for Military Vehicle Classification</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jan Erik van Woerden, Gertjan Burghouts, Lotte Nijskens, Alma M. Liezenga, Sabina van Rooij, Frank Ruis, Hugo J. Kuijf
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vision-language models (VLMs) like CLIP enable zero-shot classification by aligning images and text in a shared embedding space, offering advantages for defense applications with scarce labeled data. However, CLIP's robustness in challenging military environments, with partial occlusion and degraded signal-to-noise ratio (SNR), remains underexplored. We investigate CLIP variants' robustness to occlusion using a custom dataset of 18 military vehicle classes and evaluate using Normalized Area Under the Curve (NAUC) across occlusion percentages. Four key insights emerge: (1) Transformer-based CLIP models consistently outperform CNNs, (2) fine-grained, dispersed occlusions degrade performance more than larger contiguous occlusions, (3) despite improved accuracy, performance of linear-probed models sharply drops at around 35% occlusion, (4) by finetuning the model's backbone, this performance drop occurs at more than 60% occlusion. These results underscore the importance of occlusion-specific augmentations during training and the need for further exploration into patch-level sensitivity and architectural resilience for real-world deployment of CLIP.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T13:16:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20760v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20760v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and
  Efficient Open-Ended Text Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuanhao Ding, Esteban Garces Arias, Meimingwei Li, Julian Rodemann, Matthias Aßenmacher, Danlu Chen, Gaojuan Fan, Christian Heumann, Chongsheng Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Open-ended text generation faces a critical challenge: balancing coherence with diversity in LLM outputs. While contrastive search-based decoding strategies have emerged to address this trade-off, their practical utility is often limited by hyperparameter dependence and high computational costs. We introduce GUARD, a self-adaptive decoding method that effectively balances these competing objectives through a novel "Glocal" uncertainty-driven framework. GUARD combines global entropy estimates with local entropy deviations to integrate both long-term and short-term uncertainty signals. We demonstrate that our proposed global entropy formulation effectively mitigates abrupt variations in uncertainty, such as sudden overconfidence or high entropy spikes, and provides theoretical guarantees of unbiasedness and consistency. To reduce computational overhead, we incorporate a simple yet effective token-count-based penalty into GUARD. Experimental results demonstrate that GUARD achieves a good balance between text diversity and coherence, while exhibiting substantial improvements in generation speed. In a more nuanced comparison study across different dimensions of text quality, both human and LLM evaluators validated its remarkable performance. Our code is available at https://github.com/YecanLee/GUARD.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T13:14:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20757v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20757v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 Specializing General-purpose LLM Embeddings for Implicit Hate Speech
  Detection across Datasets</h2>
                <div class="authors">
                    <strong>Authors:</strong> Vassiliy Cheremetiev, Quang Long Ho Ngo, Chau Ying Kot, Alina Elena Baia, Andrea Cavallaro
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Implicit hate speech (IHS) is indirect language that conveys prejudice or hatred through subtle cues, sarcasm or coded terminology. IHS is challenging to detect as it does not include explicit derogatory or inflammatory words. To address this challenge, task-specific pipelines can be complemented with external knowledge or additional information such as context, emotions and sentiment data. In this paper, we show that, by solely fine-tuning recent general-purpose embedding models based on large language models (LLMs), such as Stella, Jasper, NV-Embed and E5, we achieve state-of-the-art performance. Experiments on multiple IHS datasets show up to 1.10 percentage points improvements for in-dataset, and up to 20.35 percentage points improvements in cross-dataset evaluation, in terms of F1-macro score.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T13:08:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3746275.3762209' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.20750v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20750v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 Feature Importance-Aware Deep Joint Source-Channel Coding for
  Computationally Efficient and Adjustable Image Transmission</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hansung Choi, Daewon Seo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in deep learning-based joint source-channel coding (deepJSCC) have substantially improved communication performance, but their high computational cost hinders practical deployment. Moreover, certain applications require the ability to dynamically adapt computational complexity. To address these issues, we propose a Feature Importance-Aware deepJSCC (FAJSCC) model for image transmission that is both computationally efficient and adjustable. FAJSCC employs axis-dimension specialized computation, which performs efficient operations individually for each spatial and channel axis, significantly reducing computational cost while representing features effectively. It further incorporates selective deformable self-attention, which applies self-attention only to selected and adaptively adjusted regions, leveraging the importance and relations of input features to efficiently capture complex feature correlations. Another key feature of FAJSCC is that the number of selected important areas can be controlled separately by the encoder and the decoder, depending on the available computational budget. It makes FAJSCC the first deepJSCC architecture to allow independent adjustment of encoder and decoder complexity within a single trained model. Experimental results show that FAJSCC achieves superior image transmission performance under various channel conditions while requiring less computational complexity than recent state-of-the-art models. Furthermore, experiments independently varying the encoder and decoder's computational resources reveal, for the first time in the deepJSCC literature, that understanding the meaning of noisy features in the decoder demands the greatest computational cost. The code is publicly available at github.com/hansung-choi/FAJSCCv2.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T13:05:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.04758v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.04758v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 From Law to Gherkin: A Human-Centred Quasi-Experiment on the Quality of
  LLM-Generated Behavioural Specifications from Food-Safety Regulations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shabnam Hassani, Mehrdad Sabetzadeh, Daniel Amyot
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Context: Laws and regulations increasingly affect software design and quality assurance, but legal texts are written in technology-neutral language. This creates challenges for engineers who must develop compliance artifacts such as requirements and acceptance criteria. Manual creation is labor-intensive, error-prone, and requires domain expertise. Advances in Generative AI (GenAI), especially Large Language Models (LLMs), offer a way to automate deriving such artifacts.   Objective: We present the first systematic human-subject study of LLMs' ability to derive behavioral specifications from legal texts using a quasi-experimental design. These specifications translate legal requirements into a developer-friendly form.   Methods: Ten participants evaluated specifications generated from food-safety regulations by Claude and Llama. Using Gherkin, a structured BDD language, 60 specifications were produced. Each participant assessed 12 across five criteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Each specification was reviewed by two participants, yielding 120 assessments.   Results: For Relevance, 75% of ratings were highest and 20% second-highest. Clarity reached 90% highest. Completeness: 75% highest, 19% second. Singularity: 82% highest, 12% second. Time Savings: 68% highest, 24% second. No lowest ratings occurred. Mann-Whitney U tests showed no significant differences across participants or models. Llama slightly outperformed Claude in Clarity, Completeness, and Time Savings, while Claude was stronger in Singularity. Feedback noted hallucinations and omissions but confirmed the utility of the specifications.   Conclusion: LLMs can generate high-quality Gherkin specifications from legal texts, reducing manual effort and providing structured artifacts useful for implementation, assurance, and test generation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T13:04:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20744v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20744v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 Rethinking Testing for LLM Applications: Characteristics, Challenges,
  and a Lightweight Interaction Protocol</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wei Ma, Yixiao Yang, Qiang Hu, Shi Ying, Zhi Jin, Bo Du, Zhenchang Xing, Tianlin Li, Junjie Shi, Yang Liu, Linxiao Jiang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Applications of Large Language Models~(LLMs) have evolved from simple text generators into complex software systems that integrate retrieval augmentation, tool invocation, and multi-turn interactions. Their inherent non-determinism, dynamism, and context dependence pose fundamental challenges for quality assurance. This paper decomposes LLM applications into a three-layer architecture: \textbf{\textit{System Shell Layer}}, \textbf{\textit{Prompt Orchestration Layer}}, and \textbf{\textit{LLM Inference Core}}. We then assess the applicability of traditional software testing methods in each layer: directly applicable at the shell layer, requiring semantic reinterpretation at the orchestration layer, and necessitating paradigm shifts at the inference core. A comparative analysis of Testing AI methods from the software engineering community and safety analysis techniques from the AI community reveals structural disconnects in testing unit abstraction, evaluation metrics, and lifecycle management. We identify four fundamental differences that underlie 6 core challenges. To address these, we propose four types of collaborative strategies (\emph{Retain}, \emph{Translate}, \emph{Integrate}, and \emph{Runtime}) and explore a closed-loop, trustworthy quality assurance framework that combines pre-deployment validation with runtime monitoring. Based on these strategies, we offer practical guidance and a protocol proposal to support the standardization and tooling of LLM application testing. We propose a protocol \textbf{\textit{Agent Interaction Communication Language}} (AICL) that is used to communicate between AI agents. AICL has the test-oriented features and is easily integrated in the current agent framework.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T13:00:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20737v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20737v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 Leveraging Semantic Triples for Private Document Generation with Local
  Differential Privacy Guarantees</h2>
                <div class="authors">
                    <strong>Authors:</strong> Stephen Meisenbacher, Maulik Chevli, Florian Matthes
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Many works at the intersection of Differential Privacy (DP) in Natural Language Processing aim to protect privacy by transforming texts under DP guarantees. This can be performed in a variety of ways, from word perturbations to full document rewriting, and most often under local DP. Here, an input text must be made indistinguishable from any other potential text, within some bound governed by the privacy parameter $\varepsilon$. Such a guarantee is quite demanding, and recent works show that privatizing texts under local DP can only be done reasonably under very high $\varepsilon$ values. Addressing this challenge, we introduce DP-ST, which leverages semantic triples for neighborhood-aware private document generation under local DP guarantees. Through the evaluation of our method, we demonstrate the effectiveness of the divide-and-conquer paradigm, particularly when limiting the DP notion (and privacy guarantees) to that of a privatization neighborhood. When combined with LLM post-processing, our method allows for coherent text generation even at lower $\varepsilon$ values, while still balancing privacy and utility. These findings highlight the importance of coherence in achieving balanced privatization outputs at reasonable $\varepsilon$ levels.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T12:59:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20736v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20736v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 Re4: Scientific Computing Agent with Rewriting, Resolution, Review and
  Revision</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ao Cheng, Lei Zhang, Guowei He
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) serve as an active and promising field of generative artificial intelligence and have demonstrated abilities to perform complex tasks in multiple domains, including mathematical and scientific reasoning. In this work, we construct a novel agent framework for solving representative problems in scientific computing. The proposed agent, incorporating a "rewriting-resolution-review-revision" logical chain via three reasoning LLMs (functioning as the Consultant, Reviewer, and Programmer, respectively), is integrated in a collaborative and interactive manner. The Consultant module endows the agent with knowledge transfer capabilities to link problems to professional domain insights, thereby rewriting problem descriptions through text augmentation. The Programmer module is responsible for generating and executing well-structured code to deliver the problem resolution. The Reviewer module equips the agent with the capacity for self-debugging and self-refinement through interactive feedback with code runtime outputs. By leveraging the end-to-end review mechanism, the executable code provided by the Programmer attains the iterative revision. A comprehensive evaluation is conducted on the performance of the proposed agent framework in solving PDEs, ill-conditioned linear systems, and data-driven physical analysis problems. Compared to single-model, this collaborative framework significantly improves the bug-free code generation rate and reduces the occurrence of non-physical solutions, thereby establishing a highly reliable framework for autonomous code generation based on natural language descriptions. The review mechanism improved the average execution success (bug-free code and non-NaN solutions) rate of the latest reasoning models. In summary, our agent framework establishes automatic code generation and review as a promising scientific computing paradigm.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T12:50:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>physics.comp-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20729v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20729v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 Unified Multi-task Learning for Voice-Based Detection of Diverse
  Clinical Conditions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ran Piao, Yuan Lu, Hareld Kemps, Tong Xia, Aaqib Saeed
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Voice-based health assessment offers unprecedented opportunities for scalable, non-invasive disease screening, yet existing approaches typically focus on single conditions and fail to leverage the rich, multi-faceted information embedded in speech. We present MARVEL (Multi-task Acoustic Representations for Voice-based Health Analysis), a privacy-conscious multitask learning framework that simultaneously detects nine distinct neurological, respiratory, and voice disorders using only derived acoustic features, eliminating the need for raw audio transmission. Our dual-branch architecture employs specialized encoders with task-specific heads sharing a common acoustic backbone, enabling effective cross-condition knowledge transfer. Evaluated on the large-scale Bridge2AI-Voice v2.0 dataset, MARVEL achieves an overall AUROC of 0.78, with exceptional performance on neurological disorders (AUROC = 0.89), particularly for Alzheimer's disease/mild cognitive impairment (AUROC = 0.97). Our framework consistently outperforms single-modal baselines by 5-19% and surpasses state-of-the-art self-supervised models on 7 of 9 tasks, while correlation analysis reveals that the learned representations exhibit meaningful similarities with established acoustic features, indicating that the model's internal representations are consistent with clinically recognized acoustic patterns. By demonstrating that a single unified model can effectively screen for diverse conditions, this work establishes a foundation for deployable voice-based diagnostics in resource-constrained and remote healthcare settings.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T12:37:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SD</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20717v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20717v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 Multi-Lingual Implicit Discourse Relation Recognition with Multi-Label
  Hierarchical Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nelson Filipe Costa, Leila Kosseim
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper introduces the first multi-lingual and multi-label classification model for implicit discourse relation recognition (IDRR). Our model, HArch, is evaluated on the recently released DiscoGeM 2.0 corpus and leverages hierarchical dependencies between discourse senses to predict probability distributions across all three sense levels in the PDTB 3.0 framework. We compare several pre-trained encoder backbones and find that RoBERTa-HArch achieves the best performance in English, while XLM-RoBERTa-HArch performs best in the multi-lingual setting. In addition, we compare our fine-tuned models against GPT-4o and Llama-4-Maverick using few-shot prompting across all language configurations. Our results show that our fine-tuned models consistently outperform these LLMs, highlighting the advantages of task-specific fine-tuning over prompting in IDRR. Finally, we report SOTA results on the DiscoGeM 1.0 corpus, further validating the effectiveness of our hierarchical approach.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T12:30:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20712v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20712v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 What is the Most Efficient Technique for Uplink Cell-Free Massive MIMO?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wei Jiang, Hans D. Schotten
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper seeks to determine the most efficient uplink technique for cell-free massive MIMO systems. Despite offering great advances, existing works suffer from fragmented methodologies and inconsistent assumptions (e.g., single- vs. multi-antenna access points, ideal vs. spatially correlated channels). To address these limitations, we: (1) establish a unified analytical framework compatible with centralized/distributed processing and diverse combining schemes; (2) develop a universal optimization strategy for max-min power control; and (3) conduct a holistic study among four critical metrics: worst-case user spectral efficiency (fairness), system capacity, fronthaul signaling, and computational complexity. Through analyses and evaluation, this work ultimately identifies the optimal uplink technique for practical cell-free deployments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T12:27:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>eess.SP</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20708v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20708v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 Achieving Optimal Performance-Cost Trade-Off in Hierarchical Cell-Free
  Massive MIMO</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wei Jiang, Hans D Schotten
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Cell-free (CF) massive MIMO offers uniform service via distributed access points (APs), which impose high deployment costs. A novel design called hierarchical cell-free (HCF) addresses this problem by replacing some APs with a central base station, thereby lowering the costs of fronthaul network (wireless sites and fiber cables) while preserving performance. To identify the optimal uplink configuration in HCF massive MIMO, this paper provides the first comprehensive analysis, benchmarking it against cellular and CF systems. We develop a unified analytical framework for spectral efficiency that supports arbitrary combining schemes and introduce a novel hierarchical combining approach tailored to HCF two-tier architecture. Through analysis and evaluation of user fairness, system capacity, fronthaul requirements, and computational complexity, this paper identifies that HCF using centralized zero-forcing combining achieves the optimal balance between performance and cost-efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T12:22:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>eess.SP</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20704v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20704v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social
  Interactions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Maojia Song, Tej Deep Pala, Weisheng Jin, Amir Zadeh, Chuan Li, Dorien Herremans, Soujanya Poria
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) are increasingly deployed in multi-agent systems (MAS) as components of collaborative intelligence, where peer interactions dynamically shape individual decision-making. Although prior work has focused on conformity bias, we extend the analysis to examine how LLMs form trust from previous impressions, resist misinformation, and integrate peer input during interaction, key factors for achieving collective intelligence under complex social dynamics. We present KAIROS, a benchmark simulating quiz contests with peer agents of varying reliability, offering fine-grained control over conditions such as expert-novice roles, noisy crowds, and adversarial peers. LLMs receive both historical interactions and current peer responses, allowing systematic investigation into how trust, peer action, and self-confidence influence decisions. As for mitigation strategies, we evaluate prompting, supervised fine-tuning, and reinforcement learning, Group Relative Policy Optimisation (GRPO), across multiple models. Our results reveal that GRPO with multi-agent context combined with outcome-based rewards and unconstrained reasoning achieves the best overall performance, but also decreases the robustness to social influence compared to Base models. The code and datasets are available at: https://github.com/declare-lab/KAIROS.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T12:18:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.18321v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.18321v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 Token Buncher: Shielding LLMs from Harmful Reinforcement Learning
  Fine-Tuning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weitao Feng, Lixu Wang, Tianyi Wei, Jie Zhang, Chongyang Gao, Sinong Zhan, Peizhuo Lv, Wei Dong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As large language models (LLMs) continue to grow in capability, so do the risks of harmful misuse through fine-tuning. While most prior studies assume that attackers rely on supervised fine-tuning (SFT) for such misuse, we systematically demonstrate that reinforcement learning (RL) enables adversaries to more effectively break safety alignment and facilitate advanced harmful task assistance, under matched computational budgets. To counter this emerging threat, we propose TokenBuncher, the first effective defense specifically targeting RL-based harmful fine-tuning. TokenBuncher suppresses the foundation on which RL relies: model response uncertainty. By constraining uncertainty, RL-based fine-tuning can no longer exploit distinct reward signals to drive the model toward harmful behaviors. We realize this defense through entropy-as-reward RL and a Token Noiser mechanism designed to prevent the escalation of expert-domain harmful capabilities. Extensive experiments across multiple models and RL algorithms show that TokenBuncher robustly mitigates harmful RL fine-tuning while preserving benign task utility and finetunability. Our results highlight that RL-based harmful fine-tuning poses a greater systemic risk than SFT, and that TokenBuncher provides an effective and general defense.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T12:07:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20697v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20697v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 NLKI: A lightweight Natural Language Knowledge Integration Framework for
  Improving Small VLMs in Commonsense VQA Tasks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aritra Dutta, Swapnanil Mukherjee, Deepanway Ghosal, Somak Aditya
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Commonsense visual-question answering often hinges on knowledge that is missing from the image or the question. Small vision-language models (sVLMs) such as ViLT, VisualBERT and FLAVA therefore lag behind their larger generative counterparts. To study the effect of careful commonsense knowledge integration on sVLMs, we present an end-to-end framework (NLKI) that (i) retrieves natural language facts, (ii) prompts an LLM to craft natural language explanations, and (iii) feeds both signals to sVLMs respectively across two commonsense VQA datasets (CRIC, AOKVQA) and a visual-entailment dataset (e-SNLI-VE). Facts retrieved using a fine-tuned ColBERTv2 and an object information-enriched prompt yield explanations that largely cut down hallucinations, while lifting the end-to-end answer accuracy by up to 7% (across 3 datasets), making FLAVA and other models in NLKI match or exceed medium-sized VLMs such as Qwen-2 VL-2B and SmolVLM-2.5B. As these benchmarks contain 10-25% label noise, additional finetuning using noise-robust losses (such as symmetric cross entropy and generalised cross entropy) adds another 2.5% in CRIC, and 5.5% in AOKVQA. Our findings expose when LLM-based commonsense knowledge beats retrieval from commonsense knowledge bases, how noise-aware training stabilises small models in the context of external knowledge augmentation, and why parameter-efficient commonsense reasoning is now within reach for 250M models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T12:05:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19724v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19724v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 Federated nnU-Net for Privacy-Preserving Medical Image Segmentation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Grzegorz Skorupko, Fotios Avgoustidis, Carlos Martín-Isla, Lidia Garrucho, Dimitri A. Kessler, Esmeralda Ruiz Pujadas, Oliver Díaz, Maciej Bobowicz, Katarzyna Gwoździewicz, Xavier Bargalló, Paulius Jaruševičius, Richard Osuala, Kaisar Kushibar, Karim Lekadir
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The nnU-Net framework has played a crucial role in medical image segmentation and has become the gold standard in multitudes of applications targeting different diseases, organs, and modalities. However, so far it has been used primarily in a centralized approach where the collected data is stored in the same location where nnU-Net is trained. This centralized approach has various limitations, such as potential leakage of sensitive patient information and violation of patient privacy. Federated learning has emerged as a key approach for training segmentation models in a decentralized manner, enabling collaborative development while prioritising patient privacy. In this paper, we propose FednnU-Net, a plug-and-play, federated learning extension of the nnU-Net framework. To this end, we contribute two federated methodologies to unlock decentralized training of nnU-Net, namely, Federated Fingerprint Extraction (FFE) and Asymmetric Federated Averaging (AsymFedAvg). We conduct a comprehensive set of experiments demonstrating high and consistent performance of our methods for breast, cardiac and fetal segmentation based on a multi-modal collection of 6 datasets representing samples from 18 different institutions. To democratize research as well as real-world deployments of decentralized training in clinical centres, we publicly share our framework at https://github.com/faildeny/FednnUNet .
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T12:01:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.02549v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.02549v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 Leveraging Large Language Models for Generating Research Topic
  Ontologies: A Multi-Disciplinary Study</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tanay Aggarwal, Angelo Salatino, Francesco Osborne, Enrico Motta
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Ontologies and taxonomies of research fields are critical for managing and organising scientific knowledge, as they facilitate efficient classification, dissemination and retrieval of information. However, the creation and maintenance of such ontologies are expensive and time-consuming tasks, usually requiring the coordinated effort of multiple domain experts. Consequently, ontologies in this space often exhibit uneven coverage across different disciplines, limited inter-domain connectivity, and infrequent updating cycles. In this study, we investigate the capability of several large language models to identify semantic relationships among research topics within three academic domains: biomedicine, physics, and engineering. The models were evaluated under three distinct conditions: zero-shot prompting, chain-of-thought prompting, and fine-tuning on existing ontologies. Additionally, we assessed the cross-domain transferability of fine-tuned models by measuring their performance when trained in one domain and subsequently applied to a different one. To support this analysis, we introduce PEM-Rel-8K, a novel dataset consisting of over 8,000 relationships extracted from the most widely adopted taxonomies in the three disciplines considered in this study: MeSH, PhySH, and IEEE. Our experiments demonstrate that fine-tuning LLMs on PEM-Rel-8K yields excellent performance across all disciplines.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T11:53:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DL</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20693v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20693v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 Traversing the Narrow Path: A Two-Stage Reinforcement Learning Framework
  for Humanoid Beam Walking</h2>
                <div class="authors">
                    <strong>Authors:</strong> TianChen Huang, Wei Gao, Runchen Xu, Shiwu Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Traversing narrow beams is challenging for humanoids due to sparse, safety-critical contacts and the fragility of purely learned policies. We propose a physically grounded, two-stage framework that couples an XCoM/LIPM footstep template with a lightweight residual planner and a simple low-level tracker. Stage-1 is trained on flat ground: the tracker learns to robustly follow footstep targets by adding small random perturbations to heuristic footsteps, without any hand-crafted centerline locking, so it acquires stable contact scheduling and strong target-tracking robustness. Stage-2 is trained in simulation on a beam: a high-level planner predicts a body-frame residual (Delta x, Delta y, Delta psi) for the swing foot only, refining the template step to prioritize safe, precise placement under narrow support while preserving interpretability. To ease deployment, sensing is kept minimal and consistent between simulation and hardware: the planner consumes compact, forward-facing elevation cues together with onboard IMU and joint signals. On a Unitree G1, our system reliably traverses a 0.2 m-wide, 3 m-long beam. Across simulation and real-world studies, residual refinement consistently outperforms template-only and monolithic baselines in success rate, centerline adherence, and safety margins, while the structured footstep interface enables transparent analysis and low-friction sim-to-real transfer.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T11:09:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20661v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20661v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 CodecBench: A Comprehensive Benchmark for Acoustic and Semantic
  Evaluation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruifan Deng, Yitian Gong, Qinghui Gao, Luozhijie Jin, Qinyuan Cheng, Zhaoye Fei, Shimin Li, Xipeng Qiu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the rise of multimodal large language models (LLMs), audio codec plays an increasingly vital role in encoding audio into discrete tokens, enabling integration of audio into text-based LLMs. Current audio codec captures two types of information: acoustic and semantic. As audio codec is applied to diverse scenarios in speech language model , it needs to model increasingly complex information and adapt to varied contexts, such as scenarios with multiple speakers, background noise, or richer paralinguistic information. However, existing codec's own evaluation has been limited by simplistic metrics and scenarios, and existing benchmarks for audio codec are not designed for complex application scenarios, which limits the assessment performance on complex datasets for acoustic and semantic capabilities. We introduce CodecBench, a comprehensive evaluation dataset to assess audio codec performance from both acoustic and semantic perspectives across four data domains. Through this benchmark, we aim to identify current limitations, highlight future research directions, and foster advances in the development of audio codec. The codes are available at https://github.com/RayYuki/CodecBench.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T11:07:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.AS</span><span>cs.SD</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20660v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20660v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 Improving Alignment in LVLMs with Debiased Self-Judgment</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sihan Yang, Chenhang Cui, Zihao Zhao, Yiyang Zhou, Weilong Yan, Ying Wei, Huaxiu Yao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid advancements in Large Language Models (LLMs) and Large Visual-Language Models (LVLMs) have opened up new opportunities for integrating visual and linguistic modalities. However, effectively aligning these modalities remains challenging, often leading to hallucinations--where generated outputs are not grounded in the visual input--and raising safety concerns across various domains. Existing alignment methods, such as instruction tuning and preference tuning, often rely on external datasets, human annotations, or complex post-processing, which limit scalability and increase costs. To address these challenges, we propose a novel approach that generates the debiased self-judgment score, a self-evaluation metric created internally by the model without relying on external resources. This enables the model to autonomously improve alignment. Our method enhances both decoding strategies and preference tuning processes, resulting in reduced hallucinations, enhanced safety, and improved overall capability. Empirical results show that our approach significantly outperforms traditional methods, offering a more effective solution for aligning LVLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T11:01:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20655v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20655v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 Refining Text Generation for Realistic Conversational Recommendation via
  Direct Preference Optimization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Manato Tajiri, Michimasa Inaba
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Conversational Recommender Systems (CRSs) aim to elicit user preferences via natural dialogue to provide suitable item recommendations. However, current CRSs often deviate from realistic human interactions by rapidly recommending items in brief sessions. This work addresses this gap by leveraging Large Language Models (LLMs) to generate dialogue summaries from dialogue history and item recommendation information from item description. This approach enables the extraction of both explicit user statements and implicit preferences inferred from the dialogue context. We introduce a method using Direct Preference Optimization (DPO) to ensure dialogue summary and item recommendation information are rich in information crucial for effective recommendations. Experiments on two public datasets validate our method's effectiveness in fostering more natural and realistic conversational recommendation processes.Our implementation is publicly available at: https://github.com/UEC-InabaLab/Refining-LLM-Text
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T10:57:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19918v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19918v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 CyberSleuth: Autonomous Blue-Team LLM Agent for Web Attack Forensics</h2>
                <div class="authors">
                    <strong>Authors:</strong> Stefano Fumero, Kai Huang, Matteo Boffa, Danilo Giordano, Marco Mellia, Zied Ben Houidi, Dario Rossi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Model (LLM) agents are powerful tools for automating complex tasks. In cybersecurity, researchers have primarily explored their use in red-team operations such as vulnerability discovery and penetration tests. Defensive uses for incident response and forensics have received comparatively less attention and remain at an early stage. This work presents a systematic study of LLM-agent design for the forensic investigation of realistic web application attacks. We propose CyberSleuth, an autonomous agent that processes packet-level traces and application logs to identify the targeted service, the exploited vulnerability (CVE), and attack success. We evaluate the consequences of core design decisions - spanning tool integration and agent architecture - and provide interpretable guidance for practitioners. We benchmark four agent architectures and six LLM backends on 20 incident scenarios of increasing complexity, identifying CyberSleuth as the best-performing design. In a separate set of 10 incidents from 2025, CyberSleuth correctly identifies the exact CVE in 80% of cases. At last, we conduct a human study with 22 experts, which rated the reports of CyberSleuth as complete, useful, and coherent. They also expressed a slight preference for DeepSeek R1, a good news for open source LLM. To foster progress in defensive LLM research, we release both our benchmark and the CyberSleuth platform as a foundation for fair, reproducible evaluation of forensic agents.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T10:45:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20643v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20643v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via
  Facial-Preserving Diffusion Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ayan Banerjee, Fernando Vilariño, Josep Lladós
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Preserving facial identity under extreme stylistic transformation remains a major challenge in generative art. In graffiti, a high-contrast, abstract medium, subtle distortions to the eyes, nose, or mouth can erase the subject's recognizability, undermining both personal and cultural authenticity. We present CraftGraffiti, an end-to-end text-guided graffiti generation framework designed with facial feature preservation as a primary objective. Given an input image and a style and pose descriptive prompt, CraftGraffiti first applies graffiti style transfer via LoRA-fine-tuned pretrained diffusion transformer, then enforces identity fidelity through a face-consistent self-attention mechanism that augments attention layers with explicit identity embeddings. Pose customization is achieved without keypoints, using CLIP-guided prompt extension to enable dynamic re-posing while retaining facial coherence. We formally justify and empirically validate the "style-first, identity-after" paradigm, showing it reduces attribute drift compared to the reverse order. Quantitative results demonstrate competitive facial feature consistency and state-of-the-art aesthetic and human preference scores, while qualitative analyses and a live deployment at the Cruilla Festival highlight the system's real-world creative impact. CraftGraffiti advances the goal of identity-respectful AI-assisted artistry, offering a principled approach for blending stylistic freedom with recognizability in creative AI applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T10:38:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20640v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20640v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 GDS Agent: A Graph Algorithmic Reasoning Agent</h2>
                <div class="authors">
                    <strong>Authors:</strong> Borun Shi, Ioannis Panagiotas
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have shown remarkable multimodal information processing and reasoning ability. When equipped with tools through function calling and enhanced with retrieval-augmented techniques, compound LLM-based systems can access closed data sources and answer questions about them. However, they still struggle to process and reason over large-scale graph-structure data. We introduce the GDS (Graph Data Science) agent in this technical report. The GDS agent introduces a comprehensive set of graph algorithms as tools, together with preprocessing (retrieval) and postprocessing of algorithm results, in a model context protocol (MCP) server. The server can be used with any modern LLM out-of-the-box. GDS agent allows users to ask any question that implicitly and intrinsically requires graph algorithmic reasoning about their data, and quickly obtain accurate and grounded answers. We also introduce a new benchmark that evaluates intermediate tool calls as well as final responses. The results indicate that GDS agent is able to solve a wide spectrum of graph tasks. We also provide detailed case studies for more open-ended tasks and study scenarios where the agent struggles. Finally, we discuss the remaining challenges and the future roadmap.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T10:35:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20637v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20637v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 Schema-Guided Response Generation using Multi-Frame Dialogue State for
  Motivational Interviewing Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jie Zeng, Yukiko I. Nakano
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The primary goal of Motivational Interviewing (MI) is to help clients build their own motivation for behavioral change. To support this in dialogue systems, it is essential to guide large language models (LLMs) to generate counselor responses aligned with MI principles. By employing a schema-guided approach, this study proposes a method for updating multi-frame dialogue states and a strategy decision mechanism that dynamically determines the response focus in a manner grounded in MI principles. The proposed method was implemented in a dialogue system and evaluated through a user study. Results showed that the proposed system successfully generated MI-favorable responses and effectively encouraged the user's (client's) deliberation by asking eliciting questions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T10:34:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20635v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20635v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 Continuously Steering LLMs Sensitivity to Contextual Knowledge with
  Proxy Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yilin Wang, Heng Wang, Yuyang Bai, Minnan Luo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In Large Language Models (LLMs) generation, there exist knowledge conflicts and scenarios where parametric knowledge contradicts knowledge provided in the context. Previous works studied tuning, decoding algorithms, or locating and editing context-aware neurons to adapt LLMs to be faithful to new contextual knowledge. However, they are usually inefficient or ineffective for large models, not workable for black-box models, or unable to continuously adjust LLMs' sensitivity to the knowledge provided in the context. To mitigate these problems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a simple framework that can steer LLMs' sensitivity to contextual knowledge continuously at a lightweight cost. Specifically, we tune two small LMs (i.e. proxy models) and use the difference in their output distributions to shift the original distribution of an LLM without modifying the LLM weights. In the evaluation process, we not only design synthetic data and fine-grained metrics to measure models' sensitivity to contextual knowledge but also use a real conflict dataset to validate CSKS's practical efficacy. Extensive experiments demonstrate that our framework achieves continuous and precise control over LLMs' sensitivity to contextual knowledge, enabling both increased sensitivity and reduced sensitivity, thereby allowing LLMs to prioritize either contextual or parametric knowledge as needed flexibly. Our data and code are available at https://github.com/OliveJuiceLin/CSKS.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T10:00:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19720v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19720v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 DART: Distilling Autoregressive Reasoning to Silent Thought</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nan Jiang, Ziming Wu, De-Chuan Zhan, Fuming Lai, Shaobing Lian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Chain-of-Thought (CoT) reasoning has significantly advanced Large Language Models (LLMs) in solving complex tasks. However, its autoregressive paradigm leads to significant computational overhead, hindering its deployment in latency-sensitive applications. To address this, we propose \textbf{DART} (\textbf{D}istilling \textbf{A}utoregressive \textbf{R}easoning to Silent \textbf{T}hought), a self-distillation framework that enables LLMs to replace autoregressive CoT with non-autoregressive Silent Thought (ST). Specifically, DART introduces two training pathways: the CoT pathway for traditional reasoning and the ST pathway for generating answers directly from a few ST tokens. The ST pathway utilizes a lightweight Reasoning Evolvement Module (REM) to align its hidden states with the CoT pathway, enabling the ST tokens to evolve into informative embeddings. During inference, only the ST pathway is activated, leveraging evolving ST tokens to deliver the answer directly. Extensive experimental results demonstrate that DART offers significant performance gains compared with existing non-autoregressive baselines without extra inference latency, serving as a feasible alternative for efficient reasoning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T09:45:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.11752v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.11752v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 GENRE-CMR: Generalizable Deep Learning for Diverse Multi-Domain Cardiac
  MRI Reconstruction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kian Anvari Hamedani, Narges Razizadeh, Shahabedin Nabavi, Mohsen Ebrahimi Moghaddam
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Accelerated Cardiovascular Magnetic Resonance (CMR) image reconstruction remains a critical challenge due to the trade-off between scan time and image quality, particularly when generalizing across diverse acquisition settings. We propose GENRE-CMR, a generative adversarial network (GAN)-based architecture employing a residual deep unrolled reconstruction framework to enhance reconstruction fidelity and generalization. The architecture unrolls iterative optimization into a cascade of convolutional subnetworks, enriched with residual connections to enable progressive feature propagation from shallow to deeper stages. To further improve performance, we integrate two loss functions: (1) an Edge-Aware Region (EAR) loss, which guides the network to focus on structurally informative regions and helps prevent common reconstruction blurriness; and (2) a Statistical Distribution Alignment (SDA) loss, which regularizes the feature space across diverse data distributions via a symmetric KL divergence formulation. Extensive experiments confirm that GENRE-CMR surpasses state-of-the-art methods on training and unseen data, achieving 0.9552 SSIM and 38.90 dB PSNR on unseen distributions across various acceleration factors and sampling trajectories. Ablation studies confirm the contribution of each proposed component to reconstruction quality and generalization. Our framework presents a unified and robust solution for high-quality CMR reconstruction, paving the way for clinically adaptable deployment across heterogeneous acquisition protocols.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T09:43:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.IV</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20600v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20600v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 Automated Algorithmic Discovery for Gravitational-Wave Detection Guided
  by LLM-Informed Evolutionary Monte Carlo Tree Search</h2>
                <div class="authors">
                    <strong>Authors:</strong> He Wang, Liang Zeng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Gravitational-wave signal detection with unknown source parameters buried in dynamic detector noise remains a formidable computational challenge. Existing approaches face core limitations from restrictive assumptions: traditional methods rely on predefined theoretical priors, while neural networks introduce hidden biases and lack interpretability. We propose Evolutionary Monte Carlo Tree Search (Evo-MCTS), the first integration of large language model (LLM) guidance with domain-aware physical constraints for automated gravitational wave detection. This framework systematically explores algorithmic solution spaces through tree-structured search enhanced by evolutionary optimization, combining MCTS for strategic exploration with evolutionary algorithms for solution refinement. The LLM component provides domain-aware heuristics while maintaining interpretability through explicit algorithmic pathway generation. Experimental validation demonstrates substantial performance improvements, achieving a 20.2% improvement over state-of-the-art gravitational wave detection algorithms on the MLGWSC-1 benchmark dataset and a remarkable 59.1% improvement over other LLM-based algorithm optimization frameworks. Beyond performance improvements, our framework establishes a transferable methodology for automated algorithmic discovery across computational science domains.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T09:35:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>astro-ph.HE</span><span>astro-ph.IM</span><span>gr-qc</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.03661v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.03661v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 WideSearch: Benchmarking Agentic Broad Info-Seeking</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ryan Wong, Jiawei Wang, Junjie Zhao, Li Chen, Yan Gao, Long Zhang, Xuan Zhou, Zuo Wang, Kai Xiang, Ge Zhang, Wenhao Huang, Yang Wang, Ke Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> From professional research to everyday planning, many tasks are bottlenecked by wide-scale information seeking, which is more repetitive than cognitively complex. With the rapid development of Large Language Models (LLMs), automated search agents powered by LLMs offer a promising solution to liberate humans from this tedious work. However, the capability of these agents to perform such "wide-context" collection reliably and completely remains largely unevaluated due to a lack of suitable benchmarks. To bridge this gap, we introduce WideSearch, a new benchmark engineered to evaluate agent reliability on these large-scale collection tasks. The benchmark features 200 manually curated questions (100 in English, 100 in Chinese) from over 15 diverse domains, grounded in real user queries. Each task requires agents to collect large-scale atomic information, which could be verified one by one objectively, and arrange it into a well-organized output. A rigorous five-stage quality control pipeline ensures the difficulty, completeness, and verifiability of the dataset. We benchmark over 10 state-of-the-art agentic search systems, including single-agent, multi-agent frameworks, and end-to-end commercial systems. Most systems achieve overall success rates near 0\%, with the best performer reaching just 5\%. However, given sufficient time, cross-validation by multiple human testers can achieve a near 100\% success rate. These results demonstrate that present search agents have critical deficiencies in large-scale information seeking, underscoring urgent areas for future research and development in agentic search. Our dataset, evaluation pipeline, and benchmark results have been publicly released at https://widesearch-seed.github.io/
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T09:31:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.07999v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.07999v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 Bitcoin as an Interplanetary Monetary Standard with Proof-of-Transit
  Timestamping</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jose E. Puente, Carlos Puente
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We explore the feasibility of deploying Bitcoin as the shared monetary standard between Earth and Mars, accounting for physical constraints of interplanetary communication. We introduce a novel primitive, Proof-of-Transit Timestamping (PoTT), to provide cryptographic, tamper-evident audit trails for Bitcoin data across high-latency, intermittently-connected links. Leveraging Delay/Disruption-Tolerant Networking (DTN) and optical low-Earth-orbit (LEO) mesh constellations, we propose an architecture for header-first replication, long-horizon Lightning channels with planetary watchtowers, and secure settlement through federated sidechains or blind-merge-mined (BMM) commit chains. We formalize PoTT, analyze its security model, and show how it measurably improves reliability and accountability without altering Bitcoin consensus or its monetary base. Near-term deployments favor strong federations for local settlement; longer-term, blind-merge-mined commit chains (if adopted) provide an alternative. The Earth L1 monetary base remains unchanged, while Mars can operate a pegged commit chain or strong federation with 1:1 pegged assets for local block production. For transparency, if both time-beacon regimes are simultaneously compromised, PoTT-M2 (and PoTT generally) reduces to administrative assertions rather than cryptographic time-anchoring.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T09:28:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20591v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20591v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 SemSR: Semantics aware robust Session-based Recommendations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jyoti Narwariya, Priyanka Gupta, Muskan Gupta, Jyotsana Khatri, Lovekesh Vig
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Session-based recommendation (SR) models aim to recommend items to anonymous users based on their behavior during the current session. While various SR models in the literature utilize item sequences to predict the next item, they often fail to leverage semantic information from item titles or descriptions impeding session intent identification and interpretability. Recent research has explored Large Language Models (LLMs) as promising approaches to enhance session-based recommendations, with both prompt-based and fine-tuning based methods being widely investigated. However, prompt-based methods struggle to identify optimal prompts that elicit correct reasoning and lack task-specific feedback at test time, resulting in sub-optimal recommendations. Fine-tuning methods incorporate domain-specific knowledge but incur significant computational costs for implementation and maintenance. In this paper, we present multiple approaches to utilize LLMs for session-based recommendation: (i) in-context LLMs as recommendation agents, (ii) LLM-generated representations for semantic initialization of deep learning SR models, and (iii) integration of LLMs with data-driven SR models. Through comprehensive experiments on two real-world publicly available datasets, we demonstrate that LLM-based methods excel at coarse-level retrieval (high recall values), while traditional data-driven techniques perform well at fine-grained ranking (high Mean Reciprocal Rank values). Furthermore, the integration of LLMs with data-driven SR models significantly out performs both standalone LLM approaches and data-driven deep learning models, as well as baseline SR models, in terms of both Recall and MRR metrics.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T09:25:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20587v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20587v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 A Graph Talks, But Who's Listening? Rethinking Evaluations for
  Graph-Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Soham Petkar, Hari Aakash K, Anirudh Vempati, Akshit Sinha, Ponnurangam Kumarauguru, Chirag Agarwal
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Developments in Graph-Language Models (GLMs) aim to integrate the structural reasoning capabilities of Graph Neural Networks (GNNs) with the semantic understanding of Large Language Models (LLMs). However, we demonstrate that current evaluation benchmarks for GLMs, which are primarily repurposed node-level classification datasets, are insufficient to assess multimodal reasoning. Our analysis reveals that strong performance on these benchmarks is achievable using unimodal information alone, suggesting that they do not necessitate graph-language integration. To address this evaluation gap, we introduce the CLEGR(Compositional Language-Graph Reasoning) benchmark, designed to evaluate multimodal reasoning at various complexity levels. Our benchmark employs a synthetic graph generation pipeline paired with questions that require joint reasoning over structure and textual semantics. We perform a thorough evaluation of representative GLM architectures and find that soft-prompted LLM baselines perform on par with GLMs that incorporate a full GNN backbone. This result calls into question the architectural necessity of incorporating graph structure into LLMs. We further show that GLMs exhibit significant performance degradation in tasks that require structural reasoning. These findings highlight limitations in the graph reasoning capabilities of current GLMs and provide a foundation for advancing the community toward explicit multimodal reasoning involving graph structure and language.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T09:20:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20583v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20583v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 Are formal and functional linguistic mechanisms dissociated in language
  models?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Michael Hanna, Yonatan Belinkov, Sandro Pezzelle
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Although large language models (LLMs) are increasingly capable, these capabilities are unevenly distributed: they excel at formal linguistic tasks, such as producing fluent, grammatical text, but struggle more with functional linguistic tasks like reasoning and consistent fact retrieval. Inspired by neuroscience, recent work suggests that to succeed on both formal and functional linguistic tasks, LLMs should use different mechanisms for each; such localization could either be built-in or emerge spontaneously through training. In this paper, we ask: do current models, with fast-improving functional linguistic abilities, exhibit distinct localization of formal and functional linguistic mechanisms? We answer this by finding and comparing the "circuits", or minimal computational subgraphs, responsible for various formal and functional tasks. Comparing 5 LLMs across 10 distinct tasks, we find that while there is indeed little overlap between circuits for formal and functional tasks, there is also little overlap between formal linguistic tasks, as exists in the human brain. Thus, a single formal linguistic network, unified and distinct from functional task circuits, remains elusive. However, in terms of cross-task faithfulness - the ability of one circuit to solve another's task - we observe a separation between formal and functional mechanisms, suggesting that shared mechanisms between formal tasks may exist.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T09:20:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>I.2.7</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.11302v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.11302v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 Human-AI Collaborative Bot Detection in MMORPGs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jaeman Son, Hyunsoo Kim
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In Massively Multiplayer Online Role-Playing Games (MMORPGs), auto-leveling bots exploit automated programs to level up characters at scale, undermining gameplay balance and fairness. Detecting such bots is challenging, not only because they mimic human behavior, but also because punitive actions require explainable justification to avoid legal and user experience issues. In this paper, we present a novel framework for detecting auto-leveling bots by leveraging contrastive representation learning and clustering techniques in a fully unsupervised manner to identify groups of characters with similar level-up patterns. To ensure reliable decisions, we incorporate a Large Language Model (LLM) as an auxiliary reviewer to validate the clustered groups, effectively mimicking a secondary human judgment. We also introduce a growth curve-based visualization to assist both the LLM and human moderators in assessing leveling behavior. This collaborative approach improves the efficiency of bot detection workflows while maintaining explainability, thereby supporting scalable and accountable bot regulation in MMORPGs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T09:17:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20578v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20578v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 MIDAS: Multimodal Interactive Digital-humAn Synthesis via Real-time
  Autoregressive Video Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ming Chen, Liyuan Cui, Wenyuan Zhang, Haoxian Zhang, Yan Zhou, Xiaohan Li, Songlin Tang, Jiwen Liu, Borui Liao, Hejia Chen, Xiaoqiang Liu, Pengfei Wan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recently, interactive digital human video generation has attracted widespread attention and achieved remarkable progress. However, building such a practical system that can interact with diverse input signals in real time remains challenging to existing methods, which often struggle with heavy computational cost and limited controllability. In this work, we introduce an autoregressive video generation framework that enables interactive multimodal control and low-latency extrapolation in a streaming manner. With minimal modifications to a standard large language model (LLM), our framework accepts multimodal condition encodings including audio, pose, and text, and outputs spatially and semantically coherent representations to guide the denoising process of a diffusion head. To support this, we construct a large-scale dialogue dataset of approximately 20,000 hours from multiple sources, providing rich conversational scenarios for training. We further introduce a deep compression autoencoder with up to 64$\times$ reduction ratio, which effectively alleviates the long-horizon inference burden of the autoregressive model. Extensive experiments on duplex conversation, multilingual human synthesis, and interactive world model highlight the advantages of our approach in low latency, high efficiency, and fine-grained multimodal controllability.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T09:15:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.19320v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.19320v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 Detect, Investigate, Judge and Determine: A Knowledge-guided Framework
  for Few-shot Fake News Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ye Liu, Jiajun Zhu, Xukai Liu, Haoyu Tang, Yanghai Zhang, Kai Zhang, Xiaofang Zhou, Enhong Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news from real ones in extremely low-resource scenarios. This task has garnered increased attention due to the widespread dissemination and harmful impact of fake news on social media. Large Language Models (LLMs) have demonstrated competitive performance with the help of their rich prior knowledge and excellent in-context learning abilities. However, existing methods face significant limitations, such as the Understanding Ambiguity and Information Scarcity, which significantly undermine the potential of LLMs. To address these shortcomings, we propose a Dual-perspective Knowledge-guided Fake News Detection (DKFND) model, designed to enhance LLMs from both inside and outside perspectives. Specifically, DKFND first identifies the knowledge concepts of each news article through a Detection Module. Subsequently, DKFND creatively designs an Investigation Module to retrieve inside and outside valuable information concerning to the current news, followed by another Judge Module to evaluate the relevance and confidence of them. Finally, a Determination Module further derives two respective predictions and obtain the final result. Extensive experiments on two public datasets show the efficacy of our proposed method, particularly in low-resource settings.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T09:09:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.08952v6' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.08952v6' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 Leveraging Generative Models for Real-Time Query-Driven Text
  Summarization in Large-Scale Web Search</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zeyu Xiong, Yixuan Nan, Li Gao, Hengzhu Tang, Shuaiqiang Wang, Junfeng Wang, Dawei Yin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the dynamic landscape of large-scale web search, Query-Driven Text Summarization (QDTS) aims to generate concise and informative summaries from textual documents based on a given query, which is essential for improving user engagement and facilitating rapid decision-making. Traditional extractive summarization models, based primarily on ranking candidate summary segments, have been the dominant approach in industrial applications. However, these approaches suffer from two key limitations: 1) The multi-stage pipeline often introduces cumulative information loss and architectural bottlenecks due to its weakest component; 2) Traditional models lack sufficient semantic understanding of both user queries and documents, particularly when dealing with complex search intents. In this study, we propose a novel framework to pioneer the application of generative models to address real-time QDTS in industrial web search. Our approach integrates large model distillation, supervised fine-tuning, direct preference optimization, and lookahead decoding to transform a lightweight model with only 0.1B parameters into a domain-specialized QDTS expert. Evaluated on multiple industry-relevant metrics, our model outperforms the production baseline and achieves a new state of the art. Furthermore, it demonstrates excellent deployment efficiency, requiring only 334 NVIDIA L20 GPUs to handle \textasciitilde50,000 queries per second under 55~ms average latency per query.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T08:51:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20559v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20559v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 SoAy: A Solution-based LLM API-using Methodology for Academic
  Information Seeking</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuanchun Wang, Jifan Yu, Zijun Yao, Jing Zhang, Yuyang Xie, Shangqing Tu, Yiyang Fu, Youhe Feng, Jinkai Zhang, Jingyao Zhang, Bowen Huang, Yuanyao Li, Huihui Yuan, Lei Hou, Juanzi Li, Jie Tang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Applying large language models (LLMs) for academic API usage shows promise in reducing researchers' academic information seeking efforts. However, current LLM API-using methods struggle with complex API coupling commonly encountered in academic queries. To address this, we introduce SoAy, a solution-based LLM API-using methodology for academic information seeking. It uses code with a solution as the reasoning method, where a solution is a pre-constructed API calling sequence. The addition of the solution reduces the difficulty for the model to understand the complex relationships between APIs. Code improves the efficiency of reasoning.   To evaluate SoAy, we introduce SoAyBench, an evaluation benchmark accompanied by SoAyEval, built upon a cloned environment of APIs from AMiner. Experimental results demonstrate a 34.58-75.99\% performance improvement compared to state-of-the-art LLM API-based baselines. All datasets, codes, tuned models, and deployed online services are publicly accessible at https://github.com/RUCKBReasoning/SoAy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T08:44:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3690624.3709412' target='_blank'>doi</a><a href='http://arxiv.org/abs/2405.15165v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.15165v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 Probabilistic Modeling of Jailbreak on Multimodal LLMs: From
  Quantification to Application</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wenzhuo Xu, Zhipeng Wei, Xiongtao Sun, Zonghao Ying, Deyue Zhang, Dongdong Yang, Xiangzheng Zhang, Quanchen Zou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recently, Multimodal Large Language Models (MLLMs) have demonstrated their superior ability in understanding multimodal content. However, they remain vulnerable to jailbreak attacks, which exploit weaknesses in their safety alignment to generate harmful responses. Previous studies categorize jailbreaks as successful or failed based on whether responses contain malicious content. However, given the stochastic nature of MLLM responses, this binary classification of an input's ability to jailbreak MLLMs is inappropriate. Derived from this viewpoint, we introduce jailbreak probability to quantify the jailbreak potential of an input, which represents the likelihood that MLLMs generated a malicious response when prompted with this input. We approximate this probability through multiple queries to MLLMs. After modeling the relationship between input hidden states and their corresponding jailbreak probability using Jailbreak Probability Prediction Network (JPPN), we use continuous jailbreak probability for optimization. Specifically, we propose Jailbreak-Probability-based Attack (JPA) that optimizes adversarial perturbations on input image to maximize jailbreak probability, and further enhance it as Multimodal JPA (MJPA) by including monotonic text rephrasing. To counteract attacks, we also propose Jailbreak-Probability-based Finetuning (JPF), which minimizes jailbreak probability through MLLM parameter updates. Extensive experiments show that (1) (M)JPA yields significant improvements when attacking a wide range of models under both white and black box settings. (2) JPF vastly reduces jailbreaks by at most over 60\%. Both of the above results demonstrate the significance of introducing jailbreak probability to make nuanced distinctions among input jailbreak abilities.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T08:43:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.06989v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.06989v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 Digital Scale: Open-Source On-Device BMI Estimation from Smartphone
  Camera Images Trained on a Large-Scale Real-World Dataset</h2>
                <div class="authors">
                    <strong>Authors:</strong> Frederik Rajiv Manichand, Robin Deuber, Robert Jakob, Steve Swerling, Jamie Rosen, Elgar Fleisch, Patrick Langer
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Estimating Body Mass Index (BMI) from camera images with machine learning models enables rapid weight assessment when traditional methods are unavailable or impractical, such as in telehealth or emergency scenarios. Existing computer vision approaches have been limited to datasets of up to 14,500 images. In this study, we present a deep learning-based BMI estimation method trained on our WayBED dataset, a large proprietary collection of 84,963 smartphone images from 25,353 individuals. We introduce an automatic filtering method that uses posture clustering and person detection to curate the dataset by removing low-quality images, such as those with atypical postures or incomplete views. This process retained 71,322 high-quality images suitable for training. We achieve a Mean Absolute Percentage Error (MAPE) of 7.9% on our hold-out test set (WayBED data) using full-body images, the lowest value in the published literature to the best of our knowledge. Further, we achieve a MAPE of 13% on the completely unseen~(during training) VisualBodyToBMI dataset, comparable with state-of-the-art approaches trained on it, demonstrating robust generalization. Lastly, we fine-tune our model on VisualBodyToBMI and achieve a MAPE of 8.56%, the lowest reported value on this dataset so far. We deploy the full pipeline, including image filtering and BMI estimation, on Android devices using the CLAID framework. We release our complete code for model training, filtering, and the CLAID package for mobile deployment as open-source contributions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T08:21:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20534v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20534v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 Graph-R1: Incentivizing the Zero-Shot Graph Learning Capability in LLMs
  via Explicit Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yicong Wu, Guangyue Lu, Yuan Zuo, Huarong Zhang, Junjie Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Generalizing to unseen graph tasks without task-pecific supervision remains challenging. Graph Neural Networks (GNNs) are limited by fixed label spaces, while Large Language Models (LLMs) lack structural inductive biases. Recent advances in Large Reasoning Models (LRMs) provide a zero-shot alternative via explicit, long chain-of-thought reasoning. Inspired by this, we propose a GNN-free approach that reformulates graph tasks--node classification, link prediction, and graph classification--as textual reasoning problems solved by LRMs. We introduce the first datasets with detailed reasoning traces for these tasks and develop Graph-R1, a reinforcement learning framework that leverages task-specific rethink templates to guide reasoning over linearized graphs. Experiments demonstrate that Graph-R1 outperforms state-of-the-art baselines in zero-shot settings, producing interpretable and effective predictions. Our work highlights the promise of explicit reasoning for graph learning and provides new resources for future research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T08:20:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.17387v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.17387v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 Enhancing Health Fact-Checking with LLM-Generated Synthetic Data</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jingze Zhang, Jiahe Qian, Yiliang Zhou, Yifan Peng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Fact-checking for health-related content is challenging due to the limited availability of annotated training data. In this study, we propose a synthetic data generation pipeline that leverages large language models (LLMs) to augment training data for health-related fact checking. In this pipeline, we summarize source documents, decompose the summaries into atomic facts, and use an LLM to construct sentence-fact entailment tables. From the entailment relations in the table, we further generate synthetic text-claim pairs with binary veracity labels. These synthetic data are then combined with the original data to fine-tune a BERT-based fact-checking model. Evaluation on two public datasets, PubHealth and SciFact, shows that our pipeline improved F1 scores by up to 0.019 and 0.049, respectively, compared to models trained only on the original data. These results highlight the effectiveness of LLM-driven synthetic data augmentation in enhancing the performance of health-related fact-checkers.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T08:06:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.20525v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.20525v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 RevPRAG: Revealing Poisoning Attacks in Retrieval-Augmented Generation
  through LLM Activation Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xue Tan, Hao Luan, Mingyu Luo, Xiaoyan Sun, Ping Chen, Jun Dai
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Retrieval-Augmented Generation (RAG) enriches the input to LLMs by retrieving information from the relevant knowledge database, enabling them to produce responses that are more accurate and contextually appropriate. It is worth noting that the knowledge database, being sourced from publicly available channels such as Wikipedia, inevitably introduces a new attack surface. RAG poisoning involves injecting malicious texts into the knowledge database, ultimately leading to the generation of the attacker's target response (also called poisoned response). However, there are currently limited methods available for detecting such poisoning attacks. We aim to bridge the gap in this work. Particularly, we introduce RevPRAG, a flexible and automated detection pipeline that leverages the activations of LLMs for poisoned response detection. Our investigation uncovers distinct patterns in LLMs' activations when generating correct responses versus poisoned responses. Our results on multiple benchmark datasets and RAG architectures show our approach could achieve 98% true positive rate, while maintaining false positive rates close to 1%.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-28T08:03:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.18948v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.18948v4' target='_blank'>pdf</a>
                </div>
            </div>
            
        </div>
    </body>
    </html>
    