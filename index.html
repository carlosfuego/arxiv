
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>ArXiv Results</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                margin: 0;
                padding: 0;
                display: flex;
                justify-content: center;
                align-items: center;
                min-height: 100vh;
                background-color: #f9f9f9;
            }
            .container {
                width: 17cm;
                padding: 2cm;
                background-color: #fff;
                box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            }
            h1, h2 {
                color: #333;
            }
            .paper {
                border: 1px solid #ccc;
                padding: 1em;
                margin-bottom: 1em;
                border-radius: 8px;
                box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            }
            .authors, .summary, .updated, .tags, .links {
                margin: 0.5em 0;
            }
            .tags span {
                background-color: #f0f0f0;
                border-radius: 4px;
                padding: 0.2em 0.5em;
                margin-right: 0.5em;
            }
            .links a {
                margin-right: 1em;
                text-decoration: none;
                color: #1a73e8;
            }
            .links a:hover {
                text-decoration: underline;
            }
        </style>
    </head>
    <body>
        <div class="container">
    <h1>Keyword: kv cache</h1>

            <div class="paper">
                <h2>#1 Reciprocating Locks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dave Dice, Alex Kogan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present "Reciprocating Locks", a novel mutual exclusion locking algorithm, targeting cache-coherent shared memory (CC), that enjoys a number of desirable properties. The doorway arrival phase and the release operation both run in constant-time. Waiting threads use local spinning and only a single waiting element is required per thread, regardless of the number of locks a thread might hold at a given time. While our lock does not provide strict FIFO admission, it bounds bypass and has strong anti-starvation properties. The lock is compact, space efficient, and has been intentionally designed to be readily usable in real-world general purpose computing environments such as the linux kernel, pthreads, or C++. We show the lock exhibits high throughput under contention and low latency in the uncontended case. The performance of Reciprocating Locks is competitive with and often better than the best state-of-the-art scalable spin locks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T16:44:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>D.4.1</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.02380v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.02380v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 vAttention: Dynamic Memory Management for Serving LLMs without
  PagedAttention</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ramya Prabhu, Ajay Nayak, Jayashree Mohan, Ramachandran Ramjee, Ashish Panwar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> PagedAttention is a popular approach for dynamic memory allocation in LLM serving systems. It enables on-demand allocation of GPU memory to mitigate KV cache fragmentation -- a phenomenon that crippled the batch size (and consequently throughput) in prior systems. However, in trying to allocate physical memory at runtime, PagedAttention ends up changing the virtual memory layout of the KV cache from contiguous to non-contiguous. Such a design leads to non-trivial programming and performance overheads.   We present vAttention -- an approach that mitigates fragmentation in physical memory while retaining the contiguity of KV cache in virtual memory. We achieve this by decoupling the allocation of virtual and physical memory using CUDA virtual memory management APIs. We also introduce various LLM-specific optimizations to address the limitations of CUDA virtual memory support. Overall, vAttention is a simpler, portable, and performant alternative to PagedAttention: it supports various attention kernels out-of-the-box and improves LLM serving throughput by up to 1.23x compared to the use of PagedAttention-based kernels of FlashAttention and FlashInfer.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T04:10:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.OS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2405.04437v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.04437v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 Optimizing SSD Caches for Cloud Block Storage Systems Using Machine
  Learning Approaches</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chiyu Cheng, Chang Zhou, Yang Zhao, Jin Cao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The growing demand for efficient cloud storage solutions has led to the widespread adoption of Solid-State Drives (SSDs) for caching in cloud block storage systems. The management of data writes to SSD caches plays a crucial role in improving overall system performance, reducing latency, and extending the lifespan of storage devices. A critical challenge arises from the large volume of write-only data, which significantly impacts the performance of SSD caches when handled inefficiently. Specifically, writes that have not been read for a certain period may introduce unnecessary write traffic to the SSD cache without offering substantial benefits for cache performance. This paper proposes a novel approach to mitigate this issue by leveraging machine learning techniques to dynamically optimize the write policy in cloud-based storage systems. The proposed method identifies write-only data and selectively filters it out in real-time, thereby minimizing the number of unnecessary write operations and improving the overall performance of the cache system. Experimental results demonstrate that the proposed machine learning-based policy significantly outperforms traditional approaches by reducing the number of harmful writes and optimizing cache utilization. This solution is particularly suitable for cloud environments with varying and unpredictable workloads, where traditional cache management strategies often fall short.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T20:35:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.LG</span><span>cs.OS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.14770v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.14770v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 Dynamic Adaptation in Data Storage: Real-Time Machine Learning for
  Enhanced Prefetching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chiyu Cheng, Chang Zhou, Yang Zhao, Jin Cao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The exponential growth of data storage demands has necessitated the evolution of hierarchical storage management strategies [1]. This study explores the application of streaming machine learning [3] to revolutionize data prefetching within multi-tiered storage systems. Unlike traditional batch-trained models, streaming machine learning [5] offers adaptability, real-time insights, and computational efficiency, responding dynamically to workload variations. This work designs and validates an innovative framework that integrates streaming classification models for predicting file access patterns, specifically the next file offset. Leveraging comprehensive feature engineering and real-time evaluation over extensive production traces, the proposed methodology achieves substantial improvements in prediction accuracy, memory efficiency, and system adaptability. The results underscore the potential of streaming models in real-time storage management, setting a precedent for advanced caching and tiering strategies.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T20:33:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.LG</span><span>cs.OS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.14771v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.14771v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks
  Detection: A Comparative Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tejal Joshi, Aarya Kawalay, Anvi Jamkhande, Amit Joshi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Cache side channel attacks are a sophisticated and persistent threat that exploit vulnerabilities in modern processors to extract sensitive information. These attacks leverage weaknesses in shared computational resources, particularly the last level cache, to infer patterns in data access and execution flows, often bypassing traditional security defenses. Such attacks are especially dangerous as they can be executed remotely without requiring physical access to the victim's device. This study focuses on a specific class of these threats: fingerprinting attacks, where an adversary monitors and analyzes the behavior of co-located processes via cache side channels. This can potentially reveal confidential information, such as encryption keys or user activity patterns. A comprehensive threat model illustrates how attackers sharing computational resources with target systems exploit these side channels to compromise sensitive data. To mitigate such risks, a hybrid deep learning model is proposed for detecting cache side channel attacks. Its performance is compared with five widely used deep learning models: Multi-Layer Perceptron, Convolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term Memory, and Gated Recurrent Unit. The experimental results demonstrate that the hybrid model achieves a detection rate of up to 99.96%. These findings highlight the limitations of existing models, the need for enhanced defensive mechanisms, and directions for future research to secure sensitive data against evolving side channel threats.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T18:14:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.NE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17123v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17123v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mohammad NaseriTehrani, MohammadJavad Salehi, Antti Tölli
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Integrating coded caching (CC) into multiple-input multiple-output (MIMO) communications can significantly enhance the achievable degrees of freedom (DoF) in wireless networks. This paper investigates a practical cache-aided asymmetric MIMO configuration with cache ratio $\gamma$, where a server equipped with $L$ transmit antennas communicates with $K$ users, each having $G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the \emph{min-G} scheme, which treats the system as symmetric by assuming all users have the same number of antennas, equal to the smallest among them; the \emph{Grouping} scheme, which maximizes spatial multiplexing gain separately within each user subset at the cost of some global caching gain; and the \emph{Phantom} scheme, which dynamically redistributes spatial resources using virtual or ``phantom'' antennas at the users, bridging the performance gains of the min-$G$ and Grouping schemes. These strategies jointly optimize the number of users, $\Omega$, and the parallel streams decoded by each user, $\beta_k$, ensuring linear decodability for all target users. Analytical and numerical results confirm that the proposed schemes achieve significant DoF improvements across various system configurations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T16:19:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>eess.SP</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.10854v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.10854v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 Measuring GPU utilization one level deeper</h2>
                <div class="authors">
                    <strong>Authors:</strong> Paul Elvinger, Foteini Strati, Natalie Enright Jerger, Ana Klimovic
                </div>
                <div class="summary">
                    <strong>Summary:</strong> GPU hardware is vastly underutilized. Even resource-intensive AI applications have diverse resource profiles that often leave parts of GPUs idle. While colocating applications can improve utilization, current spatial sharing systems lack performance guarantees. Providing predictable performance guarantees requires a deep understanding of how applications contend for shared GPU resources such as block schedulers, compute units, L1/L2 caches, and memory bandwidth. We propose a methodology to profile resource interference of GPU kernels across these dimensions and discuss how to build GPU schedulers that provide strict performance guarantees while colocating applications to minimize cost.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T12:57:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.16909v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.16909v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 Optimizing Smart Helper Placement for Enhanced Cache Efficiency in
  F-RANs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hesameddin Mokhtarzadeh, Mohammed Saif, Md. Jahangir Hossain, Julian Cheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Smart helpers (SHs) have been proposed to improve content delivery delays and alleviate high fronthaul loads in fog radio access networks (F-RANs). They offer an alternative to deploying additional enhanced remote radio heads (RRHs), which are often infeasible due to site constraints.} The optimal placement of SHs can significantly increase the number of users they serve which leads to enhanced cache efficiency and improved content delivery delay. In this letter, we optimize SH placement within an F-RAN to maximize the cache hit rate and further reduce the content delivery latency. We model the SH cache hit rate as a function of outage probability and user density distribution. We develop a function to estimate user density distribution leveraging the radial basis functions (RBFs) method and optimize SH placement utilizing the particle swarm optimization (PSO) algorithm. \an{Our} numerical results confirm the effectiveness of the proposed approach in maximizing the \an{SH cache hit rate}, thereby improving delivery delays and fronthaul loads of the network.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T00:22:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.16597v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.16597v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 Latency Guarantees for Caching with Delayed Hits</h2>
                <div class="authors">
                    <strong>Authors:</strong> Keerthana Gurushankar, Noah G. Singer, Bernardo Subercaseaux
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the classical caching problem, when a requested page is not present in the cache (i.e., a "miss"), it is assumed to travel from the backing store into the cache "before" the next request arrives. However, in many real-life applications, such as content delivery networks, this assumption is unrealistic.   The "delayed-hits" model for caching, introduced by Atre, Sherry, Wang, and Berger, accounts for the latency between a missed cache request and the corresponding arrival from the backing store. This theoretical model has two parameters: the "delay" $Z$, representing the ratio between the retrieval delay and the inter-request delay in an application, and the "cache size" $k$, as in classical caching. Classical caching corresponds to $Z=1$, whereas larger values of $Z$ model applications where retrieving missed requests is expensive. Despite the practical relevance of the delayed-hits model, its theoretical underpinnings are still poorly understood.   We present the first tight theoretical guarantee for optimizing delayed-hits caching: The "Least Recently Used" algorithm, a natural, deterministic, online algorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at most $O(Zk)$ times more latency than the (offline) optimal schedule. Our result extends to any so-called "marking" algorithm.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-27T22:14:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.16535v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.16535v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 SP-IMPact: A Framework for Static Partitioning Interference Mitigation
  and Performance Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Diogo Costa, Gonçalo Moreira, Afonso Oliveira, José Martins, Sandro Pinto
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modern embedded systems are evolving toward complex, heterogeneous architectures to accommodate increasingly demanding applications. Driven by SWAP-C constraints, this shift has led to consolidating multiple systems onto single hardware platforms. Static Partitioning Hypervisors offer a promising solution to partition hardware resources and provide spatial isolation between critical workloads. However, shared resources like the Last-Level Cache and system bus can introduce temporal interference between virtual machines (VMs), negatively impacting performance and predictability. Over the past decade, academia and industry have developed interference mitigation techniques, such as cache partitioning and memory bandwidth reservation. However, configuring these techniques is complex and time-consuming. Cache partitioning requires balancing cache sections across VMs, while memory bandwidth reservation needs tuning bandwidth budgets and periods. Testing all configurations is impractical and often leads to suboptimal results. Moreover, understanding how these techniques interact is limited, as their combined use can produce compounded or conflicting effects on performance. Static analysis tools estimating worst-case execution times offer guidance for configuring mitigation techniques but often fail to capture the complexity of modern multi-core systems. They typically focus on limited shared resources while neglecting others, such as IOMMUs and interrupt controllers. To address these challenges, we present SP-IMPact, an open-source framework for analyzing and guiding interference mitigation configurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth reservation, while evaluating their interactions and cumulative impact. By providing insights on real hardware, SP-IMPact helps optimize configurations for mixed-criticality systems, ensuring performance and predictability.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-27T17:42:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.PF</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.16245v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.16245v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 Recommenadation aided Caching using Combinatorial Multi-armed Bandits</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pavamana K J, Chandramani Kishore Singh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We study content caching with recommendations in a wireless network where the users are connected through a base station equipped with a finite-capacity cache. We assume a fixed set of contents with unknown user preferences and content popularities. The base station can cache a subset of the contents and can also recommend subsets of the contents to different users in order to encourage them to request the recommended contents. Recommendations, depending on their acceptability, can thus be used to increase cache hits. We first assume that the users' recommendation acceptabilities are known and formulate the cache hit optimization problem as a combinatorial multi-armed bandit (CMAB). We propose a UCB-based algorithm to decide which contents to cache and recommend and provide an upper bound on the regret of this algorithm. Subsequently, we consider a more general scenario where the users' recommendation acceptabilities are also unknown and propose another UCB-based algorithm that learns these as well. We numerically demonstrate the performance of our algorithms and compare these to state-of-the-art algorithms.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-27T14:55:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.IR</span><span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2405.00080v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.00080v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 SIC-free Multicast Scheduling for Multi-antenna Coded Caching</h2>
                <div class="authors">
                    <strong>Authors:</strong> MohammadJavad Sojdeh, MohammadJavad Salehi, Antti Tölli
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multi-antenna coded caching (CC) with multicast beamforming typically relies on a complex successive interference cancellation (SIC) structure to decode a superposition of multiple streams received by each user. Signal-level CC schemes require the regeneration and cancellation of interfering signals at the physical layer of each receiver, which complicates practical implementations. To address this, we propose a bit-level multicast scheduling scheme enabling linear, SIC-free decoding of parallel streams by repeatedly transmitting data terms with linearly independent coefficients. Two reference strategies and a novel sparse strategy are considered for constructing the coefficient matrix. The reference cases include the random strategy, which lacks control over matrix construction, and the equal-distant strategy, which balances users' interference and data terms equally. In contrast, the sparse strategy minimizes the number of multicast streams transmitted in parallel during each interval. This approach simplifies both the decoding process and the beamforming design by decoupling the desired data terms for each user and reducing the number of SINR constraints, respectively. To further enhance the symmetric rate, a successive projection algorithm is applied to exploit channel properties and optimize user ordering. With the coefficient matrix and optimized user ordering in place, multicast beamformers are devised to aggregate desired data from relevant multicast streams. Numerical simulations validate the effectiveness of the sparse strategy and user scheduling, demonstrating significant gains in symmetric rate.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-27T14:37:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.11126v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.11126v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 Random Reshuffling for Stochastic Gradient Langevin Dynamics</h2>
                <div class="authors">
                    <strong>Authors:</strong> Luke Shaw, Peter A. Whalley
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We examine the use of different randomisation policies for stochastic gradient algorithms used in sampling, based on first-order (or overdamped) Langevin dynamics, the most popular of which is known as Stochastic Gradient Langevin Dynamics. Conventionally, this algorithm is combined with a specific stochastic gradient strategy, called Robbins-Monro. In this work, we study an alternative strategy, Random Reshuffling, and show convincingly that it leads to improved performance via: a) a proof of reduced bias in the Wasserstein metric for strongly convex, gradient Lipschitz potentials; b) an analytical demonstration of reduced bias for a Gaussian model problem; and c) an empirical demonstration of reduced bias in numerical experiments for some logistic regression problems. This is especially important since Random Reshuffling is typically more efficient due to memory access and cache reasons. Such acceleration for the Random Reshuffling policy is familiar from the optimisation literature on stochastic gradient descent.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-27T13:53:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.NA</span><span>cs.NA</span><span>math.PR</span><span>stat.ML</span><span>65C05, 82C31, 62F15</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.16055v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.16055v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language
  Models Quantization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mengzhao Chen, Yi Liu, Jiahao Wang, Yi Bin, Wenqi Shao, Ping Luo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Existing weight-activation quantization methods for Large Language Models (LLMs) primarily address channel-wise outliers but often neglect token-wise outliers, which limits the accuracy of quantized models. In this work, we propose PrefixQuant, a novel quantization method that achieves state-of-the-art performance across various precision levels (W4A4KV4 and W4A8KV4) and granularities (dynamic and static quantization) by effectively isolating token-wise outliers. First, PrefixQuant eliminates token-wise outliers by prefixing outlier tokens in the KV cache, a process that is training-free and highly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant introduces new trainable parameters for block-wise training to compensate for quantization error. Our experiments show that PrefixQuant significantly outperforms existing dynamic quantization methods, even under coarser static quantization settings. For instance, PrefixQuant achieves an average accuracy improvement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on five zero-shot reasoning tasks under dynamic and static quantization settings, respectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x prefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant. Our code is available at https://github.com/ChenMnZ/PrefixQuant.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-27T13:39:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.05265v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.05265v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 Dynamic Content Caching with Waiting Costs via Restless Multi-Armed
  Bandits</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ankita Koley, Chandramani Singh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We consider a system with a local cache connected to a backend server and an end user population. A set of contents are stored at the the server where they continuously get updated. The local cache keeps copies, potentially stale, of a subset of the contents. The users make content requests to the local cache which either can serve the local version if available or can fetch a fresh version or can wait for additional requests before fetching and serving a fresh version. Serving a stale version of a content incurs an age-of-version(AoV) dependent ageing cost, fetching it from the server incurs a fetching cost, and making a request wait incurs a per unit time waiting cost. We focus on the optimal actions subject to the cache capacity constraint at each decision epoch, aiming at minimizing the long term average cost. We pose the problem as a Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based policy which is known to be asymptotically optimal. We explicitly characterize the Whittle indices. We numerically evaluate the proposed policy and also compare it to a greedy policy. We show that it is close to the optimal policy and substantially outperforms the exising policies.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-27T06:47:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.18627v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.18627v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 Online Allocation with Multi-Class Arrivals: Group Fairness vs
  Individual Welfare</h2>
                <div class="authors">
                    <strong>Authors:</strong> Faraz Zargari, Hossein Nekouyan Jazi, Bo Sun, Xiaoqi Tan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce and study a multi-class online resource allocation problem with group fairness guarantees. The problem involves allocating a fixed amount of resources to a sequence of agents, each belonging to a specific group. The primary objective is to ensure fairness across different groups in an online setting. We focus on three fairness notions: one based on quantity and two based on utility. To achieve fair allocations, we develop two threshold-based online algorithms, proving their optimality under two fairness notions and near-optimality for the more challenging one. Additionally, we demonstrate a fundamental trade-off between group fairness and individual welfare using a novel representative function-based approach. To address this trade-off, we propose a set-aside multi-threshold algorithm that reserves a portion of the resource to ensure fairness across groups while utilizing the remaining resource to optimize efficiency under utility-based fairness notions. This algorithm is proven to achieve the Pareto-optimal trade-off. We also demonstrate that our problem can model a wide range of real-world applications, including network caching and cloud computing, and empirically evaluate our proposed algorithms in the network caching problem using real datasets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-27T05:02:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.GT</span><span>cs.DS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.15782v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.15782v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language
  Model Born from Transformer</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lin Yueyu, Li Zhiyuan, Peter Yue, Liu Xiao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As is known, hybrid quadratic and subquadratic attention models in multi-head architectures have surpassed both Transformer and Linear RNN models , with these works primarily focusing on reducing KV complexity and improving efficiency. For further research on expressiveness, we introduce our series of models distilled from Qwen 2.5, based on pure native RWKV-7 attention, which aims to make RNN more expressive and demonstrates state tracking ability beyond transformers. We work with QRWK 32B based on RWKV-6 architecture, another approach that reduces the entire knowledge processing time to just 8 hours using 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the distillation process can utilize any LLM, not just Qwen, and enables knowledge transfer from larger LLMs to smaller ones with more fewer tokens. We will explain the detailed process and share our insights on building more powerful foundation models. Please note that this is an ongoing work that will be updated continuously. The model checkpoints and source code are available at \href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside}, \href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-26T15:56:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.15570v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.15570v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 Query-based versus resource-based cache strategies in tag-based browsing
  systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Joaquín Gayoso-Cabada, Mercedes Gómez-Albarrán, José-Luis Sierra
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Tag-based browsing is a popular interaction model for navigating digital libraries. According to this model, users select descriptive tags to filter resources in the collections. Typical implementations of the model are based on inverted indexes. However, these implementations can require a considerable amount of set operations to update the browsing state. To palliate this inconven-ience, it is possible to adopt suitable cache strategies. In this paper we describe and compare two of these strategies: (i) a query-based strategy, according to which previously computed browsing states are indexed by sets of selected tags; and (ii) a resource-based strategy, according to which browsing states are in-dexed by sets of filtered resources. Our comparison focused on runtime perfor-mance, and was carried out empirically, using a real-world web-based collec-tion in the field of digital humanities. The results obtained show that the re-source-based strategy clearly outperforms the query-based one.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-26T11:01:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1007/978-3-030-04257-8_4' target='_blank'>doi</a><a href='http://arxiv.org/abs/2501.15481v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.15481v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for
  Efficient LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, S. Kevin Zhou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models have excelled in various domains but face efficiency challenges due to the growing Key-Value (KV) cache required for long-sequence inference. Recent efforts aim to reduce KV cache size by evicting vast non-critical cache elements during runtime while preserving generation quality. However, these methods typically allocate compression budgets uniformly across all attention heads, ignoring the unique attention patterns of each head. In this paper, we establish a theoretical loss upper bound between pre- and post-eviction attention output, explaining the optimization target of prior cache eviction methods, while guiding the optimization of adaptive budget allocation. Base on this, we propose {\it Ada-KV}, the first head-wise adaptive budget allocation strategy. It offers plug-and-play benefits, enabling seamless integration with prior cache eviction methods. Extensive evaluations on 13 datasets from Ruler and 16 datasets from LongBench, all conducted under both question-aware and question-agnostic scenarios, demonstrate substantial quality improvements over existing methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-26T07:29:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.11550v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.11550v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 Collaborative Coded Caching for Partially Connected Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kagan Akcay, Eleftherios Lampiris, MohammadJavad Salehi, Giuseppe Caire
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Coded caching leverages the differences in user cache memories to achieve gains that scale with the total cache size, alleviating network congestion due to high-quality content requests. Additionally, distributing transmitters over a wide area can mitigate the adverse effects of path loss. In this work, we consider a partially connected network where the channel between distributed transmitters (helpers) and users is modeled as a distributed MIMO Gaussian broadcast channel. We propose a novel delivery scheme consisting of two phases: partitioning and transmission. In the partitioning phase, users with identical cache profiles are partitioned into the minimum number of sets, such that users within each set can successfully decode their desired message from a joint transmission enabled by MIMO precoding. To optimally partition the users, we employ the branch and bound method. In the transmission phase, each partition is treated as a single entity, and codewords are multicast to partitions with distinct cache profiles. The proposed delivery scheme is applicable to any partially connected network, and while the partitioning is optimal, the overall delivery scheme, including transmission, is heuristic. Interestingly, simulation results show that its performance closely approximates that of the fully connected optimal solution.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-26T01:43:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.13298v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.13298v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 ReInc: Scaling Training of Dynamic Graph Neural Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mingyu Guan, Saumia Singhal, Taesoo Kim, Anand Padmanabha Iyer
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Dynamic Graph Neural Networks (DGNNs) have gained widespread attention due to their applicability in diverse domains such as traffic network prediction, epidemiological forecasting, and social network analysis. In this paper, we present ReInc, a system designed to enable efficient and scalable training of DGNNs on large-scale graphs. ReInc introduces key innovations that capitalize on the unique combination of Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs) inherent in DGNNs. By reusing intermediate results and incrementally computing aggregations across consecutive graph snapshots, ReInc significantly enhances computational efficiency. To support these optimizations, ReInc incorporates a novel two-level caching mechanism with a specialized caching policy aligned to the DGNN execution workflow. Additionally, ReInc addresses the challenges of managing structural and temporal dependencies in dynamic graphs through a new distributed training strategy. This approach eliminates communication overheads associated with accessing remote features and redistributing intermediate results. Experimental results demonstrate that ReInc achieves up to an order of magnitude speedup compared to state-of-the-art frameworks, tested across various dynamic GNN architectures and real-world graph datasets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-25T23:16:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.15348v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.15348v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle</h2>
                <div class="authors">
                    <strong>Authors:</strong> KVS Chaithanya, Sumesh P. Thampi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Understanding the hydrodynamics of microswimmers in viscoelastic fluids and confined environments is crucial for interpreting their behaviour in natural settings and designing synthetic microswimmers for practical applications like cargo transport. In this study, we explore the hydrodynamics of a concentric active compound particle - a model microswimmer (a squirmer) positioned at the centre of a viscoelastic fluid droplet (a model cargo) suspended in another viscoelastic medium. We consider the Oldroyd-B constitutive model to characterize the fluids and employ a perturbative approach in the Deborah number to analyze viscoelastic effects analytically, assuming a small Capillary number so that the droplet remains spherical and does not deform. We examine three cases: (i) a squirmer confined within a viscoelastic fluid droplet suspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian fluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined within a viscoelastic fluid droplet suspended in another viscoelastic fluid. Our findings reveal that the swimming speeds of the squirmer and the droplet are determined by the complex interplay of viscoelasticity, the size ratio of the droplet to the squirmer (confinement strength), and the viscosity ratio of the surrounding fluid to the droplet fluid. A critical aspect of this interaction is the positioning of stagnation points within the fluid flow, which governs the distribution of polymeric stress. This distribution, in turn, plays a crucial role in determining the influence of viscoelasticity on the squirmer's dynamics. Our analysis suggests that viscoelastic effects can either enhance or hinder the swimming speed of the squirmer when confined in a droplet, depending on the specific configuration of the system.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-25T12:17:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.flu-dyn</span><span>cond-mat.soft</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.09479v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.09479v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 The Selection Problem in Multi-Query Optimization: a Comprehensive
  Survey</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sergey Zinchenko, Denis Ponomaryov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> View materialization, index selection, and plan caching are well-known techniques for optimization of query processing in database systems. The essence of these tasks is to select and save a subset of the most useful candidates (views/indexes/plans) for reuse within given space/time budget constraints. In this paper, we propose a unified view on these selection problems. We make a detailed analysis of the root causes of their complexity and summarize techniques to address them. Our survey provides a modern classification of selection algorithms known in the literature, including the latest ones based on Machine Learning. We provide a ground for reuse of the selection techniques between different optimization scenarios and highlight challenges and promising directions in the field. Based on our analysis we derive a method to exponentially accelerate some of the state-of-the-art selection algorithms.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-25T10:38:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.DM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.11828v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.11828v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 Fully-Automated Code Generation for Efficient Computation of Sparse
  Matrix Permanents on GPUs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Deniz Elbek, Kamer Kaya
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Registers are the fastest memory components within the GPU's complex memory hierarchy, accessed by names rather than addresses. They are managed entirely by the compiler through a process called register allocation, during which the compiler attempts to cache predictable data from thread-local memory into thread-private registers. Computing the permanent of a sparse matrix poses a challenge for compilers, as optimizing this process is hindered by the unpredictable distribution of nonzero elements, which only become known at runtime. In this work, we employ fully-automated code generation to address this, producing highly optimized kernels tailored to the matrix's sparsity pattern. State-of-the-art permanent computation algorithms require each thread to store a private array, denoted x, of size n. We first propose a technique that fully stores these arrays in registers, with inclusion and exclusion kernels generated for each column. To minimize control divergence and reduce the number of unique kernels within a warp, we exploit the internal structure of Gray codes, which are also used in the state-of-the-art algorithm. Our second technique reduces register pressure by utilizing both registers and global memory and introduces a matrix ordering and partitioning strategy for greater efficiency. On synthetic matrices, this approach achieves a 31x speedup over state-of-the-art CPU implementations on 112 cores, and an 8x speedup compared to our traditional GPU implementation. For real-world matrices, these speedups are 24.9x and 4.9x.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-25T08:27:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.DM</span><span>cs.NA</span><span>math.NA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.15126v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.15126v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation
  of Attention Heads</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xingyang He, Jie Liu, Shaowei Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> KV cache is a widely used acceleration technique for large language models (LLMs) inference. However, its memory requirement grows rapidly with input length. Previous studies have reduced the size of KV cache by either removing the same number of unimportant tokens for all attention heads or by allocating differentiated KV cache budgets for pre-identified attention heads. However, due to the importance of attention heads varies across different tasks, the pre-identified attention heads fail to adapt effectively to various downstream tasks. To address this issue, we propose Task-KV, a method that leverages the semantic differentiation of attention heads to allocate differentiated KV cache budgets across various tasks. We demonstrate that attention heads far from the semantic center (called heterogeneous heads) make an significant contribution to task outputs and semantic understanding. In contrast, other attention heads play the role of aggregating important information and focusing reasoning. Task-KV allocates full KV cache budget to heterogeneous heads to preserve comprehensive semantic information, while reserving a small number of recent tokens and attention sinks for non-heterogeneous heads. Furthermore, we innovatively introduce middle activations to preserve key contextual information aggregated from non-heterogeneous heads. To dynamically perceive semantic differences among attention heads, we design a semantic separator to distinguish heterogeneous heads from non-heterogeneous ones based on their distances from the semantic center. Experimental results on multiple benchmarks and different model architectures demonstrate that Task-KV significantly outperforms existing baseline methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-25T07:28:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.15113v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.15113v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 A New Construction Structure on Coded Caching with Linear
  Subpacketization: Non-Half-Sum Disjoint Packing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Minquan Cheng, Huimei Wei, Kai Wan, Giuseppe Caire
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Coded caching is a promising technique to effectively reduce peak traffic by using local caches and the multicast gains generated by these local caches. We prefer to design a coded caching scheme with the subpacketization $F$ and transmission load $R$ as small as possible since these are the key metrics for evaluating the implementation complexity and transmission efficiency of the scheme, respectively. However, most of the existing coded caching schemes have large subpacketizations which grow exponentially with the number of users $K$, and there are a few schemes with linear subpacketizations which have large transmission loads. In this paper, we focus on studying the linear subpacketization, i.e., $K=F$, coded caching scheme with low transmission load. Specifically, we first introduce a new combinatorial structure called non-half-sum disjoint packing (NHSDP) which can be used to generate a coded caching scheme with $K=F$. Then a class of new schemes is obtained by constructing NHSDP. Theoretical and numerical comparisons show that (i) compared to the existing schemes with linear subpacketization (to the number of users), the proposed scheme achieves a lower load; (ii) compared to some existing schemes with polynomial subpacketization, the proposed scheme can also achieve a lower load in some cases; (iii) compared to some existing schemes with exponential subpacketization, the proposed scheme has loads close to those of these schemes in some cases. Moreover, the new concept of NHSDP is closely related to the classical combinatorial structures such as cyclic difference packing (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash family (PHF). These connections indicate that NHSDP is an important combinatorial structure in the field of combinatorial design.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-25T04:21:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.11855v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.11855v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for
  Vision-Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zunhai Su, Wang Shen, Linge Li, Zhe Chen, Hanyu Wei, Huangqi Yu, Kehong Yuan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vision-language models (VLMs) show remarkable performance in multimodal tasks. However, excessively long multimodal inputs lead to oversized Key-Value (KV) caches, resulting in significant memory consumption and I/O bottlenecks. Previous KV quantization methods for Large Language Models (LLMs) may alleviate these issues but overlook the attention saliency differences of multimodal tokens, resulting in suboptimal performance. In this paper, we investigate the attention-aware token saliency patterns in VLM and propose AKVQ-VL. AKVQ-VL leverages the proposed Text-Salient Attention (TSA) and Pivot-Token-Salient Attention (PSA) patterns to adaptively allocate bit budgets. Moreover, achieving extremely low-bit quantization requires effectively addressing outliers in KV tensors. AKVQ-VL utilizes the Walsh-Hadamard transform (WHT) to construct outlier-free KV caches, thereby reducing quantization difficulty. Evaluations of 2-bit quantization on 12 long-context and multimodal tasks demonstrate that AKVQ-VL maintains or even improves accuracy, outperforming LLM-oriented methods. AKVQ-VL can reduce peak memory usage by 2.13x, support up to 3.25x larger batch sizes and 2.46x throughput.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-25T02:01:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.15021v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.15021v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via
  Outlier-Aware Adaptive Rotations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zunhai Su, Zhe Chen, Wang Shen, Hanyu Wei, Linge Li, Huangqi Yu, Kehong Yuan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Key-Value (KV) cache facilitates efficient large language models (LLMs) inference by avoiding recomputation of past KVs. As the batch size and context length increase, the oversized KV caches become a significant memory bottleneck, highlighting the need for efficient compression. Existing KV quantization rely on fine-grained quantization or the retention of a significant portion of high bit-widths caches, both of which compromise compression ratio and often fail to maintain robustness at extremely low average bit-widths. In this work, we explore the potential of rotation technique for 2-bit KV quantization and propose RotateKV, which achieves accurate and robust performance through the following innovations: (i) Outlier-Aware Rotation, which utilizes channel-reordering to adapt the rotations to varying channel-wise outlier distributions without sacrificing the computational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii) Pre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position embedding (RoPE) on proposed outlier-aware rotation and further smooths outliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages the massive activations to precisely identify and protect attention sinks. RotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit quantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning and long-context capabilities, with less than 1.7\% degradation on GSM8K, outperforming existing methods even at lower average bit-widths. RotateKV also showcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch sizes, and achieves a 2.32x speedup in decoding stage.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-25T01:45:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.16383v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.16383v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yifan Yu, Yu Gan, Lillian Tsai, Nikhil Sarda, Jiaming Shen, Yanqi Zhou, Arvind Krishnamurthy, Fan Lai, Henry M. Levy, David Culler
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have excelled in various applications, yet serving them at scale is challenging due to their substantial resource demands and high latency. Our real-world studies reveal that over 60% of user requests to LLMs have semantically similar counterparts, suggesting the potential for knowledge sharing among requests. However, naively caching and reusing past responses leads to large quality degradation. In this paper, we introduce EchoLM, an in-context caching system that leverages historical requests as examples to guide response generation, enabling selective offloading of requests to more efficient LLMs. However, enabling this real-time knowledge transfer leads to intricate tradeoffs between response quality, latency, and system throughput at scale. For a new request, EchoLM identifies similar, high-utility examples and efficiently prepends them to the input for better response. At scale, EchoLM adaptively routes requests to LLMs of varying capabilities, accounting for response quality and serving loads. EchoLM employs a cost-aware cache replay mechanism to improve example quality and coverage offline, maximizing cache utility and runtime efficiency. Evaluations on millions of open-source requests demonstrate that EchoLM has a throughput improvement of 1.4-5.9x while reducing latency by 28-71% without hurting response quality on average.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-24T19:13:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.12689v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.12689v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 Language-Queried Target Sound Extraction Without Parallel Training Data</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hao Ma, Zhiyuan Peng, Xu Li, Yukai Li, Mingjie Shao, Qiuqiang Kong, Ju Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Language-queried target sound extraction (TSE) aims to extract specific sounds from mixtures based on language queries. Traditional fully-supervised training schemes require extensively annotated parallel audio-text data, which are labor-intensive. We introduce a parallel-data-free training scheme, requiring only unlabelled audio clips for TSE model training by utilizing the contrastive language-audio pre-trained model (CLAP). In a vanilla parallel-data-free training stage, target audio is encoded using the pre-trained CLAP audio encoder to form a condition embedding, while during testing, user language queries are encoded by CLAP text encoder as the condition embedding. This vanilla approach assumes perfect alignment between text and audio embeddings, which is unrealistic. Two major challenges arise from training-testing mismatch: the persistent modality gap between text and audio and the risk of overfitting due to the exposure of rich acoustic details in target audio embedding during training. To address this, we propose a retrieval-augmented strategy. Specifically, we create an embedding cache using audio captions generated by a large language model (LLM). During training, target audio embeddings retrieve text embeddings from this cache to use as condition embeddings, ensuring consistent modalities between training and testing and eliminating information leakage. Extensive experiment results show that our retrieval-augmented approach achieves consistent and notable performance improvements over existing state-of-the-art with better generalizability.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-24T15:16:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.AS</span><span>cs.SD</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.09398v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.09398v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 A Programming Model for Disaggregated Memory over CXL</h2>
                <div class="authors">
                    <strong>Authors:</strong> Gal Assa, Lucas Bürgi, Michal Friedman, Ori Lahav
                </div>
                <div class="summary">
                    <strong>Summary:</strong> CXL (Compute Express Link) is an emerging open industry-standard interconnect between processing and memory devices that is expected to revolutionize the way systems are designed in the near future. It enables cache-coherent shared memory pools in a disaggregated fashion at unprecedented scales, allowing algorithms to interact with a variety of storage devices using simple loads and stores. Alongside unleashing unique opportunities for a wide range of applications, CXL introduces new challenges of data management and crash consistency. Alas, CXL lacks an adequate programming model, which makes reasoning about the correctness and expected behaviors of algorithms and systems on top of it nearly impossible.   In this work, we present CXL0, the first programming model for concurrent programs running on top of CXL. We propose a high-level abstraction for CXL memory accesses and formally define operational semantics on top of that abstraction. We perform initial measurements that provide practical insight into CXL0. We provide a set of general transformations that adapt concurrent algorithms to the new disruptive technology. These transformations enhance linearizable algorithms with durability under a general partial-failure model. We provide an additional transformation for algorithms designed for persistent main memory and full-system crashes. We believe that this work will serve as a stepping stone for systems design and modeling on top of CXL, and support the development of future models as software and hardware evolve.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-24T14:32:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.ET</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.16300v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.16300v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 Application-Aware Resource Allocation and Data Management for
  MEC-assisted IoT Service Providers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Simone Bolettieri, Raffaele Bruno, Enzo Mingozzi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> To support the growing demand for data-intensive and low-latency IoT applications, Multi-Access Edge Computing (MEC) is emerging as an effective edge-computing approach enabling the execution of delay-sensitive processing tasks close to end-users. However, most of the existing works on resource allocation and service placement in MEC systems overlook the unique characteristics of new IoT use cases. For instance, many IoT applications require the periodic execution of computing tasks on real-time data streams that originate from devices dispersed over a wide area. Thus, users requesting IoT services are typically distant from the data producers. To fill this gap, the contribution of this work is two-fold. Firstly, we propose a MEC-compliant architectural solution to support the operation of multiple IoT service providers over a common MEC platform deployment, which enables the steering and shaping of IoT data transport within the platform. Secondly, we model the problem of service placement and data management in the proposed MEC-based solution taking into account the dependencies at the data level between IoT services and sensing resources. Our model also considers that caches can be deployed on MEC hosts, to allow the sharing of the same data between different IoT services with overlapping geographical scope, and provides support for IoT services with heterogeneous QoS requirements, such as different frequencies of periodic task execution. Due to the complexity of the optimisation problem, a heuristic algorithm is proposed using linear relaxation and rounding techniques. Extensive simulation results demonstrate the efficiency of the proposed approach, especially when traffic demands generated by the service requests are not uniform.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-24T10:39:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1016/j.jnca.2021.103020' target='_blank'>doi</a><a href='http://arxiv.org/abs/2501.14387v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.14387v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 Joint System Latency and Data Freshness Optimization for Cache-enabled
  Mobile Crowdsensing Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kexin Shi, Yaru Fu, Yongna Guo, Fu Lee Wang, Yan Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Mobile crowdsensing (MCS) networks enable large-scale data collection by leveraging the ubiquity of mobile devices. However, frequent sensing and data transmission can lead to significant resource consumption. To mitigate this issue, edge caching has been proposed as a solution for storing recently collected data. Nonetheless, this approach may compromise data freshness. In this paper, we investigate the trade-off between re-using cached task results and re-sensing tasks in cache-enabled MCS networks, aiming to minimize system latency while maintaining information freshness. To this end, we formulate a weighted delay and age of information (AoI) minimization problem, jointly optimizing sensing decisions, user selection, channel selection, task allocation, and caching strategies. The problem is a mixed-integer non-convex programming problem which is intractable. Therefore, we decompose the long-term problem into sequential one-shot sub-problems and design a framework that optimizes system latency, task sensing decision, and caching strategy subproblems. When one task is re-sensing, the one-shot problem simplifies to the system latency minimization problem, which can be solved optimally. The task sensing decision is then made by comparing the system latency and AoI. Additionally, a Bayesian update strategy is developed to manage the cached task results. Building upon this framework, we propose a lightweight and time-efficient algorithm that makes real-time decisions for the long-term optimization problem. Extensive simulation results validate the effectiveness of our approach.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-24T10:00:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span><span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.14367v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.14367v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 Locality-aware Fair Scheduling in LLM Serving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shiyi Cao, Yichuan Wang, Ziming Mao, Pin-Lun Hsu, Liangsheng Yin, Tian Xia, Dacheng Li, Shu Liu, Yineng Zhang, Yang Zhou, Ying Sheng, Joseph Gonzalez, Ion Stoica
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language model (LLM) inference workload dominates a wide variety of modern AI applications, ranging from multi-turn conversation to document analysis. Balancing fairness and efficiency is critical for managing diverse client workloads with varying prefix patterns. Unfortunately, existing fair scheduling algorithms for LLM serving, such as Virtual Token Counter (VTC), fail to take prefix locality into consideration and thus suffer from poor performance. On the other hand, locality-aware scheduling algorithms in existing LLM serving frameworks tend to maximize the prefix cache hit rate without considering fair sharing among clients.   This paper introduces the first locality-aware fair scheduling algorithm, Deficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix locality with a fairness guarantee. We also introduce a novel algorithm, Double Deficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find a balance point among fairness, locality, and load-balancing. Our extensive evaluation demonstrates the superior performance of DLPM and D$^2$LPM in ensuring fairness while maintaining high throughput (up to 2.87$\times$ higher than VTC) and low per-client (up to 7.18$\times$ lower than state-of-the-art distributed LLM serving system) latency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-24T08:12:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.14312v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.14312v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement
  Learning-based Model Caching and Inference Offloading</h2>
                <div class="authors">
                    <strong>Authors:</strong> Minrui Xu, Dusit Niyato, Christopher G. Brinton
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) can perform zero-shot learning on unseen tasks and few-shot learning on complex reasoning tasks. However, resource-limited mobile edge networks struggle to support long-context LLM serving for LLM agents during multi-round interactions with users. Unlike stateless computation offloading and static service offloading in edge computing, optimizing LLM serving at edge servers is challenging because LLMs continuously learn from context which raises accuracy, latency, and resource consumption dynamics. In this paper, we propose a joint model caching and inference offloading framework that utilizes test-time deep reinforcement learning (T2DRL) to optimize deployment and execution strategies for long-context LLM serving. In this framework, we analyze the performance convergence and design an optimization problem considering the utilization of context windows in LLMs. Furthermore, the T2DRL algorithm can learn in both the training phase and the testing phase to proactively manage cached models and service requests and adapt to context changes and usage patterns during execution. To further enhance resource allocation efficiency, we propose a double Dutch auction (DDA) mechanism, which dynamically matches supply and demand while maximizing social welfare. Finally, experimental results demonstrate that the T2DRL algorithm can reduce system costs by at least 30% compared to baselines while guaranteeing the performance of LLM agents in real-world perception and reasoning tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-24T03:21:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.14205v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.14205v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 Sigma: Differential Rescaling of Query, Key and Value for Efficient
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhenghao Lin, Zihao Tang, Xiao Liu, Yeyun Gong, Yi Cheng, Qi Chen, Hang Li, Ying Xin, Ziyue Yang, Kailai Yang, Yu Yan, Xiao Liang, Shuai Lu, Yiming Huang, Zheheng Luo, Lei Qu, Xuan Feng, Yaoxiang Wang, Yuqing Xia, Feiyang Chen, Yuting Jiang, Yasen Hu, Hao Ni, Binyang Li, Guoshuai Zhao, Jui-Hao Chiang, Zhongxin Guo, Chen Lin, Kun Kuang, Wenjie Li, Yelong Shen, Jian Jiao, Peng Cheng, Mao Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce Sigma, an efficient large language model specialized for the system domain, empowered by a novel architecture including DiffQKV attention, and pre-trained on our meticulously collected system domain data. DiffQKV attention significantly enhances the inference efficiency of Sigma by optimizing the Query (Q), Key (K), and Value (V) components in the attention mechanism differentially, based on their varying impacts on the model performance and efficiency indicators. Specifically, we (1) conduct extensive experiments that demonstrate the model's varying sensitivity to the compression of K and V components, leading to the development of differentially compressed KV, and (2) propose augmented Q to expand the Q head dimension, which enhances the model's representation capacity with minimal impacts on the inference speed. Rigorous theoretical and empirical analyses reveal that DiffQKV attention significantly enhances efficiency, achieving up to a 33.36% improvement in inference speed over the conventional grouped-query attention (GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various sources, including 19.5B system domain data that we carefully collect and 1T tokens of synthesized and rewritten data. In general domains, Sigma achieves comparable performance to other state-of-arts models. In the system domain, we introduce the first comprehensive benchmark AIMicius, where Sigma demonstrates remarkable performance across all tasks, significantly outperforming GPT-4 with an absolute improvement up to 52.5%.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-23T12:58:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.13629v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.13629v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 Characterisation of the plutonium isotopic composition of a sediment
  core from Palomares, Spain, by low-energy AMS and alpha-spectrometry</h2>
                <div class="authors">
                    <strong>Authors:</strong> E. Chamizo, M. C. Jiménez-Ramos, S. M. Enamorado, M. García-León, R. García-Tenorio, J. L. Mas, P. Masqué, J. Merino, J. A. Sanchez-Cabeza
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The measurement of plutonium isotopes, 239Pu and 240Pu, at 670 kV on the compact accelerator mass spectrometry (AMS) system at the Centro Nacional de Aceleradores (CNA) in Seville, Spain, is now a reality. In this work, we present first Pu AMS results for environmental samples: a sediment core collected in a submarine canyon in the Mediterranean coast of the Spanish region of Palomares, affected by a nuclear accident in 1966. From the study of the 240Pu/239Pu atomic ratio profile, showing on average levels lower than 11%, we confirm that the weapon-grade plutonium released on land during the accident, with a characteristic 240Pu/239Pu atomic ratio of 5.8%, has found its way into the marine environment. A two-plutonium sources mixture model (Palomares and fallout) is used to elucidate the percentage of the plutonium coming from the accident. As a validation exercise of the Pu AMS measuring technique and in order to obtain the 238Pu/(239+240)Pu activity ratios, samples were also studied by alpha-spectrometry (AS). The obtained AS 239+240Pu activity concentration results fit in with the AMS ones in a wide dynamic range, thus validating the AMS technique.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-23T11:18:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.ins-det</span><span>physics.ao-ph</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1016/j.nimb.2009.10.151' target='_blank'>doi</a><a href='http://arxiv.org/abs/2501.13998v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.13998v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 POPS: From History to Mitigation of DNS Cache Poisoning Attacks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yehuda Afek, Harel Berger, Anat Bremler-Barr
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present a novel yet simple and comprehensive DNS cache POisoning Prevention System (POPS), designed to integrate as a module in Intrusion Prevention Systems (IPS). POPS addresses statistical DNS poisoning attacks, including those documented from 2002 to the present, and offers robust protection against similar future threats. It consists of two main components: a detection module that employs three simple rules, and a mitigation module that leverages the TC flag in the DNS header to enhance security. Once activated, the mitigation module has zero false positives or negatives, correcting any such errors on the side of the detection module.   We first analyze POPS against historical DNS services and attacks, showing that it would have mitigated all network-based statistical poisoning attacks, yielding a success rate of only 0.0076% for the adversary. We then simulate POPS on traffic benchmarks (PCAPs) incorporating current potential network-based statistical poisoning attacks, and benign PCAPs; the simulated attacks still succeed with a probability of 0.0076%. This occurs because five malicious packets go through before POPS detects the attack and activates the mitigation module. In addition, POPS completes its task using only 20%-50% of the time required by other tools (e.g., Suricata or Snort), and after examining just 5%-10% as many packets. Furthermore, it successfully identifies DNS cache poisoning attacks-such as fragmentation attacks-that both Suricata and Snort fail to detect, underscoring its superiority in providing comprehensive DNS protection.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-23T10:40:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.13540v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.13540v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 A Training-free Sub-quadratic Cost Transformer Model Serving Framework
  With Hierarchically Pruned Attention</h2>
                <div class="authors">
                    <strong>Authors:</strong> Heejun Lee, Geon Park, Youngwan Lee, Jaduk Suh, Jina Kim, Wonyoung Jeong, Bumsik Kim, Hyemin Lee, Myeongjae Jeon, Sung Ju Hwang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In modern large language models (LLMs), increasing the context length is crucial for improving comprehension and coherence in long-context, multi-modal, and retrieval-augmented language generation. While many recent transformer models attempt to extend their context length over a million tokens, they remain impractical due to the quadratic time and space complexities. Although recent works on linear and sparse attention mechanisms can achieve this goal, their real-world applicability is often limited by the need to re-train from scratch and significantly worse performance. In response, we propose a novel approach, Hierarchically Pruned Attention (HiP), which reduces the time complexity of the attention mechanism to $O(T \log T)$ and the space complexity to $O(T)$, where $T$ is the sequence length. We notice a pattern in the attention scores of pretrained LLMs where tokens close together tend to have similar scores, which we call ``attention locality''. Based on this observation, we utilize a novel tree-search-like algorithm that estimates the top-$k$ key tokens for a given query on the fly, which is mathematically guaranteed to have better performance than random attention pruning. In addition to improving the time complexity of the attention mechanism, we further optimize GPU memory usage by implementing KV cache offloading, which stores only $O(\log T)$ tokens on the GPU while maintaining similar decoding throughput. Experiments on benchmarks show that HiP, with its training-free nature, significantly reduces both prefill and decoding latencies, as well as memory usage, while maintaining high-quality generation with minimal degradation. HiP enables pretrained LLMs to scale up to millions of tokens on commodity GPUs, potentially unlocking long-context LLM applications previously deemed infeasible.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-23T07:25:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.CV</span><span>cs.DC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.09827v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.09827v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 Parallel Key-Value Cache Fusion for Position Invariant RAG</h2>
                <div class="authors">
                    <strong>Authors:</strong> Philhoon Oh, Jinwoo Shin, James Thorne
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in Large Language Models (LLMs) underscore the necessity of Retrieval Augmented Generation (RAG) to leverage external information. However, LLMs are sensitive to the position of relevant information within contexts and tend to generate incorrect responses when such information is placed in the middle, known as `Lost in the Middle' phenomenon. In this paper, we introduce a framework that generates consistent outputs for decoder-only models, irrespective of the input context order. Experimental results for three open domain question answering tasks demonstrate position invariance, where the model is not sensitive to input context order, and superior robustness to irrelevent passages compared to prevailing approaches for RAG pipelines.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-23T06:48:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.07523v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.07523v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 Qrazor: Reliable and effortless 4-bit llm quantization by significant
  data razoring</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dongyoung Lee, Seungkyu Choi, Ik Joon Chang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large-scale language models (LLMs) have demonstrated outstanding performance in language processing tasks, yet their deployment is often hindered by high memory demands and computational complexity. Although low-bit quantization techniques, such as 4-bit quantization, present a potential solution, they frequently lead to significant accuracy degradation or require substantial effort for such aggressive quantization approaches. To overcome these challenges, we introduce QRazor, a reliable and effortless quantization scheme designed to enable 4-bit quantization for weights, activations, and KV cache in transformer-based LLMs. The scheme involves two main stages: quantization and compression. During the quantization stage, weights, activations, and KV cache values are quantized with wider 8 or 16-bit integers as a basis to achieve nearly identical accuracy to the original full-precision LLM models, using the absolute max scaling. Subsequently, all data are compressed to 4-bit using our proposed significant data razoring (SDR) technique, which retains only the four most salient bits while discarding the others. Furthermore, we present an integer-based arithmetic unit dedicated to QRazor, enabling direct low-precision arithmetic operations without decompressing the SDR data. Despite the reduced quantization effort, QRazor achieves LLM accuracies better or comparable to state-of-the-art 4-bit methods. By also validating the hardware efficiency, our decompression-free arithmetic unit achieves 61.2% and 57.8% reduction in area and power consumption, respectively.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-23T02:20:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.13331v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.13331v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 Personalized Federated Learning for Cellular VR: Online Learning and
  Dynamic Caching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Krishnendu S. Tharakan, Hayssam Dahrouj, Nour Kouzayha, Hesham ElSawy, Tareq Y. Al-Naffouri
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Delivering an immersive experience to virtual reality (VR) users through wireless connectivity offers the freedom to engage from anywhere at any time. Nevertheless, it is challenging to ensure seamless wireless connectivity that delivers real-time and high-quality videos to the VR users. This paper proposes a field of view (FoV) aware caching for mobile edge computing (MEC)-enabled wireless VR network. In particular, the FoV of each VR user is cached/prefetched at the base stations (BSs) based on the caching strategies tailored to each BS. Specifically, decentralized and personalized federated learning (DP-FL) based caching strategies with guarantees are presented. Considering VR systems composed of multiple VR devices and BSs, a DP-FL caching algorithm is implemented at each BS to personalize content delivery for VR users. The utilized DP-FL algorithm guarantees a probably approximately correct (PAC) bound on the conditional average cache hit. Further, to reduce the cost of communicating gradients, one-bit quantization of the stochastic gradient descent (OBSGD) is proposed, and a convergence guarantee of $\mathcal{O}(1/\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is the number of iterations. Additionally, to better account for the wireless channel dynamics, the FoVs are grouped into multicast or unicast groups based on the number of requesting VR users. The performance of the proposed DP-FL algorithm is validated through realistic VR head-tracking dataset, and the proposed algorithm is shown to have better performance in terms of average delay and cache hit as compared to baseline algorithms.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-22T16:25:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>cs.LG</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.11745v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.11745v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 Efficient Prompt Compression with Evaluator Heads for Long-Context
  Transformer Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weizhi Fei, Xueyan Niu, Guoqing Xie, Yingqing Liu, Bo Bai, Wei Han
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Although applications involving long-context inputs are crucial for the effective utilization of large language models (LLMs), they also result in increased computational costs and reduced performance. To address this challenge, we propose an efficient, training-free prompt compression method that retains key information within compressed prompts. We identify specific attention heads in transformer-based LLMs, which we designate as evaluator heads, that are capable of selecting tokens in long inputs that are most significant for inference. Building on this discovery, we develop EHPC, an Evaluator Head-based Prompt Compression method, which enables LLMs to rapidly "skim through" input prompts by leveraging only the first few layers with evaluator heads during the pre-filling stage, subsequently passing only the important tokens to the model for inference. EHPC achieves state-of-the-art results across two mainstream benchmarks: prompt compression and long-context inference acceleration. Consequently, it effectively reduces the complexity and costs associated with commercial API calls. We further demonstrate that EHPC attains competitive results compared to key-value cache-based acceleration methods, thereby highlighting its potential to enhance the efficiency of LLMs for long-context tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-22T15:33:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.12959v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.12959v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 Yi-Lightning Technical Report</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alan Wake, Bei Chen, C. X. Lv, Chao Li, Chengen Huang, Chenglin Cai, Chujie Zheng, Daniel Cooper, Fan Zhou, Feng Hu, Ge Zhang, Guoyin Wang, Heng Ji, Howard Qiu, Jiangcheng Zhu, Jun Tian, Katherine Su, Lihuan Zhang, Liying Li, Ming Song, Mou Li, Peng Liu, Qicheng Hu, Shawn Wang, Shijun Zhou, Shiming Yang, Shiyong Li, Tianhang Zhu, Wen Xie, Wenhao Huang, Xiang He, Xiaobo Chen, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Yanpeng Li, Yongke Zhao, Yongzhen Luo, Yuchi Xu, Yuxuan Sha, Zhaodong Yan, Zhiyuan Liu, Zirui Zhang, Zonghong Dai
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This technical report presents Yi-Lightning, our latest flagship large language model (LLM). It achieves exceptional performance, ranking 6th overall on Chatbot Arena, with particularly strong results (2nd to 4th place) in specialized categories including Chinese, Math, Coding, and Hard Prompts. Yi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture, featuring advanced expert segmentation and routing mechanisms coupled with optimized KV-caching techniques. Our development process encompasses comprehensive pre-training, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF), where we devise deliberate strategies for multi-stage training, synthetic data construction, and reward modeling. Furthermore, we implement RAISE (Responsible AI Safety Engine), a four-component framework to address safety issues across pre-training, post-training, and serving phases. Empowered by our scalable super-computing infrastructure, all these innovations substantially reduce training, deployment and inference costs while maintaining high-performance standards. With further evaluations on public academic benchmarks, Yi-Lightning demonstrates competitive performance against top-tier LLMs, while we observe a notable disparity between traditional, static benchmark results and real-world, dynamic human preferences. This observation prompts a critical reassessment of conventional benchmarks' utility in guiding the development of more intelligent and powerful AI systems for practical applications. Yi-Lightning is now available through our developer platform at https://platform.lingyiwanwu.com.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-22T15:09:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.01253v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.01253v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic
  Wrap-Around</h2>
                <div class="authors">
                    <strong>Authors:</strong> Elizabath Peter, K. K. Krishnan Namboodiri, B. Sundar Rajan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This work explores a multiple transmit antenna setting in a multi-access coded caching (MACC) network where each user accesses more than one cache. A MACC network has $K$ users and $K$ caches, and each user has access to $r < K$ consecutive caches in a cyclic wrap-around manner. There are $L$ antennas at the server, and each cache has a normalized size of $M/N \leq 1$. The cyclic wrap-around MACC network with a single antenna at the server has been a well-investigated topic, and several coded caching schemes and improved lower bounds on the performance are known for the same. However, this MACC network has not yet been studied under multi-antenna settings in the coded caching literature. We study the multi-antenna MACC problem and propose a solution for the same by constructing a pair of arrays called caching and delivery arrays. We present three constructions of caching and delivery arrays for different scenarios and obtain corresponding multi-antenna MACC schemes for the same. Two schemes resulting from the above constructions achieve optimal performance under uncoded placement and one-shot delivery. The optimality is shown by matching the performance of the multi-antenna MACC scheme to that of an optimal multi-antenna scheme for a dedicated cache network having an identical number of users, and each user has a normalized cache size of $rM/N$. Further, as a special case, one of the proposed schemes subsumes an existing optimal MACC scheme for the single-antenna setting.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-22T15:05:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2310.08894v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2310.08894v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 Not all tokens are created equal: Perplexity Attention Weighted Networks
  for AI generated text detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pablo Miralles-González, Javier Huertas-Tato, Alejandro Martín, David Camacho
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid advancement in large language models (LLMs) has significantly enhanced their ability to generate coherent and contextually relevant text, raising concerns about the misuse of AI-generated content and making it critical to detect it. However, the task remains challenging, particularly in unseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution outputs offers a theoretically appealing approach for detection, as they encapsulate insights from the models' extensive pre-training on diverse corpora. Despite its promise, zero-shot methods that attempt to operationalize these outputs have met with limited success. We hypothesize that one of the problems is that they use the mean to aggregate next-token distribution metrics across tokens, when some tokens are naturally easier or harder to predict and should be weighted differently. Based on this idea, we propose the Perplexity Attention Weighted Network (PAWN), which uses the last hidden states of the LLM and positions to weight the sum of a series of features based on metrics from the next-token distribution across the sequence length. Although not zero-shot, our method allows us to cache the last hidden states and next-token distribution metrics on disk, greatly reducing the training resource requirements. PAWN shows competitive and even better performance in-distribution than the strongest baselines (fine-tuned LMs) with a fraction of their trainable parameters. Our model also generalizes better to unseen domains and source models, with smaller variability in the decision boundary across distribution shifts. It is also more robust to adversarial attacks, and if the backbone has multilingual capabilities, it presents decent generalization to languages not seen during supervised training, with LLaMA3-1B reaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine languages.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-22T10:39:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.03940v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.03940v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 Bright single-photon source in a silicon chip by nanoscale positioning
  of a color center in a microcavity</h2>
                <div class="authors">
                    <strong>Authors:</strong> Baptiste Lefaucher, Yoann Baron, Jean-Baptiste Jager, Vincent Calvo, Christian Elsässer, Giuliano Coppola, Frédéric Mazen, Sébastien Kerdilès, Félix Cache, Anaïs Dréau, Jean-Michel Gérard
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present an all-silicon source of near-infrared linearly-polarized single photons, fabricated by nanoscale positioning of a color center in a silicon-on-insulator microcavity. The color center consists of a single W center, created at a well-defined position by Si$^{+}$ ion implantation through a 150 nm-diameter nanohole in a mask. A circular Bragg grating cavity resonant with the W's zero-phonon line at 1217 nm is fabricated at the same location as the nanohole. Under above-gap continuous-wave excitation, a very clean photon antibunching behavior ($g{^2} \leq 0.06$) is observed over the entire power range, which highlights the absence of parasitic emitters. Purcell-enhancement of W's zero-phonon emission provides both a record-high photoluminescence count rate among Si color centers (ca $1.2 \times 10^{6}$ counts/s) and apparent Debye-Waller factor around 99%. We also demonstrate the triggered emission of single photons with 93% purity under weak pulsed laser excitation. At high pulsed laser power, we reveal a detrimental effect of repumping processes, that could be mitigated using selective pumping schemes in the future. These results represent a major step towards on-demand sources of indistinguishable near-infrared single photons within silicon photonics chips.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-22T09:25:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.optics</span><span>quant-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.12744v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.12744v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 Improved Coded Caching Scheme for Multi-User Information Retrieval
  System</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junyi Wang, Quan Zang, Jinyu Wang, Minquan Cheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper, we study the coded caching scheme for the $(L, K, M, N)$ multi-user information retrieval (MIR) system, which consists of a content library containing $N$ files, a base station (BS) with $L$ antennas that cannot access the library, and $K$ single-antenna users, each of which can cache at most $M$ files from the library. The users communicate with the others assisted by the BS to decode their required files. In this paper, we focus on designing a coded caching scheme with low communication latency measured by normalized delivery time (NDT), computational complexity, and subpacketizations. When $\frac{KM}{N}\geq L$ we first simply the precoding matrix in the downlink step to an identity matrix and use the multiple-antenna placement delivery array (MAPDA), which was originally proposed for the multiple-input single-output networks, to generate several new schemes for MIR system. Compared to the existing schemes, both the theoretical and numerical analyses show that our new schemes achieve much lower computational complexity and smaller subpacketizations with the same NDT.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-21T22:33:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.12528v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.12528v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and
  Multiple Level Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weile Luo, Ruibo Fan, Zeyu Li, Dayou Du, Hongyuan Liu, Qiang Wang, Xiaowen Chu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modern GPUs, with their specialized hardware like tensor cores, are essential for demanding AI and deep learning applications. This study presents a comprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU architecture, delving into its performance characteristics and novel features. We benchmark Hopper's memory subsystem latency and throughput, comparing its L2 partitioned cache behavior and global memory access patterns against recent GPU generations, Ampere and Ada Lovelace. Our analysis reveals significant performance differences and architectural improvements in Hopper. A core contribution of this work is a detailed evaluation of Hopper's fourth-generation tensor cores, including their FP8 precision support and the novel asynchronous wgmma instructions, assessing their impact on matrix multiply-accumulate operations. We further investigate the performance implications of other key Hopper innovations: DPX instructions for accelerating dynamic programming algorithms, distributed shared memory (DSM) for inter-SM communication, and the Tensor Memory Accelerator (TMA) for asynchronous data movement. This multi-level approach encompasses instruction-level microbenchmarks, library-level analysis of the Transformer Engine, and application-level benchmarks of tensor core performance within large language models. Our findings provide valuable, in-depth insights for software developers seeking to optimize performance and develop accurate performance models for the Hopper architecture, ultimately contributing to a deeper understanding of its potential for accelerating AI and other computationally intensive workloads.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-21T12:19:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AR</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.12084v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.12084v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 Build Optimization: A Systematic Literature Review</h2>
                <div class="authors">
                    <strong>Authors:</strong> Henri Aïdasso, Mohammed Sayagh, Francis Bordeleau
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Continuous Integration (CI) consists of an automated build process involving continuous compilation, testing, and packaging of the software system. While CI comes up with several advantages related to quality and time to delivery, CI also presents several challenges addressed by a large body of research. To better understand the literature so as to help practitioners find solutions for their problems and guide future research, we conduct a systematic review of 97 studies on build optimization published between 2006 and 2024, which we summarized according to their goals, methodologies, used datasets, and leveraged metrics. The identified build optimization studies focus on two main challenges: (1) long build durations, and (2) build failures. To meet the first challenge, existing studies have developed a range of techniques, including predicting build outcome and duration, selective build execution, and build acceleration using caching or repairing performance smells. The causes of build failures have been the subject of several studies, leading to the development of techniques for predicting build script maintenance and automating repair. Recent studies have also focused on predicting flaky build failures caused by environmental issues. The majority of these techniques use machine learning algorithms and leverage build metrics, which we classify into five categories. Additionally, we identify eight publicly available build datasets for build optimization research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-21T07:32:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.11940v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.11940v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 PDA Construction via Union of Cartesian Product Cache Configurations for
  Coded Caching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jinyu Wang, Minquan Cheng, Kai Wan, Giuseppe Caire
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Caching is an efficient technique to reduce peak traffic by storing popular content in local caches. Placement delivery array (PDA) proposed by Yan et al. is a combinatorial structure to design coded caching schemes with uncoded placement and one-shot linear delivery. By taking the $m$-fold Cartesian product of a small base PDA, Wang et al. constructed a big PDA while maintaining the memory ratio and transmission load unchanged, which achieves linear growth in both the number of users and coded caching gain. In order to achieve exponential growth in both the number of users and coded caching gain, in this paper we propose a PDA construction by taking the union operation of the cache configurations from the $m$-fold Cartesian product of a base PDA. The resulting PDA leads to a coded caching scheme with subpacketization increasing sub-exponentially with the number of users while keeping the load constant for fixed memory ratio. By applying the proposed construction to existing base PDAs, three new coded caching schemes are obtained, which cover some existing schemes as special cases and can achieve lower load with simultaneously lower subpacketization for some memory ratios.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-21T02:35:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.11834v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.11834v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pouya Hamadanian, Sadjad Fouladi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLM) have revolutionized natural language processing, but their inference demands substantial resources, while under-utilizing high-end accelerators like GPUs. A major bottleneck arises from the attention mechanism, which requires storing large key-value caches, limiting the maximum achievable throughput way below the available computing resources. Current approaches attempt to mitigate this issue through memory-efficient attention and paging mechanisms, but remained constrained by the assumption that all operations must be performed on high-end accelerators.   In this work, we propose Glinthawk, a two-tiered architecture that decouples the attention mechanism from the rest of the Transformer model. This approach allows the memory requirements for attention to scale independently, enabling larger batch sizes and more efficient use of the high-end accelerators. We prototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the other. Compared to a traditional single-tier setup, it improves throughput by $5.9\times$ and reduces cost of generation by $2.8\times$. For longer sequence lengths, it achieves $16.3\times$ throughput improvement at $2.4\times$ less cost. Our evaluation shows that this architecture can tolerate moderate network latency with minimal performance degradation, making it highly effective for latency-tolerant, throughput-oriented applications such as batch processing. We shared our prototype publicly at \url{https://github.com/microsoft/glinthawk}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-20T23:10:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.DC</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.11779v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.11779v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 Hierarchical Coded Caching in High Memory Regime with Coded Placement</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rajlaxmi Pandey, Charul Rajput, B. Sundar Rajan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We consider a two-layer hierarchical coded caching network where a server with a library of $N$ files is connected to $K_1$ mirrors, each having a cache memory of size $M_1$. Each mirror is further connected to $K_2$ users, each equipped with a dedicated cache of size $M_2$. In this paper, we propose two distinct coded caching schemes based on coded placement, corresponding to two distinct memory pairs, \( (M_1, M_2) \). We show that the proposed schemes outperform the existing schemes at these memory points given by the proposed schemes for smaller values of $K_2$. In setups where mirrors are positioned near each other, avoiding signal interference is crucial. This can be ensured by having all mirrors transmit using orthogonal carrier frequencies. To compare our schemes with existing ones, we used the composite rate metric, which accurately represents the total bandwidth utilized in such setups. The composite rate is given by $\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the rate from the server to the mirrors, and $R_2$ is the rate from the mirrors to the users, with respect to $M_1$ and $M_2$.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-20T14:19:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.11502v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.11502v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 Spineless Traversal for Layout Invalidation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Marisa Kirisame, Tiezhi Wang, Pavel Panchekha
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Latency is a major concern for web rendering engines like those in Chrome, Safari, and Firefox. These engines reduce latency by using an incremental layout algorithm to redraw the page when the user interacts with it. In such an algorithm, elements that change frame-to-frame are marked dirty; only the dirty elements need be processed to draw the next frame, dramatically reducing latency. However, the standard incremental layout algorithm must search the page for dirty elements, accessing a number of auxiliary elements in the process. These auxiliary elements add cache misses and stalled cycles, and are responsible for a sizable fraction of all layout latency. We introduce a new, faster incremental layout algorithm called Spineless Traversal. Spineless Traversal uses a more computationally demanding priority queue algorithm to avoid the need to access auxiliary nodes and thus reduces cache traffic and stalls. This leads to dramatic speedups on the most latency-critical interactions such as hovering, typing, or animations. Moreover, thanks to numerous low-level optimizations, we are able to make Spineless Traversal competitive across the whole spectrum of incremental layout workloads. As a result, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the benchmark, with a mean speedup of 3.23x concentrated in the most latency-critical interactions such as hovering, typing, and animations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-20T08:44:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.10659v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.10659v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large
  Vision-Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yassir Bendou, Amine Ouasfi, Vincent Gripon, Adnane Boukhayma
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The growing popularity of Contrastive Language-Image Pretraining (CLIP) has led to its widespread application in various visual downstream tasks. To enhance CLIP's effectiveness and versatility, efficient few-shot adaptation techniques have been widely adopted. Among these approaches, training-free methods, particularly caching methods exemplified by Tip-Adapter, have gained attention for their lightweight adaptation without the need for additional fine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective, showing that caching methods function as local adapters and are connected to a well-established kernel literature. Drawing on this insight, we offer a theoretical understanding of how these methods operate and suggest multiple avenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the importance of incorporating global information in local adapters. Therefore, we subsequently propose a global method that learns a proximal regularizer in a reproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our method, which we call ProKeR (Proximal Kernel ridge Regression), has a closed form solution and achieves state-of-the-art performances across 11 datasets in the standard few-shot adaptation benchmark.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-19T21:25:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.11175v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.11175v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 Cache Coherence Over Disaggregated Memory</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruihong Wang, Jianguo Wang, Walid G. Aref
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Disaggregating memory from compute offers the opportunity to better utilize stranded memory in cloud data centers. It is important to cache data in the compute nodes and maintain cache coherence across multiple compute nodes. However, the limited computing power on disaggregated memory servers makes traditional cache coherence protocols suboptimal, particularly in the case of stranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache Coherence protocol that maintains cache coherence without imposing any computational burden on the remote memory side. It aligns the state machine of the shared-exclusive latch protocol with the MSI protocol by introducing lazy latch-release and invalidation messages, thereby ensuring both atomicity of data access and cache coherence. SELCC embeds cache-ownership metadata directly into the RDMA latch word, enabling efficient cache ownership management via RDMA atomic operations. SELCC can serve as an abstraction layer over disaggregated memory with APIs that resemble main-memory accesses. A concurrent B-tree and three transaction concurrency control algorithms are realized using SELCC's abstraction layer. Experimental results show that SELCC significantly outperforms Remote-Procedure-Call-based protocols for cache coherence under limited remote computing power. Applications on SELCC achieve comparable or superior performance over disaggregated memory compared to competitors.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-19T19:46:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.DC</span><span>cs.ET</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.02088v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.02088v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 Coded Caching for Hierarchical Two-Layer Networks with Coded Placement</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rajlaxmi Pandey, Charul Rajput, B. Sundar Rajan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We examine a two-layered hierarchical coded caching problem, a configuration addressed in existing research. This involves a server connected to $K_1$ mirrors, each of which serves $K_2$ users. The mirrors and the users are equipped with caches of size $M_1$ and $M_2$, respectively. We propose a hierarchical coded caching scheme with coded placements that outperforms existing schemes. To ensure a fair comparison, we introduce the notion of composite rate, defined as $\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the rate from the server to mirrors and $R_2$ is the rate from mirrors to users. The composite rate has not been discussed before in the literature and is pertinent when mirrors transmit with different carrier frequencies. For the proposed scheme, we show a trade-off between the global memory $\overline{M}=K_1M_1+K_1K_2M_2$ of the system and the composite rate and compare with the existing schemes. Additionally, we conduct this comparative analysis by plotting $R_1$ + $R_2$ against global memory, which is particularly beneficial for systems wherein each mirror can utilize the same carrier frequency, given their significant spatial separation. Additionally, we propose an optimized scheme for the specific case of a single mirror, showing improved performance in this scenario.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-19T15:47:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2312.15024v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2312.15024v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 D2D Coded Caching Schemes for Multiaccess Networks with Combinatorial
  Access Topology</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rashid Ummer N. T., B. Sundar Rajan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper considers wireless device-to-device (D2D) coded caching in a multiaccess network, where the users communicate with each other and each user can access multiple cache nodes. Access topologies derived from two combinatorial designs known as the $t$-design and $t$-group divisible design ($t$-GDD), referred to as the $t$-design and $t$-GDD topologies respectively, which subsume a few other known topologies, have been studied for the multiaccess coded caching (MACC) network by Cheng \textit{et al.} in \cite{MACC_des}. These access topologies are extended to a multiaccess D2D coded caching (MADCC) network and novel MADCC schemes are proposed. MADCC network has been studied so far only for the cyclic wrap-around topology. Apart from the proposed novel MADCC schemes, MADCC schemes are also derived from the existing MACC schemes in \cite{MACC_des}. To compare the performance of different MADCC schemes, the metrics of load per user and subpacketization level are used while keeping the number of caches and cache memory size same. The proposed MADCC scheme with $t$-design topology performs better in terms of subpacketization level while achieving the same load per user compared to the MADCC scheme derived from the MACC scheme with $t$-design topology in \cite{MACC_des}. The proposed MADCC scheme with $t$-GDD topology performs better in terms of load per user while achieving the same subpacketization level compared to the MADCC scheme derived from the MACC scheme with $t$-GDD topology in \cite{MACC_des} in some cases. Compared to the existing MADCC scheme with cyclic wrap-around topology, the proposed MADCC scheme with $t$-design topology performs better in terms of load per user, and the proposed MADCC scheme with $t$-GDD topology performs better in terms of subpacketization level at the expense of an increase in load per user.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-18T13:04:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.10756v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.10756v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 SkyByte: Architecting an Efficient Memory-Semantic CXL-based SSD with OS
  and Hardware Co-design</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haoyang Zhang, Yuqi Xue, Yirui Eric Zhou, Shaobo Li, Jian Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The CXL-based solid-state drive (CXL-SSD) provides a promising approach towards scaling the main memory capacity at low cost. However, the CXL-SSD faces performance challenges due to the long flash access latency and unpredictable events such as garbage collection in the SSD device, stalling the host processor and wasting compute cycles. Although the CXL interface enables the byte-granular data access to the SSD, accessing flash chips is still at page granularity due to physical limitations. The mismatch of access granularity causes significant unnecessary I/O traffic to flash chips, worsening the suboptimal end-to-end data access performance. In this paper, we present SkyByte, an efficient CXL-based SSD that employs a holistic approach to address the aforementioned challenges by co-designing the host operating system (OS) and SSD controller. To alleviate the long memory stall when accessing the CXL-SSD, SkyByte revisits the OS context switch mechanism and enables opportunistic context switches upon the detection of long access delays. To accommodate byte-granular data accesses, SkyByte architects the internal DRAM of the SSD controller into a cacheline-level write log and a page-level data cache, and enables data coalescing upon log cleaning to reduce the I/O traffic to flash chips. SkyByte also employs optimization techniques that include adaptive page migration for exploring the performance benefits of fast host memory by promoting hot pages in CXL-SSD to the host. We implement SkyByte with a CXL-SSD simulator and evaluate its efficiency with various data-intensive applications. Our experiments show that SkyByte outperforms current CXL-based SSD by 6.11X, and reduces the I/O traffic to flash chips by 23.08X on average. SkyByte also reaches 75% of the performance of the ideal case that assumes unlimited DRAM capacity in the host, which offers an attractive cost-effective solution.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-18T07:29:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.10682v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.10682v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 Geometric rigidity of simple modules for algebraic groups</h2>
                <div class="authors">
                    <strong>Authors:</strong> Michael Bate, David I. Stewart
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Let k be a field, let G be an affine algebraic k-group and V a finite-dimensional G-module. We say V is rigid if the socle series and radical series coincide for the action of G on each indecomposable summand of V; say V is geometrically rigid (resp. absolutely rigid) if V is rigid after base change of G and V to k (resp. any field extension of k). We show that all simple G-modules are geometrically rigid, though not in general absolutely rigid. More precisely, we show that if V is a simple G-module, then there is a finite purely inseparable extension kV /k naturally attached to V such that V is absolutely rigid as a G-module after base change to kV. The proof turns on an investigation of algebras of the form K otimes E where K and E are field extensions of k; we give an example of such an algebra which is not rigid as a module over itself. We establish the existence of the purely inseparable field extension kV /k through an analogous version for artinian algebras.   In the second half of the paper we apply recent results on the structure and representation theory of pseudo-reductive groups to give a concrete description of kV when G is smooth and connected. Namely, we combine the main structure theorem of the Conrad-Prasad classification of pseudo-reductive G together with our previous high weight theory. For V a simple G-module, we calculate the minimal field of definition of the geometric Jacobson radical of EndG(V) in terms of the high weight of V and the Conrad-Prasad classification data; this gives a concrete construction of the field kV as a subextension of the minimal field of definition of the geometric unipotent radical of G. We also observe that the Conrad-Prasad classification can be used to hone the dimension formula for V we had previously established; we also use it to give a description of EndG(V) which includes a dimension formula.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-17T16:16:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.RT</span><span>math.GR</span><span>math.RA</span><span>20G05</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.05221v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.05221v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 The NIC should be part of the OS</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pengcheng Xu, Timothy Roscoe
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The network interface adapter (NIC) is a critical component of a modern cloud server which occupies a unique position. Not only is network performance vital to the efficient operation of the machine, but unlike application-oriented compute accelerators like GPUs, the network subsystem must react to unpredictable events like the arrival of a network packet and communicate with the appropriate application end point with minimal latency. Current approaches to server stacks navigate a trade-off between flexibility, efficiency, and performance: the fastest kernel-bypass approaches dedicate cores to applications, busy-wait on receive queues, etc. while more flexible approaches appropriate to more dynamic workload mixes incur much greater software overhead on the data path. However, we reject this trade-off, which we ascribe to an arbitrary (and sub-optimal) split in system state between the OS and the NIC. Instead, by exploiting the properties of cache-coherent interconnects and integrating the NIC closely with the OS kernel, we can achieve something surprising: performance for RPC workloads better than the fastest kernel-bypass approaches without sacrificing the robustness and dynamic adaptation of kernel-based network subsystems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-17T12:01:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.OS</span><span>cs.AR</span><span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.10138v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.10138v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix
  Sharing and Throughput-oriented Token Batching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhen Zheng, Xin Ji, Taosong Fang, Fanghao Zhou, Chuanjie Liu, Gang Peng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) increasingly play an important role in a wide range of information processing and management tasks. Many of these tasks are performed in large batches or even offline, and the performance indictor for which is throughput. These tasks usually show the characteristic of prefix sharing, where different prompt input can partially show the common prefix. However, the existing LLM inference engines tend to optimize the streaming requests and show limitations of supporting the large batched tasks with the prefix sharing characteristic. The existing solutions use the LRU-based cache to reuse the KV context of common prefix between requests. The KV context that are about to be reused may prematurely evicted with the implicit cache management. Besides, the streaming oriented systems do not leverage the request-batch information and can not mix the decoding tokens with the prefill chunks to the best for the batched scenarios, and thus fails to saturate the GPU. We propose BatchLLM to address the above problems. BatchLLM explicitly identifies the common prefixes globally. The requests sharing the same prefix will be scheduled together to reuse the KV context the best. BatchLLM reorders the requests and schedules the requests with larger ratio of decoding first to better mix the decoding tokens with the latter prefill chunks, and applies memory-centric token batching to enlarge the token-batch sizes, which helps to increase the GPU utilization. Finally, BatchLLM optimizes the prefix-shared Attention kernel with horizontal fusion to reduce tail effect and kernel launch overhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang by 1.3$\times$ to 10.8$\times$ on a set of microbenchmarks and a typical industry workload under different hardware environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-17T09:37:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.DC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.03594v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.03594v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 Multi-Dimensional Vector ISA Extension for Mobile In-Cache Computing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alireza Khadem, Daichi Fujiki, Hilbert Chen, Yufeng Gu, Nishil Talati, Scott Mahlke, Reetuparna Das
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In-cache computing technology transforms existing caches into long-vector compute units and offers low-cost alternatives to building expensive vector engines for mobile CPUs. Unfortunately, existing long-vector Instruction Set Architecture (ISA) extensions, such as RISC-V Vector Extension (RVV) and Arm Scalable Vector Extension (SVE), provide only one-dimensional strided and random memory accesses. While this is sufficient for typical vector engines, it fails to effectively utilize the large Single Instruction, Multiple Data (SIMD) widths of in-cache vector engines. This is because mobile data-parallel kernels expose limited parallelism across a single dimension.   Based on our analysis of mobile vector kernels, we introduce a long-vector Multi-dimensional Vector ISA Extension (MVE) for mobile in-cache computing. MVE achieves high SIMD resource utilization and enables flexible programming by abstracting cache geometry and data layout. The proposed ISA features multi-dimensional strided and random memory accesses and efficient dimension-level masked execution to encode parallelism across multiple dimensions. Using a wide range of data-parallel mobile workloads, we demonstrate that MVE offers significant performance and energy reduction benefits of 2.9x and 8.8x, on average, compared to the SIMD units of a commercial mobile processor, at an area overhead of 3.6%.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-17T01:24:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.09902v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.09902v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of
  the Atomic Layer Deposition Temperature and the Lithographic Patterning
  Method</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alessandro Paghi, Sebastiano Battisti, Simone Tortorella, Giorgio De Simoni, Francesco Giazotto
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics, have become the standard insulators in gate architectures, enhancing the electrical performance of both room temperature and cryogenic electronics. This study delves into the cryogenic (3 K) performance of high-k dielectrics commonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via Atomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and dielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on metal-insulator-metal capacitors. Our findings reveal a strong dependence of HfO2 cryogenic performance on the ALD growth temperature, while the latter shows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of the relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K. Additionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked their properties at cryogenic temperatures. The study also investigates the impact of the patterning method, namely, UV or electron-beam lithography (acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric properties.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-16T15:11:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.mtrl-sci</span><span>cond-mat.mes-hall</span><span>cond-mat.supr-con</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.04501v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.04501v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk
  Synchronization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Congcong Chen, Jinhua Cui, Gang Qu, Jiliang Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Memory-disk synchronization is a critical technology for ensuring data correctness, integrity, and security, especially in systems that handle sensitive information like financial transactions and medical records. We propose SYNC+SYNC, a group of attacks that exploit the memory-disk synchronization primitives. SYNC+SYNC works by subtly varying the timing of synchronization on the write buffer, offering several advantages: 1) implemented purely in software, enabling deployment on any hardware devices; 2) resilient against existing cache partitioning and randomization techniques; 3) unaffected by prefetching techniques and cache replacement strategies. We present the principles of SYNC+SYNC through the implementation of two write covert channel protocols, using either a single file or page, and introduce three enhanced strategies that utilize multiple files and pages. The feasibility of these channels is demonstrated in both cross-process and cross-sandbox scenarios across diverse operating systems (OSes). Experimental results show that, the average rate can reach 2.036 Kb/s (with a peak rate of 14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the average rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the error rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first high-speed write covert channel for software cache.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-16T10:35:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2312.11501v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2312.11501v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 Adaptive Contextual Caching for Mobile Edge Large Language Model Service</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guangyuan Liu, Yinqiu Liu, Jiacheng Wang, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Mobile edge Large Language Model (LLM) deployments face inherent constraints, such as limited computational resources and network bandwidth. Although Retrieval-Augmented Generation (RAG) mitigates some challenges by integrating external knowledge bases, inefficient cache management can still result in high retrieval latency and frequent cache updates. To address these issues, we propose an Adaptive Contextual Caching (ACC) framework that anticipates user needs by proactively caching semantically relevant data for mobile-edge LLMs. ACC utilizes a deep reinforcement learning (DRL) module to refine cache replacement policies, balancing user context, document similarity, and the overhead associated with cache misses. Experimental results demonstrate that ACC increases cache hit rates to over 80\% after only 11 training episodes, outperforming FIFO, LRU, and semantic-only caching while reducing retrieval latency by up to 40\%. In particular, ACC also reduces local caching overhead (i.e., the cost of updating the cache when a miss occurs) by as much as 55\%, enabling scalable, low-latency LLM services in resource-constrained edge environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-16T08:52:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.09383v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.09383v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 Interoceptive Robots for Convergent Shared Control in Collaborative
  Construction Work</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaoshan Zhou, Carol C. Menassa, Vineet R. Kamat
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Building autonomous mobile robots (AMRs) with optimized efficiency and adaptive capabilities-able to respond to changing task demands and dynamic environments-is a strongly desired goal for advancing construction robotics. Such robots can play a critical role in enabling automation, reducing operational carbon footprints, and supporting modular construction processes. Inspired by the adaptive autonomy of living organisms, we introduce interoception, which centers on the robot's internal state representation, as a foundation for developing self-reflection and conscious learning to enable continual learning and adaptability in robotic agents. In this paper, we factorize internal state variables and mathematical properties as "cognitive dissonance" in shared control paradigms, where human interventions occasionally occur. We offer a new perspective on how interoception can help build adaptive motion planning in AMRs by integrating the legacy of heuristic costs from grid/graph-based algorithms with recent advances in neuroscience and reinforcement learning. Declarative and procedural knowledge extracted from human semantic inputs is encoded into a hypergraph model that overlaps with the spatial configuration of onsite layout for path planning. In addition, we design a velocity-replay module using an encoder-decoder architecture with few-shot learning to enable robots to replicate velocity profiles in contextualized scenarios for multi-robot synchronization and handover collaboration. These "cached" knowledge representations are demonstrated in simulated environments for multi-robot motion planning and stacking tasks. The insights from this study pave the way toward artificial general intelligence in AMRs, fostering their progression from complexity to competence in construction automation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-16T04:50:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.09290v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.09290v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid
  Resolution Diffusion Serving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Desen Sun, Zepeng Zhao, Yuke Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Text-to-Image (T2I) diffusion model is one of the most popular models in the world. However, serving diffusion models at the entire image level faces several problems, especially when there are multiple candidate resolutions. First, image based serving system prevents requests with different resolutions from batching together. On the other hand, requests with hybrid resolutions also indicate diverse locality features, which makes it hard to apply the same cache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch Management Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that provides a patch-level management strategy to gather hybrid resolution requests into batches. Specifically, PATCHEDSERVE incorporates a novel patch-based processing workflow, significantly enhancing throughput for hybrid resolution inputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to fully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features an SLO-aware scheduling algorithm with lightweight online latency prediction, achieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve 30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while not hurt the image quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-16T02:40:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.09253v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.09253v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of
  Micro-UAVs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Amit Kumar Bhuyan, Hrishikesh Dutta, Subir Biswas
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content management system for disaster scenarios where communication infrastructure is generally compromised. Utilizing a hybrid network of stationary and mobile Micro-UAVs, this system aims to provide crucial content access to isolated communities. In the developed architecture, stationary anchor UAVs, equipped with vertical and lateral links, serve users in individual disaster-affected communities. and mobile micro-ferrying UAVs, with enhanced mobility, extend coverage across multiple such communities. The primary goal is to devise a content dissemination system that dynamically learns caching policies to maximize content accessibility to users left without communication infrastructure. The core contribution is an adaptive content dissemination framework that employs a decentralized Top-k Multi-Armed Bandit learning approach for efficient UAV caching decisions. This approach accounts for geo-temporal variations in content popularity and diverse user demands. Additionally, a Selective Caching Algorithm is proposed to minimize redundant content copies by leveraging inter-UAV information sharing. Through functional verification and performance evaluation, the proposed framework demonstrates improved system performance and adaptability across varying network sizes, micro-UAV swarms, and content popularity distributions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-15T21:09:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.NI</span><span>I.2.11</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.10845v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.10845v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 Towards Federated Multi-Armed Bandit Learning for Content Dissemination
  using Swarm of UAVs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Amit Kumar Bhuyan, Hrishikesh Dutta, Subir Biswas
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper introduces an Unmanned Aerial Vehicle - enabled content management architecture that is suitable for critical content access in communities of users that are communication-isolated during diverse types of disaster scenarios. The proposed architecture leverages a hybrid network of stationary anchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The anchor UAVs are equipped with both vertical and lateral communication links, and they serve local users, while the mobile micro-ferrying UAVs extend coverage across communities with increased mobility. The focus is on developing a content dissemination system that dynamically learns optimal caching policies to maximize content availability. The core innovation is an adaptive content dissemination framework based on distributed Federated Multi-Armed Bandit learning. The goal is to optimize UAV content caching decisions based on geo-temporal content popularity and user demand variations. A Selective Caching Algorithm is also introduced to reduce redundant content replication by incorporating inter-UAV information sharing. This method strategically preserves the uniqueness in user preferences while amalgamating the intelligence across a distributed learning system. This approach improves the learning algorithm's ability to adapt to diverse user preferences. Functional verification and performance evaluation confirm the proposed architecture's utility across different network sizes, UAV swarms, and content popularity patterns.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-15T20:55:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.NI</span><span>I.2.11</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.09146v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.09146v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hyucksung Kwon, Kyungmo Koo, Janghyeon Kim, Woongkyu Lee, Minjae Lee, Hyungdeok Lee, Yousub Jung, Jaehan Park, Yosub Song, Byeongsu Yang, Haerang Choi, Guhyun Kim, Jongsoon Won, Woojae Shin, Changhyun Kim, Gyeongcheol Shin, Yongkee Kwon, Ilkon Kim, Euicheol Lim, John Kim, Jungwook Choi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The expansion of large language models (LLMs) with hundreds of billions of parameters presents significant challenges to computational resources, particularly data movement and memory bandwidth. Long-context LLMs, which process sequences of tens of thousands of tokens, further increase the demand on the memory system as the complexity in attention layers and key-value cache sizes is proportional to the context length. Processing-in-Memory (PIM) maximizes memory bandwidth by moving compute to the data and can address the memory bandwidth challenges; however, PIM is not necessarily scalable to accelerate long-context LLM because of limited per-module memory capacity and the inflexibility of fixed-functional unit PIM architecture and static memory management. In this work, we propose LoL-PIM which is a multi-node PIM architecture that accelerates long context LLM through hardware-software co-design. In particular, we propose how pipeline parallelism can be exploited across a multi-PIM module while a direct PIM access (DPA) controller (or DMA for PIM) is proposed that enables dynamic PIM memory management and results in efficient PIM utilization across a diverse range of context length. We developed an MLIR-based compiler for LoL-PIM extending a commercial PIM-based compiler where the software modifications were implemented and evaluated, while the hardware changes were modeled in the simulator. Our evaluations demonstrate that LoL-PIM significantly improves throughput and reduces latency for long-context LLM inference, outperforming both multi-GPU and GPU-PIM systems (up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient deployment of LLMs in real-world applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-15T01:34:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.20166v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.20166v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 CORD: Co-design of Resource Allocation and Deadline Decomposition with
  Generative Profiling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Robert Gifford, Abby Eisenklam, Georgiy A. Bondar, Yifan Cai, Tushar Sial, Linh Thi Xuan Phan, Abhishek Halder
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As multicore hardware is becoming increasingly common in real-time systems, traditional scheduling techniques that assume a single worst-case execution time for a task are no longer adequate, since they ignore the impact of shared resources on execution time. When tasks execute concurrently on different cores, their execution times often vary substantially with their allocated budgets of shared resources, such as cache and memory bandwidth. Even under a specific resource allocation, the resource use pattern of a task also changes with time during a job execution. It is therefore important to consider the relationship between multicore resources and execution time in task modeling and scheduling algorithm design.   In this paper, we propose a much more precise execution model for DAG-based real-time tasks that captures the time-varying resource use characteristics of a task under different budgets of shared resources. We present a generative resource profiling algorithm that efficiently predicts, from limited measurement data, the resource profile of a task at any time during its execution under a given resource budget. The generative profiles can then be used to construct the execution models for tasks, using which one can make informed resource allocation decisions. We further introduce a multicore resource allocation and deadline decomposition co-design technique for DAG-based tasks that leverages the generated execution models to jointly allocate resources and deadlines to subtasks, to maximize resource efficiency and schedulability. Our evaluation results show that our generative profiling algorithm achieves high accuracy while being efficient, and that our co-allocation technique substantially improves schedulability compared to a state-of-the-art deadline decomposition method.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-14T23:13:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.OS</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.08484v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.08484v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM
  Serving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ahmet Caner Yüzügüler, Jiawei Zhuang, Lukas Cavigelli
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) are widely used across various applications, but their substantial computational requirements pose significant challenges, particularly in terms of HBM bandwidth bottlenecks and inter-device communication overhead. In this paper, we present PRESERVE, a novel prefetching framework designed to optimize LLM inference by overlapping memory reads for model weights and KV-cache with collective communication operations. Through extensive experiments conducted on commercial AI accelerators, we demonstrate up to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs. Additionally, we perform a design space exploration that identifies the optimal hardware configuration for the proposed method, showing a further 1.25x improvement in performance per cost by selecting the optimal L2 cache size. Our results show that PRESERVE has the potential to mitigate the memory bottlenecks and communication overheads, offering a solution to improve the performance and scalability of the LLM inference systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-14T15:14:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.AR</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.08192v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.08192v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out
  Context Attribution</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fengyuan Liu, Nikhil Kandpal, Colin Raffel
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The influence of contextual input on the behavior of large language models (LLMs) has prompted the development of context attribution methods that aim to quantify each context span's effect on an LLM's generations. The leave-one-out (LOO) error, which measures the change in the likelihood of the LLM's response when a given span of the context is removed, provides a principled way to perform context attribution, but can be prohibitively expensive to compute for large models. In this work, we introduce AttriBoT, a series of novel techniques for efficiently computing an approximation of the LOO error for context attribution. Specifically, AttriBoT uses cached activations to avoid redundant operations, performs hierarchical attribution to reduce computation, and emulates the behavior of large target models with smaller proxy models. Taken together, AttriBoT can provide a >300x speedup while remaining more faithful to a target model's LOO error than prior context attribution methods. This stark increase in performance makes computing context attributions for a given response 30x faster than generating the response itself, empowering real-world applications that require computing attributions at scale. We release a user-friendly and efficient implementation of AttriBoT to enable efficient LLM interpretability as well as encourage future development of efficient context attribution methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-14T14:07:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.15102v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.15102v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 TreeKV: Smooth Key-Value Cache Compression with Tree Structures</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ziwei He, Jian Yuan, Haoli Bai, Jingwen Leng, Bo Jiang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Efficient key-value (KV) cache compression is critical for scaling transformer-based Large Language Models (LLMs) in long sequences and resource-limited settings. Existing methods evict tokens based on their positions or importance scores, but position-based strategies can miss crucial information outside predefined regions, while those relying on global importance scores resulting in strong regional biases, limiting the KV cache's overall context retention and potentially impairing the performance of LLMs on complex tasks. Our wavelet analysis reveals that as tokens approach the end of sequence, their contributions to generation gradually increase and tends to diverge more from neighboring tokens, indicating a smooth transition with increasing complexity and variability from distant to nearby context. Motivated by this observation, we propose TreeKV, an intuitive, training-free method that employs a tree structure for smooth cache compression. TreeKV maintains a fixed cache size, allowing LLMs to deliver high-quality output even in long text scenarios. Unlike most compression methods, TreeKV is applicable to both the generation and prefilling stages. TreeKV consistently surpasses all baseline models in language modeling tasks on PG19 and OpenWebText2, allowing LLMs trained with short context window to generalize to longer window with a 16x cache reduction. On the Longbench benchmark, TreeKV achieves the best performance with only 6\% of the budget at optimal efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-14T12:06:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.04987v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.04987v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 Cell-level modelling of homeostasis in confined epithelial monolayers</h2>
                <div class="authors">
                    <strong>Authors:</strong> KVS Chaithanya, Jan Rozman, Andrej Košmrlj, Rastko Sknepnek
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Tissue homeostasis, the biological process of maintaining a steady state in tissue via control of cell proliferation, death, and metabolic function, is essential for the development, growth, maintenance, and proper function of living organisms. Disruptions to this process can lead to serious diseases and even death. In this study, we use the vertex model for the cell-level description of tissue mechanics to investigate the impact of the tissue microenvironment and local mechanical properties of cells on homeostasis in confined epithelial tissues. We find a dynamic steady state, where the balance between cell divisions and removals sustains homeostasis. By characterising homeostasis in terms of cell count, tissue area, and the cells' neighbour count distribution, we identify the factors that govern regulated and ordered tissue growth. This work, therefore, sheds light on the mechanisms underlying tissue homeostasis and highlights the importance of mechanics in the control of biological processes such as tissue development and disease pathology.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-14T11:41:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.bio-ph</span><span>cond-mat.soft</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2403.15896v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2403.15896v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 Multi-matrix Factorization Attention</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jingcheng Hu, Houyi Li, Yinmin Zhang, Zili Wang, Shuigeng Zhou, Xiangyu Zhang, Heung-Yeung Shum, Daxin Jiang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We propose novel attention architectures, Multi-matrix Factorization Attention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard Multi-Head Attention (MHA), including SOTA methods like MLA, fail to maintain as strong performance under stringent Key-Value cache (KV cache) constraints. MFA enhances model capacity by efficiently scaling up both the number and dimension of attention heads through low-rank matrix factorization in the Query-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory requirements by repurposing the key cache as value through value projection re-parameterization. MFA's design enables strong model capacity when working under tight KV cache budget, while MFA-KR is suitable for even harsher KV cache limits with minor performance trade-off. Notably, in our extensive and large-scale experiments, the proposed architecture outperforms MLA and performs comparably to MHA, while reducing KV cache usage by up to 56% and 93.7%, respectively.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-14T05:48:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.19255v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.19255v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 Lean Attention: Hardware-Aware Scalable Attention Mechanism for the
  Decode-Phase of Transformers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rya Sanovar, Srikant Bharadwaj, Renee St. Amant, Victor Rühle, Saravan Rajmohan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformer-based models have emerged as one of the most widely used architectures for natural language processing, natural language generation, and image generation. The size of the state-of-the-art models has increased steadily reaching billions of parameters. These huge models are memory hungry and incur significant inference latency even on cutting edge AI-accelerators, such as GPUs. Specifically, the time and memory complexity of the attention operation is quadratic in terms of the total context length, i.e., prompt and output tokens. Thus, several optimizations such as key-value tensor caching and FlashAttention computation have been proposed to deliver the low latency demands of applications relying on such large models. However, these techniques do not cater to the computationally distinct nature of different phases during inference.   To that end, we propose LeanAttention, a scalable technique of computing self-attention for the token-generation phase (decode-phase) of decoder-only transformer models. LeanAttention enables scaling the attention mechanism implementation for the challenging case of long context lengths by re-designing the execution flow for the decode-phase. We identify that the associative property of online softmax can be treated as a reduction operation thus allowing us to parallelize the attention computation over these large context lengths. We extend the "stream-K" style reduction of tiled calculation to self-attention to enable parallel computation resulting in an average of 2.6x attention execution speedup over FlashAttention-2 and up to 8.33x speedup for 512k context lengths.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-14T05:00:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>cs.LG</span><span>I.2.7; C.1.4</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2405.10480v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.10480v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 QMDB: Quick Merkle Database</h2>
                <div class="authors">
                    <strong>Authors:</strong> Isaac Zhang, Ryan Zarick, Daniel Wong, Thomas Kim, Bryan Pellegrino, Mignon Li, Kelvin Wong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain state management by integrating key-value (KV) and Merkle tree storage into a single unified architecture. QMDB delivers a significant throughput improvement over existing architectures, achieving up to 6X over the widely used RocksDB and 8X over NOMT, a leading verifiable database. Its novel append-only twig-based design enables one SSD read per state access, O(1) IOs for updates, and in-memory Merkleization on a memory footprint as small as 2.3 bytes per entry, enabling it to run on even modest consumer-grade PCs. QMDB scales seamlessly across both commodity and enterprise hardware, achieving up to 2.28 million state updates per second. This performance enables support for 1 million token transfers per second (TPS), marking QMDB as the first solution achieving such a milestone. QMDB has been benchmarked with workloads exceeding 15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to scale to 280 billion entries on a single server. Furthermore, QMDB introduces historical proofs, unlocking the ability to query its blockchain's historical state at the latest block. QMDB not only meets the demands of current blockchains but also provides a robust foundation for building scalable, efficient, and verifiable decentralized applications across diverse use cases.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-14T02:02:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span><span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05262v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05262v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 FlashRNN: Optimizing Traditional RNNs on Modern Hardware</h2>
                <div class="authors">
                    <strong>Authors:</strong> Korbinian Pöppel, Maximilian Beck, Sepp Hochreiter
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While Transformers and other sequence-parallelizable neural network architectures seem like the current state of the art in sequence modeling, they specifically lack state-tracking capabilities. These are important for time-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs, as well as modern variants like sLSTM do have these capabilities at the cost of strictly sequential processing. While this is often seen as a strong limitation, we show how fast these networks can get with our hardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the register level on modern GPUs. We extend traditional RNNs with a parallelization variant that processes multiple RNNs of smaller hidden state in parallel, similar to the head-wise processing in Transformers. To enable flexibility on different GPU variants, we introduce a new optimization framework for hardware-internal cache sizes, memory and compute handling. It models the hardware in a setting using polyhedral-like constraints, including the notion of divisibility. This speeds up the solution process in our ConstrINT library for general integer constraint satisfaction problems (integer CSPs). We show that our kernels can achieve 50x speed-ups over a vanilla PyTorch implementation and allow 40x larger hidden sizes compared to our Triton implementation. Our open-source kernels and the optimization library are released here to boost research in the direction of state-tracking enabled RNNs and sequence modeling: \url{https://github.com/NX-AI/flashrnn}
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-13T17:34:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.07752v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.07752v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 DID Link: Authentication in TLS with Decentralized Identifiers and
  Verifiable Credentials</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sandro Rodriguez Garzon, Dennis Natusch, Artur Philipp, Axel Küpper, Hans Joachim Einsiedler, Daniela Schneider
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Authentication in TLS is predominately carried out with X.509 digital certificates issued by certificate authorities (CA). The centralized nature of current public key infrastructures, however, comes along with severe risks, such as single points of failure and susceptibility to cyber-attacks, potentially undermining the security and trustworthiness of the entire system. With Decentralized Identifiers (DID) alongside distributed ledger technology, it becomes technically feasible to prove ownership of a unique identifier without requiring an attestation of the proof's public key by a centralized and therefore vulnerable CA. This article presents DID Link, a novel authentication scheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant way with self-issued X.509 certificates that are equipped with ledger-anchored DIDs instead of CA-issued identifiers. It facilitates the exchange of tamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable Credentials after the TLS handshake to complete the authentication with a full identification of the communication partner. A prototypical implementation shows comparable TLS handshake durations of DID Link if verification material is cached and reasonable prolongations if it is obtained from a ledger. The significant speed improvement of the resulting TLS channel over a widely used, DID-based alternative transport protocol on the application layer demonstrates the potential of DID Link to become a viable solution for the establishment of secure and trustful end-to-end communication links with decentrally managed digital identities.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-13T09:33:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/PST62714.2024.10788053' target='_blank'>doi</a><a href='http://arxiv.org/abs/2405.07533v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.07533v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 Generating Data Locality to Accelerate Sparse Matrix-Matrix
  Multiplication on CPUs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jordi Wolfson-Pou, Jan Laukemann, Fabrizio Petrini
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation in many applications. Current multithreaded implementations are based on Gustavson's algorithm and often perform poorly on large matrices due to limited cache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic NUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To generate locality, MAGNUS reorders the intermediate product into discrete cache-friendly chunks using a two-level hierarchical approach. The accumulator is applied to each chunk, where the chunk size is chosen such that the accumulator is cache-efficient. MAGNUS is input- and system-aware: based on the matrix characteristics and target system specifications, the optimal number of chunks is computed by minimizing the storage cost of the necessary data structures. MAGNUS allows for a hybrid accumulation strategy in which each chunk uses a different accumulator based on an input threshold. We consider two accumulators: an AVX-512 vectorized bitonic sorting algorithm and classical dense accumulation. An OpenMP implementation of MAGNUS is compared with several baselines for a variety of different matrices on three Intel x86 architectures. For matrices from the SuiteSparse collection, MAGNUS is faster than all the baselines in most cases and is orders of magnitude faster than Intel MKL for several matrices. For massive random matrices that model social network graphs, MAGNUS scales to the largest matrix sizes, while the baselines fail to do so. Furthermore, MAGNUS is close to the optimal bound for these matrices, regardless of the matrix size, structure, and density.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-13T04:31:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.07056v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.07056v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 A Unified Framework for Automated Code Transformation and Pragma
  Insertion</h2>
                <div class="authors">
                    <strong>Authors:</strong> Stéphane Pouget, Louis-Noël Pouchet, Jason Cong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> High-level synthesis, source-to-source compilers, and various Design Space Exploration techniques for pragma insertion have significantly improved the Quality of Results of generated designs. These tools offer benefits such as reduced development time and enhanced performance. However, achieving high-quality results often requires additional manual code transformations and tiling selections, which are typically performed separately or as pre-processing steps. Although DSE techniques enable code transformation upfront, the vastness of the search space often limits the exploration of all possible code transformations, making it challenging to determine which transformations are necessary. Additionally, ensuring correctness remains challenging, especially for complex transformations and optimizations.   To tackle this obstacle, we first propose a comprehensive framework leveraging HLS compilers. Our system streamlines code transformation, pragma insertion, and tiles size selection for on-chip data caching through a unified optimization problem, aiming to enhance parallelization, particularly beneficial for computation-bound kernels. Them employing a novel Non-Linear Programming (NLP) approach, we simultaneously ascertain transformations, pragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation demonstrates that our framework adeptly identifies the appropriate transformations, including scenarios where no transformation is necessary, and inserts pragmas to achieve a favorable Quality of Results.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-13T03:11:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.PL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2405.03058v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.03058v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 On Optimizing Locality of Graph Transposition on Modern Architectures</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mohsen Koohi Esfahani, Hans Vandierendonck
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper investigates the shared-memory Graph Transposition (GT) problem, a fundamental graph algorithm that is widely used in graph analytics and scientific computing.   Previous GT algorithms have significant memory requirements that are proportional to the number of vertices and threads which obstructs their use on large graphs. Moreover, atomic memory operations have become comparably fast on recent CPU architectures, which creates new opportunities for improving the performance of concurrent atomic accesses in GT.   We design PoTra, a GT algorithm which leverages graph structure and processor and memory architecture to optimize locality and performance. PoTra limits the size of additional data structures close to CPU cache sizes and utilizes the skewed degree distribution of graph datasets to optimize locality and performance. We present the performance model of PoTra to explain the connection between cache and memory response times and graph locality.   Our evaluation of PoTra on three CPU architectures and 20 real-world and synthetic graph datasets with up to 128 billion edges demonstrates that PoTra achieves up to 8.7 times speedup compared to previous works and if there is a performance loss it remains limited to 15.7%, on average.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-12T17:01:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AR</span><span>cs.DS</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06872v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06872v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large
  Language Model Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wenxuan Zeng, Ye Dong, Jinjin Zhou, Junming Ma, Jin Tan, Runsheng Wang, Meng Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Private large language model (LLM) inference based on secure multi-party computation (MPC) offers cryptographically-secure protection for both user prompt and proprietary model weights. However, it suffers from large latency overhead especially for long input sequences. While key-value (KV) cache eviction algorithms have been proposed to reduce the computation and memory cost for plaintext inference, they are not designed for MPC and cannot benefit private inference easily. In this paper, we propose an accurate and MPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on the observation that historical tokens in a long sequence may have different effects on the downstream decoding. Hence, MPCache combines a look-once static eviction algorithm to discard unimportant tokens and a query-aware dynamic selection algorithm to further select a small subset of tokens for attention computation. As existing dynamic selection algorithms incur too much latency, we propose a series of optimizations to drastically reduce the KV cache selection overhead, including MPC-friendly similarity approximation, hierarchical KV cache clustering, and cross-layer index sharing strategy. With extensive experiments, we demonstrate that MPCache consistently outperforms prior-art KV cache eviction baselines across different LLM generation tasks and achieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction on different sequence lengths, respectively.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-12T13:18:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06807v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06807v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 Linear Attention Sequence Parallelism</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weigao Sun, Zhen Qin, Dong Li, Xuyang Shen, Yu Qiao, Yiran Zhong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Sequence parallelism (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single device. However, for linear sequence modeling methods like linear attention, existing SP approaches do not take advantage of their right-product-first feature, resulting in sub-optimal communication efficiency and usability. In this paper, we introduce Linear Attention Sequence Parallelism (LASP), an efficient SP approach designed for linear attention-based transformer models. Specifically, we design an efficient point-to-point ring-style communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead, comparing with existing SP methods. We enhance the computation efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPUs. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with very-long sequences. We also discuss the generalization of LASP on other linear sequence modeling methods. Extensive experiments on linear attention-based models are conducted with varying sequence lengths from 2K to 4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\times$ longer than existing SP methods. The code is available at https://github.com/OpenNLPLab/LASP.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-12T12:01:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.02882v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.02882v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma
  Generated THz Pulses</h2>
                <div class="authors">
                    <strong>Authors:</strong> Benjamin Colmey, Rodrigo T. Paulino, Gaspard Beaufort, David G. Cooke
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Terahertz pulses generated by two-color laser plasmas have reported peak field strengths exceeding MV/cm, and when illuminating metal nanotips the near-field enhancement at the tip apex should result in extremely high bunch charges and electron energies via sub-cycle cold field emission. Here, electron emission from tungsten nanotips driven by THz pulses generated by a long filament air-plasma are reported. Electron energies up to 1.1 keV and bunch charges up to 2x$10^5$ electrons per pulse were detected, well below values expected for peak field calculated via the time averaged Poynting vector. Investigations revealed a failure in the use of the time-averaged Poynting vector when applied to long filament THz pulses, due to spatio-temporal restructuring of the THz pulse in the focus. Accounting for this restructuring significantly reduces the field strength to approximately 160 ~kV/cm, consistent with the observed electron bunch charges, peak energies and their dependence on the tip position in the THz focus. Despite these findings, our results surpass previous THz plasma-driven electron generation by an order of magnitude in both electron energy and bunch charge and a path to increasing these by an additional order of magnitude by modification of the THz optics is proposed.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-12T11:15:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.mtrl-sci</span><span>physics.plasm-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.07196v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.07196v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bohai Gu, Hao Luo, Song Guo, Peiran Dong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recently, diffusion-based methods have achieved great improvements in the video inpainting task. However, these methods still face many challenges, such as maintaining temporal consistency and the time-consuming issue. This paper proposes an advanced video inpainting framework using optical Flow-guided Efficient Diffusion, called FloED. Specifically, FloED employs a dual-branch architecture, where a flow branch first restores corrupted flow and a multi-scale flow adapter provides motion guidance to the main inpainting branch. Additionally, a training-free latent interpolation method is proposed to accelerate the multi-step denoising process using flow warping. Further introducing a flow attention cache mechanism, FLoED efficiently reduces the computational cost brought by incorporating optical flow. Comprehensive experiments in both background restoration and object removal tasks demonstrate that FloED outperforms state-of-the-art methods from the perspective of both performance and efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-12T05:25:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.00857v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.00857v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV
  Cache Management</h2>
                <div class="authors">
                    <strong>Authors:</strong> Liu Qianli, Hong Zicong, Chen Fahao, Li Peng, Guo Song
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Serving large language models (LLMs) for massive users is challenged by the significant memory footprint of the transient state, known as the key-value (KV) cache, which scales with sequence length and number of requests. Instead of renting or buying more expensive GPUs, the load imbalance of the KV cache across GPUs, coupled with recent advances in inter-GPU communication, provides an opportunity to serve more requests via request migration. However, high migration overhead and unpredictable request patterns make it challenging. Therefore, this paper proposes MELL, a memory-efficient LLM serving system via multi-GPU KV cache management. It saves the number of GPUs needed in the system by considering the dynamic KV cache load and the costly request migration. Specifically, we first develop an adaptive request migration mechanism to balance the computational and communication overheads and adapt to diverse resource conditions. Then, we design an online algorithm tailored to a multi-LLM request and multi-GPU scheduling problem with migration enabled. It aims to minimise the required GPUs while limiting the number of migrations. Finally, we implement a prototype of MELL and demonstrate that it reduces the number of GPUs by 31% and increases the GPU utilization by 43% at most compared to existing LLM serving systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-12T04:29:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06709v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06709v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 GraphSnapShot: Caching Local Structure for Fast Graph Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dong Liu, Roger Waleffe, Meng Jiang, Shivaram Venkataraman
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In our recent research, we have developed a framework called GraphSnapShot, which has been proven an useful tool for graph learning acceleration. GraphSnapShot is a framework for fast cache, storage, retrieval and computation for graph learning. It can quickly store and update the local topology of graph structure and allows us to track patterns in the structure of graph networks, just like take snapshots of the graphs. In experiments, GraphSnapShot shows efficiency, it can achieve up to 30% training acceleration and 73% memory reduction for lossless graph ML training compared to current baselines such as dgl.This technique is particular useful for large dynamic graph learning tasks such as social media analysis and recommendation systems to process complex relationships between entities.   The code for GraphSnapShot is publicly available at https://github.com/NoakLiu/GraphSnapShot.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-11T15:26:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.DC</span><span>cs.SI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.17918v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.17918v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 Accelerating Particle-Mesh Algorithms with FPGAs and OmpSs@OpenCL</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nicolas Lee Guidotti
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Due to its flexible architecture, FPGAs support unique, deep hardware pipeline implementations for accelerating HPC applications. However, these devices are quite new in the HPC space, and thus, have been scarcely explored outside some specific scientific domain, such as machine learning or biological sequence alignment. The objective of this thesis is to characterize the FPGA-based solution for accelerating particle-mesh algorithms, in which the force applied to each particle is computed based on the fields deposited in a finite mesh (or grid). Our starting point is a 2D kinetic PIC plasma simulator called ZPIC that has the same core algorithm and functionalities as OSIRIS. To create an efficient hardware design, the program keeps the particles strictly sorted by tiles (a group of cells) and uses the local memory as an explicitly managed cache. We also create multiple copies of the local current buffer to solve dependencies during the deposition phase. The resulting pipeline was replicated multiple times to explore data parallelism and increase its throughput. We then compare our hardware solution against similar implementations on GPU and multicore CPUs, showing promising results in term of power efficiency and performance.   Keywords: FPGA, OpenCL, Kinetic Plasma Simulation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-11T12:22:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.14795v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.14795v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 Optimizing digital experiences with content delivery networks:
  Architectures, performance strategies, and future trends</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anuj Tyagi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This research investigates how CDNs (Content Delivery Networks) can improve the digital experience, as consumers increasingly expect fast, efficient, and effortless access to online resources. CDNs play a crucial role in reducing latency, enhancing scalability, and optimizing delivery mechanisms, which is evident across various platforms and regions. The study focuses on key CDN concerns, such as foundational and modern CDN architectures, edge computing, hybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing topics, including caching, load balancing, and the novel features of HTTP/3 and QUIC.   Current trends, such as integrating CDNs with 5G networks, serverless architectures, and AI-driven traffic management, are examined to demonstrate how CDN technology is likely to evolve. The study also addresses challenges related to security, cost, and global regulations. Practical examples from the e-commerce, streaming, and gaming industries highlight how enhanced CDNs are transforming these sectors.   The conclusions emphasize the need to evolve CDN strategies to meet growing user expectations and adapt to the rapidly changing digital landscape. Additionally, the research identifies future research opportunities, particularly in exploring the impact of QC, the enhancement of AI services, and the sustainability of CDN solutions. Overall, the study situates architectural design, performance strategies, and emerging trends to address gaps and create a more efficient and secure approach for improving digital experiences.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-11T03:47:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06428v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06428v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 Tensor Product Attention Is All You Need</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yifan Zhang, Yifeng Liu, Huizhuo Yuan, Zhen Qin, Yang Yuan, Quanquan Gu, Andrew Chi-Chih Yao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, significantly shrinking KV cache size at inference time. By factorizing these representations into contextual low-rank components (contextual factorization) and seamlessly integrating with RoPE, TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model architecture for sequence modeling. Through extensive empirical evaluation of language modeling tasks, we demonstrate that T6 exceeds the performance of standard Transformer baselines including MHA, MQA, GQA, and MLA across various metrics, including perplexity and a range of renowned evaluation benchmarks. Notably, TPAs memory efficiency enables the processing of significantly longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. The code is available at https://github.com/tensorgi/T6.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-11T03:37:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06425v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06425v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 Unispeaker: A Unified Approach for Multimodality-driven Speaker
  Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhengyan Sheng, Zhihao Du, Heng Lu, Shiliang Zhang, Zhen-Hua Ling
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in personalized speech generation have brought synthetic speech increasingly close to the realism of target speakers' recordings, yet multimodal speaker generation remains on the rise. This paper introduces UniSpeaker, a unified approach for multimodality-driven speaker generation. Specifically, we propose a unified voice aggregator based on KV-Former, applying soft contrastive loss to map diverse voice description modalities into a shared voice space, ensuring that the generated voice aligns more closely with the input descriptions. To evaluate multimodality-driven voice control, we build the first multimodality-based voice control (MVC) benchmark, focusing on voice suitability, voice diversity, and speech quality. UniSpeaker is evaluated across five tasks using the MVC benchmark, and the experimental results demonstrate that UniSpeaker outperforms previous modality-specific models. Speech samples are available at \url{https://UniSpeaker.github.io}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-11T00:47:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SD</span><span>cs.AI</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.06394v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.06394v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 Tame fields, Graded Rings and Finite Complete Sequences of Key
  Polynomials</h2>
                <div class="authors">
                    <strong>Authors:</strong> Caio Henrique Silva de Souza
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper, we present a criterion for $(K,v)$ to be henselian and defectless in terms of finite complete sequences of key polynomials. For this, we use the theory of Mac Lane-Vaqui\'e chains and abstract key polynomials. We then prove that a valued field $(K,v)$ is tame if and only if $vK$ is $p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$ admits a finite complete sequence of key polynomials. The properties $vK$ $p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on the associated graded ring. We also make considerations on simply defectless and algebraically maximal valued fields and purely inertial and purely ramified extensions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-10T10:11:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.AC</span><span>13A18</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.01030v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.01030v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 Handover_Management_in_UAV_Networks_with_Blockages</h2>
                <div class="authors">
                    <strong>Authors:</strong> Neetu R R, Gourab Ghatak, Vivek Ashok Bohara
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We investigate the performance of unmanned aerial vehicle (UAV)-based networks in urban environments characterized by blockages, focusing on their capability to support the service demands of mobile users. The UAV-base stations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson point process (MPPP), where the marks represent the altitude of each UAV-BS. Leveraging stochastic geometry, we analyze the impact of blockages on network reliability by studying the meta distribution (MD) of the signal-to-interference noise ratio (SINR) for a specific reliability threshold and the association probabilities for both line-of-sight (LoS) and non line-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile users, we propose a novel cache-based handover management strategy that dynamically selects the cell search time and delays the received signal strength (RSS)-based base station (BS) associations. This strategy aims to minimize unnecessary handovers (HOs) experienced by users by leveraging caching capabilities at user equipment (UE), thus reducing latency, ensuring seamless connectivity, and maintaining the quality of service (QoS). This study provides valuable insights into optimizing UAV network deployments to support the stringent requirements in the network, ensuring reliable, low-latency, and high-throughput communication for next-generation smart cities.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T15:14:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.20433v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.20433v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State
  Drives</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shaobo Li, Yirui Eric Zhou, Hao Ren, Jian Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Unlike non-volatile memory that resides on the processor memory bus, memory-semantic solid-state drives (SSDs) support both byte and block access granularity via PCIe or CXL interconnects. They provide scalable memory capacity using NAND flash at a much lower cost. In addition, they have different performance characteristics for their dual byte/block interface respectively, while offering essential memory semantics for upper-level software. Such a byte-accessible storage device provides new implications on the software system design.   In this paper, we develop a new file system, named ByteFS, by rethinking the design primitives of file systems and SSD firmware to exploit the advantages of both byte and block-granular data accesses. ByteFS supports byte-granular data persistence to retain the persistence nature of SSDs. It extends the core data structure of file systems by enabling dual byte/block-granular data accesses. To facilitate the support for byte-granular writes, \pname{} manages the internal DRAM of SSD firmware in a log-structured manner and enables data coalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also enables coordinated data caching between the host page cache and SSD cache for best utilizing the precious memory resource. We implement ByteFS on both a real programmable SSD and an emulated memory-semantic SSD for sensitivity study. Compared to state-of-the-art file systems for non-volatile memory and conventional SSDs, ByteFS outperforms them by up to 2.7$\times$, while preserving the essential properties of a file system. ByteFS also reduces the write traffic to SSDs by up to 5.1$\times$ by alleviating unnecessary writes caused by both metadata and data updates in file systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T06:18:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.OS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.04993v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.04993v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 Optimal Oblivious Algorithms for Multi-way Joins</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiao Hu, Zhiang Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In cloud databases, cloud computation over sensitive data uploaded by clients inevitably causes concern about data security and privacy. Even when encryption primitives and trusted computing environments are integrated into query processing to safeguard the actual contents of the data, access patterns of algorithms can still leak private information about the data. Oblivious Random Access Memory (ORAM) and circuits are two generic approaches to address this issue, ensuring that access patterns of algorithms remain oblivious to the data. However, deploying these methods on insecure algorithms, particularly for multi-way join processing, is computationally expensive and inherently challenging.   In this paper, we propose a novel sorting-based algorithm for multi-way join processing that operates without relying on ORAM simulations or other security assumptions. Our algorithm is a non-trivial, provably oblivious composition of basic primitives, with time complexity matching the insecure worst-case optimal join algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic, with cache complexity matching the insecure lower bound, also up to a logarithmic factor. This clean and straightforward approach has the potential to be extended to other security settings and implemented in practical database systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-09T03:02:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.04216v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.04216v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 Modern Hardware Security: A Review of Attacks and Countermeasures</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jyotiprakash Mishra, Sanjay K. Sahay
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the exponential rise in the use of cloud services, smart devices, and IoT devices, advanced cyber attacks have become increasingly sophisticated and ubiquitous. Furthermore, the rapid evolution of computing architectures and memory technologies has created an urgent need to understand and address hardware security vulnerabilities. In this paper, we review the current state of vulnerabilities and mitigation strategies in contemporary computing systems. We discuss cache side-channel attacks (including Spectre and Meltdown), power side-channel attacks (such as Simple Power Analysis, Differential Power Analysis, Correlation Power Analysis, and Template Attacks), and advanced techniques like Voltage Glitching and Electromagnetic Analysis to help understand and build robust cybersecurity defense systems and guide further research. We also examine memory encryption, focusing on confidentiality, granularity, key management, masking, and re-keying strategies. Additionally, we cover Cryptographic Instruction Set Architectures, Secure Boot, Root of Trust mechanisms, Physical Unclonable Functions, and hardware fault injection techniques. The paper concludes with an analysis of the RISC-V architecture's unique security challenges. The comprehensive analysis presented in this paper is essential for building resilient hardware security solutions that can protect against both current and emerging threats in an increasingly challenging security landscape.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-08T10:14:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.04394v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.04394v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear
  Approximation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Samrat Mukhopadhyay, Debasmita Mukherjee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We consider the problem of \textit{online sparse linear approximation}, where one predicts the best sparse approximation of a sequence of measurements in terms of linear combination of columns of a given measurement matrix. Such online prediction problems are ubiquitous, ranging from medical trials to web caching to resource allocation. The inherent difficulty of offline recovery also makes the online problem challenging. In this letter, we propose Follow-The-Approximate-Sparse-Leader, an efficient online meta-policy to address this online problem. Through a detailed theoretical analysis, we prove that under certain assumptions on the measurement sequence, the proposed policy enjoys a data-dependent sublinear upper bound on the static regret, which can range from logarithmic to square-root. Numerical simulations are performed to corroborate the theoretical findings and demonstrate the efficacy of the proposed online policy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-07T17:32:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>math.OC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.00799v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.00799v2' target='_blank'>pdf</a>
                </div>
            </div>
            <h1>Keyword: LLM Inference</h1>

            <div class="paper">
                <h2>#1 Dialogue is Better Than Monologue: Instructing Medical LLMs via
  Strategical Conversations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zijie Liu, Xinyu Zhao, Jie Peng, Zhuangdi Zhu, Qingyu Chen, Xia Hu, Tianlong Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Current medical AI systems often fail to replicate real-world clinical reasoning, as they are predominantly trained and evaluated on static text and question-answer tasks. These tuning methods and benchmarks overlook critical aspects like evidence-based reasoning and handling distracting information. To bridge this gap, we introduce a novel benchmark that simulates real-world diagnostic scenarios, integrating noise and difficulty levels aligned with USMLE standards. Moreover, we explore dialogue-based fine-tuning, which transforms static datasets into conversational formats to better capture iterative reasoning processes. Experiments show that dialogue-tuned models outperform traditional methods, with improvements of $9.64\%$ in multi-round reasoning scenarios and $6.18\%$ in accuracy in a noisy environment. Our findings highlight dialogue tuning as a promising approach for advancing clinically aligned and robust medical AI systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T18:58:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17860v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17860v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 Improving Your Model Ranking on Chatbot Arena by Vote Rigging</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rui Min, Tianyu Pang, Chao Du, Qian Liu, Minhao Cheng, Min Lin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Chatbot Arena is a popular platform for evaluating LLMs by pairwise battles, where users vote for their preferred response from two randomly sampled anonymous models. While Chatbot Arena is widely regarded as a reliable LLM ranking leaderboard, we show that crowdsourced voting can be rigged to improve (or decrease) the ranking of a target model $m_{t}$. We first introduce a straightforward target-only rigging strategy that focuses on new battles involving $m_{t}$, identifying it via watermarking or a binary classifier, and exclusively voting for $m_{t}$ wins. However, this strategy is practically inefficient because there are over $190$ models on Chatbot Arena and on average only about $1\%$ of new battles will involve $m_{t}$. To overcome this, we propose omnipresent rigging strategies, exploiting the Elo rating mechanism of Chatbot Arena that any new vote on a battle can influence the ranking of the target model $m_{t}$, even if $m_{t}$ is not directly involved in the battle. We conduct experiments on around $1.7$ million historical votes from the Chatbot Arena Notebook, showing that omnipresent rigging strategies can improve model rankings by rigging only hundreds of new votes. While we have evaluated several defense mechanisms, our findings highlight the importance of continued efforts to prevent vote rigging. Our code is available at https://github.com/sail-sg/Rigging-ChatbotArena.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T18:57:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.CR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17858v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17858v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse
  Autoencoders</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhengxuan Wu, Aryaman Arora, Atticus Geiger, Zheng Wang, Jing Huang, Dan Jurafsky, Christopher D. Manning, Christopher Potts
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Fine-grained steering of language model outputs is essential for safety and reliability. Prompting and finetuning are widely used to achieve these goals, but interpretability researchers have proposed a variety of representation-based techniques as well, including sparse autoencoders (SAEs), linear artificial tomography, supervised steering vectors, linear probes, and representation finetuning. At present, there is no benchmark for making direct comparisons between these proposals. Therefore, we introduce AxBench, a large-scale benchmark for steering and concept detection, and report experiments on Gemma-2-2B and 9B. For steering, we find that prompting outperforms all existing methods, followed by finetuning. For concept detection, representation-based methods such as difference-in-means, perform the best. On both evaluations, SAEs are not competitive. We introduce a novel weakly-supervised representational method (Rank-1 Representation Finetuning; ReFT-r1), which is competitive on both tasks while providing the interpretability advantages that prompting lacks. Along with AxBench, we train and publicly release SAE-scale feature dictionaries for ReFT-r1 and DiffMean.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T18:52:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span><span>I.2.7</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17148v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17148v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 ATTNChecker: Highly-Optimized Fault Tolerant Attention for Large
  Language Model Training</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuhang Liang, Xinyi Li, Jie Ren, Ang Li, Bo Fang, Jieyang Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have demonstrated remarkable performance in various natural language processing tasks. However, the training of these models is computationally intensive and susceptible to faults, particularly in the attention mechanism, which is a critical component of transformer-based LLMs. In this paper, we investigate the impact of faults on LLM training, focusing on INF, NaN, and near-INF values in the computation results with systematic fault injection experiments. We observe the propagation patterns of these errors, which can trigger non-trainable states in the model and disrupt training, forcing the procedure to load from checkpoints. To mitigate the impact of these faults, we propose ATTNChecker, the first Algorithm-Based Fault Tolerance (ABFT) technique tailored for the attention mechanism in LLMs. ATTNChecker is designed based on fault propagation patterns of LLM and incorporates performance optimization to adapt to both system reliability and model vulnerability while providing lightweight protection for fast LLM training. Evaluations on four LLMs show that ATTNChecker incurs on average 7% overhead on training while detecting and correcting all extreme errors. Compared with the state-of-the-art checkpoint/restore approach, ATTNChecker reduces recovery overhead by up to 49x.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T18:49:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.LG</span><span>C.1.4; B.2.3; I.2.7</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.11720v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.11720v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 Large Language Models and Code Security: A Systematic Literature Review</h2>
                <div class="authors">
                    <strong>Authors:</strong> Enna Basic, Alberto Giaretta
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have emerged as powerful tools for automating various programming tasks, including security-related ones, such as detecting and fixing vulnerabilities. Despite their promising capabilities, when required to produce or modify pre-existing code, LLMs could introduce vulnerabilities unbeknown to the programmer. When analyzing code, they could miss clear vulnerabilities or signal nonexistent ones. In this Systematic Literature Review (SLR), we aim to investigate both the security benefits and potential drawbacks of using LLMs for a variety of code-related tasks. In particular, first we focus on the types of vulnerabilities that could be introduced by LLMs, when used for producing code. Second, we analyze the capabilities of LLMs to detect and fix vulnerabilities, in any given code, and how the prompting strategy of choice impacts their performance in these two tasks. Last, we provide an in-depth analysis on how data poisoning attacks on LLMs can impact performance in the aforementioned tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T18:49:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.15004v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.15004v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 DynEx: Dynamic Code Synthesis with Structured Design Exploration for
  Accelerated Exploratory Programming</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jenny Ma, Karthik Sreedhar, Vivian Liu, Pedro Alejandro Perez, Sitong Wang, Riya Sahni, Lydia B. Chilton
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in large language models have significantly expedited the process of generating front-end code. This allows users to rapidly prototype user interfaces and ideate through code, a process known as exploratory programming. However, existing LLM code generation tools focus more on technical implementation details rather than finding the right design given a particular problem. We present DynEx, an LLM-based method for design exploration in accelerated exploratory programming. DynEx introduces a technique to explore the design space through a structured Design Matrix before creating the prototype with a modular, stepwise approach to LLM code generation. Code is generated sequentially, and users can test and approve each step before moving onto the next. A user study of 10 experts found that DynEx increased design exploration and enabled the creation of more complex and varied prototypes compared to a Claude Artifact baseline. We conclude with a discussion of the implications of design exploration for exploratory programming.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T18:46:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.00400v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.00400v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 Learning Beyond the Surface: How Far Can Continual Pre-Training with
  LoRA Enhance LLMs' Domain-Specific Insight Learning?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pouya Pezeshkpour, Estevam Hruschka
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have demonstrated remarkable performance on various tasks, yet their ability to extract and internalize deeper insights from domain-specific datasets remains underexplored. In this study, we investigate how continual pre-training can enhance LLMs' capacity for insight learning across three distinct forms: declarative, statistical, and probabilistic insights. Focusing on two critical domains: medicine and finance, we employ LoRA to train LLMs on two existing datasets. To evaluate each insight type, we create benchmarks to measure how well continual pre-training helps models go beyond surface-level knowledge. We also assess the impact of document modification on capturing insights. The results show that, while continual pre-training on original documents has a marginal effect, modifying documents to retain only essential information significantly enhances the insight-learning capabilities of LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T18:40:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17840v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17840v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 Hierarchical Fallback Architecture for High Risk Online Machine Learning
  Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Gustavo Polleti, Marlesson Santana, Felipe Sassi Del Sant, Eduardo Fontes
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Open Banking powered machine learning applications require novel robustness approaches to deal with challenging stress and failure scenarios. In this paper we propose an hierarchical fallback architecture for improving robustness in high risk machine learning applications with a focus in the financial domain. We define generic failure scenarios often found in online inference that depend on external data providers and we describe in detail how to apply the hierarchical fallback architecture to address them. Finally, we offer a real world example of its applicability in the industry for near-real time transactional fraud risk evaluation using Open Banking data and under extreme stress scenarios.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T18:30:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CE</span><span>cs.SE</span><span>I.2.m</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17834v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17834v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large
  Language Models on Generated Data</h2>
                <div class="authors">
                    <strong>Authors:</strong> Atilla Akkus, Masoud Poorghaffar Aghdam, Mingjie Li, Junjie Chu, Michael Backes, Yang Zhang, Sinem Sav
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have demonstrated significant success in various domain-specific tasks, with their performance often improving substantially after fine-tuning. However, fine-tuning with real-world data introduces privacy risks. To mitigate these risks, developers increasingly rely on synthetic data generation as an alternative to using real data, as data generated by traditional models is believed to be different from real-world data. However, with the advanced capabilities of LLMs, the distinction between real data and data generated by these models has become nearly indistinguishable. This convergence introduces similar privacy risks for generated data to those associated with real data. Our study investigates whether fine-tuning with LLM-generated data truly enhances privacy or introduces additional privacy risks by examining the structural characteristics of data generated by LLMs, focusing on two primary fine-tuning approaches: supervised fine-tuning (SFT) with unstructured (plain-text) generated data and self-instruct tuning. In the scenario of SFT, the data is put into a particular instruction tuning format used by previous studies. We use Personal Information Identifier (PII) leakage and Membership Inference Attacks (MIAs) on the Pythia Model Suite and Open Pre-trained Transformer (OPT) to measure privacy risks. Notably, after fine-tuning with unstructured generated data, the rate of successful PII extractions for Pythia increased by over 20%, highlighting the potential privacy implications of such approaches. Furthermore, the ROC-AUC score of MIAs for Pythia-6.9b, the second biggest model of the suite, increases over 40% after self-instruct tuning. Our results indicate the potential privacy risks associated with fine-tuning LLMs using generated data, underscoring the need for careful consideration of privacy safeguards in such approaches.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T18:27:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11423v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11423v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 P-TAME: Explain Any Image Classifier with Trained Perturbations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mariano V. Ntrougkas, Vasileios Mezaris, Ioannis Patras
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The adoption of Deep Neural Networks (DNNs) in critical fields where predictions need to be accompanied by justifications is hindered by their inherent black-box nature. In this paper, we introduce P-TAME (Perturbation-based Trainable Attention Mechanism for Explanations), a model-agnostic method for explaining DNN-based image classifiers. P-TAME employs an auxiliary image classifier to extract features from the input image, bypassing the need to tailor the explanation method to the internal architecture of the backbone classifier being explained. Unlike traditional perturbation-based methods, which have high computational requirements, P-TAME offers an efficient alternative by generating high-resolution explanations in a single forward pass during inference. We apply P-TAME to explain the decisions of VGG-16, ResNet-50, and ViT-B-16, three distinct and widely used image classifiers. Quantitative and qualitative results show that our method matches or outperforms previous explainability methods, including model-specific approaches. Code and trained models will be released upon acceptance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T18:06:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17813v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17813v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 Cell Deformation Signatures along the Apical-Basal Axis: A 3D Continuum
  Mechanics Shell Model</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jairo M. Rojas, Mayisha Z. Nakib, Vivian W. Tang, William M. Brieher, Sascha Hilgenfeldt
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Two-dimensional (2D) mechanical models of confluent tissues have related the mechanical state of a monolayer of cells to the average perimeter length of the cell cross sections, predicting floppiness or rigidity of the material. For the well-studied system of in-vitro MDCK epithelial cells, however, we find experimentally that cells in mechanically rigid tissues display long perimeters characteristic of a floppy state in 2D models. We suggest that this discrepancy is due to mechanical effects in the third (apical-basal) dimension, including those caused by actin stress fibers near the basal membrane. To quantitatively understand cell deformations in 3D, we develop a continuum mechanics model of epithelial cells as elastic cylindrical shells, with appropriate boundary conditions reflecting both the passive confinement of neighboring cells and the active stress of actomyosin contractility. This formalism yields analytical solutions predicting cell cross sections along the entire cylinder axis. Deconvolution microscopy experimental data confirm the significant and systematic change in cell shape parameters in this apical-basal direction. In addition to providing a wealth of detailed information on deformation on the subcellular scale, the results of the approach alter our understanding of how active tissues balance requirements of their stiffness and integrity, suggesting they are more robust against loss of rigidity than previously inferred.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T17:59:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.soft</span><span>physics.bio-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17810v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17810v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 LEKA:LLM-Enhanced Knowledge Augmentation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xinhao Zhang, Jinghan Zhang, Fengran Mo, Dongjie Wang, Yanjie Fu, Kunpeng Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Humans excel in analogical learning and knowledge transfer and, more importantly, possess a unique understanding of identifying appropriate sources of knowledge. From a model's perspective, this presents an interesting challenge. If models could autonomously retrieve knowledge useful for transfer or decision-making to solve problems, they would transition from passively acquiring to actively accessing and learning from knowledge. However, filling models with knowledge is relatively straightforward -- it simply requires more training and accessible knowledge bases. The more complex task is teaching models about which knowledge can be analogized and transferred. Therefore, we design a knowledge augmentation method LEKA for knowledge transfer that actively searches for suitable knowledge sources that can enrich the target domain's knowledge. This LEKA method extracts key information from textual information from the target domain, retrieves pertinent data from external data libraries, and harmonizes retrieved data with the target domain data in feature space and marginal probability measures. We validate the effectiveness of our approach through extensive experiments across various domains and demonstrate significant improvements over traditional methods in reducing computational costs, automating data alignment, and optimizing transfer learning outcomes.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T17:44:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17802v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17802v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 Leveraging Multimodal LLM for Inspirational User Interface Search</h2>
                <div class="authors">
                    <strong>Authors:</strong> Seokhyeon Park, Yumin Song, Soohyun Lee, Jaeyoung Kim, Jinwook Seo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Inspirational search, the process of exploring designs to inform and inspire new creative work, is pivotal in mobile user interface (UI) design. However, exploring the vast space of UI references remains a challenge. Existing AI-based UI search methods often miss crucial semantics like target users or the mood of apps. Additionally, these models typically require metadata like view hierarchies, limiting their practical use. We used a multimodal large language model (MLLM) to extract and interpret semantics from mobile UI images. We identified key UI semantics through a formative study and developed a semantic-based UI search system. Through computational and human evaluations, we demonstrate that our approach significantly outperforms existing UI retrieval methods, offering UI designers a more enriched and contextually relevant search experience. We enhance the understanding of mobile UI design semantics and highlight MLLMs' potential in inspirational search, providing a rich dataset of UI semantics for future studies.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T17:38:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3706598.3714213' target='_blank'>doi</a><a href='http://arxiv.org/abs/2501.17799v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17799v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 BreezyVoice: Adapting TTS for Taiwanese Mandarin with Enhanced Polyphone
  Disambiguation -- Challenges and Insights</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chan-Jan Hsu, Yi-Cheng Lin, Chia-Chun Lin, Wei-Chih Chen, Ho Lam Chung, Chen-An Li, Yi-Chang Chen, Chien-Yu Yu, Ming-Ji Lee, Chien-Cheng Chen, Ru-Heng Huang, Hung-yi Lee, Da-Shan Shiu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present BreezyVoice, a Text-to-Speech (TTS) system specifically adapted for Taiwanese Mandarin, highlighting phonetic control abilities to address the unique challenges of polyphone disambiguation in the language. Building upon CosyVoice, we incorporate a $S^{3}$ tokenizer, a large language model (LLM), an optimal-transport conditional flow matching model (OT-CFM), and a grapheme to phoneme prediction model, to generate realistic speech that closely mimics human utterances. Our evaluation demonstrates BreezyVoice's superior performance in both general and code-switching contexts, highlighting its robustness and effectiveness in generating high-fidelity speech. Additionally, we address the challenges of generalizability in modeling long-tail speakers and polyphone disambiguation. Our approach significantly enhances performance and offers valuable insights into the workings of neural codec TTS systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T17:31:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17790v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17790v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 WARP: An Efficient Engine for Multi-Vector Retrieval</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jan Luca Scheerer, Matei Zaharia, Christopher Potts, Gustavo Alonso, Omar Khattab
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We study the efficiency of multi-vector retrieval methods like ColBERT and its recent variant XTR. We introduce WARP, a retrieval engine that drastically improves the efficiency of XTR-based ColBERT retrievers through three key innovations: (1) WARP$_\text{SELECT}$ for dynamic similarity imputation, (2) implicit decompression to bypass costly vector reconstruction, and (3) a two-stage reduction process for efficient scoring. Combined with optimized C++ kernels and specialized inference runtimes, WARP reduces end-to-end latency by 41x compared to XTR's reference implementation and thereby achieves a 3x speedup over PLAID from the the official ColBERT implementation.   We study the efficiency of multi-vector retrieval methods like ColBERT and its recent variant XTR. We introduce WARP, a retrieval engine that drastically improves the efficiency of XTR-based ColBERT retrievers through three key innovations: (1) WARP$_\text{SELECT}$ for dynamic similarity imputation, (2) implicit decompression during retrieval, and (3) a two-stage reduction process for efficient scoring. Thanks also to highly-optimized C++ kernels and to the adoption of specialized inference runtimes, WARP can reduce end-to-end query latency relative to XTR's reference implementation by 41x. And it thereby achieves a 3x speedup over the official ColBERTv2 PLAID engine, while preserving retrieval quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T17:26:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17788v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17788v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 Reasoning Over the Glyphs: Evaluation of LLM's Decipherment of Rare
  Scripts</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yu-Fei Shih, Zheng-Lin Lin, Shu-Kai Hsieh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We explore the capabilities of LVLMs and LLMs in deciphering rare scripts not encoded in Unicode. We introduce a novel approach to construct a multimodal dataset of linguistic puzzles involving such scripts, utilizing a tokenization method for language glyphs. Our methods include the Picture Method for LVLMs and the Description Method for LLMs, enabling these models to tackle these challenges. We conduct experiments using prominent models, GPT-4o, Gemini, and Claude 3.5 Sonnet, on linguistic puzzles. Our findings reveal the strengths and limitations of current AI methods in linguistic decipherment, highlighting the impact of Unicode encoding on model performance and the challenges of modeling visual language tokens through descriptions. Our study advances understanding of AI's potential in linguistic decipherment and underscores the need for further research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T17:24:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span><span>J.5; I.2.7</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17785v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17785v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 Large Language Models Can Solve Real-World Planning Rigorously with
  Formal Verification Tools</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yilun Hao, Yongchao Chen, Yang Zhang, Chuchu Fan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) struggle to directly generate correct plans for complex multi-constraint planning problems, even with self-verification and self-critique. For example, a U.S. domestic travel planning benchmark TravelPlanner was proposed in Xie et al. (2024), where the best LLM OpenAI o1-preview can only find viable travel plans with a 10% success rate given all needed information. In this work, we tackle this by proposing an LLM-based planning framework that formalizes and solves complex multi-constraint planning problems as constrained satisfiability problems, which are further consumed by sound and complete satisfiability solvers. We start with TravelPlanner as the primary use case and show that our framework achieves a success rate of 93.9% and is effective with diverse paraphrased prompts. More importantly, our framework has strong zero-shot generalizability, successfully handling unseen constraints in our newly created unseen international travel dataset and generalizing well to new fundamentally different domains. Moreover, when user input queries are infeasible, our framework can identify the unsatisfiable core, provide failure reasons, and offers personalized modification suggestions. We show that our framework can modify and solve for an average of 81.6% and 91.7% unsatisfiable queries from two datasets and prove with ablations that all key components of our framework are effective and necessary. Project page: https://sites.google.com/view/llm-rwplanning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T17:24:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.11891v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.11891v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 More is Less: Inducing Sparsity via Overparameterization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hung-Hsu Chou, Johannes Maly, Holger Rauhut
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In deep learning it is common to overparameterize neural networks, that is, to use more parameters than training samples. Quite surprisingly training the neural network via (stochastic) gradient descent leads to models that generalize very well, while classical statistics would suggest overfitting. In order to gain understanding of this implicit bias phenomenon we study the special case of sparse recovery (compressed sensing) which is of interest on its own. More precisely, in order to reconstruct a vector from underdetermined linear measurements, we introduce a corresponding overparameterized square loss functional, where the vector to be reconstructed is deeply factorized into several vectors. We show that, if there exists an exact solution, vanilla gradient flow for the overparameterized loss functional converges to a good approximation of the solution of minimal $\ell_1$-norm. The latter is well-known to promote sparse solutions. As a by-product, our results significantly improve the sample complexity for compressed sensing via gradient flow/descent on overparameterized models derived in previous works. The theory accurately predicts the recovery rate in numerical experiments. Our proof relies on analyzing a certain Bregman divergence of the flow. This bypasses the obstacles caused by non-convexity and should be of independent interest.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T17:22:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.OC</span><span>cs.IT</span><span>cs.LG</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2112.11027v6' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2112.11027v6' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 2SSP: A Two-Stage Framework for Structured Pruning of LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fabrizio Sandri, Elia Cunegatti, Giovanni Iacca
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We propose a novel Two-Stage framework for Structured Pruning (2SSP) for pruning Large Language Models (LLMs), which combines two different strategies of pruning, namely Width and Depth Pruning. The first stage (Width Pruning) removes entire neurons, hence their corresponding rows and columns, aiming to preserve the connectivity among the pruned structures in the intermediate state of the Feed-Forward Networks in each Transformer block. This is done based on an importance score measuring the impact of each neuron over the output magnitude. The second stage (Depth Pruning), instead, removes entire Attention submodules. This is done by applying an iterative process that removes the Attention submodules with the minimum impact on a given metric of interest (in our case, perplexity). We also propose a novel mechanism to balance the sparsity rate of the two stages w.r.t. to the desired global sparsity. We test 2SSP on four LLM families and three sparsity rates (25\%, 37.5\%, and 50\%), measuring the resulting perplexity over three language modeling datasets as well as the performance over six downstream tasks. Our method consistently outperforms five state-of-the-art competitors over three language modeling and six downstream tasks, with an up to two-order-of-magnitude gain in terms of pruning time. The code is available at available at \url{https://github.com/FabrizioSandri/2SSP}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T17:05:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17771v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17771v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 Modelling the evolution of the Galactic disc scale height traced by open
  clusters</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sandro Moreira, André Moitinho, André Silva, Duarte Almeida
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Context. The scale height of the spatial distribution of open clusters (OCs) in the Milky Way exhibits a well known increase with age which is usually interpreted as evidence for dynamical heating of the disc or of the disc having been thicker in the past.   Aims. We address the increase of the scale height with age of the OC population from a different angle. We propose that the apparent thickening of the disc can be largely explained as a consequence of a stronger disruption of OCs near the Galactic plane by disc phenomena, namely encounters with giant molecular clouds (GMCs).   Methods. We present a computational model that forms OCs with different initial masses and follows their orbits while subjecting them to different disruption mechanisms. To setup the model and infer its parameters, we use and analyse a Gaia-based OC catalogue (Dias et al. 2021). We investigate both the spatial and age distributions of the OC population and discuss the completeness of the sample. The simulation results are then compared to the observations.   Results. Consistent with previous studies, the observations reveal that the SH of the spatial distribution of OCs increases with age. We find that it is very likely that the OC sample is incomplete even for the solar neighbourhood. The model simulations successfully reproduce the SH increase with age and the total number of OCs that survive with age up to 1 Gyr. For older OCs, the predicted SH from the model starts deviating from the observations, although remaining within the uncertainties of the observations. This can be related with effects of incompleteness and/or simplifications in the model.   Conclusions. A selective disruption of OCs near the galactic plane through GMC encounters is able to explain the SH evolution of the OC population.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T17:00:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.GA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.14661v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.14661v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 Hybrid Graphs for Table-and-Text based Question Answering using LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ankush Agarwal, Ganesh S, Chaitanya Devaguptapu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Answering questions that require reasoning and aggregation across both structured (tables) and unstructured (raw text) data sources presents significant challenges. Current methods rely on fine-tuning and high-quality, human-curated data, which is difficult to obtain. Recent advances in Large Language Models (LLMs) have shown promising results for multi-hop question answering (QA) over single-source text data in a zero-shot setting, yet exploration into multi-source Table-Text QA remains limited. In this paper, we present a novel Hybrid Graph-based approach for Table-Text QA that leverages LLMs without fine-tuning. Our method constructs a unified Hybrid Graph from textual and tabular data, pruning information based on the input question to provide the LLM with relevant context concisely. We evaluate our approach on the challenging Hybrid-QA and OTT-QA datasets using state-of-the-art LLMs, including GPT-3.5, GPT-4, and LLaMA-3. Our method achieves the best zero-shot performance on both datasets, improving Exact Match scores by up to 10% on Hybrid-QA and 5.4% on OTT-QA. Moreover, our approach reduces token usage by up to 53% compared to the original context.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T16:58:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17767v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17767v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 Prompt Obfuscation for Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> David Pape, Sina Mavali, Thorsten Eisenhofer, Lea Schönherr
                </div>
                <div class="summary">
                    <strong>Summary:</strong> System prompts that include detailed instructions to describe the task performed by the underlying LLM can easily transform foundation models into tools and services with minimal overhead. Because of their crucial impact on the utility, they are often considered intellectual property, similar to the code of a software product. However, extracting system prompts is easily possible. As of today, there is no effective countermeasure to prevent the stealing of system prompts and all safeguarding efforts could be evaded. In this work, we propose an alternative to conventional system prompts. We introduce prompt obfuscation to prevent the extraction of the system prompt with only little overhead. The core idea is to find a representation of the original system prompt that leads to the same functionality, while the obfuscated system prompt does not contain any information that allows conclusions to be drawn about the original system prompt. We evaluate our approach by comparing our obfuscated prompt output with the output of the original prompt, using eight distinct metrics, to measure the lexical, character-level, and semantic similarity. We show that the obfuscated version is constantly on par with the original one. We further perform three different deobfuscation attacks with varying attacker knowledge--covering both black-box and white-box conditions--and show that in realistic attack scenarios an attacker is not able to extract meaningful information. Overall, we demonstrate that prompt obfuscation is an effective mechanism to safeguard the intellectual property of a system prompt while maintaining the same utility as the original prompt.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T16:57:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11026v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11026v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 Bayesian Inference of the Critical Endpoint in 2+1-Flavor System from
  Holographic QCD</h2>
                <div class="authors">
                    <strong>Authors:</strong> Liqiang Zhu, Xun Chen, Kai Zhou, Hanzhong Zhang, Mei Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present a Bayesian holographic model constructed by integrating the equation of state and baryon number susceptibility at zero chemical potential from lattice QCD. The model incorporates error estimates derived from lattice data. With this model, we systematically investigate the thermodynamic properties of the 2+1-flavor QCD system. Using Bayesian Inference, we perform precise calibration of the model parameters and determined the critical endpoint (CEP) position under the maximum a posterior (MAP) estimation to be $(T^{c},\mu_{B}^{c})=(0.0859\;\mathrm{GeV},0.742\;\mathrm{GeV})$. Additionally, we predict the CEP positions within 68\% and 95\% confidence levels, yielding $(T^{c},\; \mu_{B}^{c})_{68\%}$=$(0.0820\text{-}0.0889, 0.71\text{-}0.77)\;\mathrm{GeV}$ and $(T^{c},\; \mu_{B}^{c})_{95\%}$=$(0.0816\text{-}0.0898,\; 0.71\text{-}0.79)\;\mathrm{GeV}$, respectively. Moreover, to validate the reliability and predictive power of our approach, we conduct a comprehensive comparison between our predictions and potential CEP locations proposed by other theoretical models. This work not only establishes a novel Bayesian framework for holographic modeling but also provides valuable insights and theoretical support for exploring phase transitions in strongly-interacting matter under extreme conditions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T16:54:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>hep-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17763v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17763v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 Online Prompt Selection for Program Synthesis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yixuan Li, Lewis Frampton, Federico Mora, Elizabeth Polgreen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) demonstrate impressive capabilities in the domain of program synthesis. This level of performance is not, however, universal across all tasks, all LLMs and all prompting styles. There are many areas where one LLM dominates, one prompting style dominates, or where calling a symbolic solver is a better choice than an LLM. A key challenge for the user then, is to identify not only when an LLM is the right choice of solver, and the appropriate LLM to call for a given synthesis task, but also the right way to call it. A non-expert user who makes the wrong choice, incurs a cost both in terms of results (number of tasks solved, and the time it takes to solve them) and financial cost, if using a closed-source language model via a commercial API. We frame this choice as an online learning problem. We use a multi-armed bandit algorithm to select which symbolic solver, or LLM and prompt combination to deploy in order to maximize a given reward function (which may prioritize solving time, number of synthesis tasks solved, or financial cost of solving). We implement an instance of this approach, called CYANEA, and evaluate it on synthesis queries from the literature in ranking function synthesis, from the syntax-guided synthesis competition, and fresh, unseen queries generated from SMT problems. CYANEA solves 37.2% more queries than the best single solver and achieves results within 4% of the virtual best solver.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T16:52:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05247v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05247v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 On the Partitioning of GPU Power among Multi-Instances</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tirth Vamja, Kaustabha Ray, Felix George, UmaMaheswari C Devi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Efficient power management in cloud data centers is essential for reducing costs, enhancing performance, and minimizing environmental impact. GPUs, critical for tasks like machine learning (ML) and GenAI, are major contributors to power consumption. NVIDIA's Multi-Instance GPU (MIG) technology improves GPU utilization by enabling isolated partitions with per-partition resource tracking, facilitating GPU sharing by multiple tenants. However, accurately apportioning GPU power consumption among MIG instances remains challenging due to a lack of hardware support. This paper addresses this challenge by developing software methods to estimate power usage per MIG partition. We analyze NVIDIA GPU utilization metrics and find that light-weight methods with good accuracy can be difficult to construct. We hence explore the use of ML-based power models to enable accurate, partition-level power estimation. Our findings reveal that a single generic offline power model or modeling method is not applicable across diverse workloads, especially with concurrent MIG usage, and that online models constructed using partition-level utilization metrics of workloads under execution can significantly improve accuracy. Using NVIDIA A100 GPUs, we demonstrate this approach for accurate partition-level power estimation for workloads including matrix multiplication and Large Language Model inference, contributing to transparent and fair carbon reporting.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T16:41:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17752v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17752v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 Early External Safety Testing of OpenAI's o3-mini: Insights from the
  Pre-Deployment Evaluation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aitor Arrieta, Miriam Ugarte, Pablo Valle, José Antonio Parejo, Sergio Segura
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have become an integral part of our daily lives. However, they impose certain risks, including those that can harm individuals' privacy, perpetuate biases and spread misinformation. These risks highlight the need for robust safety mechanisms, ethical guidelines, and thorough testing to ensure their responsible deployment. Safety of LLMs is a key property that needs to be thoroughly tested prior the model to be deployed and accessible to the general users. This paper reports the external safety testing experience conducted by researchers from Mondragon University and University of Seville on OpenAI's new o3-mini LLM as part of OpenAI's early access for safety testing program. In particular, we apply our tool, ASTRAL, to automatically and systematically generate up to date unsafe test inputs (i.e., prompts) that helps us test and assess different safety categories of LLMs. We automatically generate and execute a total of 10,080 unsafe test input on a early o3-mini beta version. After manually verifying the test cases classified as unsafe by ASTRAL, we identify a total of 87 actual instances of unsafe LLM behavior. We highlight key insights and findings uncovered during the pre-deployment external testing phase of OpenAI's latest LLM.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T16:36:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17749v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17749v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 Planning Anything with Rigor: General-Purpose Zero-Shot Planning with
  LLM-based Formalized Programming</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yilun Hao, Yang Zhang, Chuchu Fan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While large language models (LLMs) have recently demonstrated strong potential in solving planning problems, there is a trade-off between flexibility and complexity. LLMs, as zero-shot planners themselves, are still not capable of directly generating valid plans for complex planning problems such as multi-constraint or long-horizon tasks. On the other hand, many frameworks aiming to solve complex planning problems often rely on task-specific preparatory efforts, such as task-specific in-context examples and pre-defined critics/verifiers, which limits their cross-task generalization capability. In this paper, we tackle these challenges by observing that the core of many planning problems lies in optimization problems: searching for the optimal solution (best plan) with goals subject to constraints (preconditions and effects of decisions). With LLMs' commonsense, reasoning, and programming capabilities, this opens up the possibilities of a universal LLM-based approach to planning problems. Inspired by this observation, we propose LLMFP, a general-purpose framework that leverages LLMs to capture key information from planning problems and formally formulate and solve them as optimization problems from scratch, with no task-specific examples needed. We apply LLMFP to 9 planning problems, ranging from multi-constraint decision making to multi-step planning problems, and demonstrate that LLMFP achieves on average 83.7% and 86.8% optimal rate across 9 tasks for GPT-4o and Claude 3.5 Sonnet, significantly outperforming the best baseline (direct planning with OpenAI o1-preview) with 37.6% and 40.7% improvements. We also validate components of LLMFP with ablation experiments and analyzed the underlying success and failure reasons. Project page: https://sites.google.com/view/llmfp.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T16:31:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.12112v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.12112v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 Conformal Distributed Remote Inference in Sensor Networks Under
  Reliability and Communication Constraints</h2>
                <div class="authors">
                    <strong>Authors:</strong> Meiyi Zhu, Matteo Zecchin, Sangwoo Park, Caili Guo, Chunyan Feng, Petar Popovski, Osvaldo Simeone
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper presents communication-constrained distributed conformal risk control (CD-CRC) framework, a novel decision-making framework for sensor networks under communication constraints. Targeting multi-label classification problems, such as segmentation, CD-CRC dynamically adjusts local and global thresholds used to identify significant labels with the goal of ensuring a target false negative rate (FNR), while adhering to communication capacity limits. CD-CRC builds on online exponentiated gradient descent to estimate the relative quality of the observations of different sensors, and on online conformal risk control (CRC) as a mechanism to control local and global thresholds. CD-CRC is proved to offer deterministic worst-case performance guarantees in terms of FNR and communication overhead, while the regret performance in terms of false positive rate (FPR) is characterized as a function of the key hyperparameters. Simulation results highlight the effectiveness of CD-CRC, particularly in communication resource-constrained environments, making it a valuable tool for enhancing the performance and reliability of distributed sensor networks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T16:24:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span><span>cs.IT</span><span>cs.LG</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.07902v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.07902v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 Real-Time Video Generation with Pyramid Attention Broadcast</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xuanlei Zhao, Xiaolong Jin, Kai Wang, Yang You
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present Pyramid Attention Broadcast (PAB), a real-time, high quality and training-free approach for DiT-based video generation. Our method is founded on the observation that attention difference in the diffusion process exhibits a U-shaped pattern, indicating significant redundancy. We mitigate this by broadcasting attention outputs to subsequent steps in a pyramid style. It applies different broadcast strategies to each attention based on their variance for best efficiency. We further introduce broadcast sequence parallel for more efficient distributed inference. PAB demonstrates up to 10.5x speedup across three models compared to baselines, achieving real-time generation for up to 720p videos. We anticipate that our simple yet effective method will serve as a robust baseline and facilitate future research and application for video generation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T16:02:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.12588v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.12588v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 Using Code Generation to Solve Open Instances of Combinatorial Design
  Problems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Christopher D. Rosin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Handbook of Combinatorial Designs catalogs many types of combinatorial designs, together with lists of open instances for which existence has not yet been determined. We develop a constructive protocol CPro1, which uses Large Language Models (LLMs) to generate code that constructs combinatorial designs and resolves some of these open instances. The protocol starts from a definition of a particular type of design, and a verifier that reliably confirms whether a proposed design is valid. The LLM selects strategies and implements them in code, and scaffolding provides automated hyperparameter tuning and execution feedback using the verifier. Most generated code fails, but by generating many candidates, the protocol automates exploration of a variety of standard methods (e.g. simulated annealing, genetic algorithms) and experimentation with variations (e.g. cost functions) to find successful approaches. Testing on 16 different types of designs, CPro1 constructs solutions to open instances for 6 of them: Symmetric and Skew Weighing Matrices, Equidistant Permutation Arrays, Packing Arrays, Balanced Ternary Designs, and Florentine Rectangles.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T15:57:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span><span>cs.DM</span><span>math.CO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17725v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17725v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 Fundamentals of non-parametric statistical inference for integrated
  quantiles</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nadezhda Gribkova, Mengqi Wang, Ričardas Zitikis
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present a general non-parametric statistical inference theory for integrals of quantiles without assuming any specific sampling design or dependence structure. Technical considerations are accompanied by examples and discussions, including those pertaining to the bias of empirical estimators. To illustrate how the general results can be adapted to specific situations, we derive - at a stroke and under minimal conditions - consistency and asymptotic normality of the empirical tail-value-at-risk, Lorenz and Gini curves at any probability level in the case of the simple random sampling, thus facilitating a comparison of our results with what is already known in the literature. Notes and references concerning underlying technicalities in the case of dependent (i.e., time series) data are offered. As a by-product, our general results provide new and unified proofs of large-sample properties of a number of classical statistical estimators, such as trimmed means, and give additional insights into the origins of, and the reasons for, various necessary and sufficient conditions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T15:47:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.ST</span><span>stat.TH</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17722v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17722v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 Bayesian Variable Selection in Distributed Lag Models: A Focus on Binary
  Quantile and Count Data Regressions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Daniel Dempsey, Jason Wyse
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Distributed Lag Models (DLMs) and similar regression approaches such as MIDAS have been used for many decades in econometrics and more recently to investigate how poor air quality adversely affects human health. In this paper we describe how to expand the utility of these models for Bayesian inference by leveraging latent variables. In particular we explain how to perform binary regression to better handle imbalanced data, how to incorporate negative binomial regression, and how to estimate the probability of predictor inclusion. Extra parameters introduced through the DLM framework may require calibration for the MCMC algorithm, but this will not be the case in DLM-based analyses often seen in pollution exposure literature. In these cases, the parameters are inferred through a fully automatic Gibbs sampling procedure.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T15:43:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>stat.AP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2403.03646v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2403.03646v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 LogLLM: Log-based Anomaly Detection Using Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wei Guan, Jian Cao, Shiyou Qian, Jianqi Gao, Chun Ouyang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Software systems often record important runtime information in logs to help with troubleshooting. Log-based anomaly detection has become a key research area that aims to identify system issues through log data, ultimately enhancing the reliability of software systems. Traditional deep learning methods often struggle to capture the semantic information embedded in log data, which is typically organized in natural language. In this paper, we propose LogLLM, a log-based anomaly detection framework that leverages large language models (LLMs). LogLLM employs BERT for extracting semantic vectors from log messages, while utilizing Llama, a transformer decoder-based model, for classifying log sequences. Additionally, we introduce a projector to align the vector representation spaces of BERT and Llama, ensuring a cohesive understanding of log semantics. Unlike conventional methods that require log parsers to extract templates, LogLLM preprocesses log messages with regular expressions, streamlining the entire process. Our framework is trained through a novel three-stage procedure designed to enhance performance and adaptability. Experimental results across four public datasets demonstrate that LogLLM outperforms state-of-the-art methods. Even when handling unstable logs, it effectively captures the semantic meaning of log messages and detects anomalies accurately.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T15:41:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.08561v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.08561v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 A Two-Step Procedure to Detect Cosmological Gravitational Wave
  Backgrounds with Next-Generation Terrestrial Gravitational-Wave Detectors</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haowen Zhong, Luca Reali, Bei Zhou, Emanuele Berti, Vuk Mandic
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Cosmological gravitational-wave backgrounds are an exciting science target for next-generation ground-based detectors, as they encode invaluable information about the primordial Universe. However, any such background is expected to be obscured by the astrophysical foreground from compact-binary coalescences. We propose a novel framework to detect a cosmological gravitational-wave background in the presence of binary black holes and binary neutron star signals with next-generation ground-based detectors, including Cosmic Explorer and the Einstein Telescope. Our procedure involves first removing all the individually resolved binary black hole signals by notching them out in the time-frequency domain. Then, we perform joint Bayesian inference on the individually resolved binary neutron star signals, the unresolved binary neutron star foreground, and the cosmological background. For a flat cosmological background, we find that we can claim detection at $5\,\sigma$ level when $\Omega_\mathrm{ref}\geqslant 2.7\times 10^{-12}/\sqrt{T_\mathrm{obs}/\mathrm{yr}}$, where $T_\mathrm{obs}$ is the observation time (in years), which is within a factor of $\lesssim2$ from the sensitivity reached in absence of these astrophysical foregrounds.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T15:36:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>gr-qc</span><span>astro-ph.CO</span><span>astro-ph.HE</span><span>astro-ph.IM</span><span>hep-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17717v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17717v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 Towards Lifelong Dialogue Agents via Timeline-based Memory Management</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kai Tzu-iunn Ong, Namyoung Kim, Minju Gwak, Hyungjoo Chae, Taeyoon Kwon, Yohan Jo, Seung-won Hwang, Dongha Lee, Jinyoung Yeo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> To achieve lifelong human-agent interaction, dialogue agents need to constantly memorize perceived information and properly retrieve it for response generation (RG). While prior studies focus on getting rid of outdated memories to improve retrieval quality, we argue that such memories provide rich, important contextual cues for RG (e.g., changes in user behaviors) in long-term conversations. We present THEANINE, a framework for LLM-based lifelong dialogue agents. THEANINE discards memory removal and manages large-scale memories by linking them based on their temporal and cause-effect relation. Enabled by this linking structure, THEANINE augments RG with memory timelines - series of memories representing the evolution or causality of relevant past events. Along with THEANINE, we introduce TeaFarm, a counterfactual-driven evaluation scheme, addressing the limitation of G-Eval and human efforts when assessing agent performance in integrating past memories into RG. A supplementary video for THEANINE and data for TeaFarm are at https://huggingface.co/spaces/ResearcherScholar/Theanine.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T15:34:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.10996v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.10996v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 RICoTA: Red-teaming of In-the-wild Conversation with Test Attempts</h2>
                <div class="authors">
                    <strong>Authors:</strong> Eujeong Choi, Younghun Jeong, Soomin Kim, Won Ik Cho
                </div>
                <div class="summary">
                    <strong>Summary:</strong> User interactions with conversational agents (CAs) evolve in the era of heavily guardrailed large language models (LLMs). As users push beyond programmed boundaries to explore and build relationships with these systems, there is a growing concern regarding the potential for unauthorized access or manipulation, commonly referred to as "jailbreaking." Moreover, with CAs that possess highly human-like qualities, users show a tendency toward initiating intimate sexual interactions or attempting to tame their chatbots. To capture and reflect these in-the-wild interactions into chatbot designs, we propose RICoTA, a Korean red teaming dataset that consists of 609 prompts challenging LLMs with in-the-wild user-made dialogues capturing jailbreak attempts. We utilize user-chatbot conversations that were self-posted on a Korean Reddit-like community, containing specific testing and gaming intentions with a social chatbot. With these prompts, we aim to evaluate LLMs' ability to identify the type of conversation and users' testing purposes to derive chatbot design implications for mitigating jailbreaking risks. Our dataset will be made publicly available via GitHub.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T15:32:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17715v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17715v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 STGCN-LSTM for Olympic Medal Prediction: Dynamic Power Modeling and
  Causal Policy Optimization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yiquan Wang, Jiaying Wang, Jingyi Yang, Zihao Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper proposes a novel hybrid model, STGCN-LSTM, to forecast Olympic medal distributions by integrating the spatio-temporal relationships among countries and the long-term dependencies of national performance. The Spatial-Temporal Graph Convolution Network (STGCN) captures geographic and interactive factors-such as coaching exchange and socio-economic links-while the Long Short-Term Memory (LSTM) module models historical trends in medal counts, economic data, and demographics. To address zero-inflated outputs (i.e., the disparity between countries that consistently yield wins and those never having won medals), a Zero-Inflated Compound Poisson (ZICP) framework is incorporated to separate random zeros from structural zeros, providing a clearer view of potential breakthrough performances. Validation includes historical backtracking, policy shock simulations, and causal inference checks, confirming the robustness of the proposed method. Results shed light on the influence of coaching mobility, event specialization, and strategic investment on medal forecasts, offering a data-driven foundation for optimizing sports policies and resource allocation in diverse Olympic contexts.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T15:28:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17711v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17711v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 Inferring Implicit Goals Across Differing Task Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Silvia Tulli, Stylianos Loukas Vasileiou, Mohamed Chetouani, Sarath Sreedharan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> One of the significant challenges to generating value-aligned behavior is to not only account for the specified user objectives but also any implicit or unspecified user requirements. The existence of such implicit requirements could be particularly common in settings where the user's understanding of the task model may differ from the agent's estimate of the model. Under this scenario, the user may incorrectly expect some agent behavior to be inevitable or guaranteed. This paper addresses such expectation mismatch in the presence of differing models by capturing the possibility of unspecified user subgoal in the context of a task captured as a Markov Decision Process (MDP) and querying for it as required. Our method identifies bottleneck states and uses them as candidates for potential implicit subgoals. We then introduce a querying strategy that will generate the minimal number of queries required to identify a policy guaranteed to achieve the underlying goal. Our empirical evaluations demonstrate the effectiveness of our approach in inferring and achieving unstated goals across various tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T15:20:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.RO</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17704v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17704v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 Uniform Inference on High-dimensional Spatial Panel Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Victor Chernozhukov, Chen Huang, Weining Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We propose employing a debiased-regularized, high-dimensional generalized method of moments (GMM) framework to perform inference on large-scale spatial panel networks. In particular, network structure with a flexible sparse deviation, which can be regarded either as latent or as misspecified from a predetermined adjacency matrix, is estimated using debiased machine learning approach. The theoretical analysis establishes the consistency and asymptotic normality of our proposed estimator, taking into account general temporal and spatial dependency inherent in the data-generating processes. A primary contribution of our study is the development of uniform inference theory that enables hypothesis testing on the parameters of interest, including zero or non-zero elements in the network structure. Additionally, the asymptotic properties for the estimator are derived for both linear and nonlinear moments. Simulations demonstrate superior performance of our proposed approach. Lastly, we apply our methodology to investigate the spatial network effect of stock returns.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T14:59:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>econ.EM</span><span>stat.ME</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2105.07424v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2105.07424v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 ContourFormer:Real-Time Contour-Based End-to-End Instance Segmentation
  Transformer</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weiwei yao, Chen Li, Minjun Xiong, Wenbo Dong, Hao Chen, Xiong Xiao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper presents Contourformer, a real-time contour-based instance segmentation algorithm. The method is fully based on the DETR paradigm and achieves end-to-end inference through iterative and progressive mechanisms to optimize contours. To improve efficiency and accuracy, we develop two novel techniques: sub-contour decoupling mechanisms and contour fine-grained distribution refinement.In the sub-contour decoupling mechanism, we propose a deformable attention-based module that adaptively selects sampling regions based on the current predicted contour, enabling more effective capturing of object boundary information. Additionally, we design a multi-stage optimization process to enhance segmentation precision by progressively refining sub-contours. The contour fine-grained distribution refinement technique aims to further improve the ability to express fine details of contours.These innovations enable Contourformer to achieve stable and precise segmentation for each instance while maintaining real-time performance. Extensive experiments demonstrate the superior performance of Contourformer on multiple benchmark datasets, including SBD, COCO, and KINS. We conduct comprehensive evaluations and comparisons with existing state-of-the-art methods, showing significant improvements in both accuracy and inference speed.This work provides a new solution for contour-based instance segmentation tasks and lays a foundation for future research, with the potential to become a strong baseline method in this field.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T14:56:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17688v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17688v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 Continuously Learning New Words in Automatic Speech Recognition</h2>
                <div class="authors">
                    <strong>Authors:</strong> Christian Huber, Alexander Waibel
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Despite recent advances, Automatic Speech Recognition (ASR) systems are still far from perfect. Typical errors include acronyms, named entities, and domain-specific special words for which little or no labeled data is available. To address the problem of recognizing these words, we propose a self-supervised continual learning approach: Given the audio of a lecture talk with the corresponding slides, we bias the model towards decoding new words from the slides by using a memory-enhanced ASR model from the literature. Then, we perform inference on the talk, collecting utterances that contain detected new words into an adaptation data set. Continual learning is then performed by training adaptation weights added to the model on this data set. The whole procedure is iterated for many talks. We show that with this approach, we obtain increasing performance on the new words when they occur more frequently (more than 80% recall) while preserving the general performance of the model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T14:55:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2401.04482v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2401.04482v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 Algorithmic syntactic causal identification</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dhurim Cakiqi, Max A. Little
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Causal identification in causal Bayes nets (CBNs) is an important tool in causal inference allowing the derivation of interventional distributions from observational distributions where this is possible in principle. However, most existing formulations of causal identification using techniques such as d-separation and do-calculus are expressed within the mathematical language of classical probability theory on CBNs. However, there are many causal settings where probability theory and hence current causal identification techniques are inapplicable such as relational databases, dataflow programs such as hardware description languages, distributed systems and most modern machine learning algorithms. We show that this restriction can be lifted by replacing the use of classical probability theory with the alternative axiomatic foundation of symmetric monoidal categories. In this alternative axiomatization, we show how an unambiguous and clean distinction can be drawn between the general syntax of causal models and any specific semantic implementation of that causal model. This allows a purely syntactic algorithmic description of general causal identification by a translation of recent formulations of the general ID algorithm through fixing. Our description is given entirely in terms of the non-parametric ADMG structure specifying a causal model and the algebraic signature of the corresponding monoidal category, to which a sequence of manipulations is then applied so as to arrive at a modified monoidal category in which the desired, purely syntactic interventional causal model, is obtained. We use this idea to derive purely syntactic analogues of classical back-door and front-door causal adjustment, and illustrate an application to a more complex causal model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T14:41:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.LG</span><span>stat.ME</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2403.09580v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2403.09580v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 Deterministic and statistical calibration of constitutive models from
  full-field data with parametric physics-informed neural networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> David Anton, Jendrik-Alexander Tröger, Henning Wessels, Ulrich Römer, Alexander Henkes, Stefan Hartmann
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The calibration of constitutive models from full-field data has recently gained increasing interest due to improvements in full-field measurement capabilities. In addition to the experimental characterization of novel materials, continuous structural health monitoring is another application that is of great interest. However, monitoring is usually associated with severe time constraints, difficult to meet with standard numerical approaches. Therefore, parametric physics-informed neural networks (PINNs) for constitutive model calibration from full-field displacement data are investigated. In an offline stage, a parametric PINN can be trained to learn a parameterized solution of the underlying partial differential equation. In the subsequent online stage, the parametric PINN then acts as a surrogate for the parameters-to-state map in calibration. We test the proposed approach for the deterministic least-squares calibration of a linear elastic as well as a hyperelastic constitutive model from noisy synthetic displacement data. We further carry out Markov chain Monte Carlo-based Bayesian inference to quantify the uncertainty. A proper statistical evaluation of the results underlines the high accuracy of the deterministic calibration and that the estimated uncertainty is valid. Finally, we consider experimental data and show that the results are in good agreement with a finite element method-based calibration. Due to the fast evaluation of PINNs, calibration can be performed in near real-time. This advantage is particularly evident in many-query applications such as Markov chain Monte Carlo-based Bayesian inference.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T14:21:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2405.18311v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.18311v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 Planning with Vision-Language Models and a Use Case in Robot-Assisted
  Teaching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xuzhe Dang, Lada Kudláčková, Stefan Edelkamp
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Automating the generation of Planning Domain Definition Language (PDDL) with Large Language Model (LLM) opens new research topic in AI planning, particularly for complex real-world tasks. This paper introduces Image2PDDL, a novel framework that leverages Vision-Language Models (VLMs) to automatically convert images of initial states and descriptions of goal states into PDDL problems. By providing a PDDL domain alongside visual inputs, Imasge2PDDL addresses key challenges in bridging perceptual understanding with symbolic planning, reducing the expertise required to create structured problem instances, and improving scalability across tasks of varying complexity. We evaluate the framework on various domains, including standard planning domains like blocksworld and sliding tile puzzles, using datasets with multiple difficulty levels. Performance is assessed on syntax correctness, ensuring grammar and executability, and content correctness, verifying accurate state representation in generated PDDL problems. The proposed approach demonstrates promising results across diverse task complexities, suggesting its potential for broader applications in AI planning. We will discuss a potential use case in robot-assisted teaching of students with Autism Spectrum Disorder.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T14:04:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17665v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17665v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 GalSBI: Phenomenological galaxy population model for cosmology using
  simulation-based inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Silvan Fischbacher, Tomasz Kacprzak, Luca Tortorelli, Beatrice Moser, Alexandre Refregier, Patrick Gebhardt, Daniel Gruen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present GalSBI, a phenomenological model of the galaxy population for cosmological applications using simulation-based inference. The model is based on analytical parametrizations of galaxy luminosity functions, morphologies and spectral energy distributions. Model constraints are derived through iterative Approximate Bayesian Computation, by comparing Hyper Suprime-Cam deep field images with simulations which include a forward model of instrumental, observational and source extraction effects. We developed an emulator trained on image simulations using a normalizing flow. We use it to accelerate the inference by predicting detection probabilities, including blending effects and photometric properties of each object, while accounting for background and PSF variations. This enables robustness tests for all elements of the forward model and the inference. The model demonstrates excellent performance when comparing photometric properties from simulations with observed imaging data for key parameters such as magnitudes, colors and sizes. The redshift distribution of simulated galaxies agrees well with high-precision photometric redshifts in the COSMOS field within $1.5\sigma$ for all magnitude cuts. Additionally, we demonstrate how GalSBI's redshifts can be utilized for splitting galaxy catalogs into tomographic bins, highlighting its potential for current and upcoming surveys. GalSBI is fully open-source, with the accompanying Python package, $\texttt{galsbi}$, offering an easy interface to quickly generate realistic, survey-independent galaxy catalogs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T13:52:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.CO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.08701v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.08701v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models
  into Assembly Code Obfuscation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Seyedreza Mohseni, Seyedali Mohammadi, Deepa Tilwani, Yash Saxena, Gerald Ketu Ndawula, Sriram Vema, Edward Raff, Manas Gaur
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Malware authors often employ code obfuscations to make their malware harder to detect. Existing tools for generating obfuscated code often require access to the original source code (e.g., C++ or Java), and adding new obfuscations is a non-trivial, labor-intensive process. In this study, we ask the following question: Can Large Language Models (LLMs) potentially generate a new obfuscated assembly code? If so, this poses a risk to anti-virus engines and potentially increases the flexibility of attackers to create new obfuscation patterns. We answer this in the affirmative by developing the MetamorphASM benchmark comprising MetamorphASM Dataset (MAD) along with three code obfuscation techniques: dead code, register substitution, and control flow change. The MetamorphASM systematically evaluates the ability of LLMs to generate and analyze obfuscated code using MAD, which contains 328,200 obfuscated assembly code samples. We release this dataset and analyze the success rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder, CodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly code. The evaluation was performed using established information-theoretic metrics and manual human review to ensure correctness and provide the foundation for researchers to study and develop remediations to this risk.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T13:52:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.16135v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.16135v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 Network Fission Ensembles for Low-Cost Self-Ensembles</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hojung Lee, Jong-Seok Lee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent ensemble learning methods for image classification have been shown to improve classification accuracy with low extra cost. However, they still require multiple trained models for ensemble inference, which eventually becomes a significant burden when the model size increases. In this paper, we propose a low-cost ensemble learning and inference, called Network Fission Ensembles (NFE), by converting a conventional network itself into a multi-exit structure. Starting from a given initial network, we first prune some of the weights to reduce the training burden. We then group the remaining weights into several sets and create multiple auxiliary paths using each set to construct multi-exits. We call this process Network Fission. Through this, multiple outputs can be obtained from a single network, which enables ensemble learning. Since this process simply changes the existing network structure to multi-exits without using additional networks, there is no extra computational burden for ensemble learning and inference. Moreover, by learning from multiple losses of all exits, the multi-exits improve performance via regularization, and high performance can be achieved even with increased network sparsity. With our simple yet effective method, we achieve significant improvement compared to existing ensemble methods. The code is available at https://github.com/hjdw2/NFE.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T13:27:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.02301v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.02301v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 A Unified Representation of Density-Power-Based Divergences Reducible to
  M-Estimation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Masahiro Kobayashi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Density-power-based divergences are known to provide robust inference procedures against outliers, and their extensions have been widely studied. A characteristic of successful divergences is that the estimation problem can be reduced to M-estimation. In this paper, we define a norm-based Bregman density power divergence (NB-DPD) -- density-power-based divergence with functional flexibility within the framework of Bregman divergences that can be reduced to M-estimation. We show that, by specifying the function $\phi_\gamma$, NB-DPD reduces to well-known divergences, such as the density power divergence and the $\gamma$-divergence. Furthermore, by examining the combinations of functions $\phi_\gamma$ corresponding to existing divergences, we show that a new divergence connecting these existing divergences can be derived. Finally, we show that the redescending property, one of the key indicators of robustness, holds only for the $\gamma$-divergence.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T13:22:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>math.IT</span><span>math.ST</span><span>stat.ML</span><span>stat.TH</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.16287v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.16287v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 In-Context Meta LoRA Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yihua Shao, Minxi Yan, Yang Liu, Siyu Chen, Wenjie Chen, Xinwei Long, Ziyang Yan, Lei Li, Chenyu Zhang, Nicu Sebe, Hao Tang, Yan Wang, Hao Zhao, Mengzhu Wang, Jingcai Guo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Low-rank Adaptation (LoRA) has demonstrated remarkable capabilities for task specific fine-tuning. However, in scenarios that involve multiple tasks, training a separate LoRA model for each one results in considerable inefficiency in terms of storage and inference. Moreover, existing parameter generation methods fail to capture the correlations among these tasks, making multi-task LoRA parameter generation challenging. To address these limitations, we propose In-Context Meta LoRA (ICM-LoRA), a novel approach that efficiently achieves task-specific customization of large language models (LLMs). Specifically, we use training data from all tasks to train a tailored generator, Conditional Variational Autoencoder (CVAE). CVAE takes task descriptions as inputs and produces task-aware LoRA weights as outputs. These LoRA weights are then merged with LLMs to create task-specialized models without the need for additional fine-tuning. Furthermore, we utilize in-context meta-learning for knowledge enhancement and task mapping, to capture the relationship between tasks and parameter distributions. As a result, our method achieves more accurate LoRA parameter generation for diverse tasks using CVAE. ICM-LoRA enables more accurate LoRA parameter reconstruction than current parameter reconstruction methods and is useful for implementing task-specific enhancements of LoRA parameters. At the same time, our method occupies 283MB, only 1\% storage compared with the original LoRA.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T13:12:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17635v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17635v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 The Imitation Game According To Turing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sharon Temtsin, Diane Proudfoot, David Kaber, Christoph Bartneck
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The current cycle of hype and anxiety concerning the benefits and risks to human society of Artificial Intelligence is fuelled, not only by the increasing use of generative AI and other AI tools by the general public, but also by claims made on behalf of such technology by popularizers and scientists. In particular, recent studies have claimed that Large Language Models (LLMs) can pass the Turing Test-a goal for AI since the 1950s-and therefore can "think". Large-scale impacts on society have been predicted as a result. Upon detailed examination, however, none of these studies has faithfully applied Turing's original instructions. Consequently, we conducted a rigorous Turing Test with GPT-4-Turbo that adhered closely to Turing's instructions for a three-player imitation game. We followed established scientific standards where Turing's instructions were ambiguous or missing. For example, we performed a Computer-Imitates-Human Game (CIHG) without constraining the time duration and conducted a Man-Imitates-Woman Game (MIWG) as a benchmark. All but one participant correctly identified the LLM, showing that one of today's most advanced LLMs is unable to pass a rigorous Turing Test. We conclude that recent extravagant claims for such models are unsupported, and do not warrant either optimism or concern about the social impact of thinking machines.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T13:08:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span><span>cs.CY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17629v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17629v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 Uncertainty Quantification and Decomposition for LLM-based
  Recommendation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wonbin Kweon, Sanghwan Jang, SeongKu Kang, Hwanjo Yu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Despite the widespread adoption of large language models (LLMs) for recommendation, we demonstrate that LLMs often exhibit uncertainty in their recommendations. To ensure the trustworthy use of LLMs in generating recommendations, we emphasize the importance of assessing the reliability of recommendations generated by LLMs. We start by introducing a novel framework for estimating the predictive uncertainty to quantitatively measure the reliability of LLM-based recommendations. We further propose to decompose the predictive uncertainty into recommendation uncertainty and prompt uncertainty, enabling in-depth analyses of the primary source of uncertainty. Through extensive experiments, we (1) demonstrate predictive uncertainty effectively indicates the reliability of LLM-based recommendations, (2) investigate the origins of uncertainty with decomposed uncertainty measures, and (3) propose uncertainty-aware prompting for a lower predictive uncertainty and enhanced recommendation. Our source code and model weights are available at https://github.com/WonbinKweon/UNC_LLM_REC_WWW2025
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T13:08:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3696410.3714601' target='_blank'>doi</a><a href='http://arxiv.org/abs/2501.17630v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17630v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 AdaSociety: An Adaptive Environment with Social Structures for
  Multi-Agent Decision-Making</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yizhe Huang, Xingbo Wang, Hao Liu, Fanqi Kong, Aoyang Qin, Min Tang, Song-Chun Zhu, Mingjie Bi, Siyuan Qi, Xue Feng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Traditional interactive environments limit agents' intelligence growth with fixed tasks. Recently, single-agent environments address this by generating new tasks based on agent actions, enhancing task diversity. We consider the decision-making problem in multi-agent settings, where tasks are further influenced by social connections, affecting rewards and information access. However, existing multi-agent environments lack a combination of adaptive physical surroundings and social connections, hindering the learning of intelligent behaviors. To address this, we introduce AdaSociety, a customizable multi-agent environment featuring expanding state and action spaces, alongside explicit and alterable social structures. As agents progress, the environment adaptively generates new tasks with social structures for agents to undertake. In AdaSociety, we develop three mini-games showcasing distinct social structures and tasks. Initial results demonstrate that specific social structures can promote both individual and collective benefits, though current reinforcement learning and LLM-based algorithms show limited effectiveness in leveraging social structures to enhance performance. Overall, AdaSociety serves as a valuable research platform for exploring intelligence in diverse physical and social settings. The code is available at https://github.com/bigai-ai/AdaSociety.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T12:52:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MA</span><span>cs.AI</span><span>cs.GT</span><span>cs.LG</span><span>cs.SI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.03865v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.03865v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 Structured Context Recomposition for Large Language Models Using
  Probabilistic Layer Realignment</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jonathan Teel, Jocasta Cumberbatch, Raphael Benington, Quentin Baskerville
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Extended sequence generation often leads to degradation in contextual consistency due to the inability of conventional self-attention mechanisms to effectively retain long-range dependencies. Existing approaches, including memory compression and retrieval-augmented conditioning, introduce computational trade-offs that either increase inference latency or impose additional storage overhead. Structured Context Recomposition (SCR) introduces a probabilistic layer realignment strategy that dynamically adjusts learned representations within transformer layers, ensuring that semantically relevant embeddings persist throughout extended transformations. The proposed method enhances coherence retention through a recursive weighting function that redistributes representational emphasis based on inferred contextual relevance rather than relying on fixed token-level attention scores. Empirical results indicate that probabilistic realignment mitigates abrupt topic shifts and logical inconsistencies, particularly in scenarios where sequences exceed standard attention window constraints. Sequence-level entropy analysis further reveals that SCR moderates representational variability without introducing excessive output regularization, allowing models to sustain generative diversity while preserving contextual alignment. Attention head deviation measurements confirm that hierarchical reweighting contributes to smoother token dependency transitions across transformer layers, reinforcing the stability of multi-turn interactions and document-level reasoning. Computational resource assessments show that while SCR incurs a moderate increase in processing time, memory overhead remains within feasible limits, making it suitable for practical deployment in autoregressive generative applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T12:46:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17617v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17617v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 LLMs in the Heart of Differential Testing: A Case Study on a Medical
  Rule Engine</h2>
                <div class="authors">
                    <strong>Authors:</strong> Erblin Isaku, Christoph Laaber, Hassan Sartaj, Shaukat Ali, Thomas Schwitalla, Jan F. Nygård
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Cancer Registry of Norway (CRN) uses an automated cancer registration support system (CaReSS) to support core cancer registry activities, i.e, data capture, data curation, and producing data products and statistics for various stakeholders. GURI is a core component of CaReSS, which is responsible for validating incoming data with medical rules. Such medical rules are manually implemented by medical experts based on medical standards, regulations, and research. Since large language models (LLMs) have been trained on a large amount of public information, including these documents, they can be employed to generate tests for GURI. Thus, we propose an LLM-based test generation and differential testing approach (LLMeDiff) to test GURI. We experimented with four different LLMs, two medical rule engine implementations, and 58 real medical rules to investigate the hallucination, success, time efficiency, and robustness of the LLMs to generate tests, and these tests' ability to find potential issues in GURI. Our results showed that GPT-3.5 hallucinates the least, is the most successful, and is generally the most robust; however, it has the worst time efficiency. Our differential testing revealed 22 medical rules where implementation inconsistencies were discovered (e.g., regarding handling rule versions). Finally, we provide insights for practitioners and researchers based on the results.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T12:36:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.03664v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.03664v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 VoicePrompter: Robust Zero-Shot Voice Conversion with Voice Prompt and
  Conditional Flow Matching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ha-Yeong Choi, Jaehan Park
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Despite remarkable advancements in recent voice conversion (VC) systems, enhancing speaker similarity in zero-shot scenarios remains challenging. This challenge arises from the difficulty of generalizing and adapting speaker characteristics in speech within zero-shot environments, which is further complicated by mismatch between the training and inference processes. To address these challenges, we propose VoicePrompter, a robust zero-shot VC model that leverages in-context learning with voice prompts. VoicePrompter is composed of (1) a factorization method that disentangles speech components and (2) a DiT-based conditional flow matching (CFM) decoder that conditions on these factorized features and voice prompts. Additionally, (3) latent mixup is used to enhance in-context learning by combining various speaker features. This approach improves speaker similarity and naturalness in zero-shot VC by applying mixup to latent representations. Experimental results demonstrate that VoicePrompter outperforms existing zero-shot VC systems in terms of speaker similarity, speech intelligibility, and audio quality. Our demo is available at \url{https://hayeong0.github.io/VoicePrompter-demo/}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T12:34:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SD</span><span>cs.AI</span><span>eess.AS</span><span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17612v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17612v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 FeedSign: Robust Full-parameter Federated Fine-tuning of Large Models
  with Extremely Low Communication Overhead of One Bit</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhijie Cai, Haolong Chen, Guangxu Zhu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Federated fine-tuning (FFT) attempts to fine-tune a pre-trained model with private data from distributed clients by exchanging models rather than data under the orchestration of a parameter server (PS). To overcome the bottleneck forged by the growing communication and memory overhead for clients in such systems due to the growing model sizes, we propose \textit{FeedSign}, an FFT algorithm in which the upload and download payload for an aggregation step is exactly $1$ bit per step, while the memory overhead is squeezed to the amount needed for inference. This is realized by utilizing zeroth-order (ZO) optimizers on large models and shared pseudo-random number generators (PRNG) across devices to represent the gradient estimates as seed-sign pairs. We conduct theoretical analysis on FeedSign and show that it converges at an exponential rate $\mathcal{O}(e^{-t})$, where $t$ is the number of elapsed steps under widely used assumptions. Moreover, FeedSign is found to be robust against data heterogeneity and Byzantine attacks. We conducted extensive experiments on models across different structures and sizes (11M to 13B) and found that the proposed method performs better or closely, depending on scenarios, compared to its ZO and FO counterparts, albeit with an orders-of-magnitude lower communication overhead. We also discuss some interesting advantages as byproducts guaranteed by the minimalistic design of \textit{FeedSign}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T12:33:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17610v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17610v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 medIKAL: Integrating Knowledge Graphs as Assistants of LLMs for Enhanced
  Clinical Diagnosis on EMRs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mingyi Jia, Junwen Duan, Yan Song, Jianxin Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Electronic Medical Records (EMRs), while integral to modern healthcare, present challenges for clinical reasoning and diagnosis due to their complexity and information redundancy. To address this, we proposed medIKAL (Integrating Knowledge Graphs as Assistants of LLMs), a framework that combines Large Language Models (LLMs) with knowledge graphs (KGs) to enhance diagnostic capabilities. medIKAL assigns weighted importance to entities in medical records based on their type, enabling precise localization of candidate diseases within KGs. It innovatively employs a residual network-like approach, allowing initial diagnosis by the LLM to be merged into KG search results. Through a path-based reranking algorithm and a fill-in-the-blank style prompt template, it further refined the diagnostic process. We validated medIKAL's effectiveness through extensive experiments on a newly introduced open-sourced Chinese EMR dataset, demonstrating its potential to improve clinical diagnosis in real-world settings.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T12:27:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.14326v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.14326v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 A Neyman-Orthogonalization Approach to the Incidental Parameter Problem</h2>
                <div class="authors">
                    <strong>Authors:</strong> Stéphane Bonhomme, Koen Jochmans, Martin Weidner
                </div>
                <div class="summary">
                    <strong>Summary:</strong> A popular approach to perform inference on a target parameter in the presence of nuisance parameters is to construct estimating equations that are orthogonal to the nuisance parameters, in the sense that their expected first derivative is zero. Such first-order orthogonalization may, however, not suffice when the nuisance parameters are very imprecisely estimated. Leading examples where this is the case are models for panel and network data that feature fixed effects. In this paper, we show how, in the conditional-likelihood setting, estimating equations can be constructed that are orthogonal to any chosen order. Combining these equations with sample splitting yields higher-order bias-corrected estimators of target parameters. In an empirical application we apply our method to a fixed-effect model of team production and obtain estimates of complementarity in production and impacts of counterfactual re-allocations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T12:18:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>econ.EM</span><span>stat.ME</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.10304v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.10304v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 How well can a large language model explain business processes as
  perceived by users?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dirk Fahland, Fabiana Fournier, Lior Limonad, Inna Skarbovsky, Ava J. E. Swevels
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are trained on a vast amount of text to interpret and generate human-like textual content. They are becoming a vital vehicle in realizing the vision of the autonomous enterprise, with organizations today actively adopting LLMs to automate many aspects of their operations. LLMs are likely to play a prominent role in future AI-augmented business process management systems, catering functionalities across all system lifecycle stages. One such system's functionality is Situation-Aware eXplainability (SAX), which relates to generating causally sound and human-interpretable explanations. In this paper, we present the SAX4BPM framework developed to generate SAX explanations. The SAX4BPM suite consists of a set of services and a central knowledge repository. The functionality of these services is to elicit the various knowledge ingredients that underlie SAX explanations. A key innovative component among these ingredients is the causal process execution view. In this work, we integrate the framework with an LLM to leverage its power to synthesize the various input ingredients for the sake of improved SAX explanations. Since the use of LLMs for SAX is also accompanied by a certain degree of doubt related to its capacity to adequately fulfill SAX along with its tendency for hallucination and lack of inherent capacity to reason, we pursued a methodological evaluation of the perceived quality of the generated explanations. We developed a designated scale and conducted a rigorous user study. Our findings show that the input presented to the LLMs aided with the guard-railing of its performance, yielding SAX explanations having better-perceived fidelity. This improvement is moderated by the perception of trust and curiosity. More so, this improvement comes at the cost of the perceived interpretability of the explanation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T12:15:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>68T01</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2401.12846v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2401.12846v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 Semantic Consistency Regularization with Large Language Models for
  Semi-supervised Sentiment Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kunrong Li, Xinyu Liu, Zhen Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Accurate sentiment analysis of texts is crucial for a variety of applications, such as understanding customer feedback, monitoring market trends, and detecting public sentiment. However, manually annotating large sentiment corpora for supervised learning is labor-intensive and time-consuming. Therefore, it is essential and effective to develop a semi-supervised method for the sentiment analysis task. Although some methods have been proposed for semi-supervised text classification, they rely on the intrinsic information within the unlabeled data and the learning capability of the NLP model, which lack generalization ability to the sentiment analysis scenario and may prone to overfit. Inspired by the ability of pretrained Large Language Models (LLMs) in following instructions and generating coherent text, we propose a Semantic Consistency Regularization with Large Language Models (SCR) framework for semi-supervised sentiment analysis. We introduce two prompting strategies to semantically enhance unlabeled text using LLMs. The first is Entity-based Enhancement (SCR-EE), which involves extracting entities and numerical information, and querying the LLM to reconstruct the textual information. The second is Concept-based Enhancement (SCR-CE), which directly queries the LLM with the original sentence for semantic reconstruction. Subsequently, the LLM-augmented data is utilized for a consistency loss with confidence thresholding, which preserves high-quality agreement samples to provide additional supervision signals during training. Furthermore, to fully utilize the uncertain unlabeled data samples, we propose a class re-assembling strategy inspired by the class space shrinking theorem. Experiments show our method achieves remarkable performance over prior semi-supervised methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T12:03:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17598v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17598v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 GLLM: Self-Corrective G-Code Generation using Large Language Models with
  User Feedback</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mohamed Abdelaal, Samuel Lokadjaja, Gilbert Engert
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper introduces GLLM, an innovative tool that leverages Large Language Models (LLMs) to automatically generate G-code from natural language instructions for Computer Numerical Control (CNC) machining. GLLM addresses the challenges of manual G-code writing by bridging the gap between human-readable task descriptions and machine-executable code. The system incorporates a fine-tuned StarCoder-3B model, enhanced with domain-specific training data and a Retrieval-Augmented Generation (RAG) mechanism. GLLM employs advanced prompting strategies and a novel self-corrective code generation approach to ensure both syntactic and semantic correctness of the generated G-code. The architecture includes robust validation mechanisms, including syntax checks, G-code-specific verifications, and functional correctness evaluations using Hausdorff distance. By combining these techniques, GLLM aims to democratize CNC programming, making it more accessible to users without extensive programming experience while maintaining high accuracy and reliability in G-code generation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T11:40:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17584v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17584v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 CSEval: Towards Automated, Multi-Dimensional, and Reference-Free
  Counterspeech Evaluation using Auto-Calibrated LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Amey Hengle, Aswini Kumar, Anil Bandhakavi, Tanmoy Chakraborty
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Counterspeech has been popular as an effective approach to counter online hate speech, leading to increasing research interest in automated counterspeech generation using language models. However, this field lacks standardised evaluation protocols and robust automated evaluation metrics that align with human judgement. Current automatic evaluation methods, primarily based on similarity metrics, do not effectively capture the complex and independent attributes of counterspeech quality, such as contextual relevance, aggressiveness, or argumentative coherence. This has led to an increased dependency on labor-intensive human evaluations to assess automated counter-speech generation methods. To address these challenges, we introduce CSEval, a novel dataset and framework for evaluating counterspeech quality across four dimensions: contextual-relevance, aggressiveness, argument-coherence, and suitableness. Furthermore, we propose Auto-Calibrated COT for Counterspeech Evaluation (ACE), a prompt-based method with auto-calibrated chain-of-thoughts (CoT) for scoring counterspeech using large language models. Our experiments show that ACE outperforms traditional metrics like ROUGE, METEOR, and BertScore in correlating with human judgement, indicating a significant advancement in automated counterspeech evaluation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T11:38:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.CY</span><span>cs.SI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17581v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17581v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 Magnetic reconnection: an alternative explanation of radio emission in
  galaxy clusters</h2>
                <div class="authors">
                    <strong>Authors:</strong> Subham Ghosh, Pallavi Bhat
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Observations of galaxy clusters show radio emission extended over almost the system scale, necessitating mechanisms for particle acceleration. Previous models for acceleration such as diffusive shock acceleration and that due to turbulence can fall short in terms of efficiency. In this letter, we propose the possibility of acceleration via magnetic reconnection. In particular, we invoke the plasmoid instability which has been previously applied to understand particle energization in high energy systems. Turbulence in galaxy clusters lead to fluctuation dynamos that are known to generate magnetic fields structures consisting of sharp reversals. These form natural sites of reconnection. We perform Particle-In-Cell (PIC) simulations of the plasmoid instability in collisionless and nonrelativistic plasmas. We show that the resulting electron energy spectra have power law indices that are consistent with that inferred from observations. Our estimates show that the acceleration timescales are much smaller than the lifetime of the reconnecting magnetic structures indicating the feasibility of our model. The synchrotron radio luminosity estimate is about $10^{41}$ ergs/s, agreeing with observations. Finally, we find that the maximum achievable Lorentz factor can go upto $10^5$ indicating that acceleration due magnetic reconnection is a promising avenue for understanding the origin of nonthermal emission in galaxy clusters.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T11:32:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.CO</span><span>astro-ph.HE</span><span>physics.plasm-ph</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.3847/2041-8213/ad9f2d' target='_blank'>doi</a><a href='http://arxiv.org/abs/2407.11156v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.11156v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 Fast Word Error Rate Estimation Using Self-Supervised Representations
  for Speech and Text</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chanho Park, Chengsong Lu, Mingjie Chen, Thomas Hain
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Word error rate (WER) estimation aims to evaluate the quality of an automatic speech recognition (ASR) system's output without requiring ground-truth labels. This task has gained increasing attention as advanced ASR systems are trained on large amounts of data. In this context, the computational efficiency of a WER estimator becomes essential in practice. However, previous works have not prioritised this aspect. In this paper, a Fast estimator for WER (Fe-WER) is introduced, utilizing average pooling over self-supervised learning representations for speech and text. Our results demonstrate that Fe-WER outperformed a baseline relatively by 14.10% in root mean square error and 1.22% in Pearson correlation coefficient on Ted-Lium3. Moreover, a comparative analysis of the distributions of target WER and WER estimates was conducted, including an examination of the average values per speaker. Lastly, the inference speed was approximately 3.4 times faster in the real-time factor.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T11:28:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.AS</span><span>cs.CL</span><span>cs.SD</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2310.08225v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2310.08225v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 Synthesizing 3D Abstractions by Inverting Procedural Buildings with
  Transformers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Maximilian Dax, Jordi Berbel, Jan Stria, Leonidas Guibas, Urs Bergmann
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We generate abstractions of buildings, reflecting the essential aspects of their geometry and structure, by learning to invert procedural models. We first build a dataset of abstract procedural building models paired with simulated point clouds and then learn the inverse mapping through a transformer. Given a point cloud, the trained transformer then infers the corresponding abstracted building in terms of a programmatic language description. This approach leverages expressive procedural models developed for gaming and animation, and thereby retains desirable properties such as efficient rendering of the inferred abstractions and strong priors for regularity and symmetry. Our approach achieves good reconstruction accuracy in terms of geometry and structure, as well as structurally consistent inpainting.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T11:06:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17044v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17044v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 Is Conversational XAI All You Need? Human-AI Decision Making With a
  Conversational XAI Assistant</h2>
                <div class="authors">
                    <strong>Authors:</strong> Gaole He, Nilay Aishwarya, Ujwal Gadiraju
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Explainable artificial intelligence (XAI) methods are being proposed to help interpret and understand how AI systems reach specific predictions. Inspired by prior work on conversational user interfaces, we argue that augmenting existing XAI methods with conversational user interfaces can increase user engagement and boost user understanding of the AI system. In this paper, we explored the impact of a conversational XAI interface on users' understanding of the AI system, their trust, and reliance on the AI system. In comparison to an XAI dashboard, we found that the conversational XAI interface can bring about a better understanding of the AI system among users and higher user trust. However, users of both the XAI dashboard and conversational XAI interfaces showed clear overreliance on the AI system. Enhanced conversations powered by large language model (LLM) agents amplified over-reliance. Based on our findings, we reason that the potential cause of such overreliance is the illusion of explanatory depth that is concomitant with both XAI interfaces. Our findings have important implications for designing effective conversational XAI interfaces to facilitate appropriate reliance and improve human-AI collaboration. Code can be found at https://github.com/delftcrowd/IUI2025_ConvXAI
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T10:29:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3708359.3712133' target='_blank'>doi</a><a href='http://arxiv.org/abs/2501.17546v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17546v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling
  MiXed Emotions and Discourse Dynamics</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chenwei Wan, Matthieu Labeau, Chloé Clavel
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Designing emotionally intelligent conversational systems to provide comfort and advice to people experiencing distress is a compelling area of research. Recently, with advancements in large language models (LLMs), end-to-end dialogue agents without explicit strategy prediction steps have become prevalent. However, implicit strategy planning lacks transparency, and recent studies show that LLMs' inherent preference bias towards certain socio-emotional strategies hinders the delivery of high-quality emotional support. To address this challenge, we propose decoupling strategy prediction from language generation, and introduce a novel dialogue strategy prediction framework, EmoDynamiX, which models the discourse dynamics between user fine-grained emotions and system strategies using a heterogeneous graph for better performance and transparency. Experimental results on two ESC datasets show EmoDynamiX outperforms previous state-of-the-art methods with a significant margin (better proficiency and lower preference bias). Our approach also exhibits better transparency by allowing backtracing of decision making.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T10:25:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.08782v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.08782v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 Towards Supporting Penetration Testing Education with Large Language
  Models: an Evaluation and Comparison</h2>
                <div class="authors">
                    <strong>Authors:</strong> Martin Nizon-Deladoeuille, Brynjólfur Stefánsson, Helmut Neukirchen, Thomas Welsh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Cybersecurity education is challenging and it is helpful for educators to understand Large Language Models' (LLMs') capabilities for supporting education. This study evaluates the effectiveness of LLMs in conducting a variety of penetration testing tasks. Fifteen representative tasks were selected to cover a comprehensive range of real-world scenarios. We evaluate the performance of 6 models (GPT-4o mini, GPT-4o, Gemini 1.5 Flash, Llama 3.1 405B, Mixtral 8x7B and WhiteRabbitNeo) upon the Metasploitable v3 Ubuntu image and OWASP WebGOAT. Our findings suggest that GPT-4o mini currently offers the most consistent support making it a valuable tool for educational purposes. However, its use in conjonction with WhiteRabbitNeo should be considered, because of its innovative approach to tool and command recommendations. This study underscores the need for continued research into optimising LLMs for complex, domain-specific tasks in cybersecurity education.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T10:12:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17539v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17539v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 Scale-Invariant Learning-to-Rank</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alessio Petrozziello, Christian Sommeregger, Ye-Sheen Lim
                </div>
                <div class="summary">
                    <strong>Summary:</strong> At Expedia, learning-to-rank (LTR) models plays a key role on our website in sorting and presenting information more relevant to users, such as search filters, property rooms, amenities, and images. A major challenge in deploying these models is ensuring consistent feature scaling between training and production data, as discrepancies can lead to unreliable rankings when deployed. Normalization techniques like feature standardization and batch normalization could address these issues but are impractical in production due to latency impacts and the difficulty of distributed real-time inference. To address consistent feature scaling issue, we introduce a scale-invariant LTR framework which combines a deep and a wide neural network to mathematically guarantee scale-invariance in the model at both training and prediction time. We evaluate our framework in simulated real-world scenarios with injected feature scale issues by perturbing the test set at prediction time, and show that even with inconsistent train-test scaling, using framework achieves better performance than without.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T10:09:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.01959v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.01959v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 Wireless Network Topology Inference: A Markov Chains Approach</h2>
                <div class="authors">
                    <strong>Authors:</strong> James Martin, Tristan Pryer, Luca Zanetti
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this work, we address the problem of inferring the topology of a wireless network using limited observational data. Specifically, we assume that we can detect when a node is transmitting, but no further information regarding the transmission is available. We propose a novel network estimation procedure grounded in the following abstract problem: estimating the parameters of a finite discrete-time Markov chain by observing, at each time step, which states are visited by multiple ``anonymous'' copies of the chain. We develop a consistent estimator that approximates the transition matrix of the chain in the operator norm, with the number of required samples scaling roughly linearly with the size of the state space. Applying this estimation procedure to wireless networks, our numerical experiments demonstrate that the proposed method accurately infers network topology across a wide range of parameters, consistently outperforming transfer entropy, particularly under conditions of high network congestion.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T10:04:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span><span>math.PR</span><span>math.ST</span><span>stat.TH</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17532v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17532v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 Evaluating Telugu Proficiency in Large Language Models_ A Comparative
  Analysis of ChatGPT and Gemini</h2>
                <div class="authors">
                    <strong>Authors:</strong> Katikela Sreeharsha Kishore, Rahimanuddin Shaik
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The growing prominence of large language models (LLMs) necessitates the exploration of their capabilities beyond English. This research investigates the Telugu language proficiency of ChatGPT and Gemini, two leading LLMs. Through a designed set of 20 questions encompassing greetings, grammar, vocabulary, common phrases, task completion, and situational reasoning, the study delves into their strengths and weaknesses in handling Telugu. The analysis aims to identify the LLM that demonstrates a deeper understanding of Telugu grammatical structures, possesses a broader vocabulary, and exhibits superior performance in tasks like writing and reasoning. By comparing their ability to comprehend and use everyday Telugu expressions, the research sheds light on their suitability for real-world language interaction. Furthermore, the evaluation of adaptability and reasoning capabilities provides insights into how each LLM leverages Telugu to respond to dynamic situations. This comparative analysis contributes to the ongoing discussion on multilingual capabilities in AI and paves the way for future research in developing LLMs that can seamlessly integrate with Telugu-speaking communities.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T09:54:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.19369v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.19369v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 Consistent time travel for realistic interactions with historical data:
  reinforcement learning for market making</h2>
                <div class="authors">
                    <strong>Authors:</strong> Vincent Ragel, Damien Challet
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reinforcement learning works best when the impact of the agent's actions on its environment can be perfectly simulated or fully appraised from available data. Some systems are however both hard to simulate and very sensitive to small perturbations. An additional difficulty arises when a RL agent is trained offline to be part of a multi-agent system using only anonymous data, which makes it impossible to infer the state of each agent, thus to use data directly. Typical examples are competitive systems without agent-resolved data such as financial markets. We introduce consistent data time travel for offline RL as a remedy for these problems: instead of using historical data in a sequential way, we argue that one needs to perform time travel in historical data, i.e., to adjust the time index so that both the past state and the influence of the RL agent's action on the system coincide with real data. This both alleviates the need to resort to imperfect models and consistently accounts for both the immediate and long-term reactions of the system when using anonymous historical data. We apply this idea to market making in limit order books, a notoriously difficult task for RL; it turns out that the gain of the agent is significantly higher with data time travel than with naive sequential data, which suggests that the difficulty of this task for RL may have been overestimated.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T09:43:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>q-fin.TR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.02322v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.02322v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 Principled model selection for stochastic dynamics</h2>
                <div class="authors">
                    <strong>Authors:</strong> Andonis Gerardos, Pierre Ronceray
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Complex dynamical systems, from macromolecules to ecosystems, are often modeled by stochastic differential equations. To learn such models from data, a common approach involves sparse selection among a large function library. However, we show that overfitting arises - not just from individual model complexity, but also from the combinatorial growth of possible models. To address this, we introduce Parsimonious Stochastic Inference (PASTIS), a principled method combining likelihood-estimation statistics with extreme value theory to suppress superfluous parameters. PASTIS outperforms existing methods and reliably identifies minimal models, even with low sampling rates or measurement error. It extends to stochastic partial differential equations, and applies to ecological networks and reaction-diffusion dynamics.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T09:40:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.soft</span><span>cond-mat.stat-mech</span><span>physics.data-an</span><span>stat.ML</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.10339v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.10339v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 Semiparametric principal stratification analysis beyond monotonicity</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiaqi Tong, Brennan Kahan, Michael O. Harhay, Fan Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Intercurrent events, common in clinical trials and observational studies, affect the existence or interpretation of final outcomes. Principal stratification addresses these challenges by defining local average treatment effects within latent subpopulations, but often relies on restrictive assumptions such as monotonicity and counterfactual intermediate independence. To address these limitations, we propose a unified semiparametric framework for principal stratification analysis leveraging a margin-free, conditional odds ratio sensitivity parameter. Under principal ignorability, we derive nonparametric identification formulas and develop efficient estimation methods, including a conditionally doubly robust parametric estimator and a de-biased machine learning estimator with data-adaptive nuisance estimators. Simulations show that incorrectly assuming monotonicity can often lead to suboptimal inference, while specifying non-trivial odds ratio sensitivity parameter can enable approximately valid inference under monotonicity. We apply our methods to a critical care trial and further suggest a semiparametric sensitivity analysis approach under violation of principal ignorability.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T09:38:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17514v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17514v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 Admissible online closed testing must employ e-values</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lasse Fischer, Aaditya Ramdas
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In contemporary research, data scientists often test an infinite sequence of hypotheses $H_1,H_2,\ldots $ one by one, and are required to make real-time decisions without knowing the future hypotheses or data. In this paper, we consider such an online multiple testing problem with the goal of providing simultaneous lower bounds for the number of true discoveries in data-adaptively chosen rejection sets. In offline multiple testing, it has been recently established that such simultaneous inference is admissible iff it proceeds through (offline) closed testing. We establish an analogous result in this paper using the recent online closure principle. In particular, we show that it is necessary to use an anytime-valid test for each intersection hypothesis. This connects two distinct branches of the literature: online testing of multiple hypotheses (where the hypotheses appear online), and sequential anytime-valid testing of a single hypothesis (where the data for a fixed hypothesis appears online). Motivated by this result, we construct a new online closed testing procedure and a corresponding short-cut with a true discovery guarantee based on multiplying sequential e-values. This general but simple procedure gives uniform improvements over the state-of-the-art methods but also allows to construct entirely new and powerful procedures. In addition, we introduce new ideas for hedging and boosting of sequential e-values that provably increase power. Finally, we also propose the first online true discovery procedures for exchangeable and arbitrarily dependent e-values.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T09:27:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.15733v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.15733v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 LLM Assistance for Pediatric Depression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mariia Ignashina, Paulina Bondaronek, Dan Santel, John Pestian, Julia Ive
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Traditional depression screening methods, such as the PHQ-9, are particularly challenging for children in pediatric primary care due to practical limitations. AI has the potential to help, but the scarcity of annotated datasets in mental health, combined with the computational costs of training, highlights the need for efficient, zero-shot approaches. In this work, we investigate the feasibility of state-of-the-art LLMs for depressive symptom extraction in pediatric settings (ages 6-24). This approach aims to complement traditional screening and minimize diagnostic errors.   Our findings show that all LLMs are 60% more efficient than word match, with Flan leading in precision (average F1: 0.65, precision: 0.78), excelling in the extraction of more rare symptoms like "sleep problems" (F1: 0.92) and "self-loathing" (F1: 0.8). Phi strikes a balance between precision (0.44) and recall (0.60), performing well in categories like "Feeling depressed" (0.69) and "Weight change" (0.78). Llama 3, with the highest recall (0.90), overgeneralizes symptoms, making it less suitable for this type of analysis. Challenges include the complexity of clinical notes and overgeneralization from PHQ-9 scores. The main challenges faced by LLMs include navigating the complex structure of clinical notes with content from different times in the patient trajectory, as well as misinterpreting elevated PHQ-9 scores.   We finally demonstrate the utility of symptom annotations provided by Flan as features in an ML algorithm, which differentiates depression cases from controls with high precision of 0.78, showing a major performance boost compared to a baseline that does not use these features.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T09:27:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17510v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17510v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 A Sampling Complexity-aware Framework for Discrete-time Fractional-Order
  Dynamical System Identification</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaole Zhang, Vijay Gupta, Paul Bogdan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> A variety of complex biological, natural and man-made systems exhibit non-Markovian dynamics that can be modeled through fractional order differential equations, yet, we lack sample comlexity aware system identification strategies. Towards this end, we propose an affine discrete-time fractional order dynamical system (FoDS) identification algorithm and provide a detailed sample complexity analysis. The algorithm effectively addresses the challenges of FoDS identification in the presence of noisy data. The proposed algorithm consists of two key steps. Firstly, it avoids solving higher-order polynomial equations, which would otherwise result in multiple potential solutions for the fractional orders. Secondly, the identification problem is reformulated as a least squares estimation, allowing us to infer the system parameters. We derive the expectation and probabilistic bounds for the FoDS parameter estimation error, assuming prior knowledge of the functions \( f \) and \( g \) in the FoDS model. The error decays at a rate of \( N = O\left( \frac{d}{\epsilon} \right) \), where \( N \) is the number of samples, \( d \) is the dimension of the state variable, and \( \epsilon \) represents the desired estimation accuracy. Simulation results demonstrate that our theoretical bounds are tight, validating the accuracy and robustness of this algorithm.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T09:12:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SY</span><span>cs.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17499v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17499v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 Velocity Structure of Circumstellar Environment around Class 0/I
  Protostars: Uncertainty in the Protostellar Mass Estimation Using
  Circumstellar Velocities</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shingo Hirano, Yuri Aikawa, Masahiro N. Machida
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent high-resolution observations have enabled detailed investigations of the circumstellar environments around Class 0/I protostars. Several studies have reported that the infall velocity of the envelope is a few times smaller than the free-fall velocity inferred from protostellar masses estimated via the observed rotational velocity of their Keplerian disks. To explore the physical origins of the slow infall, we perform a set of three-dimensional non-ideal magnetohydrodynamic simulations of the star formation process, extending to $10^5$ yr after protostar formation. Our simulations show that the infall velocity decreases markedly at the outer edge of the pseudo-disk (at radii of $\sim \! 100-1000$ au) and is much slower than the expected free-fall velocity. The degree of this reduction depends on (1) the initial magnetic field strength, (2) the alignment between the initial field and the rotation axis, and (3) the evolutionary stage of the system. Across our parameter space, the ratio of the infall velocity to the free-fall velocity is as small as $0.2-0.5$, which is consistent with the observations. We further examine the reliability of protostellar mass estimates derived from infall and rotational velocities. While the mass derived from disk rotation closely matches the true value, deviation by a factor of $0.3-2$ is found for the estimates using the infall velocity; it is underestimated due to slow infall, but could also be overestimated due to the contribution of disk mass. These findings underscore the critical role of magnetic fields in shaping star formation dynamics and highlight the uncertainties associated with protostellar mass estimates.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T09:00:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.SR</span><span>astro-ph.GA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17492v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17492v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 Pricing Carbon Allowance Options on Futures: Insights from
  High-Frequency Data</h2>
                <div class="authors">
                    <strong>Authors:</strong> Simone Serafini, Giacomo Bormetti
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Leveraging a unique dataset of carbon futures option prices traded on the ICE market from December 2015 until December 2020, we present the results from an unprecedented calibration exercise. Within a multifactor stochastic volatility framework with jumps, we employ a three-dimensional pricing kernel compensating for equity and variance components' risk to derive an analytically tractable and numerically practical approach to pricing. To the best of our knowledge, we are the first to provide an estimate of the equity and variance risk premia for the carbon futures option market. We gain insights into daily option and futures dynamics by exploiting the information from tick-by-tick futures trade data. Decomposing the realized measure of futures volatility into continuous and jump components, we employ them as auxiliary variables for estimating futures dynamics via indirect inference. Our approach provides a realistic description of carbon futures price, volatility, and jump dynamics and an insightful understanding of the carbon option market.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T09:00:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>q-fin.PR</span><span>q-fin.GN</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17490v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17490v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 Neural Spelling: A Spell-Based BCI System for Language Neural Decoding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaowei Jiang, Charles Zhou, Yiqun Duan, Ziyi Zhao, Thomas Do, Chin-Teng Lin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Brain-computer interfaces (BCIs) present a promising avenue by translating neural activity directly into text, eliminating the need for physical actions. However, existing non-invasive BCI systems have not successfully covered the entire alphabet, limiting their practicality. In this paper, we propose a novel non-invasive EEG-based BCI system with Curriculum-based Neural Spelling Framework, which recognizes all 26 alphabet letters by decoding neural signals associated with handwriting first, and then apply a Generative AI (GenAI) to enhance spell-based neural language decoding tasks. Our approach combines the ease of handwriting with the accessibility of EEG technology, utilizing advanced neural decoding algorithms and pre-trained large language models (LLMs) to translate EEG patterns into text with high accuracy. This system show how GenAI can improve the performance of typical spelling-based neural language decoding task, and addresses the limitations of previous methods, offering a scalable and user-friendly solution for individuals with communication impairments, thereby enhancing inclusive communication options.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T08:57:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17489v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17489v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 Regularized Best-of-N Sampling with Minimum Bayes Risk Objective for
  Language Model Alignment</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuu Jinnai, Tetsuro Morimura, Kaito Ariu, Kenshi Abe
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) to human preferences at the time of decoding. BoN sampling is susceptible to a problem known as reward hacking when the accuracy of the reward model is not high enough due to the quality or the quantity of the preference dataset. Because the reward model is an imperfect proxy for the true objective, over-optimizing its value can compromise its performance on the true objective. In this research, we propose MBR-BoN, a variant of BoN that aims to mitigate reward hacking at inference time by incorporating the Minimum Bayes Risk (MBR) objective as a proximity regularization term. We show empirically and analytically that the MBR objective quantifies the proximity of the response to the reference policy, serving as a proximity regularizer. We evaluate MBR-BoN on the AlpacaFarm and Anthropic's hh-rlhf datasets and show that it outperforms both BoN sampling and MBR decoding. We also evaluate MBR-BoN to generate a pairwise preference learning dataset for Direct Preference Optimization (DPO). Empirical results show that models trained on a dataset generated with MBR-BoN outperform those with vanilla BoN. Our code is available at https://github.com/CyberAgentAILab/regularized-bon
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T08:52:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.01054v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.01054v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 DFPE: A Diverse Fingerprint Ensemble for Enhancing LLM Performance</h2>
                <div class="authors">
                    <strong>Authors:</strong> Seffi Cohen, Niv Goldshlager, Nurit Cohen-Inger, Bracha Shapira, Lior Rokach
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have shown remarkable capabilities across various natural language processing tasks but often struggle to excel uniformly in diverse or complex domains. We propose a novel ensemble method - Diverse Fingerprint Ensemble (DFPE), which leverages the complementary strengths of multiple LLMs to achieve more robust performance. Our approach involves: (1) clustering models based on response "fingerprints" patterns, (2) applying a quantile-based filtering mechanism to remove underperforming models at a per-subject level, and (3) assigning adaptive weights to remaining models based on their subject-wise validation accuracy. In experiments on the Massive Multitask Language Understanding (MMLU) benchmark, DFPE outperforms the best single model by 3% overall accuracy and 5% in discipline-level accuracy. This method increases the robustness and generalization of LLMs and underscores how model selection, diversity preservation, and performance-driven weighting can effectively address challenging, multi-faceted language understanding tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T08:44:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17479v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17479v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 AugmenTest: Enhancing Tests with LLM-Driven Oracles</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shaker Mahmud Khandaker, Fitsum Kifetew, Davide Prandi, Angelo Susi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Automated test generation is crucial for ensuring the reliability and robustness of software applications while at the same time reducing the effort needed. While significant progress has been made in test generation research, generating valid test oracles still remains an open problem. To address this challenge, we present AugmenTest, an approach leveraging Large Language Models (LLMs) to infer correct test oracles based on available documentation of the software under test. Unlike most existing methods that rely on code, AugmenTest utilizes the semantic capabilities of LLMs to infer the intended behavior of a method from documentation and developer comments, without looking at the code. AugmenTest includes four variants: Simple Prompt, Extended Prompt, RAG with a generic prompt (without the context of class or method under test), and RAG with Simple Prompt, each offering different levels of contextual information to the LLMs. To evaluate our work, we selected 142 Java classes and generated multiple mutants for each. We then generated tests from these mutants, focusing only on tests that passed on the mutant but failed on the original class, to ensure that the tests effectively captured bugs. This resulted in 203 unique tests with distinct bugs, which were then used to evaluate AugmenTest. Results show that in the most conservative scenario, AugmenTest's Extended Prompt consistently outperformed the Simple Prompt, achieving a success rate of 30\% for generating correct assertions. In comparison, the state-of-the-art TOGA approach achieved 8.2\%. Contrary to our expectations, the RAG-based approaches did not lead to improvements, with performance of 18.2\% success rate for the most conservative scenario.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T07:45:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17461v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17461v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 Large Language Models for Single-Step and Multi-Step Flight Trajectory
  Prediction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kaiwei Luo, Jiliu Zhou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Flight trajectory prediction is a critical time series task in aviation. While deep learning methods have shown significant promise, the application of large language models (LLMs) to this domain remains underexplored. This study pioneers the use of LLMs for flight trajectory prediction by reframing it as a language modeling problem. Specifically, We extract features representing the aircraft's position and status from ADS-B flight data to construct a prompt-based dataset, where trajectory waypoints are converted into language tokens. The dataset is then employed to fine-tune LLMs, enabling them to learn complex spatiotemporal patterns for accurate predictions. Comprehensive experiments demonstrate that LLMs achieve notable performance improvements in both single-step and multi-step predictions compared to traditional methods, with LLaMA-3.1 model achieving the highest overall accuracy. However, the high inference latency of LLMs poses a challenge for real-time applications, underscoring the need for further research in this promising direction.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T07:35:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17459v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17459v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 Uniform Confidence Band for Marginal Treatment Effect Function</h2>
                <div class="authors">
                    <strong>Authors:</strong> Toshiki Tsuda, Yanchun Jin, Ryo Okui
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper presents a method for constructing uniform confidence bands for the marginal treatment effect function. Our approach visualizes statistical uncertainty, facilitating inferences about the function's shape. We derive a Gaussian approximation for a local quadratic estimator, enabling computationally inexpensive construction of these bands. Monte Carlo simulations demonstrate that our bands provide the desired coverage and are less conservative than those based on the Gumbel approximation. An empirical illustration is included.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T07:32:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>econ.EM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17455v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17455v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 MACI: Multi-Agent Collaborative Intelligence for Adaptive Reasoning and
  Temporal Planning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Edward Y. Chang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Artificial intelligence requires deliberate reasoning, temporal awareness, and effective constraint management, capabilities traditional LLMs often lack due to their reliance on pattern matching, limited self-verification, and inconsistent constraint handling. We introduce Multi-Agent Collaborative Intelligence (MACI), a framework comprising three key components: 1) a meta-planner (MP) that identifies, formulates, and refines all roles and constraints of a task (e.g., wedding planning) while generating a dependency graph, with common-sense augmentation to ensure realistic and practical constraints; 2) a collection of agents to facilitate planning and address task-specific requirements; and 3) a run-time monitor that manages plan adjustments as needed. By decoupling planning from validation, maintaining minimal agent context, and integrating common-sense reasoning, MACI overcomes the aforementioned limitations and demonstrates robust performance in two scheduling problems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T07:23:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>F.2.2</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.16689v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.16689v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 Inferring additional physics through unmodelled signal reconstructions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rimo Das, V. Gayathri, Divyajyoti, Sijil Jose, Imre Bartos, Sergey Klimenko, Chandra Kant Mishra
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Parameter estimation of gravitational wave data is often computationally expensive, requiring simplifying assumptions such as circularisation of binary orbits. Although, if included, the sub-dominant effects like orbital eccentricity may provide crucial insights into the formation channels of compact binary mergers. To address these challenges, we present a pipeline strategy leveraging minimally modelled waveform reconstruction to identify the presence of eccentricity in real time. Using injected signals, we demonstrate that ignoring eccentricity ($e_{\rm 20Hz} \gtrsim 0.1$) leads to significant biases in parameter recovery, including chirp mass estimates falling outside the 90% credible interval. Waveform reconstruction shows inconsistencies increase with eccentricity, and this behaviour is consistent for different mass ratios. Our method enables low-latency inferences of binary properties supporting targeted follow-up analyses and can be applied to identify any physical effect of measurable strength.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T07:13:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>gr-qc</span><span>astro-ph.HE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.11749v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.11749v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 COBias and Debias: Balancing Class Accuracies for Language Models in
  Inference Time via Nonlinear Integer Programming</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruixi Lin, Yang You
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) are good knowledge bases but struggle to perform equally well for all classes in text classification tasks. This paper investigates a fundamental inference-time problem in language models: imbalanced class accuracies. We find what's underneath the issue is a tendency to over-predict some classes while under-predicting some others. This class accuracy imbalance is difficult to solve from the root via better pre-training or fine-tuning strategies, but we show it can be effectively mitigated via inference-time combinatorial optimization. To this end, we conceptualize and quantify the over- and under-prediction issue as the Contextual Oddity Bias (COBias), and propose the Debiasing as Nonlinear Integer Programming (DNIP) model to correct in-context learned class probabilities based on minimizing COBias and maximizing overall accuracy, without LLM parameter update. Considering that the DNIP model implicitly contains non-differentiable elements, we therefore use the simulated annealing algorithm to solve it. Extensive evaluations on three LLMs across seven NLP classification tasks in different prompting settings show that DNIP simultaneously achieves significant COBias reduction (-27%) and accuracy improvement (+12%) over the conventional ICL approach, suggesting that inference-time mitigation of class accuracy imbalance is a promising direction to push forward LLM performances.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T07:07:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2405.07623v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.07623v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 Beyond Text: Implementing Multimodal Large Language Model-Powered
  Multi-Agent Systems Using a No-Code Platform</h2>
                <div class="authors">
                    <strong>Authors:</strong> Cheonsu Jeong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This study proposes the design and implementation of a multimodal LLM-based Multi-Agent System (MAS) leveraging a No-Code platform to address the practical constraints and significant entry barriers associated with AI adoption in enterprises. Advanced AI technologies, such as Large Language Models (LLMs), often pose challenges due to their technical complexity and high implementation costs, making them difficult for many organizations to adopt. To overcome these limitations, this research develops a No-Code-based Multi-Agent System designed to enable users without programming knowledge to easily build and manage AI systems. The study examines various use cases to validate the applicability of AI in business processes, including code generation from image-based notes, Advanced RAG-based question-answering systems, text-based image generation, and video generation using images and prompts. These systems lower the barriers to AI adoption, empowering not only professional developers but also general users to harness AI for significantly improved productivity and efficiency. By demonstrating the scalability and accessibility of No-Code platforms, this study advances the democratization of AI technologies within enterprises and validates the practical applicability of Multi-Agent Systems, ultimately contributing to the widespread adoption of AI across various industries.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T06:49:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.00750v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.00750v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 Bayesian BIM-Guided Construction Robot Navigation with NLP Safety
  Prompts in Dynamic Environments</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mani Amani, Reza Akhavian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Construction robotics increasingly relies on natural language processing for task execution, creating a need for robust methods to interpret commands in complex, dynamic environments. While existing research primarily focuses on what tasks robots should perform, less attention has been paid to how these tasks should be executed safely and efficiently. This paper presents a novel probabilistic framework that uses sentiment analysis from natural language commands to dynamically adjust robot navigation policies in construction environments. The framework leverages Building Information Modeling (BIM) data and natural language prompts to create adaptive navigation strategies that account for varying levels of environmental risk and uncertainty. We introduce an object-aware path planning approach that combines exponential potential fields with a grid-based representation of the environment, where the potential fields are dynamically adjusted based on the semantic analysis of user prompts. The framework employs Bayesian inference to consolidate multiple information sources: the static data from BIM, the semantic content of natural language commands, and the implied safety constraints from user prompts. We demonstrate our approach through experiments comparing three scenarios: baseline shortest-path planning, safety-oriented navigation, and risk-aware routing. Results show that our method successfully adapts path planning based on natural language sentiment, achieving a 50\% improvement in minimum distance to obstacles when safety is prioritized, while maintaining reasonable path lengths. Scenarios with contrasting prompts, such as "dangerous" and "safe", demonstrate the framework's ability to modify paths. This approach provides a flexible foundation for integrating human knowledge and safety considerations into construction robot navigation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T06:32:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17437v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17437v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 Geodesic Difference-in-Differences</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yidong Zhou, Daisuke Kurisu, Taisuke Otsu, Hans-Georg Müller
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Difference-in-differences (DID) is a widely used quasi-experimental design for causal inference, traditionally applied to scalar or Euclidean outcomes, while extensions to outcomes residing in non-Euclidean spaces remain limited. Existing methods for such outcomes have primarily focused on univariate distributions, leveraging linear operations in the space of quantile functions, but these approaches cannot be directly extended to outcomes in general metric spaces. In this paper, we propose geodesic DID, a novel DID framework for outcomes in geodesic metric spaces, such as distributions, networks, and manifold-valued data. To address the absence of algebraic operations in these spaces, we use geodesics as proxies for differences and introduce the geodesic average treatment effect on the treated (ATT) as the causal estimand. We establish the identification of the geodesic ATT and derive the convergence rate of its sample versions, employing tools from metric geometry and empirical process theory. This framework is further extended to the case of staggered DID settings, allowing for multiple time periods and varying treatment timings. To illustrate the practical utility of geodesic DID, we analyze health impacts of the Soviet Union's collapse using age-at-death distributions and assess effects of U.S. electricity market liberalization on electricity generation compositions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T06:31:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>62D20, 62R20</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17436v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17436v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing
  Guardrail Moderation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, Ling Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent research shows that Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks -- models lose their safety alignment ability after fine-tuning on a few harmful samples. For risk mitigation, a guardrail is typically used to filter out harmful samples before fine-tuning. By designing a new red-teaming method, we in this paper show that purely relying on the moderation guardrail for data filtration is not reliable. Our proposed attack method, dubbed Virus, easily bypasses the guardrail moderation by slightly modifying the harmful data. Experimental results show that the harmful data optimized by Virus is not detectable by the guardrail with up to 100\% leakage ratio, and can simultaneously achieve superior attack performance. Finally, the key message we want to convey through this paper is that: \textbf{it is reckless to consider guardrail moderation as a clutch at straws towards harmful fine-tuning attack}, as it cannot solve the inherent safety issue of the pre-trained LLMs. Our code is available at https://github.com/git-disl/Virus
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T06:24:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span><span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17433v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17433v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 From Decision to Action in Surgical Autonomy: Multi-Modal Large Language
  Models for Robot-Assisted Blood Suction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sadra Zargarzadeh, Maryam Mirzaei, Yafei Ou, Mahdi Tavakoli
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rise of Large Language Models (LLMs) has impacted research in robotics and automation. While progress has been made in integrating LLMs into general robotics tasks, a noticeable void persists in their adoption in more specific domains such as surgery, where critical factors such as reasoning, explainability, and safety are paramount. Achieving autonomy in robotic surgery, which entails the ability to reason and adapt to changes in the environment, remains a significant challenge. In this work, we propose a multi-modal LLM integration in robot-assisted surgery for autonomous blood suction. The reasoning and prioritization are delegated to the higher-level task-planning LLM, and the motion planning and execution are handled by the lower-level deep reinforcement learning model, creating a distributed agency between the two components. As surgical operations are highly dynamic and may encounter unforeseen circumstances, blood clots and active bleeding were introduced to influence decision-making. Results showed that using a multi-modal LLM as a higher-level reasoning unit can account for these surgical complexities to achieve a level of reasoning previously unattainable in robot-assisted surgeries. These findings demonstrate the potential of multi-modal LLMs to significantly enhance contextual understanding and decision-making in robotic-assisted surgeries, marking a step toward autonomous surgical systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T06:13:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/LRA.2025.3535184' target='_blank'>doi</a><a href='http://arxiv.org/abs/2408.07806v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07806v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges
  and Future Directions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hung Du, Srikanth Thudumu, Rajesh Vasa, Kon Mouzakis
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Research interest in autonomous agents is on the rise as an emerging topic. The notable achievements of Large Language Models (LLMs) have demonstrated the considerable potential to attain human-like intelligence in autonomous agents. However, the challenge lies in enabling these agents to learn, reason, and navigate uncertainties in dynamic environments. Context awareness emerges as a pivotal element in fortifying multi-agent systems when dealing with dynamic situations. Despite existing research focusing on both context-aware systems and multi-agent systems, there is a lack of comprehensive surveys outlining techniques for integrating context-aware systems with multi-agent systems. To address this gap, this survey provides a comprehensive overview of state-of-the-art context-aware multi-agent systems. First, we outline the properties of both context-aware systems and multi-agent systems that facilitate integration between these systems. Subsequently, we propose a general process for context-aware systems, with each phase of the process encompassing diverse approaches drawn from various application domains such as collision avoidance in autonomous driving, disaster relief management, utility management, supply chain management, human-AI interaction, and others. Finally, we discuss the existing challenges of context-aware multi-agent systems and provide future research directions in this field.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T05:41:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MA</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.01968v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.01968v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases
  in Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuxuan Li, Hirokazu Shirado, Sauvik Das
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While advances in fairness and alignment have helped mitigate overt biases exhibited by large language models (LLMs) when explicitly prompted, we hypothesize that these models may still exhibit implicit biases when simulating human behavior. To test this hypothesis, we propose a technique to systematically uncover such biases across a broad range of sociodemographic categories by assessing decision-making disparities among agents with LLM-generated, sociodemographically-informed personas. Using our technique, we tested six LLMs across three sociodemographic groups and four decision-making scenarios. Our results show that state-of-the-art LLMs exhibit significant sociodemographic disparities in nearly all simulations, with more advanced models exhibiting greater implicit biases despite reducing explicit biases. Furthermore, when comparing our findings to real-world disparities reported in empirical studies, we find that the biases we uncovered are directionally aligned but markedly amplified. This directional alignment highlights the utility of our technique in uncovering systematic biases in LLMs rather than random variations; moreover, the presence and amplification of implicit biases emphasizes the need for novel strategies to address these biases.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T05:21:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17420v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17420v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 CoCoNUT: Structural Code Understanding does not fall out of a tree</h2>
                <div class="authors">
                    <strong>Authors:</strong> Claas Beger, Saikat Dutta
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have shown impressive performance across a wide array of tasks involving both structured and unstructured textual data. Recent results on various benchmarks for code generation, repair, or completion suggest that certain models have programming abilities comparable to or even surpass humans. In this work, we demonstrate that high performance on such benchmarks does not correlate to humans' innate ability to understand structural control flow in code. To this end, we extract solutions from the HumanEval benchmark, which the relevant models perform strongly on, and trace their execution path using function calls sampled from the respective test set. Using this dataset, we investigate the ability of seven state-of-the-art LLMs to match the execution trace and find that, despite their ability to generate semantically identical code, they possess limited ability to trace execution paths, especially for longer traces and specific control structures. We find that even the top-performing model, Gemini, can fully and correctly generate only 47% of HumanEval task traces. Additionally, we introduce a subset for three key structures not contained in HumanEval: Recursion, Parallel Processing, and Object-Oriented Programming, including concepts like Inheritance and Polymorphism. Besides OOP, we show that none of the investigated models achieve an accuracy over 5% on the relevant traces. Aggregating these specialized parts with HumanEval tasks, we present CoCoNUT: Code Control Flow for Navigation Understanding and Testing, which measures a model's ability to trace execution of code upon relevant calls, including advanced structural components. We conclude that current LLMs need significant improvement to enhance code reasoning abilities. We hope our dataset helps researchers bridge this gap.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T05:15:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.16456v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.16456v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 si4onnx: A Python package for Selective Inference in Deep Learning
  Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Teruyuki Katsuoka, Tomohiro Shiraishi, Daiki Miwa, Shuichi Nishino, Ichiro Takeuchi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper, we introduce si4onnx, a package for performing selective inference on deep learning models. Techniques such as CAM in XAI and reconstruction-based anomaly detection using VAE can be interpreted as methods for identifying significant regions within input images. However, the identified regions may not always carry meaningful significance. Therefore, evaluating the statistical significance of these regions represents a crucial challenge in establishing the reliability of AI systems. si4onnx is a Python package that enables straightforward implementation of hypothesis testing with controlled type I error rates through selective inference. It is compatible with deep learning models constructed using common frameworks such as PyTorch and TensorFlow.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T05:04:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>stat.ML</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17415v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17415v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 Optimizing Code Runtime Performance through Context-Aware
  Retrieval-Augmented Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Manish Acharya, Yifan Zhang, Kevin Leach, Yu Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Optimizing software performance through automated code refinement offers a promising avenue for enhancing execution speed and efficiency. Despite recent advancements in LLMs, a significant gap remains in their ability to perform in-depth program analysis. This study introduces AUTOPATCH, an in-context learning approach designed to bridge this gap by enabling LLMs to automatically generate optimized code. Inspired by how programmers learn and apply knowledge to optimize software, AUTOPATCH incorporates three key components: (1) an analogy-driven framework to align LLM optimization with human cognitive processes, (2) a unified approach that integrates historical code examples and CFG analysis for context-aware learning, and (3) an automated pipeline for generating optimized code through in-context prompting. Experimental results demonstrate that AUTOPATCH achieves a 7.3% improvement in execution efficiency over GPT-4o across common generated executable code, highlighting its potential to advance automated program runtime optimization.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T04:36:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.16692v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.16692v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 Value Function Decomposition in Markov Recommendation Process</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaobei Wang, Shuchang Liu, Qingpeng Cai, Xiang Li, Lantao Hu, Han li, Guangming Xie
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in recommender systems have shown that user-system interaction essentially formulates long-term optimization problems, and online reinforcement learning can be adopted to improve recommendation performance. The general solution framework incorporates a value function that estimates the user's expected cumulative rewards in the future and guides the training of the recommendation policy. To avoid local maxima, the policy may explore potential high-quality actions during inference to increase the chance of finding better future rewards. To accommodate the stepwise recommendation process, one widely adopted approach to learning the value function is learning from the difference between the values of two consecutive states of a user. However, we argue that this paradigm involves an incorrect approximation in the stochastic process. Specifically, between the current state and the next state in each training sample, there exist two separate random factors from the stochastic policy and the uncertain user environment. Original temporal difference (TD) learning under these mixed random factors may result in a suboptimal estimation of the long-term rewards. As a solution, we show that these two factors can be separately approximated by decomposing the original temporal difference loss. The disentangled learning framework can achieve a more accurate estimation with faster learning and improved robustness against action exploration. As empirical verification of our proposed method, we conduct offline experiments with online simulated environments built based on public datasets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T04:22:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3696410.3714807' target='_blank'>doi</a><a href='http://arxiv.org/abs/2501.17409v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17409v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 vAttention: Dynamic Memory Management for Serving LLMs without
  PagedAttention</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ramya Prabhu, Ajay Nayak, Jayashree Mohan, Ramachandran Ramjee, Ashish Panwar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> PagedAttention is a popular approach for dynamic memory allocation in LLM serving systems. It enables on-demand allocation of GPU memory to mitigate KV cache fragmentation -- a phenomenon that crippled the batch size (and consequently throughput) in prior systems. However, in trying to allocate physical memory at runtime, PagedAttention ends up changing the virtual memory layout of the KV cache from contiguous to non-contiguous. Such a design leads to non-trivial programming and performance overheads.   We present vAttention -- an approach that mitigates fragmentation in physical memory while retaining the contiguity of KV cache in virtual memory. We achieve this by decoupling the allocation of virtual and physical memory using CUDA virtual memory management APIs. We also introduce various LLM-specific optimizations to address the limitations of CUDA virtual memory support. Overall, vAttention is a simpler, portable, and performant alternative to PagedAttention: it supports various attention kernels out-of-the-box and improves LLM serving throughput by up to 1.23x compared to the use of PagedAttention-based kernels of FlashAttention and FlashInfer.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T04:10:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.OS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2405.04437v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.04437v3' target='_blank'>pdf</a>
                </div>
            </div>
            <h1>Keyword: LLM Deployment</h1>

            <div class="paper">
                <h2>#1 Dialogue is Better Than Monologue: Instructing Medical LLMs via
  Strategical Conversations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zijie Liu, Xinyu Zhao, Jie Peng, Zhuangdi Zhu, Qingyu Chen, Xia Hu, Tianlong Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Current medical AI systems often fail to replicate real-world clinical reasoning, as they are predominantly trained and evaluated on static text and question-answer tasks. These tuning methods and benchmarks overlook critical aspects like evidence-based reasoning and handling distracting information. To bridge this gap, we introduce a novel benchmark that simulates real-world diagnostic scenarios, integrating noise and difficulty levels aligned with USMLE standards. Moreover, we explore dialogue-based fine-tuning, which transforms static datasets into conversational formats to better capture iterative reasoning processes. Experiments show that dialogue-tuned models outperform traditional methods, with improvements of $9.64\%$ in multi-round reasoning scenarios and $6.18\%$ in accuracy in a noisy environment. Our findings highlight dialogue tuning as a promising approach for advancing clinically aligned and robust medical AI systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T18:58:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17860v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17860v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 Improving Your Model Ranking on Chatbot Arena by Vote Rigging</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rui Min, Tianyu Pang, Chao Du, Qian Liu, Minhao Cheng, Min Lin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Chatbot Arena is a popular platform for evaluating LLMs by pairwise battles, where users vote for their preferred response from two randomly sampled anonymous models. While Chatbot Arena is widely regarded as a reliable LLM ranking leaderboard, we show that crowdsourced voting can be rigged to improve (or decrease) the ranking of a target model $m_{t}$. We first introduce a straightforward target-only rigging strategy that focuses on new battles involving $m_{t}$, identifying it via watermarking or a binary classifier, and exclusively voting for $m_{t}$ wins. However, this strategy is practically inefficient because there are over $190$ models on Chatbot Arena and on average only about $1\%$ of new battles will involve $m_{t}$. To overcome this, we propose omnipresent rigging strategies, exploiting the Elo rating mechanism of Chatbot Arena that any new vote on a battle can influence the ranking of the target model $m_{t}$, even if $m_{t}$ is not directly involved in the battle. We conduct experiments on around $1.7$ million historical votes from the Chatbot Arena Notebook, showing that omnipresent rigging strategies can improve model rankings by rigging only hundreds of new votes. While we have evaluated several defense mechanisms, our findings highlight the importance of continued efforts to prevent vote rigging. Our code is available at https://github.com/sail-sg/Rigging-ChatbotArena.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T18:57:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.CR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17858v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17858v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse
  Autoencoders</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhengxuan Wu, Aryaman Arora, Atticus Geiger, Zheng Wang, Jing Huang, Dan Jurafsky, Christopher D. Manning, Christopher Potts
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Fine-grained steering of language model outputs is essential for safety and reliability. Prompting and finetuning are widely used to achieve these goals, but interpretability researchers have proposed a variety of representation-based techniques as well, including sparse autoencoders (SAEs), linear artificial tomography, supervised steering vectors, linear probes, and representation finetuning. At present, there is no benchmark for making direct comparisons between these proposals. Therefore, we introduce AxBench, a large-scale benchmark for steering and concept detection, and report experiments on Gemma-2-2B and 9B. For steering, we find that prompting outperforms all existing methods, followed by finetuning. For concept detection, representation-based methods such as difference-in-means, perform the best. On both evaluations, SAEs are not competitive. We introduce a novel weakly-supervised representational method (Rank-1 Representation Finetuning; ReFT-r1), which is competitive on both tasks while providing the interpretability advantages that prompting lacks. Along with AxBench, we train and publicly release SAE-scale feature dictionaries for ReFT-r1 and DiffMean.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T18:52:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span><span>I.2.7</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17148v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17148v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 ATTNChecker: Highly-Optimized Fault Tolerant Attention for Large
  Language Model Training</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuhang Liang, Xinyi Li, Jie Ren, Ang Li, Bo Fang, Jieyang Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have demonstrated remarkable performance in various natural language processing tasks. However, the training of these models is computationally intensive and susceptible to faults, particularly in the attention mechanism, which is a critical component of transformer-based LLMs. In this paper, we investigate the impact of faults on LLM training, focusing on INF, NaN, and near-INF values in the computation results with systematic fault injection experiments. We observe the propagation patterns of these errors, which can trigger non-trainable states in the model and disrupt training, forcing the procedure to load from checkpoints. To mitigate the impact of these faults, we propose ATTNChecker, the first Algorithm-Based Fault Tolerance (ABFT) technique tailored for the attention mechanism in LLMs. ATTNChecker is designed based on fault propagation patterns of LLM and incorporates performance optimization to adapt to both system reliability and model vulnerability while providing lightweight protection for fast LLM training. Evaluations on four LLMs show that ATTNChecker incurs on average 7% overhead on training while detecting and correcting all extreme errors. Compared with the state-of-the-art checkpoint/restore approach, ATTNChecker reduces recovery overhead by up to 49x.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T18:49:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.LG</span><span>C.1.4; B.2.3; I.2.7</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.11720v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.11720v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 Large Language Models and Code Security: A Systematic Literature Review</h2>
                <div class="authors">
                    <strong>Authors:</strong> Enna Basic, Alberto Giaretta
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have emerged as powerful tools for automating various programming tasks, including security-related ones, such as detecting and fixing vulnerabilities. Despite their promising capabilities, when required to produce or modify pre-existing code, LLMs could introduce vulnerabilities unbeknown to the programmer. When analyzing code, they could miss clear vulnerabilities or signal nonexistent ones. In this Systematic Literature Review (SLR), we aim to investigate both the security benefits and potential drawbacks of using LLMs for a variety of code-related tasks. In particular, first we focus on the types of vulnerabilities that could be introduced by LLMs, when used for producing code. Second, we analyze the capabilities of LLMs to detect and fix vulnerabilities, in any given code, and how the prompting strategy of choice impacts their performance in these two tasks. Last, we provide an in-depth analysis on how data poisoning attacks on LLMs can impact performance in the aforementioned tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T18:49:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.15004v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.15004v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 DynEx: Dynamic Code Synthesis with Structured Design Exploration for
  Accelerated Exploratory Programming</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jenny Ma, Karthik Sreedhar, Vivian Liu, Pedro Alejandro Perez, Sitong Wang, Riya Sahni, Lydia B. Chilton
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in large language models have significantly expedited the process of generating front-end code. This allows users to rapidly prototype user interfaces and ideate through code, a process known as exploratory programming. However, existing LLM code generation tools focus more on technical implementation details rather than finding the right design given a particular problem. We present DynEx, an LLM-based method for design exploration in accelerated exploratory programming. DynEx introduces a technique to explore the design space through a structured Design Matrix before creating the prototype with a modular, stepwise approach to LLM code generation. Code is generated sequentially, and users can test and approve each step before moving onto the next. A user study of 10 experts found that DynEx increased design exploration and enabled the creation of more complex and varied prototypes compared to a Claude Artifact baseline. We conclude with a discussion of the implications of design exploration for exploratory programming.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T18:46:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.00400v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.00400v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 acoupi: An Open-Source Python Framework for Deploying Bioacoustic AI
  Models on Edge Devices</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aude Vuilliomenet, Santiago Martínez Balvanera, Oisin Mac Aodha, Kate E. Jones, Duncan Wilson
                </div>
                <div class="summary">
                    <strong>Summary:</strong> 1. Passive acoustic monitoring (PAM) coupled with artificial intelligence (AI) is becoming an essential tool for biodiversity monitoring. Traditional PAM systems require manual data offloading and impose substantial demands on storage and computing infrastructure. The combination of on-device AI-based processing and network connectivity enables local data analysis and transmission of only relevant information, greatly reducing storage needs. However, programming these devices for robust operation is challenging, requiring expertise in embedded systems and software engineering. Despite the increase in AI-based models for bioacoustics, their full potential remains unrealized without accessible tools to deploy them on custom hardware and tailor device behaviour to specific monitoring goals. 2. To address this challenge, we develop acoupi, an open-source Python framework that simplifies the creation and deployment of smart bioacoustic devices. acoupi integrates audio recording, AI-based data processing, data management, and real-time wireless messaging into a unified and configurable framework. By modularising key elements of the bioacoustic monitoring workflow, acoupi allows users to easily customise, extend, or select specific components to fit their unique monitoring needs. 3. We demonstrate the flexibility of acoupi by integrating two bioacoustic classifiers: BirdNET, for the classification of bird species, and BatDetect2, for the classification of UK bat species. We test the reliability of acoupi over a month-long deployment of two acoupi-powered devices in a UK urban park. 4. acoupi can be deployed on low-cost hardware such as the Raspberry Pi and can be customised for various applications. acoupi standardised framework and simplified tools facilitate the adoption of AI-powered PAM systems for researchers and conservationists. acoupi is on GitHub at https://github.com/acoupi/acoupi.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T18:44:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SD</span><span>cs.LG</span><span>eess.AS</span><span>H.5.5; I.2.m; J.3</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17841v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17841v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 Learning Beyond the Surface: How Far Can Continual Pre-Training with
  LoRA Enhance LLMs' Domain-Specific Insight Learning?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pouya Pezeshkpour, Estevam Hruschka
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have demonstrated remarkable performance on various tasks, yet their ability to extract and internalize deeper insights from domain-specific datasets remains underexplored. In this study, we investigate how continual pre-training can enhance LLMs' capacity for insight learning across three distinct forms: declarative, statistical, and probabilistic insights. Focusing on two critical domains: medicine and finance, we employ LoRA to train LLMs on two existing datasets. To evaluate each insight type, we create benchmarks to measure how well continual pre-training helps models go beyond surface-level knowledge. We also assess the impact of document modification on capturing insights. The results show that, while continual pre-training on original documents has a marginal effect, modifying documents to retain only essential information significantly enhances the insight-learning capabilities of LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T18:40:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17840v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17840v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large
  Language Models on Generated Data</h2>
                <div class="authors">
                    <strong>Authors:</strong> Atilla Akkus, Masoud Poorghaffar Aghdam, Mingjie Li, Junjie Chu, Michael Backes, Yang Zhang, Sinem Sav
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have demonstrated significant success in various domain-specific tasks, with their performance often improving substantially after fine-tuning. However, fine-tuning with real-world data introduces privacy risks. To mitigate these risks, developers increasingly rely on synthetic data generation as an alternative to using real data, as data generated by traditional models is believed to be different from real-world data. However, with the advanced capabilities of LLMs, the distinction between real data and data generated by these models has become nearly indistinguishable. This convergence introduces similar privacy risks for generated data to those associated with real data. Our study investigates whether fine-tuning with LLM-generated data truly enhances privacy or introduces additional privacy risks by examining the structural characteristics of data generated by LLMs, focusing on two primary fine-tuning approaches: supervised fine-tuning (SFT) with unstructured (plain-text) generated data and self-instruct tuning. In the scenario of SFT, the data is put into a particular instruction tuning format used by previous studies. We use Personal Information Identifier (PII) leakage and Membership Inference Attacks (MIAs) on the Pythia Model Suite and Open Pre-trained Transformer (OPT) to measure privacy risks. Notably, after fine-tuning with unstructured generated data, the rate of successful PII extractions for Pythia increased by over 20%, highlighting the potential privacy implications of such approaches. Furthermore, the ROC-AUC score of MIAs for Pythia-6.9b, the second biggest model of the suite, increases over 40% after self-instruct tuning. Our results indicate the potential privacy risks associated with fine-tuning LLMs using generated data, underscoring the need for careful consideration of privacy safeguards in such approaches.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T18:27:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11423v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11423v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 LEKA:LLM-Enhanced Knowledge Augmentation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xinhao Zhang, Jinghan Zhang, Fengran Mo, Dongjie Wang, Yanjie Fu, Kunpeng Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Humans excel in analogical learning and knowledge transfer and, more importantly, possess a unique understanding of identifying appropriate sources of knowledge. From a model's perspective, this presents an interesting challenge. If models could autonomously retrieve knowledge useful for transfer or decision-making to solve problems, they would transition from passively acquiring to actively accessing and learning from knowledge. However, filling models with knowledge is relatively straightforward -- it simply requires more training and accessible knowledge bases. The more complex task is teaching models about which knowledge can be analogized and transferred. Therefore, we design a knowledge augmentation method LEKA for knowledge transfer that actively searches for suitable knowledge sources that can enrich the target domain's knowledge. This LEKA method extracts key information from textual information from the target domain, retrieves pertinent data from external data libraries, and harmonizes retrieved data with the target domain data in feature space and marginal probability measures. We validate the effectiveness of our approach through extensive experiments across various domains and demonstrate significant improvements over traditional methods in reducing computational costs, automating data alignment, and optimizing transfer learning outcomes.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T17:44:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17802v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17802v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 Leveraging Multimodal LLM for Inspirational User Interface Search</h2>
                <div class="authors">
                    <strong>Authors:</strong> Seokhyeon Park, Yumin Song, Soohyun Lee, Jaeyoung Kim, Jinwook Seo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Inspirational search, the process of exploring designs to inform and inspire new creative work, is pivotal in mobile user interface (UI) design. However, exploring the vast space of UI references remains a challenge. Existing AI-based UI search methods often miss crucial semantics like target users or the mood of apps. Additionally, these models typically require metadata like view hierarchies, limiting their practical use. We used a multimodal large language model (MLLM) to extract and interpret semantics from mobile UI images. We identified key UI semantics through a formative study and developed a semantic-based UI search system. Through computational and human evaluations, we demonstrate that our approach significantly outperforms existing UI retrieval methods, offering UI designers a more enriched and contextually relevant search experience. We enhance the understanding of mobile UI design semantics and highlight MLLMs' potential in inspirational search, providing a rich dataset of UI semantics for future studies.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T17:38:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3706598.3714213' target='_blank'>doi</a><a href='http://arxiv.org/abs/2501.17799v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17799v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 BreezyVoice: Adapting TTS for Taiwanese Mandarin with Enhanced Polyphone
  Disambiguation -- Challenges and Insights</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chan-Jan Hsu, Yi-Cheng Lin, Chia-Chun Lin, Wei-Chih Chen, Ho Lam Chung, Chen-An Li, Yi-Chang Chen, Chien-Yu Yu, Ming-Ji Lee, Chien-Cheng Chen, Ru-Heng Huang, Hung-yi Lee, Da-Shan Shiu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present BreezyVoice, a Text-to-Speech (TTS) system specifically adapted for Taiwanese Mandarin, highlighting phonetic control abilities to address the unique challenges of polyphone disambiguation in the language. Building upon CosyVoice, we incorporate a $S^{3}$ tokenizer, a large language model (LLM), an optimal-transport conditional flow matching model (OT-CFM), and a grapheme to phoneme prediction model, to generate realistic speech that closely mimics human utterances. Our evaluation demonstrates BreezyVoice's superior performance in both general and code-switching contexts, highlighting its robustness and effectiveness in generating high-fidelity speech. Additionally, we address the challenges of generalizability in modeling long-tail speakers and polyphone disambiguation. Our approach significantly enhances performance and offers valuable insights into the workings of neural codec TTS systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T17:31:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17790v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17790v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 Reasoning Over the Glyphs: Evaluation of LLM's Decipherment of Rare
  Scripts</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yu-Fei Shih, Zheng-Lin Lin, Shu-Kai Hsieh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We explore the capabilities of LVLMs and LLMs in deciphering rare scripts not encoded in Unicode. We introduce a novel approach to construct a multimodal dataset of linguistic puzzles involving such scripts, utilizing a tokenization method for language glyphs. Our methods include the Picture Method for LVLMs and the Description Method for LLMs, enabling these models to tackle these challenges. We conduct experiments using prominent models, GPT-4o, Gemini, and Claude 3.5 Sonnet, on linguistic puzzles. Our findings reveal the strengths and limitations of current AI methods in linguistic decipherment, highlighting the impact of Unicode encoding on model performance and the challenges of modeling visual language tokens through descriptions. Our study advances understanding of AI's potential in linguistic decipherment and underscores the need for further research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T17:24:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span><span>J.5; I.2.7</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17785v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17785v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 Large Language Models Can Solve Real-World Planning Rigorously with
  Formal Verification Tools</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yilun Hao, Yongchao Chen, Yang Zhang, Chuchu Fan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) struggle to directly generate correct plans for complex multi-constraint planning problems, even with self-verification and self-critique. For example, a U.S. domestic travel planning benchmark TravelPlanner was proposed in Xie et al. (2024), where the best LLM OpenAI o1-preview can only find viable travel plans with a 10% success rate given all needed information. In this work, we tackle this by proposing an LLM-based planning framework that formalizes and solves complex multi-constraint planning problems as constrained satisfiability problems, which are further consumed by sound and complete satisfiability solvers. We start with TravelPlanner as the primary use case and show that our framework achieves a success rate of 93.9% and is effective with diverse paraphrased prompts. More importantly, our framework has strong zero-shot generalizability, successfully handling unseen constraints in our newly created unseen international travel dataset and generalizing well to new fundamentally different domains. Moreover, when user input queries are infeasible, our framework can identify the unsatisfiable core, provide failure reasons, and offers personalized modification suggestions. We show that our framework can modify and solve for an average of 81.6% and 91.7% unsatisfiable queries from two datasets and prove with ablations that all key components of our framework are effective and necessary. Project page: https://sites.google.com/view/llm-rwplanning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T17:24:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.11891v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.11891v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 SafePR: Unified Approach for Safe Parallel Robots by Contact Detection
  and Reaction with Redundancy Resolution</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aran Mohammad, Tim-Lukas Habich, Thomas Seel, Moritz Schappler
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Fast and safe motion is crucial for the successful deployment of physically interactive robots. Parallel robots (PRs) offer the potential for higher speeds while maintaining the same energy limits due to their low moving masses. However, they require methods for contact detection and reaction while avoiding singularities and self-collisions. We address this issue and present SafePR - a unified approach for the detection and localization, including the distinction between collision and clamping to perform a reaction that is safe for humans and feasible for PRs. Our approach uses information from the encoders and motor currents to estimate forces via a generalized-momentum observer. Neural networks and particle filters classify and localize the contacts. We introduce reactions with redundancy resolution to avoid type-II singularities and self-collisions. Our approach detected and terminated 72 real-world collision and clamping contacts with end-effector speeds of up to 1.5 m/s, each within 25-275 ms. The forces were below the thresholds from ISO/TS 15066. By using built-in sensors, SafePR enables safe interaction with already assembled PRs without the need for new hardware components.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T17:09:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17773v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17773v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 2SSP: A Two-Stage Framework for Structured Pruning of LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fabrizio Sandri, Elia Cunegatti, Giovanni Iacca
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We propose a novel Two-Stage framework for Structured Pruning (2SSP) for pruning Large Language Models (LLMs), which combines two different strategies of pruning, namely Width and Depth Pruning. The first stage (Width Pruning) removes entire neurons, hence their corresponding rows and columns, aiming to preserve the connectivity among the pruned structures in the intermediate state of the Feed-Forward Networks in each Transformer block. This is done based on an importance score measuring the impact of each neuron over the output magnitude. The second stage (Depth Pruning), instead, removes entire Attention submodules. This is done by applying an iterative process that removes the Attention submodules with the minimum impact on a given metric of interest (in our case, perplexity). We also propose a novel mechanism to balance the sparsity rate of the two stages w.r.t. to the desired global sparsity. We test 2SSP on four LLM families and three sparsity rates (25\%, 37.5\%, and 50\%), measuring the resulting perplexity over three language modeling datasets as well as the performance over six downstream tasks. Our method consistently outperforms five state-of-the-art competitors over three language modeling and six downstream tasks, with an up to two-order-of-magnitude gain in terms of pruning time. The code is available at available at \url{https://github.com/FabrizioSandri/2SSP}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T17:05:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17771v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17771v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 Hybrid Graphs for Table-and-Text based Question Answering using LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ankush Agarwal, Ganesh S, Chaitanya Devaguptapu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Answering questions that require reasoning and aggregation across both structured (tables) and unstructured (raw text) data sources presents significant challenges. Current methods rely on fine-tuning and high-quality, human-curated data, which is difficult to obtain. Recent advances in Large Language Models (LLMs) have shown promising results for multi-hop question answering (QA) over single-source text data in a zero-shot setting, yet exploration into multi-source Table-Text QA remains limited. In this paper, we present a novel Hybrid Graph-based approach for Table-Text QA that leverages LLMs without fine-tuning. Our method constructs a unified Hybrid Graph from textual and tabular data, pruning information based on the input question to provide the LLM with relevant context concisely. We evaluate our approach on the challenging Hybrid-QA and OTT-QA datasets using state-of-the-art LLMs, including GPT-3.5, GPT-4, and LLaMA-3. Our method achieves the best zero-shot performance on both datasets, improving Exact Match scores by up to 10% on Hybrid-QA and 5.4% on OTT-QA. Moreover, our approach reduces token usage by up to 53% compared to the original context.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T16:58:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17767v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17767v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 Prompt Obfuscation for Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> David Pape, Sina Mavali, Thorsten Eisenhofer, Lea Schönherr
                </div>
                <div class="summary">
                    <strong>Summary:</strong> System prompts that include detailed instructions to describe the task performed by the underlying LLM can easily transform foundation models into tools and services with minimal overhead. Because of their crucial impact on the utility, they are often considered intellectual property, similar to the code of a software product. However, extracting system prompts is easily possible. As of today, there is no effective countermeasure to prevent the stealing of system prompts and all safeguarding efforts could be evaded. In this work, we propose an alternative to conventional system prompts. We introduce prompt obfuscation to prevent the extraction of the system prompt with only little overhead. The core idea is to find a representation of the original system prompt that leads to the same functionality, while the obfuscated system prompt does not contain any information that allows conclusions to be drawn about the original system prompt. We evaluate our approach by comparing our obfuscated prompt output with the output of the original prompt, using eight distinct metrics, to measure the lexical, character-level, and semantic similarity. We show that the obfuscated version is constantly on par with the original one. We further perform three different deobfuscation attacks with varying attacker knowledge--covering both black-box and white-box conditions--and show that in realistic attack scenarios an attacker is not able to extract meaningful information. Overall, we demonstrate that prompt obfuscation is an effective mechanism to safeguard the intellectual property of a system prompt while maintaining the same utility as the original prompt.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T16:57:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11026v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11026v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 Online Prompt Selection for Program Synthesis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yixuan Li, Lewis Frampton, Federico Mora, Elizabeth Polgreen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) demonstrate impressive capabilities in the domain of program synthesis. This level of performance is not, however, universal across all tasks, all LLMs and all prompting styles. There are many areas where one LLM dominates, one prompting style dominates, or where calling a symbolic solver is a better choice than an LLM. A key challenge for the user then, is to identify not only when an LLM is the right choice of solver, and the appropriate LLM to call for a given synthesis task, but also the right way to call it. A non-expert user who makes the wrong choice, incurs a cost both in terms of results (number of tasks solved, and the time it takes to solve them) and financial cost, if using a closed-source language model via a commercial API. We frame this choice as an online learning problem. We use a multi-armed bandit algorithm to select which symbolic solver, or LLM and prompt combination to deploy in order to maximize a given reward function (which may prioritize solving time, number of synthesis tasks solved, or financial cost of solving). We implement an instance of this approach, called CYANEA, and evaluate it on synthesis queries from the literature in ranking function synthesis, from the syntax-guided synthesis competition, and fresh, unseen queries generated from SMT problems. CYANEA solves 37.2% more queries than the best single solver and achieves results within 4% of the virtual best solver.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T16:52:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.05247v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.05247v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 Early External Safety Testing of OpenAI's o3-mini: Insights from the
  Pre-Deployment Evaluation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aitor Arrieta, Miriam Ugarte, Pablo Valle, José Antonio Parejo, Sergio Segura
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have become an integral part of our daily lives. However, they impose certain risks, including those that can harm individuals' privacy, perpetuate biases and spread misinformation. These risks highlight the need for robust safety mechanisms, ethical guidelines, and thorough testing to ensure their responsible deployment. Safety of LLMs is a key property that needs to be thoroughly tested prior the model to be deployed and accessible to the general users. This paper reports the external safety testing experience conducted by researchers from Mondragon University and University of Seville on OpenAI's new o3-mini LLM as part of OpenAI's early access for safety testing program. In particular, we apply our tool, ASTRAL, to automatically and systematically generate up to date unsafe test inputs (i.e., prompts) that helps us test and assess different safety categories of LLMs. We automatically generate and execute a total of 10,080 unsafe test input on a early o3-mini beta version. After manually verifying the test cases classified as unsafe by ASTRAL, we identify a total of 87 actual instances of unsafe LLM behavior. We highlight key insights and findings uncovered during the pre-deployment external testing phase of OpenAI's latest LLM.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T16:36:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17749v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17749v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 Planning Anything with Rigor: General-Purpose Zero-Shot Planning with
  LLM-based Formalized Programming</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yilun Hao, Yang Zhang, Chuchu Fan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While large language models (LLMs) have recently demonstrated strong potential in solving planning problems, there is a trade-off between flexibility and complexity. LLMs, as zero-shot planners themselves, are still not capable of directly generating valid plans for complex planning problems such as multi-constraint or long-horizon tasks. On the other hand, many frameworks aiming to solve complex planning problems often rely on task-specific preparatory efforts, such as task-specific in-context examples and pre-defined critics/verifiers, which limits their cross-task generalization capability. In this paper, we tackle these challenges by observing that the core of many planning problems lies in optimization problems: searching for the optimal solution (best plan) with goals subject to constraints (preconditions and effects of decisions). With LLMs' commonsense, reasoning, and programming capabilities, this opens up the possibilities of a universal LLM-based approach to planning problems. Inspired by this observation, we propose LLMFP, a general-purpose framework that leverages LLMs to capture key information from planning problems and formally formulate and solve them as optimization problems from scratch, with no task-specific examples needed. We apply LLMFP to 9 planning problems, ranging from multi-constraint decision making to multi-step planning problems, and demonstrate that LLMFP achieves on average 83.7% and 86.8% optimal rate across 9 tasks for GPT-4o and Claude 3.5 Sonnet, significantly outperforming the best baseline (direct planning with OpenAI o1-preview) with 37.6% and 40.7% improvements. We also validate components of LLMFP with ablation experiments and analyzed the underlying success and failure reasons. Project page: https://sites.google.com/view/llmfp.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T16:31:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.12112v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.12112v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 Revisiting Differential Verification: Equivalence Verification with
  Confidence</h2>
                <div class="authors">
                    <strong>Authors:</strong> Samuel Teuber, Philipp Kern, Marvin Janzen, Bernhard Beckert
                </div>
                <div class="summary">
                    <strong>Summary:</strong> When validated neural networks (NNs) are pruned (and retrained) before deployment, it is desirable to prove that the new NN behaves equivalently to the (original) reference NN. To this end, our paper revisits the idea of differential verification which performs reasoning on differences between NNs: On the one hand, our paper proposes a novel abstract domain for differential verification admitting more efficient reasoning about equivalence. On the other hand, we investigate empirically and theoretically which equivalence properties are (not) efficiently solved using differential reasoning. Based on the gained insights, and following a recent line of work on confidence-based verification, we propose a novel equivalence property that is amenable to Differential Verification while providing guarantees for large parts of the input space instead of small-scale guarantees constructed w.r.t. predetermined input points. We implement our approach in a new tool called VeryDiff and perform an extensive evaluation on numerous old and new benchmark families, including new pruned NNs for particle jet classification in the context of CERN's LHC where we observe median speedups >300x over the State-of-the-Art verifier alpha,beta-CROWN.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T16:21:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.LO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.20207v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.20207v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 Gateways for Institutional-Grade Commerce and Interoperability of
  Digital Assets</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rafael Belchior, Thomas Hardjono, Alex Chiriac, Venkatraman Ranakrishna
                </div>
                <div class="summary">
                    <strong>Summary:</strong> It is time for the legacy financial infrastructure to seamlessly connect with modern, decentralized infrastructure. Although it is increasingly evident that decentralized infrastructure for finance (namely distributed ledgers) will coexist with and complement legacy infrastructure, it is also clear that such interoperability efforts carry new risks and concerns. In particular, managing the range of heterogeneous (and not well-established) infrastructure brings security, privacy, and regulatory issues. The first step to overcome some of these challenges is to recognize that in many deployment instances using distributed ledgers, the purpose of the ledger is to share resources among the community members. The second step after recognizing that borders exist is to understand that interoperability across systems can be best achieved through the use of standardized service interfaces (or application programming interfaces (API)). In this paper we use the term ledger gateways (or simply gateways) to denote the computer and software systems that implement the standardized service interfaces into a distributed ledger. The main purpose of a gateway is to communicate with other peer gateways that implement the same standardized service interface. Among others, peer gateways perform the transfer of data and value across borders (legal or national borders). Gateways also become a mechanism to manage a permissioned environment, where abiding by laws and regulations is crucial for business compliance (e.g., EU General Data Protection Regulations (GDPR), EU MiCa regulation on digital assets, FAFT Recommendation 15, ISO 27001.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T16:15:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17732v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17732v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 Using Code Generation to Solve Open Instances of Combinatorial Design
  Problems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Christopher D. Rosin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Handbook of Combinatorial Designs catalogs many types of combinatorial designs, together with lists of open instances for which existence has not yet been determined. We develop a constructive protocol CPro1, which uses Large Language Models (LLMs) to generate code that constructs combinatorial designs and resolves some of these open instances. The protocol starts from a definition of a particular type of design, and a verifier that reliably confirms whether a proposed design is valid. The LLM selects strategies and implements them in code, and scaffolding provides automated hyperparameter tuning and execution feedback using the verifier. Most generated code fails, but by generating many candidates, the protocol automates exploration of a variety of standard methods (e.g. simulated annealing, genetic algorithms) and experimentation with variations (e.g. cost functions) to find successful approaches. Testing on 16 different types of designs, CPro1 constructs solutions to open instances for 6 of them: Symmetric and Skew Weighing Matrices, Equidistant Permutation Arrays, Packing Arrays, Balanced Ternary Designs, and Florentine Rectangles.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T15:57:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span><span>cs.DM</span><span>math.CO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17725v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17725v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 LogLLM: Log-based Anomaly Detection Using Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wei Guan, Jian Cao, Shiyou Qian, Jianqi Gao, Chun Ouyang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Software systems often record important runtime information in logs to help with troubleshooting. Log-based anomaly detection has become a key research area that aims to identify system issues through log data, ultimately enhancing the reliability of software systems. Traditional deep learning methods often struggle to capture the semantic information embedded in log data, which is typically organized in natural language. In this paper, we propose LogLLM, a log-based anomaly detection framework that leverages large language models (LLMs). LogLLM employs BERT for extracting semantic vectors from log messages, while utilizing Llama, a transformer decoder-based model, for classifying log sequences. Additionally, we introduce a projector to align the vector representation spaces of BERT and Llama, ensuring a cohesive understanding of log semantics. Unlike conventional methods that require log parsers to extract templates, LogLLM preprocesses log messages with regular expressions, streamlining the entire process. Our framework is trained through a novel three-stage procedure designed to enhance performance and adaptability. Experimental results across four public datasets demonstrate that LogLLM outperforms state-of-the-art methods. Even when handling unstable logs, it effectively captures the semantic meaning of log messages and detects anomalies accurately.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T15:41:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.08561v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.08561v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 Towards Lifelong Dialogue Agents via Timeline-based Memory Management</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kai Tzu-iunn Ong, Namyoung Kim, Minju Gwak, Hyungjoo Chae, Taeyoon Kwon, Yohan Jo, Seung-won Hwang, Dongha Lee, Jinyoung Yeo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> To achieve lifelong human-agent interaction, dialogue agents need to constantly memorize perceived information and properly retrieve it for response generation (RG). While prior studies focus on getting rid of outdated memories to improve retrieval quality, we argue that such memories provide rich, important contextual cues for RG (e.g., changes in user behaviors) in long-term conversations. We present THEANINE, a framework for LLM-based lifelong dialogue agents. THEANINE discards memory removal and manages large-scale memories by linking them based on their temporal and cause-effect relation. Enabled by this linking structure, THEANINE augments RG with memory timelines - series of memories representing the evolution or causality of relevant past events. Along with THEANINE, we introduce TeaFarm, a counterfactual-driven evaluation scheme, addressing the limitation of G-Eval and human efforts when assessing agent performance in integrating past memories into RG. A supplementary video for THEANINE and data for TeaFarm are at https://huggingface.co/spaces/ResearcherScholar/Theanine.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T15:34:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.10996v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.10996v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 RICoTA: Red-teaming of In-the-wild Conversation with Test Attempts</h2>
                <div class="authors">
                    <strong>Authors:</strong> Eujeong Choi, Younghun Jeong, Soomin Kim, Won Ik Cho
                </div>
                <div class="summary">
                    <strong>Summary:</strong> User interactions with conversational agents (CAs) evolve in the era of heavily guardrailed large language models (LLMs). As users push beyond programmed boundaries to explore and build relationships with these systems, there is a growing concern regarding the potential for unauthorized access or manipulation, commonly referred to as "jailbreaking." Moreover, with CAs that possess highly human-like qualities, users show a tendency toward initiating intimate sexual interactions or attempting to tame their chatbots. To capture and reflect these in-the-wild interactions into chatbot designs, we propose RICoTA, a Korean red teaming dataset that consists of 609 prompts challenging LLMs with in-the-wild user-made dialogues capturing jailbreak attempts. We utilize user-chatbot conversations that were self-posted on a Korean Reddit-like community, containing specific testing and gaming intentions with a social chatbot. With these prompts, we aim to evaluate LLMs' ability to identify the type of conversation and users' testing purposes to derive chatbot design implications for mitigating jailbreaking risks. Our dataset will be made publicly available via GitHub.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T15:32:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17715v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17715v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 Temperature-Free Loss Function for Contrastive Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bum Jun Kim, Sang Woo Kim
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As one of the most promising methods in self-supervised learning, contrastive learning has achieved a series of breakthroughs across numerous fields. A predominant approach to implementing contrastive learning is applying InfoNCE loss: By capturing the similarities between pairs, InfoNCE loss enables learning the representation of data. Albeit its success, adopting InfoNCE loss requires tuning a temperature, which is a core hyperparameter for calibrating similarity scores. Despite its significance and sensitivity to performance being emphasized by several studies, searching for a valid temperature requires extensive trial-and-error-based experiments, which increases the difficulty of adopting InfoNCE loss. To address this difficulty, we propose a novel method to deploy InfoNCE loss without temperature. Specifically, we replace temperature scaling with the inverse hyperbolic tangent function, resulting in a modified InfoNCE loss. In addition to hyperparameter-free deployment, we observed that the proposed method even yielded a performance gain in contrastive learning. Our detailed theoretical analysis discovers that the current practice of temperature scaling in InfoNCE loss causes serious problems in gradient descent, whereas our method provides desirable gradient properties. The proposed method was validated on five benchmarks on contrastive learning, yielding satisfactory results without temperature tuning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T14:43:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17683v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17683v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 SafeSteps: Learning Safer Footstep Planning Policies for Legged Robots
  via Model-Based Priors</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shafeef Omar, Lorenzo Amatucci, Victor Barasuol, Giulio Turrisi, Claudio Semini
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present a footstep planning policy for quadrupedal locomotion that is able to directly take into consideration a-priori safety information in its decisions. At its core, a learning process analyzes terrain patches, classifying each landing location by its kinematic feasibility, shin collision, and terrain roughness. This information is then encoded into a small vector representation and passed as an additional state to the footstep planning policy, which furthermore proposes only safe footstep location by applying a masked variant of the Proximal Policy Optimization algorithm. The performance of the proposed approach is shown by comparative simulations and experiments on an electric quadruped robot walking in different rough terrain scenarios. We show that violations of the above safety conditions are greatly reduced both during training and the successive deployment of the policy, resulting in an inherently safer footstep planner. Furthermore, we show how, as a byproduct, fewer reward terms are needed to shape the behavior of the policy, which in return is able to achieve both better final performances and sample efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T14:42:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/Humanoids57100.2023.10375218' target='_blank'>doi</a><a href='http://arxiv.org/abs/2307.12664v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2307.12664v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 Planning with Vision-Language Models and a Use Case in Robot-Assisted
  Teaching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xuzhe Dang, Lada Kudláčková, Stefan Edelkamp
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Automating the generation of Planning Domain Definition Language (PDDL) with Large Language Model (LLM) opens new research topic in AI planning, particularly for complex real-world tasks. This paper introduces Image2PDDL, a novel framework that leverages Vision-Language Models (VLMs) to automatically convert images of initial states and descriptions of goal states into PDDL problems. By providing a PDDL domain alongside visual inputs, Imasge2PDDL addresses key challenges in bridging perceptual understanding with symbolic planning, reducing the expertise required to create structured problem instances, and improving scalability across tasks of varying complexity. We evaluate the framework on various domains, including standard planning domains like blocksworld and sliding tile puzzles, using datasets with multiple difficulty levels. Performance is assessed on syntax correctness, ensuring grammar and executability, and content correctness, verifying accurate state representation in generated PDDL problems. The proposed approach demonstrates promising results across diverse task complexities, suggesting its potential for broader applications in AI planning. We will discuss a potential use case in robot-assisted teaching of students with Autism Spectrum Disorder.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T14:04:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17665v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17665v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models
  into Assembly Code Obfuscation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Seyedreza Mohseni, Seyedali Mohammadi, Deepa Tilwani, Yash Saxena, Gerald Ketu Ndawula, Sriram Vema, Edward Raff, Manas Gaur
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Malware authors often employ code obfuscations to make their malware harder to detect. Existing tools for generating obfuscated code often require access to the original source code (e.g., C++ or Java), and adding new obfuscations is a non-trivial, labor-intensive process. In this study, we ask the following question: Can Large Language Models (LLMs) potentially generate a new obfuscated assembly code? If so, this poses a risk to anti-virus engines and potentially increases the flexibility of attackers to create new obfuscation patterns. We answer this in the affirmative by developing the MetamorphASM benchmark comprising MetamorphASM Dataset (MAD) along with three code obfuscation techniques: dead code, register substitution, and control flow change. The MetamorphASM systematically evaluates the ability of LLMs to generate and analyze obfuscated code using MAD, which contains 328,200 obfuscated assembly code samples. We release this dataset and analyze the success rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder, CodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly code. The evaluation was performed using established information-theoretic metrics and manual human review to ensure correctness and provide the foundation for researchers to study and develop remediations to this risk.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T13:52:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.16135v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.16135v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 In-Context Meta LoRA Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yihua Shao, Minxi Yan, Yang Liu, Siyu Chen, Wenjie Chen, Xinwei Long, Ziyang Yan, Lei Li, Chenyu Zhang, Nicu Sebe, Hao Tang, Yan Wang, Hao Zhao, Mengzhu Wang, Jingcai Guo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Low-rank Adaptation (LoRA) has demonstrated remarkable capabilities for task specific fine-tuning. However, in scenarios that involve multiple tasks, training a separate LoRA model for each one results in considerable inefficiency in terms of storage and inference. Moreover, existing parameter generation methods fail to capture the correlations among these tasks, making multi-task LoRA parameter generation challenging. To address these limitations, we propose In-Context Meta LoRA (ICM-LoRA), a novel approach that efficiently achieves task-specific customization of large language models (LLMs). Specifically, we use training data from all tasks to train a tailored generator, Conditional Variational Autoencoder (CVAE). CVAE takes task descriptions as inputs and produces task-aware LoRA weights as outputs. These LoRA weights are then merged with LLMs to create task-specialized models without the need for additional fine-tuning. Furthermore, we utilize in-context meta-learning for knowledge enhancement and task mapping, to capture the relationship between tasks and parameter distributions. As a result, our method achieves more accurate LoRA parameter generation for diverse tasks using CVAE. ICM-LoRA enables more accurate LoRA parameter reconstruction than current parameter reconstruction methods and is useful for implementing task-specific enhancements of LoRA parameters. At the same time, our method occupies 283MB, only 1\% storage compared with the original LoRA.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T13:12:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17635v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17635v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 The Imitation Game According To Turing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sharon Temtsin, Diane Proudfoot, David Kaber, Christoph Bartneck
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The current cycle of hype and anxiety concerning the benefits and risks to human society of Artificial Intelligence is fuelled, not only by the increasing use of generative AI and other AI tools by the general public, but also by claims made on behalf of such technology by popularizers and scientists. In particular, recent studies have claimed that Large Language Models (LLMs) can pass the Turing Test-a goal for AI since the 1950s-and therefore can "think". Large-scale impacts on society have been predicted as a result. Upon detailed examination, however, none of these studies has faithfully applied Turing's original instructions. Consequently, we conducted a rigorous Turing Test with GPT-4-Turbo that adhered closely to Turing's instructions for a three-player imitation game. We followed established scientific standards where Turing's instructions were ambiguous or missing. For example, we performed a Computer-Imitates-Human Game (CIHG) without constraining the time duration and conducted a Man-Imitates-Woman Game (MIWG) as a benchmark. All but one participant correctly identified the LLM, showing that one of today's most advanced LLMs is unable to pass a rigorous Turing Test. We conclude that recent extravagant claims for such models are unsupported, and do not warrant either optimism or concern about the social impact of thinking machines.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T13:08:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span><span>cs.CY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17629v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17629v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 Uncertainty Quantification and Decomposition for LLM-based
  Recommendation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wonbin Kweon, Sanghwan Jang, SeongKu Kang, Hwanjo Yu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Despite the widespread adoption of large language models (LLMs) for recommendation, we demonstrate that LLMs often exhibit uncertainty in their recommendations. To ensure the trustworthy use of LLMs in generating recommendations, we emphasize the importance of assessing the reliability of recommendations generated by LLMs. We start by introducing a novel framework for estimating the predictive uncertainty to quantitatively measure the reliability of LLM-based recommendations. We further propose to decompose the predictive uncertainty into recommendation uncertainty and prompt uncertainty, enabling in-depth analyses of the primary source of uncertainty. Through extensive experiments, we (1) demonstrate predictive uncertainty effectively indicates the reliability of LLM-based recommendations, (2) investigate the origins of uncertainty with decomposed uncertainty measures, and (3) propose uncertainty-aware prompting for a lower predictive uncertainty and enhanced recommendation. Our source code and model weights are available at https://github.com/WonbinKweon/UNC_LLM_REC_WWW2025
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T13:08:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3696410.3714601' target='_blank'>doi</a><a href='http://arxiv.org/abs/2501.17630v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17630v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 Adaptively Weighted Averaging Over-the-Air Computation and Its
  Application to Distributed Gaussian Process Regression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Koya Sato, Koji Ishibashi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper introduces a noise-tolerant computing method for over-the-air computation (AirComp) aimed at weighted averaging, which is critical in various Internet of Things (IoT) applications such as environmental monitoring. Traditional AirComp approaches, while efficient, suffer significantly in accuracy due to noise enhancement in the normalization by the sum of weights. Our proposed method allows nodes to adaptively truncate their weights based on the channel conditions, thereby enhancing noise tolerance. Applied to distributed Gaussian process regression, the method facilitates low-latency, low-complexity, and high-accuracy distributed regression across a range of signal-to-noise ratios. We evaluate the performance of the proposed method in a radio map construction problem, which is a task for visualizing the radio environment based on limited sensing information and spatial interpolation. Numerical results demonstrate that our approach not only maintains computational accuracy in high noise scenarios but also achieves performance close to ideal conditions in high SNR environments. This advancement holds significant implications for the deployment of real-time, large-scale IoT systems, providing a scalable solution to enhance reliability and efficiency in data-driven environmental monitoring.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T13:06:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17627v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17627v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 AdaSociety: An Adaptive Environment with Social Structures for
  Multi-Agent Decision-Making</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yizhe Huang, Xingbo Wang, Hao Liu, Fanqi Kong, Aoyang Qin, Min Tang, Song-Chun Zhu, Mingjie Bi, Siyuan Qi, Xue Feng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Traditional interactive environments limit agents' intelligence growth with fixed tasks. Recently, single-agent environments address this by generating new tasks based on agent actions, enhancing task diversity. We consider the decision-making problem in multi-agent settings, where tasks are further influenced by social connections, affecting rewards and information access. However, existing multi-agent environments lack a combination of adaptive physical surroundings and social connections, hindering the learning of intelligent behaviors. To address this, we introduce AdaSociety, a customizable multi-agent environment featuring expanding state and action spaces, alongside explicit and alterable social structures. As agents progress, the environment adaptively generates new tasks with social structures for agents to undertake. In AdaSociety, we develop three mini-games showcasing distinct social structures and tasks. Initial results demonstrate that specific social structures can promote both individual and collective benefits, though current reinforcement learning and LLM-based algorithms show limited effectiveness in leveraging social structures to enhance performance. Overall, AdaSociety serves as a valuable research platform for exploring intelligence in diverse physical and social settings. The code is available at https://github.com/bigai-ai/AdaSociety.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T12:52:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MA</span><span>cs.AI</span><span>cs.GT</span><span>cs.LG</span><span>cs.SI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.03865v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.03865v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 Structured Context Recomposition for Large Language Models Using
  Probabilistic Layer Realignment</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jonathan Teel, Jocasta Cumberbatch, Raphael Benington, Quentin Baskerville
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Extended sequence generation often leads to degradation in contextual consistency due to the inability of conventional self-attention mechanisms to effectively retain long-range dependencies. Existing approaches, including memory compression and retrieval-augmented conditioning, introduce computational trade-offs that either increase inference latency or impose additional storage overhead. Structured Context Recomposition (SCR) introduces a probabilistic layer realignment strategy that dynamically adjusts learned representations within transformer layers, ensuring that semantically relevant embeddings persist throughout extended transformations. The proposed method enhances coherence retention through a recursive weighting function that redistributes representational emphasis based on inferred contextual relevance rather than relying on fixed token-level attention scores. Empirical results indicate that probabilistic realignment mitigates abrupt topic shifts and logical inconsistencies, particularly in scenarios where sequences exceed standard attention window constraints. Sequence-level entropy analysis further reveals that SCR moderates representational variability without introducing excessive output regularization, allowing models to sustain generative diversity while preserving contextual alignment. Attention head deviation measurements confirm that hierarchical reweighting contributes to smoother token dependency transitions across transformer layers, reinforcing the stability of multi-turn interactions and document-level reasoning. Computational resource assessments show that while SCR incurs a moderate increase in processing time, memory overhead remains within feasible limits, making it suitable for practical deployment in autoregressive generative applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T12:46:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17617v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17617v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 LLMs in the Heart of Differential Testing: A Case Study on a Medical
  Rule Engine</h2>
                <div class="authors">
                    <strong>Authors:</strong> Erblin Isaku, Christoph Laaber, Hassan Sartaj, Shaukat Ali, Thomas Schwitalla, Jan F. Nygård
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Cancer Registry of Norway (CRN) uses an automated cancer registration support system (CaReSS) to support core cancer registry activities, i.e, data capture, data curation, and producing data products and statistics for various stakeholders. GURI is a core component of CaReSS, which is responsible for validating incoming data with medical rules. Such medical rules are manually implemented by medical experts based on medical standards, regulations, and research. Since large language models (LLMs) have been trained on a large amount of public information, including these documents, they can be employed to generate tests for GURI. Thus, we propose an LLM-based test generation and differential testing approach (LLMeDiff) to test GURI. We experimented with four different LLMs, two medical rule engine implementations, and 58 real medical rules to investigate the hallucination, success, time efficiency, and robustness of the LLMs to generate tests, and these tests' ability to find potential issues in GURI. Our results showed that GPT-3.5 hallucinates the least, is the most successful, and is generally the most robust; however, it has the worst time efficiency. Our differential testing revealed 22 medical rules where implementation inconsistencies were discovered (e.g., regarding handling rule versions). Finally, we provide insights for practitioners and researchers based on the results.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T12:36:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.03664v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.03664v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 medIKAL: Integrating Knowledge Graphs as Assistants of LLMs for Enhanced
  Clinical Diagnosis on EMRs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mingyi Jia, Junwen Duan, Yan Song, Jianxin Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Electronic Medical Records (EMRs), while integral to modern healthcare, present challenges for clinical reasoning and diagnosis due to their complexity and information redundancy. To address this, we proposed medIKAL (Integrating Knowledge Graphs as Assistants of LLMs), a framework that combines Large Language Models (LLMs) with knowledge graphs (KGs) to enhance diagnostic capabilities. medIKAL assigns weighted importance to entities in medical records based on their type, enabling precise localization of candidate diseases within KGs. It innovatively employs a residual network-like approach, allowing initial diagnosis by the LLM to be merged into KG search results. Through a path-based reranking algorithm and a fill-in-the-blank style prompt template, it further refined the diagnostic process. We validated medIKAL's effectiveness through extensive experiments on a newly introduced open-sourced Chinese EMR dataset, demonstrating its potential to improve clinical diagnosis in real-world settings.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T12:27:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.14326v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.14326v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 How well can a large language model explain business processes as
  perceived by users?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dirk Fahland, Fabiana Fournier, Lior Limonad, Inna Skarbovsky, Ava J. E. Swevels
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are trained on a vast amount of text to interpret and generate human-like textual content. They are becoming a vital vehicle in realizing the vision of the autonomous enterprise, with organizations today actively adopting LLMs to automate many aspects of their operations. LLMs are likely to play a prominent role in future AI-augmented business process management systems, catering functionalities across all system lifecycle stages. One such system's functionality is Situation-Aware eXplainability (SAX), which relates to generating causally sound and human-interpretable explanations. In this paper, we present the SAX4BPM framework developed to generate SAX explanations. The SAX4BPM suite consists of a set of services and a central knowledge repository. The functionality of these services is to elicit the various knowledge ingredients that underlie SAX explanations. A key innovative component among these ingredients is the causal process execution view. In this work, we integrate the framework with an LLM to leverage its power to synthesize the various input ingredients for the sake of improved SAX explanations. Since the use of LLMs for SAX is also accompanied by a certain degree of doubt related to its capacity to adequately fulfill SAX along with its tendency for hallucination and lack of inherent capacity to reason, we pursued a methodological evaluation of the perceived quality of the generated explanations. We developed a designated scale and conducted a rigorous user study. Our findings show that the input presented to the LLMs aided with the guard-railing of its performance, yielding SAX explanations having better-perceived fidelity. This improvement is moderated by the perception of trust and curiosity. More so, this improvement comes at the cost of the perceived interpretability of the explanation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T12:15:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>68T01</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2401.12846v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2401.12846v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 Semantic Consistency Regularization with Large Language Models for
  Semi-supervised Sentiment Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kunrong Li, Xinyu Liu, Zhen Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Accurate sentiment analysis of texts is crucial for a variety of applications, such as understanding customer feedback, monitoring market trends, and detecting public sentiment. However, manually annotating large sentiment corpora for supervised learning is labor-intensive and time-consuming. Therefore, it is essential and effective to develop a semi-supervised method for the sentiment analysis task. Although some methods have been proposed for semi-supervised text classification, they rely on the intrinsic information within the unlabeled data and the learning capability of the NLP model, which lack generalization ability to the sentiment analysis scenario and may prone to overfit. Inspired by the ability of pretrained Large Language Models (LLMs) in following instructions and generating coherent text, we propose a Semantic Consistency Regularization with Large Language Models (SCR) framework for semi-supervised sentiment analysis. We introduce two prompting strategies to semantically enhance unlabeled text using LLMs. The first is Entity-based Enhancement (SCR-EE), which involves extracting entities and numerical information, and querying the LLM to reconstruct the textual information. The second is Concept-based Enhancement (SCR-CE), which directly queries the LLM with the original sentence for semantic reconstruction. Subsequently, the LLM-augmented data is utilized for a consistency loss with confidence thresholding, which preserves high-quality agreement samples to provide additional supervision signals during training. Furthermore, to fully utilize the uncertain unlabeled data samples, we propose a class re-assembling strategy inspired by the class space shrinking theorem. Experiments show our method achieves remarkable performance over prior semi-supervised methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T12:03:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17598v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17598v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 GLLM: Self-Corrective G-Code Generation using Large Language Models with
  User Feedback</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mohamed Abdelaal, Samuel Lokadjaja, Gilbert Engert
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper introduces GLLM, an innovative tool that leverages Large Language Models (LLMs) to automatically generate G-code from natural language instructions for Computer Numerical Control (CNC) machining. GLLM addresses the challenges of manual G-code writing by bridging the gap between human-readable task descriptions and machine-executable code. The system incorporates a fine-tuned StarCoder-3B model, enhanced with domain-specific training data and a Retrieval-Augmented Generation (RAG) mechanism. GLLM employs advanced prompting strategies and a novel self-corrective code generation approach to ensure both syntactic and semantic correctness of the generated G-code. The architecture includes robust validation mechanisms, including syntax checks, G-code-specific verifications, and functional correctness evaluations using Hausdorff distance. By combining these techniques, GLLM aims to democratize CNC programming, making it more accessible to users without extensive programming experience while maintaining high accuracy and reliability in G-code generation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T11:40:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17584v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17584v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 CSEval: Towards Automated, Multi-Dimensional, and Reference-Free
  Counterspeech Evaluation using Auto-Calibrated LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Amey Hengle, Aswini Kumar, Anil Bandhakavi, Tanmoy Chakraborty
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Counterspeech has been popular as an effective approach to counter online hate speech, leading to increasing research interest in automated counterspeech generation using language models. However, this field lacks standardised evaluation protocols and robust automated evaluation metrics that align with human judgement. Current automatic evaluation methods, primarily based on similarity metrics, do not effectively capture the complex and independent attributes of counterspeech quality, such as contextual relevance, aggressiveness, or argumentative coherence. This has led to an increased dependency on labor-intensive human evaluations to assess automated counter-speech generation methods. To address these challenges, we introduce CSEval, a novel dataset and framework for evaluating counterspeech quality across four dimensions: contextual-relevance, aggressiveness, argument-coherence, and suitableness. Furthermore, we propose Auto-Calibrated COT for Counterspeech Evaluation (ACE), a prompt-based method with auto-calibrated chain-of-thoughts (CoT) for scoring counterspeech using large language models. Our experiments show that ACE outperforms traditional metrics like ROUGE, METEOR, and BertScore in correlating with human judgement, indicating a significant advancement in automated counterspeech evaluation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T11:38:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.CY</span><span>cs.SI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17581v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17581v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 Is Conversational XAI All You Need? Human-AI Decision Making With a
  Conversational XAI Assistant</h2>
                <div class="authors">
                    <strong>Authors:</strong> Gaole He, Nilay Aishwarya, Ujwal Gadiraju
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Explainable artificial intelligence (XAI) methods are being proposed to help interpret and understand how AI systems reach specific predictions. Inspired by prior work on conversational user interfaces, we argue that augmenting existing XAI methods with conversational user interfaces can increase user engagement and boost user understanding of the AI system. In this paper, we explored the impact of a conversational XAI interface on users' understanding of the AI system, their trust, and reliance on the AI system. In comparison to an XAI dashboard, we found that the conversational XAI interface can bring about a better understanding of the AI system among users and higher user trust. However, users of both the XAI dashboard and conversational XAI interfaces showed clear overreliance on the AI system. Enhanced conversations powered by large language model (LLM) agents amplified over-reliance. Based on our findings, we reason that the potential cause of such overreliance is the illusion of explanatory depth that is concomitant with both XAI interfaces. Our findings have important implications for designing effective conversational XAI interfaces to facilitate appropriate reliance and improve human-AI collaboration. Code can be found at https://github.com/delftcrowd/IUI2025_ConvXAI
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T10:29:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3708359.3712133' target='_blank'>doi</a><a href='http://arxiv.org/abs/2501.17546v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17546v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling
  MiXed Emotions and Discourse Dynamics</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chenwei Wan, Matthieu Labeau, Chloé Clavel
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Designing emotionally intelligent conversational systems to provide comfort and advice to people experiencing distress is a compelling area of research. Recently, with advancements in large language models (LLMs), end-to-end dialogue agents without explicit strategy prediction steps have become prevalent. However, implicit strategy planning lacks transparency, and recent studies show that LLMs' inherent preference bias towards certain socio-emotional strategies hinders the delivery of high-quality emotional support. To address this challenge, we propose decoupling strategy prediction from language generation, and introduce a novel dialogue strategy prediction framework, EmoDynamiX, which models the discourse dynamics between user fine-grained emotions and system strategies using a heterogeneous graph for better performance and transparency. Experimental results on two ESC datasets show EmoDynamiX outperforms previous state-of-the-art methods with a significant margin (better proficiency and lower preference bias). Our approach also exhibits better transparency by allowing backtracing of decision making.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T10:25:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.08782v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.08782v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 Towards Supporting Penetration Testing Education with Large Language
  Models: an Evaluation and Comparison</h2>
                <div class="authors">
                    <strong>Authors:</strong> Martin Nizon-Deladoeuille, Brynjólfur Stefánsson, Helmut Neukirchen, Thomas Welsh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Cybersecurity education is challenging and it is helpful for educators to understand Large Language Models' (LLMs') capabilities for supporting education. This study evaluates the effectiveness of LLMs in conducting a variety of penetration testing tasks. Fifteen representative tasks were selected to cover a comprehensive range of real-world scenarios. We evaluate the performance of 6 models (GPT-4o mini, GPT-4o, Gemini 1.5 Flash, Llama 3.1 405B, Mixtral 8x7B and WhiteRabbitNeo) upon the Metasploitable v3 Ubuntu image and OWASP WebGOAT. Our findings suggest that GPT-4o mini currently offers the most consistent support making it a valuable tool for educational purposes. However, its use in conjonction with WhiteRabbitNeo should be considered, because of its innovative approach to tool and command recommendations. This study underscores the need for continued research into optimising LLMs for complex, domain-specific tasks in cybersecurity education.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T10:12:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17539v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17539v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 Evaluating Telugu Proficiency in Large Language Models_ A Comparative
  Analysis of ChatGPT and Gemini</h2>
                <div class="authors">
                    <strong>Authors:</strong> Katikela Sreeharsha Kishore, Rahimanuddin Shaik
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The growing prominence of large language models (LLMs) necessitates the exploration of their capabilities beyond English. This research investigates the Telugu language proficiency of ChatGPT and Gemini, two leading LLMs. Through a designed set of 20 questions encompassing greetings, grammar, vocabulary, common phrases, task completion, and situational reasoning, the study delves into their strengths and weaknesses in handling Telugu. The analysis aims to identify the LLM that demonstrates a deeper understanding of Telugu grammatical structures, possesses a broader vocabulary, and exhibits superior performance in tasks like writing and reasoning. By comparing their ability to comprehend and use everyday Telugu expressions, the research sheds light on their suitability for real-world language interaction. Furthermore, the evaluation of adaptability and reasoning capabilities provides insights into how each LLM leverages Telugu to respond to dynamic situations. This comparative analysis contributes to the ongoing discussion on multilingual capabilities in AI and paves the way for future research in developing LLMs that can seamlessly integrate with Telugu-speaking communities.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T09:54:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.19369v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.19369v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 LLM Assistance for Pediatric Depression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mariia Ignashina, Paulina Bondaronek, Dan Santel, John Pestian, Julia Ive
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Traditional depression screening methods, such as the PHQ-9, are particularly challenging for children in pediatric primary care due to practical limitations. AI has the potential to help, but the scarcity of annotated datasets in mental health, combined with the computational costs of training, highlights the need for efficient, zero-shot approaches. In this work, we investigate the feasibility of state-of-the-art LLMs for depressive symptom extraction in pediatric settings (ages 6-24). This approach aims to complement traditional screening and minimize diagnostic errors.   Our findings show that all LLMs are 60% more efficient than word match, with Flan leading in precision (average F1: 0.65, precision: 0.78), excelling in the extraction of more rare symptoms like "sleep problems" (F1: 0.92) and "self-loathing" (F1: 0.8). Phi strikes a balance between precision (0.44) and recall (0.60), performing well in categories like "Feeling depressed" (0.69) and "Weight change" (0.78). Llama 3, with the highest recall (0.90), overgeneralizes symptoms, making it less suitable for this type of analysis. Challenges include the complexity of clinical notes and overgeneralization from PHQ-9 scores. The main challenges faced by LLMs include navigating the complex structure of clinical notes with content from different times in the patient trajectory, as well as misinterpreting elevated PHQ-9 scores.   We finally demonstrate the utility of symptom annotations provided by Flan as features in an ML algorithm, which differentiates depression cases from controls with high precision of 0.78, showing a major performance boost compared to a baseline that does not use these features.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T09:27:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17510v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17510v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 Neural Spelling: A Spell-Based BCI System for Language Neural Decoding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaowei Jiang, Charles Zhou, Yiqun Duan, Ziyi Zhao, Thomas Do, Chin-Teng Lin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Brain-computer interfaces (BCIs) present a promising avenue by translating neural activity directly into text, eliminating the need for physical actions. However, existing non-invasive BCI systems have not successfully covered the entire alphabet, limiting their practicality. In this paper, we propose a novel non-invasive EEG-based BCI system with Curriculum-based Neural Spelling Framework, which recognizes all 26 alphabet letters by decoding neural signals associated with handwriting first, and then apply a Generative AI (GenAI) to enhance spell-based neural language decoding tasks. Our approach combines the ease of handwriting with the accessibility of EEG technology, utilizing advanced neural decoding algorithms and pre-trained large language models (LLMs) to translate EEG patterns into text with high accuracy. This system show how GenAI can improve the performance of typical spelling-based neural language decoding task, and addresses the limitations of previous methods, offering a scalable and user-friendly solution for individuals with communication impairments, thereby enhancing inclusive communication options.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T08:57:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17489v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17489v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 Regularized Best-of-N Sampling with Minimum Bayes Risk Objective for
  Language Model Alignment</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuu Jinnai, Tetsuro Morimura, Kaito Ariu, Kenshi Abe
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) to human preferences at the time of decoding. BoN sampling is susceptible to a problem known as reward hacking when the accuracy of the reward model is not high enough due to the quality or the quantity of the preference dataset. Because the reward model is an imperfect proxy for the true objective, over-optimizing its value can compromise its performance on the true objective. In this research, we propose MBR-BoN, a variant of BoN that aims to mitigate reward hacking at inference time by incorporating the Minimum Bayes Risk (MBR) objective as a proximity regularization term. We show empirically and analytically that the MBR objective quantifies the proximity of the response to the reference policy, serving as a proximity regularizer. We evaluate MBR-BoN on the AlpacaFarm and Anthropic's hh-rlhf datasets and show that it outperforms both BoN sampling and MBR decoding. We also evaluate MBR-BoN to generate a pairwise preference learning dataset for Direct Preference Optimization (DPO). Empirical results show that models trained on a dataset generated with MBR-BoN outperform those with vanilla BoN. Our code is available at https://github.com/CyberAgentAILab/regularized-bon
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T08:52:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.01054v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.01054v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 DFPE: A Diverse Fingerprint Ensemble for Enhancing LLM Performance</h2>
                <div class="authors">
                    <strong>Authors:</strong> Seffi Cohen, Niv Goldshlager, Nurit Cohen-Inger, Bracha Shapira, Lior Rokach
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have shown remarkable capabilities across various natural language processing tasks but often struggle to excel uniformly in diverse or complex domains. We propose a novel ensemble method - Diverse Fingerprint Ensemble (DFPE), which leverages the complementary strengths of multiple LLMs to achieve more robust performance. Our approach involves: (1) clustering models based on response "fingerprints" patterns, (2) applying a quantile-based filtering mechanism to remove underperforming models at a per-subject level, and (3) assigning adaptive weights to remaining models based on their subject-wise validation accuracy. In experiments on the Massive Multitask Language Understanding (MMLU) benchmark, DFPE outperforms the best single model by 3% overall accuracy and 5% in discipline-level accuracy. This method increases the robustness and generalization of LLMs and underscores how model selection, diversity preservation, and performance-driven weighting can effectively address challenging, multi-faceted language understanding tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T08:44:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17479v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17479v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 AugmenTest: Enhancing Tests with LLM-Driven Oracles</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shaker Mahmud Khandaker, Fitsum Kifetew, Davide Prandi, Angelo Susi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Automated test generation is crucial for ensuring the reliability and robustness of software applications while at the same time reducing the effort needed. While significant progress has been made in test generation research, generating valid test oracles still remains an open problem. To address this challenge, we present AugmenTest, an approach leveraging Large Language Models (LLMs) to infer correct test oracles based on available documentation of the software under test. Unlike most existing methods that rely on code, AugmenTest utilizes the semantic capabilities of LLMs to infer the intended behavior of a method from documentation and developer comments, without looking at the code. AugmenTest includes four variants: Simple Prompt, Extended Prompt, RAG with a generic prompt (without the context of class or method under test), and RAG with Simple Prompt, each offering different levels of contextual information to the LLMs. To evaluate our work, we selected 142 Java classes and generated multiple mutants for each. We then generated tests from these mutants, focusing only on tests that passed on the mutant but failed on the original class, to ensure that the tests effectively captured bugs. This resulted in 203 unique tests with distinct bugs, which were then used to evaluate AugmenTest. Results show that in the most conservative scenario, AugmenTest's Extended Prompt consistently outperformed the Simple Prompt, achieving a success rate of 30\% for generating correct assertions. In comparison, the state-of-the-art TOGA approach achieved 8.2\%. Contrary to our expectations, the RAG-based approaches did not lead to improvements, with performance of 18.2\% success rate for the most conservative scenario.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T07:45:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17461v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17461v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 Large Language Models for Single-Step and Multi-Step Flight Trajectory
  Prediction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kaiwei Luo, Jiliu Zhou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Flight trajectory prediction is a critical time series task in aviation. While deep learning methods have shown significant promise, the application of large language models (LLMs) to this domain remains underexplored. This study pioneers the use of LLMs for flight trajectory prediction by reframing it as a language modeling problem. Specifically, We extract features representing the aircraft's position and status from ADS-B flight data to construct a prompt-based dataset, where trajectory waypoints are converted into language tokens. The dataset is then employed to fine-tune LLMs, enabling them to learn complex spatiotemporal patterns for accurate predictions. Comprehensive experiments demonstrate that LLMs achieve notable performance improvements in both single-step and multi-step predictions compared to traditional methods, with LLaMA-3.1 model achieving the highest overall accuracy. However, the high inference latency of LLMs poses a challenge for real-time applications, underscoring the need for further research in this promising direction.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T07:35:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17459v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17459v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 MACI: Multi-Agent Collaborative Intelligence for Adaptive Reasoning and
  Temporal Planning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Edward Y. Chang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Artificial intelligence requires deliberate reasoning, temporal awareness, and effective constraint management, capabilities traditional LLMs often lack due to their reliance on pattern matching, limited self-verification, and inconsistent constraint handling. We introduce Multi-Agent Collaborative Intelligence (MACI), a framework comprising three key components: 1) a meta-planner (MP) that identifies, formulates, and refines all roles and constraints of a task (e.g., wedding planning) while generating a dependency graph, with common-sense augmentation to ensure realistic and practical constraints; 2) a collection of agents to facilitate planning and address task-specific requirements; and 3) a run-time monitor that manages plan adjustments as needed. By decoupling planning from validation, maintaining minimal agent context, and integrating common-sense reasoning, MACI overcomes the aforementioned limitations and demonstrates robust performance in two scheduling problems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T07:23:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>F.2.2</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.16689v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.16689v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 COBias and Debias: Balancing Class Accuracies for Language Models in
  Inference Time via Nonlinear Integer Programming</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruixi Lin, Yang You
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) are good knowledge bases but struggle to perform equally well for all classes in text classification tasks. This paper investigates a fundamental inference-time problem in language models: imbalanced class accuracies. We find what's underneath the issue is a tendency to over-predict some classes while under-predicting some others. This class accuracy imbalance is difficult to solve from the root via better pre-training or fine-tuning strategies, but we show it can be effectively mitigated via inference-time combinatorial optimization. To this end, we conceptualize and quantify the over- and under-prediction issue as the Contextual Oddity Bias (COBias), and propose the Debiasing as Nonlinear Integer Programming (DNIP) model to correct in-context learned class probabilities based on minimizing COBias and maximizing overall accuracy, without LLM parameter update. Considering that the DNIP model implicitly contains non-differentiable elements, we therefore use the simulated annealing algorithm to solve it. Extensive evaluations on three LLMs across seven NLP classification tasks in different prompting settings show that DNIP simultaneously achieves significant COBias reduction (-27%) and accuracy improvement (+12%) over the conventional ICL approach, suggesting that inference-time mitigation of class accuracy imbalance is a promising direction to push forward LLM performances.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T07:07:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2405.07623v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.07623v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 Beyond Text: Implementing Multimodal Large Language Model-Powered
  Multi-Agent Systems Using a No-Code Platform</h2>
                <div class="authors">
                    <strong>Authors:</strong> Cheonsu Jeong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This study proposes the design and implementation of a multimodal LLM-based Multi-Agent System (MAS) leveraging a No-Code platform to address the practical constraints and significant entry barriers associated with AI adoption in enterprises. Advanced AI technologies, such as Large Language Models (LLMs), often pose challenges due to their technical complexity and high implementation costs, making them difficult for many organizations to adopt. To overcome these limitations, this research develops a No-Code-based Multi-Agent System designed to enable users without programming knowledge to easily build and manage AI systems. The study examines various use cases to validate the applicability of AI in business processes, including code generation from image-based notes, Advanced RAG-based question-answering systems, text-based image generation, and video generation using images and prompts. These systems lower the barriers to AI adoption, empowering not only professional developers but also general users to harness AI for significantly improved productivity and efficiency. By demonstrating the scalability and accessibility of No-Code platforms, this study advances the democratization of AI technologies within enterprises and validates the practical applicability of Multi-Agent Systems, ultimately contributing to the widespread adoption of AI across various industries.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T06:49:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.00750v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.00750v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing
  Guardrail Moderation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, Ling Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent research shows that Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks -- models lose their safety alignment ability after fine-tuning on a few harmful samples. For risk mitigation, a guardrail is typically used to filter out harmful samples before fine-tuning. By designing a new red-teaming method, we in this paper show that purely relying on the moderation guardrail for data filtration is not reliable. Our proposed attack method, dubbed Virus, easily bypasses the guardrail moderation by slightly modifying the harmful data. Experimental results show that the harmful data optimized by Virus is not detectable by the guardrail with up to 100\% leakage ratio, and can simultaneously achieve superior attack performance. Finally, the key message we want to convey through this paper is that: \textbf{it is reckless to consider guardrail moderation as a clutch at straws towards harmful fine-tuning attack}, as it cannot solve the inherent safety issue of the pre-trained LLMs. Our code is available at https://github.com/git-disl/Virus
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T06:24:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span><span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17433v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17433v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 From Decision to Action in Surgical Autonomy: Multi-Modal Large Language
  Models for Robot-Assisted Blood Suction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sadra Zargarzadeh, Maryam Mirzaei, Yafei Ou, Mahdi Tavakoli
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rise of Large Language Models (LLMs) has impacted research in robotics and automation. While progress has been made in integrating LLMs into general robotics tasks, a noticeable void persists in their adoption in more specific domains such as surgery, where critical factors such as reasoning, explainability, and safety are paramount. Achieving autonomy in robotic surgery, which entails the ability to reason and adapt to changes in the environment, remains a significant challenge. In this work, we propose a multi-modal LLM integration in robot-assisted surgery for autonomous blood suction. The reasoning and prioritization are delegated to the higher-level task-planning LLM, and the motion planning and execution are handled by the lower-level deep reinforcement learning model, creating a distributed agency between the two components. As surgical operations are highly dynamic and may encounter unforeseen circumstances, blood clots and active bleeding were introduced to influence decision-making. Results showed that using a multi-modal LLM as a higher-level reasoning unit can account for these surgical complexities to achieve a level of reasoning previously unattainable in robot-assisted surgeries. These findings demonstrate the potential of multi-modal LLMs to significantly enhance contextual understanding and decision-making in robotic-assisted surgeries, marking a step toward autonomous surgical systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T06:13:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/LRA.2025.3535184' target='_blank'>doi</a><a href='http://arxiv.org/abs/2408.07806v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07806v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 TAID: Temporally Adaptive Interpolated Distillation for Efficient
  Knowledge Transfer in Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Makoto Shing, Kou Misaki, Han Bao, Sho Yokoi, Takuya Akiba
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Causal language models have demonstrated remarkable capabilities, but their size poses significant challenges for deployment in resource-constrained environments. Knowledge distillation, a widely-used technique for transferring knowledge from a large teacher model to a small student model, presents a promising approach for model compression. A significant remaining issue lies in the major differences between teacher and student models, namely the substantial capacity gap, mode averaging, and mode collapse, which pose barriers during distillation. To address these issues, we introduce $\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel knowledge distillation approach that dynamically interpolates student and teacher distributions through an adaptive intermediate distribution, gradually shifting from the student's initial distribution towards the teacher's distribution. We provide a theoretical analysis demonstrating TAID's ability to prevent mode collapse and empirically show its effectiveness in addressing the capacity gap while balancing mode averaging and mode collapse. Our comprehensive experiments demonstrate TAID's superior performance across various model sizes and architectures in both instruction tuning and pre-training scenarios. Furthermore, we showcase TAID's practical impact by developing two state-of-the-art compact foundation models: $\texttt{TAID-LLM-1.5B}$ for language tasks and $\texttt{TAID-VLM-2B}$ for vision-language tasks. These results demonstrate TAID's effectiveness in creating high-performing and efficient models, advancing the development of more accessible AI technologies.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T05:51:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.16937v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.16937v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges
  and Future Directions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hung Du, Srikanth Thudumu, Rajesh Vasa, Kon Mouzakis
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Research interest in autonomous agents is on the rise as an emerging topic. The notable achievements of Large Language Models (LLMs) have demonstrated the considerable potential to attain human-like intelligence in autonomous agents. However, the challenge lies in enabling these agents to learn, reason, and navigate uncertainties in dynamic environments. Context awareness emerges as a pivotal element in fortifying multi-agent systems when dealing with dynamic situations. Despite existing research focusing on both context-aware systems and multi-agent systems, there is a lack of comprehensive surveys outlining techniques for integrating context-aware systems with multi-agent systems. To address this gap, this survey provides a comprehensive overview of state-of-the-art context-aware multi-agent systems. First, we outline the properties of both context-aware systems and multi-agent systems that facilitate integration between these systems. Subsequently, we propose a general process for context-aware systems, with each phase of the process encompassing diverse approaches drawn from various application domains such as collision avoidance in autonomous driving, disaster relief management, utility management, supply chain management, human-AI interaction, and others. Finally, we discuss the existing challenges of context-aware multi-agent systems and provide future research directions in this field.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T05:41:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MA</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.01968v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.01968v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases
  in Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuxuan Li, Hirokazu Shirado, Sauvik Das
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While advances in fairness and alignment have helped mitigate overt biases exhibited by large language models (LLMs) when explicitly prompted, we hypothesize that these models may still exhibit implicit biases when simulating human behavior. To test this hypothesis, we propose a technique to systematically uncover such biases across a broad range of sociodemographic categories by assessing decision-making disparities among agents with LLM-generated, sociodemographically-informed personas. Using our technique, we tested six LLMs across three sociodemographic groups and four decision-making scenarios. Our results show that state-of-the-art LLMs exhibit significant sociodemographic disparities in nearly all simulations, with more advanced models exhibiting greater implicit biases despite reducing explicit biases. Furthermore, when comparing our findings to real-world disparities reported in empirical studies, we find that the biases we uncovered are directionally aligned but markedly amplified. This directional alignment highlights the utility of our technique in uncovering systematic biases in LLMs rather than random variations; moreover, the presence and amplification of implicit biases emphasizes the need for novel strategies to address these biases.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T05:21:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17420v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17420v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 CoCoNUT: Structural Code Understanding does not fall out of a tree</h2>
                <div class="authors">
                    <strong>Authors:</strong> Claas Beger, Saikat Dutta
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have shown impressive performance across a wide array of tasks involving both structured and unstructured textual data. Recent results on various benchmarks for code generation, repair, or completion suggest that certain models have programming abilities comparable to or even surpass humans. In this work, we demonstrate that high performance on such benchmarks does not correlate to humans' innate ability to understand structural control flow in code. To this end, we extract solutions from the HumanEval benchmark, which the relevant models perform strongly on, and trace their execution path using function calls sampled from the respective test set. Using this dataset, we investigate the ability of seven state-of-the-art LLMs to match the execution trace and find that, despite their ability to generate semantically identical code, they possess limited ability to trace execution paths, especially for longer traces and specific control structures. We find that even the top-performing model, Gemini, can fully and correctly generate only 47% of HumanEval task traces. Additionally, we introduce a subset for three key structures not contained in HumanEval: Recursion, Parallel Processing, and Object-Oriented Programming, including concepts like Inheritance and Polymorphism. Besides OOP, we show that none of the investigated models achieve an accuracy over 5% on the relevant traces. Aggregating these specialized parts with HumanEval tasks, we present CoCoNUT: Code Control Flow for Navigation Understanding and Testing, which measures a model's ability to trace execution of code upon relevant calls, including advanced structural components. We conclude that current LLMs need significant improvement to enhance code reasoning abilities. We hope our dataset helps researchers bridge this gap.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T05:15:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.16456v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.16456v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 Optimizing Code Runtime Performance through Context-Aware
  Retrieval-Augmented Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Manish Acharya, Yifan Zhang, Kevin Leach, Yu Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Optimizing software performance through automated code refinement offers a promising avenue for enhancing execution speed and efficiency. Despite recent advancements in LLMs, a significant gap remains in their ability to perform in-depth program analysis. This study introduces AUTOPATCH, an in-context learning approach designed to bridge this gap by enabling LLMs to automatically generate optimized code. Inspired by how programmers learn and apply knowledge to optimize software, AUTOPATCH incorporates three key components: (1) an analogy-driven framework to align LLM optimization with human cognitive processes, (2) a unified approach that integrates historical code examples and CFG analysis for context-aware learning, and (3) an automated pipeline for generating optimized code through in-context prompting. Experimental results demonstrate that AUTOPATCH achieves a 7.3% improvement in execution efficiency over GPT-4o across common generated executable code, highlighting its potential to advance automated program runtime optimization.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T04:36:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.16692v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.16692v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 vAttention: Dynamic Memory Management for Serving LLMs without
  PagedAttention</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ramya Prabhu, Ajay Nayak, Jayashree Mohan, Ramachandran Ramjee, Ashish Panwar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> PagedAttention is a popular approach for dynamic memory allocation in LLM serving systems. It enables on-demand allocation of GPU memory to mitigate KV cache fragmentation -- a phenomenon that crippled the batch size (and consequently throughput) in prior systems. However, in trying to allocate physical memory at runtime, PagedAttention ends up changing the virtual memory layout of the KV cache from contiguous to non-contiguous. Such a design leads to non-trivial programming and performance overheads.   We present vAttention -- an approach that mitigates fragmentation in physical memory while retaining the contiguity of KV cache in virtual memory. We achieve this by decoupling the allocation of virtual and physical memory using CUDA virtual memory management APIs. We also introduce various LLM-specific optimizations to address the limitations of CUDA virtual memory support. Overall, vAttention is a simpler, portable, and performant alternative to PagedAttention: it supports various attention kernels out-of-the-box and improves LLM serving throughput by up to 1.23x compared to the use of PagedAttention-based kernels of FlashAttention and FlashInfer.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T04:10:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.OS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2405.04437v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.04437v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 General Scene Adaptation for Vision-and-Language Navigation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haodong Hong, Yanyuan Qiao, Sen Wang, Jiajun Liu, Qi Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vision-and-Language Navigation (VLN) tasks mainly evaluate agents based on one-time execution of individual instructions across multiple environments, aiming to develop agents capable of functioning in any environment in a zero-shot manner. However, real-world navigation robots often operate in persistent environments with relatively consistent physical layouts, visual observations, and language styles from instructors. Such a gap in the task setting presents an opportunity to improve VLN agents by incorporating continuous adaptation to specific environments. To better reflect these real-world conditions, we introduce GSA-VLN, a novel task requiring agents to execute navigation instructions within a specific scene and simultaneously adapt to it for improved performance over time. To evaluate the proposed task, one has to address two challenges in existing VLN datasets: the lack of OOD data, and the limited number and style diversity of instructions for each scene. Therefore, we propose a new dataset, GSA-R2R, which significantly expands the diversity and quantity of environments and instructions for the R2R dataset to evaluate agent adaptability in both ID and OOD contexts. Furthermore, we design a three-stage instruction orchestration pipeline that leverages LLMs to refine speaker-generated instructions and apply role-playing techniques to rephrase instructions into different speaking styles. This is motivated by the observation that each individual user often has consistent signatures or preferences in their instructions. We conducted extensive experiments on GSA-R2R to thoroughly evaluate our dataset and benchmark various methods. Based on our findings, we propose a novel method, GR-DUET, which incorporates memory-based navigation graphs with an environment-specific training strategy, achieving state-of-the-art results on all GSA-R2R splits.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T03:57:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17403v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17403v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 MultiChallenge: A Realistic Multi-Turn Conversation Evaluation Benchmark
  Challenging to Frontier LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ved Sirdeshmukh, Kaustubh Deshpande, Johannes Mols, Lifeng Jin, Ed-Yeremai Cardona, Dean Lee, Jeremy Kritz, Willow Primack, Summer Yue, Chen Xing
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present MultiChallenge, a pioneering benchmark evaluating large language models (LLMs) on conducting multi-turn conversations with human users, a crucial yet underexamined capability for their applications. MultiChallenge identifies four categories of challenges in multi-turn conversations that are not only common and realistic among current human-LLM interactions, but are also challenging to all current frontier LLMs. All 4 challenges require accurate instruction-following, context allocation, and in-context reasoning at the same time. We also develop LLM as judge with instance-level rubrics to facilitate an automatic evaluation method with fair agreement with experienced human raters. Despite achieving near-perfect scores on existing multi-turn evaluation benchmarks, all frontier models have less than 50% accuracy on MultiChallenge, with the top-performing Claude 3.5 Sonnet (June 2024) achieving just a 41.4% average accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T03:29:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17399v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17399v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, Dacheng Tao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recently, long-thought reasoning LLMs, such as OpenAI's O1, adopt extended reasoning processes similar to how humans ponder over complex problems. This reasoning paradigm significantly enhances the model's problem-solving abilities and has achieved promising results. However, long-thought reasoning process leads to a substantial increase in inference time. A pressing challenge is reducing the inference overhead of long-thought LLMs while ensuring accuracy. In this paper, we experimentally demonstrate that long-thought reasoning models struggle to effectively allocate token budgets based on problem difficulty and reasoning redundancies. To address this, we propose Length-Harmonizing Fine-Tuning (O1-Pruner), aiming at minimizing reasoning overhead while maintaining accuracy. This effective fine-tuning method first estimates the LLM's baseline performance through pre-sampling and then uses RL-style fine-tuning to encourage the model to generate shorter reasoning processes under accuracy constraints. This allows the model to achieve efficient reasoning with lower redundancy while maintaining accuracy. Experiments on various mathematical reasoning benchmarks show that O1-Pruner not only significantly reduces inference overhead but also achieves higher accuracy, providing a novel and promising solution to this challenge. Our code is coming soon at https://github.com/StarDewXXX/O1-Pruner
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T03:11:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.12570v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.12570v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 How Efficient is LLM-Generated Code? A Rigorous & High-Standard
  Benchmark</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruizhong Qiu, Weiliang Will Zeng, James Ezick, Christopher Lott, Hanghang Tong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The emergence of large language models (LLMs) has significantly pushed the frontiers of program synthesis. Advancement of LLM-based program synthesis calls for a thorough evaluation of LLM-generated code. Most evaluation frameworks focus on the (functional) correctness of generated code; efficiency, as an important measure of code quality, has been overlooked in existing evaluations. In this work, we develop ENAMEL (EfficeNcy AutoMatic EvaLuator), a rigorous and high-standard benchmark for evaluating the capability of LLMs in generating efficient code. Firstly, we propose a new efficiency metric called eff@k, which generalizes the pass@k metric from correctness to efficiency and appropriately handles right-censored execution time. Furthermore, we derive an unbiased and variance-reduced estimator of eff@k via Rao--Blackwellization; we also provide a numerically stable implementation for the new estimator. Secondly, to set a high-standard for efficiency evaluation, we employ a human expert to design best algorithms and implementations as our reference solutions of efficiency, many of which are much more efficient than existing canonical solutions in HumanEval and HumanEval+. Moreover, to ensure a rigorous evaluation, we employ a human expert to curate strong test case generators to filter out wrong code and differentiate suboptimal algorithms. An extensive study across 30 popular LLMs using our benchmark ENAMEL shows that LLMs still fall short of generating expert-level efficient code. Using two subsets of our problem set, we demonstrate that such deficiency is because current LLMs struggle in designing advanced algorithms and are barely aware of implementation optimization. Our benchmark is publicly available at https://github.com/q-rz/enamel .
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T03:10:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.06647v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.06647v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 Ask, and it shall be given: On the Turing completeness of prompting</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruizhong Qiu, Zhe Xu, Wenxuan Bao, Hanghang Tong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Since the success of GPT, large language models (LLMs) have been revolutionizing machine learning and have initiated the so-called LLM prompting paradigm. In the era of LLMs, people train a single general-purpose LLM and provide the LLM with different prompts to perform different tasks. However, such empirical success largely lacks theoretical understanding. Here, we present the first theoretical study on the LLM prompting paradigm to the best of our knowledge. In this work, we show that prompting is in fact Turing-complete: there exists a finite-size Transformer such that for any computable function, there exists a corresponding prompt following which the Transformer computes the function. Furthermore, we show that even though we use only a single finite-size Transformer, it can still achieve nearly the same complexity bounds as that of the class of all unbounded-size Transformers. Overall, our result reveals that prompting can enable a single finite-size Transformer to be efficiently universal, which establishes a theoretical underpinning for prompt engineering in practice.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T02:53:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.01992v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.01992v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 Learning Free Token Reduction for Multi-Modal LLM</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zihui Zhao, Yingxin Li, Yang Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vision-Language Models (VLMs) have achieved remarkable success across a range of multimodal tasks; however, their practical deployment is often constrained by high computational costs and prolonged inference times. Since the vision modality typically carries more information than the text modality, compressing visual prompts offers a promising solution to alleviate these challenges. Existing approaches predominantly focus on refining model architectures or directly reducing the number of visual tokens. However, these methods often compromise inference performance due to a lack of consideration for the unique spatial and temporal characteristics of visual data. In this work, we propose a token compression paradigm that operates on both spatial and temporal dimensions. Our approach includes a learning-free, plug-and-play compression pipeline that can be seamlessly integrated into most Multimodal Large Language Model (MLLM) frameworks. By leveraging this method, we enhance the model inference capability while simultaneously reducing its computational cost. Experimental results on the Video-QA task demonstrate the effectiveness of the proposed approach, showcasing significant improvements in efficiency without sacrificing performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T02:52:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17391v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17391v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 On the Workflows and Smells of Leaderboard Operations (LBOps): An
  Exploratory Study of Foundation Model Leaderboards</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhimin Zhao, Abdul Ali Bangash, Filipe Roseiro Côgo, Bram Adams, Ahmed E. Hassan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Foundation models (FM), such as large language models (LLMs), which are large-scale machine learning (ML) models, have demonstrated remarkable adaptability in various downstream software engineering (SE) tasks, such as code completion, code understanding, and software development. As a result, FM leaderboards have become essential tools for SE teams to compare and select the best third-party FMs for their specific products and purposes. However, the lack of standardized guidelines for FM evaluation and comparison threatens the transparency of FM leaderboards and limits stakeholders' ability to perform effective FM selection. As a first step towards addressing this challenge, our research focuses on understanding how these FM leaderboards operate in real-world scenarios ("leaderboard operations") and identifying potential pitfalls and areas for improvement ("leaderboard smells"). In this regard, we collect up to 1,045 FM leaderboards from five different sources: GitHub, Hugging Face Spaces, Papers With Code, spreadsheet and independent platform, to examine their documentation and engage in direct communication with leaderboard operators to understand their workflows. Through card sorting and negotiated agreement, we identify five distinct workflow patterns and develop a domain model that captures the key components and their interactions within these workflows. We then identify eight unique types of leaderboard smells in LBOps. By mitigating these smells, SE teams can improve transparency, accountability, and collaboration in current LBOps practices, fostering a more robust and responsible ecosystem for FM comparison and selection.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T02:47:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/TSE.2025.3533972' target='_blank'>doi</a><a href='http://arxiv.org/abs/2407.04065v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.04065v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 Assessing the Capability of YOLO- and Transformer-based Object Detectors
  for Real-time Weed Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alicia Allmendinger, Ahmet Oğuz Saltık, Gerassimos G. Peteinatos, Anthony Stein, Roland Gerhards
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Spot spraying represents an efficient and sustainable method for reducing the amount of pesticides, particularly herbicides, used in agricultural fields. To achieve this, it is of utmost importance to reliably differentiate between crops and weeds, and even between individual weed species in situ and under real-time conditions. To assess suitability for real-time application, different object detection models that are currently state-of-the-art are compared. All available models of YOLOv8, YOLOv9, YOLOv10, and RT-DETR are trained and evaluated with images from a real field situation. The images are separated into two distinct datasets: In the initial data set, each species of plants is trained individually; in the subsequent dataset, a distinction is made between monocotyledonous weeds, dicotyledonous weeds, and three chosen crops. The results demonstrate that while all models perform equally well in the metrics evaluated, the YOLOv9 models, particularly the YOLOv9s and YOLOv9e, stand out in terms of their strong recall scores (66.58 \% and 72.36 \%), as well as mAP50 (73.52 \% and 79.86 \%), and mAP50-95 (43.82 \% and 47.00 \%) in dataset 2. However, the RT-DETR models, especially RT-DETR-l, excel in precision with reaching 82.44 \% on dataset 1 and 81.46 \% in dataset 2, making them particularly suitable for scenarios where minimizing false positives is critical. In particular, the smallest variants of the YOLO models (YOLOv8n, YOLOv9t, and YOLOv10n) achieve substantially faster inference times down to 7.58 ms for dataset 2 on the NVIDIA GeForce RTX 4090 GPU for analyzing one frame, while maintaining competitive accuracy, highlighting their potential for deployment in resource-constrained embedded computing devices as typically used in productive setups.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-29T02:39:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17387v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17387v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 Post-Training Quantization for 3D Medical Image Segmentation: A
  Practical Study on Real Inference Engines</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chongyu Qu, Ritchie Zhao, Ye Yu, Bin Liu, Tianyuan Yao, Junchao Zhu, Bennett A. Landman, Yucheng Tang, Yuankai Huo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Quantizing deep neural networks ,reducing the precision (bit-width) of their computations, can remarkably decrease memory usage and accelerate processing, making these models more suitable for large-scale medical imaging applications with limited computational resources. However, many existing methods studied "fake quantization", which simulates lower precision operations during inference, but does not actually reduce model size or improve real-world inference speed. Moreover, the potential of deploying real 3D low-bit quantization on modern GPUs is still unexplored. In this study, we introduce a real post-training quantization (PTQ) framework that successfully implements true 8-bit quantization on state-of-the-art (SOTA) 3D medical segmentation models, i.e., U-Net, SegResNet, SwinUNETR, nnU-Net, UNesT, TransUNet, ST-UNet,and VISTA3D. Our approach involves two main steps. First, we use TensorRT to perform fake quantization for both weights and activations with unlabeled calibration dataset. Second, we convert this fake quantization into real quantization via TensorRT engine on real GPUs, resulting in real-world reductions in model size and inference latency. Extensive experiments demonstrate that our framework effectively performs 8-bit quantization on GPUs without sacrificing model performance. This advancement enables the deployment of efficient deep learning models in medical imaging applications where computational resources are constrained. The code and models have been released, including U-Net, TransUNet pretrained on the BTCV dataset for abdominal (13-label) segmentation, UNesT pretrained on the Whole Brain Dataset for whole brain (133-label) segmentation, and nnU-Net, SegResNet, SwinUNETR and VISTA3D pretrained on TotalSegmentator V2 for full body (104-label) segmentation. https://github.com/hrlblab/PTQ.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T23:29:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17343v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17343v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 WALLABY Pilot Survey: kNN identification of perturbed galaxies through
  HI morphometrics</h2>
                <div class="authors">
                    <strong>Authors:</strong> B. W. Holwerda, Helga Dénes, J. Rhee, D. Leahy, B. Koribalski, N. Yu, N. Deg, T. Westmeier, K. Lee-Waddell, Y. Ascasibar, M. Saraf, X. Lin, B. Catinella, K. Hess
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Galaxy morphology in stellar light can be described by a series of "non-parametric" or "morphometric" parameters, such as concentration-asymmetry-smoothness, Gini, $M_{20}$, and Sersic fit. These parameters can be applied to column density maps of atomic hydrogen (HI). The HI distribution is susceptible to perturbations by environmental effects, e.g. inter-galactic medium pressure and tidal interactions. Therefore, HI morphology can potentially identify galaxies undergoing ram-pressure stripping or tidal interactions. We explore three fields in the WALLABY Pilot HI survey and identify perturbed galaxies based on a k-nearest Neighbor (kNN) algorithm using an HI morphometric feature space. For training, we used labeled galaxies in the combined NGC 4808 and NGC 4636 fields with six HI morphometrics to train and test a kNN classifier. The kNN classification is proficient in classifying perturbed galaxies with all metrics -- accuracy, precision and recall -- at 70-80%. By using the kNN method to identify perturbed galaxies in the deployment field, the NGC 5044 mosaic, we find that in most regards, the scaling relations of perturbed and unperturbed galaxies have similar distribution in the scaling relations of stellar mass vs star formation rate and the Baryonic Tully-Fisher relation, but the HI and stellar mass relation flatter than of the unperturbed galaxies. Our results for NGC 5044 provide a prediction for future studies on the fraction of galaxies undergoing interaction in this catalogue and to build a training sample to classify such galaxies in the full WALLABY survey.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T23:05:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.GA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.10563v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.10563v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 Compact Neural TTS Voices for Accessibility</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kunal Jain, Eoin Murphy, Deepanshu Gupta, Jonathan Dyke, Saumya Shah, Vasilieios Tsiaras, Petko Petkov, Alistair Conkie
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Contemporary text-to-speech solutions for accessibility applications can typically be classified into two categories: (i) device-based statistical parametric speech synthesis (SPSS) or unit selection (USEL) and (ii) cloud-based neural TTS. SPSS and USEL offer low latency and low disk footprint at the expense of naturalness and audio quality. Cloud-based neural TTS systems provide significantly better audio quality and naturalness but regress in terms of latency and responsiveness, rendering these impractical for real-world applications. More recently, neural TTS models were made deployable to run on handheld devices. Nevertheless, latency remains higher than SPSS and USEL, while disk footprint prohibits pre-installation for multiple voices at once. In this work, we describe a high-quality compact neural TTS system achieving latency on the order of 15 ms with low disk footprint. The proposed solution is capable of running on low-power devices.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T22:51:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SD</span><span>cs.LG</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17332v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17332v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 Attribution analysis of legal language as used by LLM</h2>
                <div class="authors">
                    <strong>Authors:</strong> Richard K. Belew
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Three publicly-available LLM specifically designed for legal tasks have been implemented and shown that classification accuracy can benefit from training over legal corpora, but why and how? Here we use two publicly-available legal datasets, a simpler binary classification task of ``overruling'' texts, and a more elaborate multiple choice task identifying ``holding'' judicial decisions. We report on experiments contrasting the legal LLM and a generic BERT model for comparison, against both datasets. We use integrated gradient attribution techniques to impute ``causes'' of variation in the models' perfomance, and characterize them in terms of the tokenizations each use. We find that while all models can correctly classify some test examples from the casehold task, other examples can only be identified by only one, model, and attribution can be used to highlight the reasons for this. We find that differential behavior of the models' tokenizers accounts for most of the difference and analyze these differences in terms of the legal language they process. Frequency analysis of tokens generated by dataset texts, combined with use of known ``stop word'' lists, allow identification of tokens that are clear signifiers of legal topics.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T22:48:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17330v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17330v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 WASUP: Interpretable Classification with Weight-Input Alignment and
  Class-Discriminative SUPports Vectors</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tom Nuno Wolf, Christian Wachinger
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The deployment of deep learning models in critical domains necessitates a balance between high accuracy and interpretability. We introduce WASUP, an inherently interpretable neural network that provides local and global explanations of its decision-making process. We prove that these explanations are faithful by fulfilling established axioms for explanations. Leveraging the concept of case-based reasoning, WASUP extracts class-representative support vectors from training images, ensuring they capture relevant features while suppressing irrelevant ones. Classification decisions are made by calculating and aggregating similarity scores between these support vectors and the input's latent feature vector. We employ B-Cos transformations, which align model weights with inputs to enable faithful mappings of latent features back to the input space, facilitating local explanations in addition to global explanations of case-based reasoning. We evaluate WASUP on three tasks: fine-grained classification on Stanford Dogs, multi-label classification on Pascal VOC, and pathology detection on the RSNA dataset. Results indicate that WASUP not only achieves competitive accuracy compared to state-of-the-art black-box models but also offers insightful explanations verified through theoretical analysis. Our findings underscore WASUP's potential for applications where understanding model decisions is as critical as the decisions themselves.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T22:39:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17328v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17328v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 Memorize and Rank: Elevating Large Language Models for Clinical
  Diagnosis Prediction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mingyu Derek Ma, Xiaoxuan Wang, Yijia Xiao, Anthony Cuturrufo, Vijay S Nori, Eran Halperin, Wei Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Clinical diagnosis prediction models, when provided with a patient's medical history, aim to detect potential diseases early, facilitating timely intervention and improving prognostic outcomes. However, the inherent scarcity of patient data and large disease candidate space often pose challenges in developing satisfactory models for this intricate task. The exploration of leveraging Large Language Models (LLMs) for encapsulating clinical decision processes has been limited. We introduce MERA, a clinical diagnosis prediction model that bridges pertaining natural language knowledge with medical practice. We apply hierarchical contrastive learning on a disease candidate ranking list to alleviate the large decision space issue. With concept memorization through fine-tuning, we bridge the natural language clinical knowledge with medical codes. Experimental results on MIMIC-III and IV datasets show that MERA achieves the state-of-the-art diagnosis prediction performance and dramatically elevates the diagnosis prediction capabilities of generative LMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T22:38:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17326v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17326v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 Safe and Trustworthy Robot Pathfinding with BIM, MHA*, and NLP</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mani Amani, Reza Akhavian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Construction robots have gained significant traction in recent years in research and development. However, the application of industrial robots has unique challenges. Dynamic environments, domain-specific tasks, and complex localization and mapping are significant obstacles in their development. In construction job sites, moving objects and complex machinery can make pathfinding a difficult task due to the possibility of object collisions. Existing methods such as simultaneous localization and mapping are viable solutions to this problem, however, due to the precision and data quality required by the sensors and the processing of the information, they can be very computationally expensive. We propose using spatial and semantic information in building information modeling (BIM) to develop domain-specific pathfinding strategies. In this work, we integrate a multi-heuristic A* (MHA*) algorithm using APFs from the BIM spatial information and process textual information from the BIM using large language models (LLMs) to adjust the algorithm for dynamic object avoidance. We show increased robot object proximity by 80% while maintaining similar path lengths.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T22:26:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2411.15371v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2411.15371v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 A sketch of an AI control safety case</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tomek Korbak, Joshua Clymer, Benjamin Hilton, Buck Shlegeris, Geoffrey Irving
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As LLM agents gain a greater capacity to cause harm, AI developers might increasingly rely on control measures such as monitoring to justify that they are safe. We sketch how developers could construct a "control safety case", which is a structured argument that models are incapable of subverting control measures in order to cause unacceptable outcomes. As a case study, we sketch an argument that a hypothetical LLM agent deployed internally at an AI company won't exfiltrate sensitive information. The sketch relies on evidence from a "control evaluation,"' where a red team deliberately designs models to exfiltrate data in a proxy for the deployment environment. The safety case then hinges on several claims: (1) the red team adequately elicits model capabilities to exfiltrate data, (2) control measures remain at least as effective in deployment, and (3) developers conservatively extrapolate model performance to predict the probability of data exfiltration in deployment. This safety case sketch is a step toward more concrete arguments that can be used to show that a dangerously capable LLM agent is safe to deploy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T21:52:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CR</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17315v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17315v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 RLPP: A Residual Method for Zero-Shot Real-World Autonomous Racing on
  Scaled Platforms</h2>
                <div class="authors">
                    <strong>Authors:</strong> Edoardo Ghignone, Nicolas Baumann, Cheng Hu, Jonathan Wang, Lei Xie, Andrea Carron, Michele Magno
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Autonomous racing presents a complex environment requiring robust controllers capable of making rapid decisions under dynamic conditions. While traditional controllers based on tire models are reliable, they often demand extensive tuning or system identification. RL methods offer significant potential due to their ability to learn directly from interaction, yet they typically suffer from the Sim-to-Reall gap, where policies trained in simulation fail to perform effectively in the real world. In this paper, we propose RLPP, a residual RL framework that enhances a PP controller with an RL-based residual. This hybrid approach leverages the reliability and interpretability of PP while using RL to fine-tune the controller's performance in real-world scenarios. Extensive testing on the F1TENTH platform demonstrates that RLPP improves lap times by up to 6.37 %, closing the gap to the SotA methods by more than 52 % and providing reliable performance in zero-shot real-world deployment, overcoming key challenges associated with the Sim-to-Real transfer and reducing the performance gap from simulation to reality by more than 8-fold when compared to the baseline RL controller. The RLPP framework is made available as an open-source tool, encouraging further exploration and advancement in autonomous racing research. The code is available at: www.github.com/forzaeth/rlpp.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T21:48:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.LG</span><span>68T40</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17311v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17311v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 Towards Integrating Emerging AI Applications in SE Education</h2>
                <div class="authors">
                    <strong>Authors:</strong> Michael Vierhauser, Iris Groher, Tobias Antensteiner, Clemens Sauerwein
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Artificial Intelligence (AI) approaches have been incorporated into modern learning environments and software engineering (SE) courses and curricula for several years. However, with the significant rise in popularity of large language models (LLMs) in general, and OpenAI's LLM-powered chatbot ChatGPT in particular in the last year, educators are faced with rapidly changing classroom environments and disrupted teaching principles. Examples range from programming assignment solutions that are fully generated via ChatGPT, to various forms of cheating during exams. However, despite these negative aspects and emerging challenges, AI tools in general, and LLM applications in particular, can also provide significant opportunities in a wide variety of SE courses, supporting both students and educators in meaningful ways. In this early research paper, we present preliminary results of a systematic analysis of current trends in the area of AI, and how they can be integrated into university-level SE curricula, guidelines, and approaches to support both instructors and learners. We collected both teaching and research papers and analyzed their potential usage in SE education, using the ACM Computer Science Curriculum Guidelines CS2023. As an initial outcome, we discuss a series of opportunities for AI applications and further research areas.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T21:46:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/CSEET62301.2024.10663045' target='_blank'>doi</a><a href='http://arxiv.org/abs/2405.18062v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.18062v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 Probing LLM World Models: Enhancing Guesstimation with Wisdom of Crowds
  Decoding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yun-Shiuan Chuang, Nikunj Harlalka, Sameer Narendran, Alexander Cheung, Sizhe Gao, Siddharth Suresh, Junjie Hu, Timothy T. Rogers
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Guesstimation, the task of making approximate quantity estimates, is a common real-world challenge. However, it has been largely overlooked in large language models (LLMs) and vision language models (VLMs) research. We introduce a novel guesstimation dataset, MARBLES. This dataset requires one to estimate how many items (e.g., marbles) can fit into containers (e.g., a one-cup measuring cup), both with and without accompanying images. Inspired by the social science concept of the ``{Wisdom of Crowds'' (WOC) - taking the median from estimates from a crowd), which has proven effective in guesstimation, we propose ``WOC decoding'' strategy for LLM guesstimation. We show that LLMs/VLMs perform well on guesstimation, suggesting that they possess some level of a "world model" necessary for guesstimation. Moreover, similar to human performance, the WOC decoding method improves LLM/VLM guesstimation accuracy. Furthermore, the inclusion of images in the multimodal condition enhances model performance. These results highlight the value of WOC decoding strategy for LLMs/VLMs and position guesstimation as a probe for evaluating LLMs/VLMs' world model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T21:43:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17310v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17310v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and
  Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence</h2>
                <div class="authors">
                    <strong>Authors:</strong> Robert Kaufman, Aaron Broukhim, David Kirsh, Nadir Weibel
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Explanations for autonomous vehicle (AV) decisions may build trust, however, explanations can contain errors. In a simulated driving study (n = 232), we tested how AV explanation errors, driving context characteristics (perceived harm and driving difficulty), and personal traits (prior trust and expertise) affected a passenger's comfort in relying on an AV, preference for control, confidence in the AV's ability, and explanation satisfaction. Errors negatively affected all outcomes. Surprisingly, despite identical driving, explanation errors reduced ratings of the AV's driving ability. Severity and potential harm amplified the negative impact of errors. Contextual harm and driving difficulty directly impacted outcome ratings and influenced the relationship between errors and outcomes. Prior trust and expertise were positively associated with outcome ratings. Results emphasize the need for accurate, contextually adaptive, and personalized AV explanations to foster trust, reliance, satisfaction, and confidence. We conclude with design, research, and deployment recommendations for trustworthy AV explanation systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T21:31:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3706598.3713088' target='_blank'>doi</a><a href='http://arxiv.org/abs/2409.05731v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.05731v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 Generating Streamlining Constraints with Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Florentina Voboril, Vaidyanathan Peruvemba Ramaswamy, Stefan Szeider
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Streamlining constraints (or streamliners, for short) narrow the search space, enhancing the speed and feasibility of solving complex constraint satisfaction problems. Traditionally, streamliners were crafted manually or generated through systematically combined atomic constraints with high-effort offline testing. Our approach utilizes the creativity of Large Language Models (LLMs) to propose effective streamliners for problems specified in the MiniZinc constraint programming language and integrates feedback to the LLM with quick empirical tests for validation. Evaluated across seven diverse constraint satisfaction problems, our method achieves substantial runtime reductions. We compare the results to obfuscated and disguised variants of the problem to see whether the results depend on LLM memorization. We also analyze whether longer off-line runs improve the quality of streamliners and whether the LLM can propose good combinations of streamliners.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T21:31:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.10268v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.10268v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 "Ownership, Not Just Happy Talk": Co-Designing a Participatory Large
  Language Model for Journalism</h2>
                <div class="authors">
                    <strong>Authors:</strong> Emily Tseng, Meg Young, Marianne Aubin Le Quéré, Aimee Rinehart, Harini Suresh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Journalism has emerged as an essential domain for understanding the uses, limitations, and impacts of large language models (LLMs) in the workplace. News organizations face divergent financial incentives: LLMs already permeate newswork processes within financially constrained organizations, even as ongoing legal challenges assert that AI companies violate their copyright. At stake are key questions about what LLMs are created to do, and by whom: How might a journalist-led LLM work, and what can participatory design illuminate about the present-day challenges about adapting ``one-size-fits-all'' foundation models to a given context of use? In this paper, we undertake a co-design exploration to understand how a participatory approach to LLMs might address opportunities and challenges around AI in journalism. Our 20 interviews with reporters, data journalists, editors, labor organizers, product leads, and executives highlight macro, meso, and micro tensions that designing for this opportunity space must address. From these desiderata, we describe the result of our co-design work: organizational structures and functionality for a journalist-controlled LLM. In closing, we discuss the limitations of commercial foundation models for workplace use, and the methodological implications of applying participatory methods to LLM co-design.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T21:06:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.CL</span><span>cs.CY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17299v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17299v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 Mitigating Hallucinated Translations in Large Language Models with
  Hallucination-focused Preference Optimization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zilu Tang, Rajen Chatterjee, Sarthak Garg
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Machine Translation (MT) is undergoing a paradigm shift, with systems based on fine-tuned large language models (LLM) becoming increasingly competitive with traditional encoder-decoder models trained specifically for translation tasks. However, LLM-based systems are at a higher risk of generating hallucinations, which can severely undermine user's trust and safety. Most prior research on hallucination mitigation focuses on traditional MT models, with solutions that involve post-hoc mitigation - detecting hallucinated translations and re-translating them. While effective, this approach introduces additional complexity in deploying extra tools in production and also increases latency. To address these limitations, we propose a method that intrinsically learns to mitigate hallucinations during the model training phase. Specifically, we introduce a data creation framework to generate hallucination focused preference datasets. Fine-tuning LLMs on these preference datasets reduces the hallucination rate by an average of 96% across five language pairs, while preserving overall translation quality. In a zero-shot setting our approach reduces hallucinations by 89% on an average across three unseen target languages.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T20:58:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17295v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17295v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 Fine-Tuning Open-Source Large Language Models to Improve Their
  Performance on Radiation Oncology Tasks: A Feasibility Study to Investigate
  Their Potential Clinical Applications in Radiation Oncology</h2>
                <div class="authors">
                    <strong>Authors:</strong> Peilong Wang, Zhengliang Liu, Yiwei Li, Jason Holmes, Peng Shu, Lian Zhang, Xiang Li, Quanzheng Li, Brady S. Laughlin, Diego Santos Toesca, Sujay A. Vora, Samir H. Patel, Terence T. Sio, Tianming Liu, Wei Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Background: The radiation oncology clinical practice involves many steps relying on the dynamic interplay of abundant text data. Large language models have displayed remarkable capabilities in processing complex text information. But their direct applications in specific fields like radiation oncology remain underexplored.   Purpose: This study aims to investigate whether fine-tuning LLMs with domain knowledge can improve the performance on Task (1) treatment regimen generation, Task (2) treatment modality selection (photon, proton, electron, or brachytherapy), and Task (3) ICD-10 code prediction in radiation oncology.   Methods: Data for 15,724 patient cases were extracted. Cases where patients had a single diagnostic record, and a clearly identifiable primary treatment plan were selected for preprocessing and manual annotation to have 7,903 cases of the patient diagnosis, treatment plan, treatment modality, and ICD-10 code. Each case was used to construct a pair consisting of patient diagnostics details and an answer (treatment regimen, treatment modality, or ICD-10 code respectively) for the supervised fine-tuning of these three tasks. Open source LLaMA2-7B and Mistral-7B models were utilized for the fine-tuning with the Low-Rank Approximations method. Accuracy and ROUGE-1 score were reported for the fine-tuned models and original models. Clinical evaluation was performed on Task (1) by radiation oncologists, while precision, recall, and F-1 score were evaluated for Task (2) and (3). One-sided Wilcoxon signed-rank tests were used to statistically analyze the results.   Results: Fine-tuned LLMs outperformed original LLMs across all tasks with p-value <= 0.001. Clinical evaluation demonstrated that over 60% of the fine-tuned LLMs-generated treatment regimens were clinically acceptable. Precision, recall, and F1-score showed improved performance of fine-tuned LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T20:37:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.med-ph</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17286v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17286v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 From Natural Language to Extensive-Form Game Representations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shilong Deng, Yongzhao Wang, Rahul Savani
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce a framework for translating game descriptions in natural language into extensive-form representations in game theory, leveraging Large Language Models (LLMs) and in-context learning. Given the varying levels of strategic complexity in games, such as perfect versus imperfect information, directly applying in-context learning would be insufficient. To address this, we introduce a two-stage framework with specialized modules to enhance in-context learning, enabling it to divide and conquer the problem effectively. In the first stage, we tackle the challenge of imperfect information by developing a module that identifies information sets along and the corresponding partial tree structure. With this information, the second stage leverages in-context learning alongside a self-debugging module to produce a complete extensive-form game tree represented using pygambit, the Python API of a recognized game-theoretic analysis tool called Gambit. Using this python representation enables the automation of tasks such as computing Nash equilibria directly from natural language descriptions. We evaluate the performance of the full framework, as well as its individual components, using various LLMs on games with different levels of strategic complexity. Our experimental results show that the framework significantly outperforms baseline models in generating accurate extensive-form games, with each module playing a critical role in its success.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T20:30:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span><span>cs.GT</span><span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17282v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17282v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 On Accelerating Edge AI: Optimizing Resource-Constrained Environments</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jacob Sander, Achraf Cohen, Venkat R. Dasari, Brent Venable, Brian Jalaian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Resource-constrained edge deployments demand AI solutions that balance high performance with stringent compute, memory, and energy limitations. In this survey, we present a comprehensive overview of the primary strategies for accelerating deep learning models under such constraints. First, we examine model compression techniques-pruning, quantization, tensor decomposition, and knowledge distillation-that streamline large models into smaller, faster, and more efficient variants. Next, we explore Neural Architecture Search (NAS), a class of automated methods that discover architectures inherently optimized for particular tasks and hardware budgets. We then discuss compiler and deployment frameworks, such as TVM, TensorRT, and OpenVINO, which provide hardware-tailored optimizations at inference time. By integrating these three pillars into unified pipelines, practitioners can achieve multi-objective goals, including latency reduction, memory savings, and energy efficiency-all while maintaining competitive accuracy. We also highlight emerging frontiers in hierarchical NAS, neurosymbolic approaches, and advanced distillation tailored to large language models, underscoring open challenges like pre-training pruning for massive networks. Our survey offers practical insights, identifies current research gaps, and outlines promising directions for building scalable, platform-independent frameworks to accelerate deep learning models at the edge.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T20:29:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.NE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.15014v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.15014v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 Tailored Truths: Optimizing LLM Persuasion with Personalization and
  Fabricated Statistics</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jasper Timm, Chetan Talele, Jacob Haimes
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are becoming increasingly persuasive, demonstrating the ability to personalize arguments in conversation with humans by leveraging their personal data. This may have serious impacts on the scale and effectiveness of disinformation campaigns. We studied the persuasiveness of LLMs in a debate setting by having humans $(n=33)$ engage with LLM-generated arguments intended to change the human's opinion. We quantified the LLM's effect by measuring human agreement with the debate's hypothesis pre- and post-debate and analyzing both the magnitude of opinion change, as well as the likelihood of an update in the LLM's direction. We compare persuasiveness across established persuasion strategies, including personalized arguments informed by user demographics and personality, appeal to fabricated statistics, and a mixed strategy utilizing both personalized arguments and fabricated statistics. We found that static arguments generated by humans and GPT-4o-mini have comparable persuasive power. However, the LLM outperformed static human-written arguments when leveraging the mixed strategy in an interactive debate setting. This approach had a $\mathbf{51\%}$ chance of persuading participants to modify their initial position, compared to $\mathbf{32\%}$ for the static human-written arguments. Our results highlight the concerning potential for LLMs to enable inexpensive and persuasive large-scale disinformation campaigns.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T20:06:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17273v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17273v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 Rango: Adaptive Retrieval-Augmented Proving for Automated Software
  Verification</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kyle Thompson, Nuno Saavedra, Pedro Carrott, Kevin Fisher, Alex Sanchez-Stern, Yuriy Brun, João F. Ferreira, Sorin Lerner, Emily First
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Formal verification using proof assistants, such as Coq, enables the creation of high-quality software. However, the verification process requires significant expertise and manual effort to write proofs. Recent work has explored automating proof synthesis using machine learning and large language models (LLMs). This work has shown that identifying relevant premises, such as lemmas and definitions, can aid synthesis. We present Rango, a fully automated proof synthesis tool for Coq that automatically identifies relevant premises and also similar proofs from the current project and uses them during synthesis. Rango uses retrieval augmentation at every step of the proof to automatically determine which proofs and premises to include in the context of its fine-tuned LLM. In this way, Rango adapts to the project and to the evolving state of the proof. We create a new dataset, CoqStoq, of 2,226 open-source Coq projects and 196,929 theorems from GitHub, which includes both training data and a curated evaluation benchmark of well-maintained projects. On this benchmark, Rango synthesizes proofs for 32.0% of the theorems, which is 29% more theorems than the prior state-of-the-art tool Tactician. Our evaluation also shows that Rango adding relevant proofs to its context leads to a 47% increase in the number of theorems proven.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T19:56:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span><span>D.2.4; I.2.7; I.2.3</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.14063v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.14063v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 Accelerating Large Language Model Pretraining via LFR Pedagogy: Learn,
  Focus, and Review</h2>
                <div class="authors">
                    <strong>Authors:</strong> Neha Prakriya, Jui-Nan Yen, Cho-Jui Hsieh, Jason Cong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Traditional Large Language Model (LLM) pretraining relies on autoregressive language modeling with randomly sampled data from web-scale datasets. Inspired by human learning techniques like spaced repetition, we hypothesize that random sampling leads to high training costs, lower-quality models, and significant data forgetting. To address these inefficiencies, we propose the Learn-Focus-Review (LFR) paradigm -- a dynamic training approach that adapts to the model's learning progress. LFR tracks the model's learning performance across data blocks (sequences of tokens) and prioritizes revisiting challenging regions of the dataset that are more prone to being forgotten, enabling better retention and more efficient learning. Using the LFR paradigm, we pretrained Llama and GPT models on the SlimPajama and OpenWebText datasets, respectively. These models were evaluated on downstream tasks across various domains, including question answering, problem-solving, commonsense reasoning, language modeling, and translation. Compared to baseline models trained on the full datasets, LFR consistently achieved lower perplexity and higher accuracy, while using only 5%--19% of the training tokens. Furthermore, LFR matched the performance of industry-standard Pythia models with up to 2$\times$ the parameter count, using just 3.2% of the training tokens, demonstrating its effectiveness and efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T19:18:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.06131v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.06131v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 FactCG: Enhancing Fact Checkers with Graph-Based Multi-Hop Data</h2>
                <div class="authors">
                    <strong>Authors:</strong> Deren Lei, Yaxi Li, Siyao Li, Mengya Hu, Rui Xu, Ken Archer, Mingyu Wang, Emily Ching, Alex Deng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Prior research on training grounded factuality classification models to detect hallucinations in large language models (LLMs) has relied on public natural language inference (NLI) data and synthetic data. However, conventional NLI datasets are not well-suited for document-level reasoning, which is critical for detecting LLM hallucinations. Recent approaches to document-level synthetic data generation involve iteratively removing sentences from documents and annotating factuality using LLM-based prompts. While effective, this method is computationally expensive for long documents and limited by the LLM's capabilities. In this work, we analyze the differences between existing synthetic training data used in state-of-the-art models and real LLM output claims. Based on our findings, we propose a novel approach for synthetic data generation, CG2C, that leverages multi-hop reasoning on context graphs extracted from documents. Our fact checker model, FactCG, demonstrates improved performance with more connected reasoning, using the same backbone models. Experiments show it even outperforms GPT-4-o on the LLM-Aggrefact benchmark with much smaller model size.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T18:45:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17144v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17144v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 Abstract Operations Research Modeling Using Natural Language Inputs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junxuan Li, Ryan Wickman, Sahil Bhatnagar, Raj Kumar Maity, Arko Mukherjee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Operations research (OR) uses mathematical models to enhance decision-making, but developing these models requires expert knowledge and can be time-consuming. Automated mathematical programming (AMP) has emerged to simplify this process, but existing systems have limitations. This paper introduces a novel methodology that uses recent advances in Large Language Model (LLM) to create and edit OR solutions from non-expert user queries expressed using Natural Language. This reduces the need for domain expertise and the time to formulate a problem. The paper presents an end-to-end pipeline, named NL2OR, that generates solutions to OR problems from natural language input, and shares experimental results on several important OR problems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T18:40:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07272v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07272v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 ASTRAL: Automated Safety Testing of Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Miriam Ugarte, Pablo Valle, José Antonio Parejo, Sergio Segura, Aitor Arrieta
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have recently gained attention due to their ability to understand and generate sophisticated human-like content. However, ensuring their safety is paramount as they might provide harmful and unsafe responses. Existing LLM testing frameworks address various safety-related concerns (e.g., drugs, terrorism, animal abuse) but often face challenges due to unbalanced and obsolete datasets. In this paper, we present ASTRAL, a tool that automates the generation and execution of test cases (i.e., prompts) for testing the safety of LLMs. First, we introduce a novel black-box coverage criterion to generate balanced and diverse unsafe test inputs across a diverse set of safety categories as well as linguistic writing characteristics (i.e., different style and persuasive writing techniques). Second, we propose an LLM-based approach that leverages Retrieval Augmented Generation (RAG), few-shot prompting strategies and web browsing to generate up-to-date test inputs. Lastly, similar to current LLM test automation techniques, we leverage LLMs as test oracles to distinguish between safe and unsafe test outputs, allowing a fully automated testing approach. We conduct an extensive evaluation on well-known LLMs, revealing the following key findings: i) GPT3.5 outperforms other LLMs when acting as the test oracle, accurately detecting unsafe responses, and even surpassing more recent LLMs (e.g., GPT-4), as well as LLMs that are specifically tailored to detect unsafe LLM outputs (e.g., LlamaGuard); ii) the results confirm that our approach can uncover nearly twice as many unsafe LLM behaviors with the same number of test inputs compared to currently used static datasets; and iii) our black-box coverage criterion combined with web browsing can effectively guide the LLM on generating up-to-date unsafe test inputs, significantly increasing the number of unsafe LLM behaviors.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T18:25:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17132v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17132v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 Scenario Understanding of Traffic Scenes Through Large Visual Language
  Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rivera Esteban, Lübberstedt Jannik, Nico Uhlemann, Markus Lienkamp
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Deep learning models for autonomous driving, encompassing perception, planning, and control, depend on vast datasets to achieve their high performance. However, their generalization often suffers due to domain-specific data distributions, making an effective scene-based categorization of samples necessary to improve their reliability across diverse domains. Manual captioning, though valuable, is both labor-intensive and time-consuming, creating a bottleneck in the data annotation process. Large Visual Language Models (LVLMs) present a compelling solution by automating image analysis and categorization through contextual queries, often without requiring retraining for new categories. In this study, we evaluate the capabilities of LVLMs, including GPT-4 and LLaVA, to understand and classify urban traffic scenes on both an in-house dataset and the BDD100K. We propose a scalable captioning pipeline that integrates state-of-the-art models, enabling a flexible deployment on new datasets. Our analysis, combining quantitative metrics with qualitative insights, demonstrates the effectiveness of LVLMs to understand urban traffic scenarios and highlights their potential as an efficient tool for data-driven advancements in autonomous driving.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T18:23:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17131v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17131v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 Histoires Morales: A French Dataset for Assessing Moral Alignment</h2>
                <div class="authors">
                    <strong>Authors:</strong> Thibaud Leteno, Irina Proskurina, Antoine Gourru, Julien Velcin, Charlotte Laclau, Guillaume Metzler, Christophe Gravier
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Aligning language models with human values is crucial, especially as they become more integrated into everyday life. While models are often adapted to user preferences, it is equally important to ensure they align with moral norms and behaviours in real-world social situations. Despite significant progress in languages like English and Chinese, French has seen little attention in this area, leaving a gap in understanding how LLMs handle moral reasoning in this language. To address this gap, we introduce Histoires Morales, a French dataset derived from Moral Stories, created through translation and subsequently refined with the assistance of native speakers to guarantee grammatical accuracy and adaptation to the French cultural context. We also rely on annotations of the moral values within the dataset to ensure their alignment with French norms. Histoires Morales covers a wide range of social situations, including differences in tipping practices, expressions of honesty in relationships, and responsibilities toward animals. To foster future research, we also conduct preliminary experiments on the alignment of multilingual models on French and English data and the robustness of the alignment. We find that while LLMs are generally aligned with human moral norms by default, they can be easily influenced with user-preference optimization for both moral and immoral data.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T18:07:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17117v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17117v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 Optimizing Large Language Model Training Using FP4 Quantization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruizhe Wang, Yeyun Gong, Xiao Liu, Guoshuai Zhao, Ziyue Yang, Baining Guo, Zhengjun Zha, Peng Cheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The growing computational demands of training large language models (LLMs) necessitate more efficient methods. Quantized training presents a promising solution by enabling low-bit arithmetic operations to reduce these costs. While FP8 precision has demonstrated feasibility, leveraging FP4 remains a challenge due to significant quantization errors and limited representational capacity. This work introduces the first FP4 training framework for LLMs, addressing these challenges with two key innovations: a differentiable quantization estimator for precise weight updates and an outlier clamping and compensation strategy to prevent activation collapse. To ensure stability, the framework integrates a mixed-precision training scheme and vector-wise quantization. Experimental results demonstrate that our FP4 framework achieves accuracy comparable to BF16 and FP8, with minimal degradation, scaling effectively to 13B-parameter LLMs trained on up to 100B tokens. With the emergence of next-generation hardware supporting FP4, our framework sets a foundation for efficient ultra-low precision training.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T18:04:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.17116v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.17116v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 Self-reflecting Large Language Models: A Hegelian Dialectical Approach</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sara Abdali, Can Goksen, Saeed Amizadeh, Kazuhito Koishida
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Investigating NLP through a philosophical lens has recently caught researcher's eyes as it connects computational methods with classical schools of philosophy. This paper introduces a philosophical approach inspired by the Hegelian Dialectic for LLMs' self-reflection, utilizing a self-dialectical approach to emulate internal critiques and then synthesize new ideas by resolving the contradicting points. Moreover, this paper investigates the effect of LLMs' temperature for generation by establishing a dynamic annealing approach, which promotes the creativity in the early stages and gradually refines it by focusing on the nuances, as well as a fixed temperature strategy for generation. Our proposed approach is examined to determine its ability to generate novel ideas from an initial proposition. Additionally, a Multi Agent Majority Voting (MAMV) strategy is leveraged to assess the validity and novelty of the generated ideas, which proves beneficial in the absence of domain experts. Our experiments show promise in generating new ideas and provide a stepping stone for future research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-01-28T18:00:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.HC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.14917v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.14917v2' target='_blank'>pdf</a>
                </div>
            </div>
            
        </div>
    </body>
    </html>
    