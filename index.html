
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>ArXiv Results</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                margin: 0;
                padding: 0;
                display: flex;
                justify-content: center;
                align-items: center;
                min-height: 100vh;
                background-color: #f9f9f9;
            }
            .container {
                width: 17cm;
                padding: 2cm;
                background-color: #fff;
                box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            }
            h1, h2 {
                color: #333;
            }
            .paper {
                border: 1px solid #ccc;
                padding: 1em;
                margin-bottom: 1em;
                border-radius: 8px;
                box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            }
            .authors, .summary, .updated, .tags, .links {
                margin: 0.5em 0;
            }
            .tags span {
                background-color: #f0f0f0;
                border-radius: 4px;
                padding: 0.2em 0.5em;
                margin-right: 0.5em;
            }
            .links a {
                margin-right: 1em;
                text-decoration: none;
                color: #1a73e8;
            }
            .links a:hover {
                text-decoration: underline;
            }
        </style>
    </head>
    <body>
        <div class="container">
    <h1>Keyword: kv cache</h1>

            <div class="paper">
                <h2>#1 Autonomous Navigation in Ice-Covered Waters with Learned Predictions on
  Ship-Ice Interactions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ninghan Zhong, Alessandro Potenza, Stephen L. Smith
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Autonomous navigation in ice-covered waters poses significant challenges due to the frequent lack of viable collision-free trajectories. When complete obstacle avoidance is infeasible, it becomes imperative for the navigation strategy to minimize collisions. Additionally, the dynamic nature of ice, which moves in response to ship maneuvers, complicates the path planning process. To address these challenges, we propose a novel deep learning model to estimate the coarse dynamics of ice movements triggered by ship actions through occupancy estimation. To ensure real-time applicability, we propose a novel approach that caches intermediate prediction results and seamlessly integrates the predictive model into a graph search planner. We evaluate the proposed planner both in simulation and in a physical testbed against existing approaches and show that our planner significantly reduces collisions with ice when compared to the state-of-the-art. Codes and demos of this work are available at https://github.com/IvanIZ/predictive-asv-planner.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T16:22:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11326v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11326v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 Attacking Slicing Network via Side-channel Reinforcement Learning Attack</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wei Shao, Chandra Thapa, Rayne Holland, Sarah Ali Siddiqui, Seyit Camtepe
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Network slicing in 5G and the future 6G networks will enable the creation of multiple virtualized networks on a shared physical infrastructure. This innovative approach enables the provision of tailored networks to accommodate specific business types or industry users, thus delivering more customized and efficient services. However, the shared memory and cache in network slicing introduce security vulnerabilities that have yet to be fully addressed. In this paper, we introduce a reinforcement learning-based side-channel cache attack framework specifically designed for network slicing environments. Unlike traditional cache attack methods, our framework leverages reinforcement learning to dynamically identify and exploit cache locations storing sensitive information, such as authentication keys and user registration data. We assume that one slice network is compromised and demonstrate how the attacker can induce another shared slice to send registration requests, thereby estimating the cache locations of critical data. By formulating the cache timing channel attack as a reinforcement learning-driven guessing game between the attack slice and the victim slice, our model efficiently explores possible actions to pinpoint memory blocks containing sensitive information. Experimental results showcase the superiority of our approach, achieving a success rate of approximately 95\% to 98\% in accurately identifying the storage locations of sensitive data. This high level of accuracy underscores the potential risks in shared network slicing environments and highlights the need for robust security measures to safeguard against such advanced side-channel attacks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T15:07:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11258v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11258v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 Electron-beam-induced adatom-vacancy-complexes in mono- and bilayer
  phosphorene</h2>
                <div class="authors">
                    <strong>Authors:</strong> Carsten Speckmann, Andrea Angeletti, Lukáš Kývala, David Lamprecht, Felix Herterich, Clemens Mangler, Lado Filipovic, Christoph Dellago, Cesare Franchini, Jani Kotakoski
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Phosphorene, a puckered two-dimensional allotrope of phosphorus, has sparked considerable interest in recent years due to its potential especially for optoelectronic applications with its layer-number-dependant direct band gap and strongly bound excitons. However, detailed experimental characterization of its intrinsic defects as well as its defect creation characteristics under electron irradiation are scarce. Here, we report on the creation and stability of a variety of defect configurations under 60 kV electron irradiation in mono- and bilayer phosphorene including the first experimental reports of stable adatom-vacancy-complexes. Displacement cross section measurements in bilayer phosphorene yield a value of 7.7 +- 1.4 barn with an estimated lifetime of adatom-vacancy-complexes of 19.9 +- 0.7 s, while some are stable for up to 68 s under continuous electron irradiation. Surprisingly, ab initio-based simulations indicate that the complexes should readily recombine, even in structures strained by up to 3 %. The presented results will help to improve the understanding of the wide variety of defects in phosphorene, their creation, and their stability, which may enable new pathways for defect engineered phosphorene devices.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T11:54:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.mes-hall</span><span>cond-mat.mtrl-sci</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11102v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11102v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 KVPruner: Structural Pruning for Faster and Memory-Efficient Large
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bo Lv, Quan Zhou, Xuanang Ding, Yan Wang, Zeming Ma
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The bottleneck associated with the key-value(KV) cache presents a significant challenge during the inference processes of large language models. While depth pruning accelerates inference, it requires extensive recovery training, which can take up to two weeks. On the other hand, width pruning retains much of the performance but offers slight speed gains. To tackle these challenges, we propose KVPruner to improve model efficiency while maintaining performance. Our method uses global perplexity-based analysis to determine the importance ratio for each block and provides multiple strategies to prune non-essential KV channels within blocks. Compared to the original model, KVPruner reduces runtime memory usage by 50% and boosts throughput by over 35%. Additionally, our method requires only two hours of LoRA fine-tuning on small datasets to recover most of the performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T10:35:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11057v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11057v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 Skip TLB flushes for reused pages within mmap's</h2>
                <div class="authors">
                    <strong>Authors:</strong> Frederic Schimmelpfennig, André Brinkmann, Hossein Asadi, Reza Salkhordeh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Memory access efficiency is significantly enhanced by caching recent address translations in the CPUs' Translation Lookaside Buffers (TLBs). However, since the operating system is not aware of which core is using a particular mapping, it flushes TLB entries across all cores where the application runs whenever addresses are unmapped, ensuring security and consistency. These TLB flushes, known as TLB shootdowns, are costly and create a performance and scalability bottleneck. A key contributor to TLB shootdowns is memory-mapped I/O, particularly during mmap-munmap cycles and page cache evictions. Often, the same physical pages are reassigned to the same process post-eviction, presenting an opportunity for the operating system to reduce the frequency of TLB shootdowns. We demonstrate, that by slightly extending the mmap function, TLB shootdowns for these "recycled pages" can be avoided.   Therefore we introduce and implement the "fast page recycling" (FPR) feature within the mmap system call. FPR-mmaps maintain security by only triggering TLB shootdowns when a page exits its recycling cycle and is allocated to a different process. To ensure consistency when FPR-mmap pointers are used, we made minor adjustments to virtual memory management to avoid the ABA problem. Unlike previous methods to mitigate shootdown effects, our approach does not require any hardware modifications and operates transparently within the existing Linux virtual memory framework.   Our evaluations across a variety of CPU, memory, and storage setups, including persistent memory and Optane SSDs, demonstrate that FPR delivers notable performance gains, with improvements of up to 28% in real-world applications and 92% in micro-benchmarks. Additionally, we show that TLB shootdowns are a significant source of bottlenecks, previously misattributed to other components of the Linux kernel.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T07:28:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.OS</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10946v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10946v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 Resources on the Move for Smart City: A Disruptive Perspective on the
  Grand Convergence of Sensing, Communications, Computing, Storage, and
  Intelligence</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuguang Fang, Yiqin Deng, Xianhao Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The most commonly seen things on streets in any city are vehicles. However, most of them are used to transport people or goods. What if they also carry resources and capabilities for sensing, communications, computing, storage, and intelligence (SCCSI)? We will have a web of sensors to monitor the city, a network of powerful communicators to transport data around, a grid of computing power to conduct data analytics and machine learning (ML), a network of distributed storage to buffer/cache data/job for optimization, and a set of movable AI/ML toolboxes made available for specialized smart applications. This perspective article presents how to leverage SCCSI-empowered vehicles to design such a service network, simply called SCCSI network, to help build a smart city with a cost-effective and sustainable solution. It showcases how multi-dimensional technologies, namely, sensing, communications, computing, storage, and intelligence, converge to a unifying technology to solve grand challenges for resource demands from emerging large-scale applications. Thus, with SCCSI-empowered vehicles on the ground, over the air, and on the sea, SCCSI network can make resources and capabilities on the move, practically pushing SCCSI services to the edge! We hope this article serves as a spark to stimulate more disruptive thinking to address grand challenges of paramount importance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T04:39:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.09417v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.09417v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 RetrievalAttention: Accelerating Long-Context LLM Inference via Vector
  Retrieval</h2>
                <div class="authors">
                    <strong>Authors:</strong> Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang, Yuqing Yang, Lili Qiu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformer-based large Language Models (LLMs) become increasingly important in various domains. However, the quadratic time complexity of attention operation poses a significant challenge for scaling to longer contexts due to the extremely high inference latency and GPU memory consumption for caching key-value (KV) vectors. This paper proposes RetrievalAttention, a training-free approach to accelerate attention computation. To leverage the dynamic sparse property of attention, RetrievalAttention builds approximate nearest neighbor search (ANNS) indexes upon KV vectors in CPU memory and retrieves the most relevant ones via vector search during generation. Due to the out-of-distribution (OOD) between query vectors and key vectors, off-the-shelf ANNS indexes still need to scan O(N) (usually 30% of all keys) data for accurate retrieval, which fails to exploit the high sparsity. RetrievalAttention first identifies the OOD challenge of ANNS-based attention, and addresses it via an attention-aware vector search algorithm that can adapt to queries and only access 1--3% of data, thus achieving a sub-linear time complexity. RetrievalAttention greatly reduces the inference cost of long-context LLM with much lower GPU memory requirements while maintaining the model accuracy. Especially, RetrievalAttention only needs 16GB GPU memory for serving 128K tokens in LLMs with 8B parameters, which is capable of generating one token in 0.188 seconds on a single NVIDIA RTX4090 (24GB).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-16T17:59:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10516v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10516v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context
  Scenarios</h2>
                <div class="authors">
                    <strong>Authors:</strong> Luning Wang, Shiyao Li, Xuefei Ning, Zhihang Yuan, Shengen Yan, Guohao Dai, Yu Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have been widely adopted to process long-context tasks. However, the large memory overhead of the key-value (KV) cache poses significant challenges in long-context scenarios. Existing training-free KV cache compression methods typically focus on quantization and token pruning, which have compression limits, and excessive sparsity can lead to severe performance degradation. Other methods design new architectures with less KV overhead but require significant training overhead. To address the above two drawbacks, we further explore the redundancy in the channel dimension and apply an architecture-level design with minor training costs. Therefore, we introduce CSKV, a training-efficient Channel Shrinking technique for KV cache compression: (1) We first analyze the singular value distribution of the KV cache, revealing significant redundancy and compression potential along the channel dimension. Based on this observation, we propose using low-rank decomposition for key and value layers and storing the low-dimension features. (2) To preserve model performance, we introduce a bi-branch KV cache, including a window-based full-precision KV cache and a low-precision compressed KV cache. (3) To reduce the training costs, we minimize the layer-wise reconstruction loss for the compressed KV cache instead of retraining the entire LLMs. Extensive experiments show that CSKV can reduce the memory overhead of the KV cache by 80% while maintaining the model's long-context capability. Moreover, we show that our method can be seamlessly combined with quantization to further reduce the memory overhead, achieving a compression ratio of up to 95%.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-16T17:36:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10593v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10593v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 Ejected Particles after Impact Splash on Mars: Electrification</h2>
                <div class="authors">
                    <strong>Authors:</strong> T. Becker, F. C. Onyeagusi, J. Teiser, T. Jardiel, M. Peiteado, O. Munoz, J. Martikainen, J. C. Gomez Martin, J. Merrison, G. Wurm
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Within the RoadMap project we investigated the microphysical aspects of particle collisions during saltation on the Martian surface in laboratory experiments. Following the size distribution of ejected particles, their aerodynamic properties and aggregation status upon ejection, we now focus on the electrification and charge distribution of ejected particles. We analyzed rebound and ejection trajectories of grains in a vacuum setup with a strong electric field of 100 kV/m and deduced particle charges from their acceleration. The ejected particles have sizes of about 10 to 100 microns. They carry charges up to $10^5$ e or charge densities up to $> 10^7$ e/mm$^2$. Within the given size range, we find a small bias towards positive charges.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-16T13:52:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.EP</span><span>astro-ph.IM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10287v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10287v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 Decoupling DNS Update Timing from TTL Values</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yehuda Afek, Ariel Litmanovich
                </div>
                <div class="summary">
                    <strong>Summary:</strong> A relatively simple safety-belt mechanism for improving DNS system availability and efficiency is proposed here. While it may seem ambitious, a careful examination shows it is both feasible and beneficial for the DNS system. The mechanism called "DNS Real-time Update" (DNSRU), a service that facilitates real-time and secure updates of cached domain records in DNS resolvers worldwide, even before the expiration of the corresponding Time To Live (TTL) values. This service allows Internet domain owners to quickly rectify any erroneous global IP address distribution, even if a long TTL value is associated with it. By addressing this critical DNS high availability issue, DNSRU eliminates the need for short TTL values and their associated drawbacks. Therefore, DNSRU DNSRU reduces the traffic load on authoritative servers while enhancing the system's fault tolerance. In this paper we show that our DNSRU design is backward compatible, supports gradual deployment, secure, efficient, and feasible.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-16T11:56:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10207v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10207v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shahriar Rifat, Jonathan Ashdown, Francesco Restuccia
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Test Time Adaptation (TTA) has emerged as a practical solution to mitigate the performance degradation of Deep Neural Networks (DNNs) in the presence of corruption/ noise affecting inputs. Existing approaches in TTA continuously adapt the DNN, leading to excessive resource consumption and performance degradation due to accumulation of error stemming from lack of supervision. In this work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to address such issues. Our key approach is to proactively learn latent representations of some corruption types, each one associated with a sub-network state tailored to correctly classify inputs affected by that corruption. After deployment, DARDA adapts the DNN to previously unseen corruptions in an unsupervised fashion by (i) estimating the latent representation of the ongoing corruption; (ii) selecting the sub-network whose associated corruption is the closest in the latent space to the ongoing corruption; and (iii) adapting DNN state, so that its representation matches the ongoing corruption. This way, DARDA is more resource efficient and can swiftly adapt to new distributions caused by different corruptions without requiring a large variety of input data. Through experiments with two popular mobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA reduces energy consumption and average cache memory footprint respectively by 1.74x and 2.64x with respect to the state of the art, while increasing the performance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-15T14:49:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.09753v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.09753v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 Language-Queried Target Sound Extraction Without Parallel Training Data</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hao Ma, Zhiyuan Peng, Xu Li, Yukai Li, Mingjie Shao, Qiuqiang Kong, Ju Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Language-queried target sound extraction (TSE) aims to extract specific sounds from mixtures based on language queries. Traditional fully-supervised training schemes require extensively annotated parallel audio-text data, which are labor-intensive. We introduce a language-free training scheme, requiring only unlabelled audio clips for TSE model training by utilizing the multi-modal representation alignment nature of the contrastive language-audio pre-trained model (CLAP). In a vanilla language-free training stage, target audio is encoded using the pre-trained CLAP audio encoder to form a condition embedding for the TSE model, while during inference, user language queries are encoded by CLAP text encoder. This straightforward approach faces challenges due to the modality gap between training and inference queries and information leakage from direct exposure to target audio during training. To address this, we propose a retrieval-augmented strategy. Specifically, we create an embedding cache using audio captions generated by a large language model (LLM). During training, target audio embeddings retrieve text embeddings from this cache to use as condition embeddings, ensuring consistent modalities between training and inference and eliminating information leakage. Extensive experiment results show that our retrieval-augmented approach achieves consistent and notable performance improvements over existing state-of-the-art with better generalizability.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-14T10:15:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.AS</span><span>cs.SD</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.09398v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.09398v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 A Compressive Memory-based Retrieval Approach for Event Argument
  Extraction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wanlong Liu, Enqi Zhang, Li Zhou, Dingyi Zeng, Shaohuan Cheng, Chen Zhang, Malu Zhang, Wenyu Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent works have demonstrated the effectiveness of retrieval augmentation in the Event Argument Extraction (EAE) task. However, existing retrieval-based EAE methods have two main limitations: (1) input length constraints and (2) the gap between the retriever and the inference model. These issues limit the diversity and quality of the retrieved information. In this paper, we propose a Compressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the two limitations mentioned above. Our compressive memory, designed as a dynamic matrix that effectively caches retrieved information and supports continuous updates, overcomes the limitations of the input length. Additionally, after pre-loading all candidate demonstrations into the compressive memory, the model further retrieves and filters relevant information from memory based on the input query, bridging the gap between the retriever and the inference model. Extensive experiments show that our method achieves new state-of-the-art performance on three public datasets (RAMS, WikiEvents, ACE05), significantly outperforming existing retrieval-based EAE methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-14T05:51:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.09322v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.09322v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 WarmSwap: Sharing Dependencies for Accelerating Cold Starts in
  Serverless Functions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rui Li, Devesh Tiwari, Gene Cooperman
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This work presents WarmSwap, a novel provider-side cold-start optimization for serverless computing. This optimization reduces cold-start time when booting and loading dependencies at runtime inside a function container. Previous approaches to the optimization of cold starts tend to fall into two categories: optimizing the infrastructure of serverless computing to benefit all serverless functions; or function-specific tuning for individual serverless functions. In contrast, WarmSwap offers a broad middle ground, which optimizes entire categories of serverless functions. WarmSwap eliminates the need to initialize middleware or software dependencies when launching a new serverless container, by migrating a pre-initialized live dependency image to the new function instance. WarmSwap respects the provider's cache constraints, as a single pre-warmed dependency image in the cache is shared among all serverless functions requiring that software dependency image. WarmSwap has been tested on seven representative functions from FunctionBench. The functions are chosen to compare with previous work. In those tests, WarmSwap accelerates cold-start executions for those serverless functions with large dependency requirements by a factor ranging from 1.2 to 2.2.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-13T21:31:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.09202v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.09202v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent
  Interconnects</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anastasiia Ruzhanskaia, Pengcheng Xu, David Cock, Timothy Roscoe
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Conventional wisdom holds that an efficient interface between an OS running on a CPU and a high-bandwidth I/O device should be based on Direct Memory Access (DMA), descriptor rings, and interrupts: DMA offloads transfers from the CPU, descriptor rings provide buffering and queuing, and interrupts facilitate asynchronous interaction between cores and device with a lightweight notification mechanism. In this paper we question this wisdom in the light of modern hardware and workloads, particularly in cloud servers. We argue that the assumptions that led to this model are obsolete, and in many use-cases use of programmed I/O, where the CPU explicitly transfers data and control information to and from a device via loads and stores, actually results in a more efficient system. We quantitatively demonstrate these advantages using three use-cases: fine-grained RPC-style invocation of functions on an accelerator, offloading of operators in a streaming dataflow engine, and a network interface targeting for serverless functions. Moreover, we show that while these advantages are significant over a modern PCIe peripheral bus, a truly cache-coherent interconnect offers significant additional efficiency gains.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-12T15:34:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>cs.OS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.08141v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.08141v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect</h2>
                <div class="authors">
                    <strong>Authors:</strong> Priya Sharma, Alexander V. Balatsky
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present a microscopic calculation of the inverse Faraday effect in metals. We derive a static local magnetic moment induced on the application of high-frequency light, using the Eilenberger formulation of quasiclassical theory. We include the effect of disorder and formulate a theory applicable across the entire temperature range, in the absence of external applied fields. For light-induced electric fields of amplitude $\sim 100 kV/cm$, the induced fields are large, $\sim 0.1 T$ for metallic Nb! The predictions of our theory agree with recent experimental and theoretical results [1]. An extension of this approach to superconductors would open a new route of inducing orbital magnetic field and potentially vortices in superconductors.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-12T10:35:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.supr-con</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1103/PhysRevB.110.094302' target='_blank'>doi</a><a href='http://arxiv.org/abs/2303.01699v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2303.01699v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 Super Monotonic Alignment Search</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junhyeok Lee, Hyeongju Kim
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most popular algorithm in TTS to estimate unknown alignments between text and speech. Since this algorithm needs to search for the most probable alignment with dynamic programming by caching all paths, the time complexity of the algorithm is $O(T \times S)$. The authors of Glow-TTS run this algorithm on CPU, and while they mentioned it is difficult to parallelize, we found that MAS can be parallelized in text-length dimension and CPU execution consumes an inordinate amount of time for inter-device copy. Therefore, we implemented a Triton kernel and PyTorch JIT script to accelerate MAS on GPU without inter-device copy. As a result, Super-MAS Triton kernel is up to 72 times faster in the extreme-length case. The code is available at \url{https://github.com/supertone-inc/super-monotonic-align}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-12T02:13:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.AS</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.07704v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.07704v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 Learning to Compress Contexts for Efficient Knowledge-based Visual
  Question Answering</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weixi Weng, Jieming Zhu, Hao Zhang, Xiaojun Meng, Rui Zhang, Chun Yuan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot performance on visual question answering (VQA). However, when it comes to knowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized domain knowledge to answer such questions and require obtaining necessary information from external knowledge sources. Previous works like Retrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input information, such as image-based textual descriptions and retrieved knowledge, as possible to improve performance, but they all overlook the issue that with the number of input tokens increasing, inference efficiency significantly decreases, which contradicts the demands of practical applications. To address this issue, we propose Retrieval-Augmented MLLM with Compressed Contexts (RACC). RACC learns to compress and aggregate retrieved contexts, from which it generates a compact modulation in the form of Key-Value (KV) cache. This modulation is then used to adapt the downstream frozen MLLM, thereby achieving effective and efficient inference. RACC achieves a state-of-the-art (SOTA) performance of 62.9% on OK-VQA. Moreover, it significantly reduces inference latency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments show RACC's broad applicability. It is compatible with various off-the-shelf MLLMs and can also handle different knowledge sources including textual and multimodal documents.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-11T15:11:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.07331v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.07331v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language
  Models on a Single GPU</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhenyu Ning, Jieru Zhao, Qihao Jin, Wenchao Ding, Minyi Guo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multimodal Large Language Models (MLLMs) are distinguished by their multimodal comprehensive ability and widely used in many real-world applications including GPT-4o, autonomous driving and robotics. Despite their impressive performance, the multimodal inputs always incur long context. The inference under long context requires caching massive Key and Value states (KV cache) of previous tokens, which introduces high latency and excessive memory consumption. Due to this reason, it is challenging to deploy streaming inference of MLLMs on edge devices, which largely constrains the power and usage of MLLMs in real-world applications. In this paper, we introduce Inf-MLLM, an efficient inference framework for MLLMs, which enable streaming inference of MLLM on a single GPU with infinite context. Inf-MLLM is based on our key observation of the attention pattern in both LLMs and MLLMs called "attention saddles". Thanks to the newly discovered attention pattern, Inf-MLLM maintains a size-constrained KV cache by dynamically caching recent tokens and relevant tokens. Furthermore, Inf-MLLM proposes attention bias, a novel approach to enable MLLMs to capture long-term dependency. We show that Inf-MLLM enables multiple LLMs and MLLMs to achieve stable performance over 4M-token long texts and multi-round conversations with 1-hour-long videos on a single GPU. In addition, Inf-MLLM exhibits superior streaming reasoning quality than existing methods such as StreamingLLM and 2x speedup than H2O.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-11T12:44:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CV</span><span>cs.DC</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.09086v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.09086v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma
  Generated THz Pulses</h2>
                <div class="authors">
                    <strong>Authors:</strong> Benjamin Colmey, Rodrigo T. Paulino, David G. Cooke
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Terahertz pulses generated by two-color laser plasmas have reported peak field strengths exceeding MV/cm, and when illuminating metal nanotips the near-field enhancement at the tip apex should result in extremely high bunch charges and electron energies via sub-cycle cold field emission. Here, electron emission from tungsten nanotips driven by THz pulses generated by a long filament air-plasma are reported. Electron energies up to 1.1 keV and bunch charges up to 2x$10^5$ electrons per pulse were detected, well below values expected for peak field calculated via the time averaged Poynting vector. Investigations revealed a failure in the use of the time-averaged Poynting vector when applied to long filament THz pulses, due to spatio-temporal restructuring of the THz pulse in the focus. Accounting for this restructuring significantly reduces the field strength to approximately 160 ~kV/cm, consistent with the observed electron bunch charges, peak energies and their dependence on the tip position in the THz focus. Despite these findings, our results surpass previous THz plasma-driven electron generation by an order of magnitude in both electron energy and bunch charge and a path to increasing these by an additional order of magnitude by modification of the THz optics is proposed.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-11T11:40:23Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.mtrl-sci</span><span>physics.plasm-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.07196v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.07196v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 In-Loop Filtering via Trained Look-Up Tables</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhuoyuan Li, Jiacheng Li, Yao Li, Li Li, Dong Liu, Feng Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In-loop filtering (ILF) is a key technology for removing the artifacts in image/video coding standards. Recently, neural network-based in-loop filtering methods achieve remarkable coding gains beyond the capability of advanced video coding standards, which becomes a powerful coding tool candidate for future video coding standards. However, the utilization of deep neural networks brings heavy time and computational complexity, and high demands of high-performance hardware, which is challenging to apply to the general uses of coding scene. To address this limitation, inspired by explorations in image restoration, we propose an efficient and practical in-loop filtering scheme by adopting the Look-up Table (LUT). We train the DNN of in-loop filtering within a fixed filtering reference range, and cache the output values of the DNN into a LUT via traversing all possible inputs. At testing time in the coding process, the filtered pixel is generated by locating input pixels (to-be-filtered pixel with reference pixels) and interpolating cached filtered pixel values. To further enable the large filtering reference range with the limited storage cost of LUT, we introduce the enhanced indexing mechanism in the filtering process, and clipping/finetuning mechanism in the training. The proposed method is implemented into the Versatile Video Coding (VVC) reference software, VTM-11.0. Experimental results show that the ultrafast, very fast, and fast mode of the proposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39% BD-rate reduction, under the all intra (AI) and random access (RA) configurations. Especially, our method has friendly time and computational complexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel, and only 164-1148 KB storage cost for a single model. Our solution may shed light on the journey of practical neural network-based coding tool evolution.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-11T08:12:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.IV</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.10926v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.10926v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free
  Massive MIMO Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yu Zhang, Shuaifei Chen, Jiayi Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Cell-free massive multiple-input-multiple-output is promising to meet the stringent quality-of-experience (QoE) requirements of railway wireless communications by coordinating many successional access points (APs) to serve the onboard users coherently. A key challenge is how to deliver the desired contents timely due to the radical changing propagation environment caused by the growing train speed. In this paper, we propose to proactively cache the likely-requesting contents at the upcoming APs which perform the coherent transmission to reduce end-to-end delay. A long-term QoE-maximization problem is formulated and two cache placement algorithms are proposed. One is based on heuristic convex optimization (HCO) and the other exploits deep reinforcement learning (DRL) with soft actor-critic (SAC). Compared to the conventional benchmark, numerical results show the advantage of our proposed algorithms on QoE and hit probability. With the advanced DRL model, SAC outperforms HCO on QoE by predicting the user requests accurately.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-11T02:33:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>cs.AI</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2208.12453v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2208.12453v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 With Greater Text Comes Greater Necessity: Inference-Time Training Helps
  Long Text Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Y. Wang, D. Ma, D. Cai
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Long text generation, such as novel writing and discourse-level translation with extremely long contexts, presents significant challenges to current language models. Existing methods mainly focus on extending the model's context window through strategies like length extrapolation. However, these approaches demand substantial hardware resources during the training and/or inference phases. Our proposed method, Temp-Lora, introduces an alternative concept. Instead of relying on the KV cache to store all context information, we embeds this information directly into a temporary Lora module. In the process of long text generation, this module is progressively trained with text generated previously. This approach not only efficiently preserves contextual knowledge but also prevents any permanent alteration to the model's parameters given that the module is discarded post-generation. Extensive experiments on the PG19 language modeling benchmark and the GuoFeng discourse-level translation benchmark validate the effectiveness of Temp-Lora. Our results show that: 1) Temp-Lora substantially enhances generation quality for long text, as indicated by a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3% decrease in PPL along with a 113.2% increase in BLEU score on a subset of GuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text generation methods, and 3) Temp-Lora can greatly reduce computational costs by shortening the context window. For example, we can ensure a moderate improvement in generation quality (a decrease of 3.8% in PPL) while enabling a 51.5% memory usage reduction and a 60.0% decrease in latency for inference.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-11T02:22:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2401.11504v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2401.11504v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online
  Surgical Phase Recognition</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kaixiang Yang, Qiang Li, Zhiwei Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Surgical phase recognition has become a crucial requirement in laparoscopic surgery, enabling various clinical applications like surgical risk forecasting. Current methods typically identify the surgical phase using individual frame-wise embeddings as the fundamental unit for time modeling. However, this approach is overly sensitive to current observations, often resulting in discontinuous and erroneous predictions within a complete surgical phase. In this paper, we propose DACAT, a novel dual-stream model that adaptively learns clip-aware context information to enhance the temporal relationship. In one stream, DACAT pretrains a frame encoder, caching all historical frame-wise features. In the other stream, DACAT fine-tunes a new frame encoder to extract the frame-wise feature at the current moment. Additionally, a max clip-response read-out (Max-R) module is introduced to bridge the two streams by using the current frame-wise feature to adaptively fetch the most relevant past clip from the feature cache. The clip-aware context feature is then encoded via cross-attention between the current frame and its fetched adaptive clip, and further utilized to enhance the time modeling for accurate online surgical phase recognition. The benchmark results on three public datasets, i.e., Cholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed DACAT over existing state-of-the-art methods, with improvements in Jaccard scores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have been released at https://github.com/kk42yy/DACAT.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-10T04:58:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.06217v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.06217v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 Design and Implementation of Online Live Streaming System Using A 3D
  Engine</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aizierjiang Aiersilan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the growing demand for live video streaming, there is an increasing need for low-latency and high-quality transmission, especially with the advent of 5G networks. While 5G offers hardware-level improvements, effective software solutions for minimizing latency remain essential. Current methods, such as multi-channel streaming, fail to address latency issues fundamentally, often only adding new channels without optimizing overall performance. This thesis proposes a novel approach using a 3D engine (e.g., Unity 3D) to stream multi-input video data through a single channel with reduced latency. By leveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D Canvases, and Webcam Textures, the proposed system consolidates video streams from multiple external cameras into a unified, low-latency output. The affiliated project of this thesis demonstrates the implementation of a low-latency multi-channel live video streaming system. It employs the RTSP protocol and examines video encoding techniques, alongside a client-side application based on Unity 3D. The system architecture includes a WebSocket server for persistent connections, an HTTP server for communication, a MySQL database for storage, Redis for caching, and Nginx for load balancing. Each module operates independently, ensuring flexibility and scalability in the system's design. A key innovation of this system is its use of a 3D scene to map multiple video inputs onto a virtual canvas, recorded by an in-engine camera for transmission. This design minimizes redundant data, enabling an efficient and director-guided live streaming network. The thesis concludes by discussing challenges encountered during the project and provides solutions for future improvement.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-10T04:24:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span><span>cs.MM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.06207v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.06207v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering</h2>
                <div class="authors">
                    <strong>Authors:</strong> Benjamin Attal, Dor Verbin, Ben Mildenhall, Peter Hedman, Jonathan T. Barron, Matthew O'Toole, Pratul P. Srinivasan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> State-of-the-art techniques for 3D reconstruction are largely based on volumetric scene representations, which require sampling multiple points to compute the color arriving along a ray. Using these representations for more general inverse rendering -- reconstructing geometry, materials, and lighting from observed images -- is challenging because recursively path-tracing such volumetric representations is expensive. Recent works alleviate this issue through the use of radiance caches: data structures that store the steady-state, infinite-bounce radiance arriving at any point from any direction. However, these solutions rely on approximations that introduce bias into the renderings and, more importantly, into the gradients used for optimization. We present a method that avoids these approximations while remaining computationally efficient. In particular, we leverage two techniques to reduce variance for unbiased estimators of the rendering equation: (1) an occlusion-aware importance sampler for incoming illumination and (2) a fast cache architecture that can be used as a control variate for the radiance from a high-quality, but more expensive, volumetric cache. We show that by removing these biases our approach improves the generality of radiance cache based inverse rendering, as well as increasing quality in the presence of challenging light transport effects such as specular reflections.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-09T17:59:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.GR</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.05867v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.05867v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuntian Deng, Wenting Zhao, Jack Hessel, Xiang Ren, Claire Cardie, Yejin Choi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The increasing availability of real-world conversation data offers exciting opportunities for researchers to study user-chatbot interactions. However, the sheer volume of this data makes manually examining individual conversations impractical. To overcome this challenge, we introduce WildVis, an interactive tool that enables fast, versatile, and large-scale conversation analysis. WildVis provides search and visualization capabilities in the text and embedding spaces based on a list of criteria. To manage million-scale datasets, we implemented optimizations including search index construction, embedding precomputation and compression, and caching to ensure responsive user interactions within seconds. We demonstrate WildVis' utility through three case studies: facilitating chatbot misuse research, visualizing and comparing topic distributions across datasets, and characterizing user-specific conversation patterns. WildVis is open-source and designed to be extendable, supporting additional datasets and customized search and visualization functionalities.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-09T10:04:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.HC</span><span>cs.IR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.03753v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.03753v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 Cooperative Learning-Based Framework for VNF Caching and Placement
  Optimization over Low Earth Orbit Satellite Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Khai Doan, Marios Avgeris, Aris Leivadeas, Ioannis Lambadaris, Wonjae Shin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad range of modern applications, which are typically modeled as Service Function Chains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where each VNF performs a specific task. In this work, we tackle two key challenges in deploying SFCs across an LSN. Firstly, we aim to optimize the long-term system performance by minimizing the average end-to-end SFC execution delay, given that each satellite comes with a pre-installed/cached subset of VNFs. To achieve optimal SFC placement, we formulate an offline Dynamic Programming (DP) equation. To overcome the challenges associated with DP, such as its complexity, the need for probability knowledge, and centralized decision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution. Our MAQL approach addresses convergence issues in the non-stationary LSN environment by enabling satellites to share learning parameters and update their Q-tables based on distinct rules for their selected actions. Secondly, to determine the optimal VNF subsets for satellite caching, we develop a Bayesian Optimization (BO)-based learning mechanism that operates both offline and continuously in the background during runtime. Extensive experiments demonstrate that our MAQL approach achieves near-optimal performance comparable to the DP model and significantly outperforms existing baselines. Moreover, the BO-based approach effectively enhances the request serving rate over time.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-08T08:39:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>cs.SY</span><span>eess.SY</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.05025v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.05025v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 InstInfer: In-Storage Attention Offloading for Cost-Effective
  Long-Context LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiurui Pan, Endian Li, Qiao Li, Shengwen Liang, Yizhou Shan, Ke Zhou, Yingwei Luo, Xiaolin Wang, Jie Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The widespread of Large Language Models (LLMs) marks a significant milestone in generative AI. Nevertheless, the increasing context length and batch size in offline LLM inference escalate the memory requirement of the key-value (KV) cache, which imposes a huge burden on the GPU VRAM, especially for resource-constraint scenarios (e.g., edge computing and personal devices). Several cost-effective solutions leverage host memory or SSDs to reduce storage costs for offline inference scenarios and improve the throughput. Nevertheless, they suffer from significant performance penalties imposed by intensive KV cache accesses due to limited PCIe bandwidth. To address these issues, we propose InstInfer, a novel LLM inference system that offloads the most performance-critical computation (i.e., attention in decoding phase) and data (i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize the enormous KV transfer overheads. InstInfer designs a dedicated flash-aware in-storage attention engine with KV cache management mechanisms to exploit the high internal bandwidths of CSDs instead of being limited by the PCIe bandwidth. The optimized P2P transmission between GPU and CSDs further reduces data migration overheads. Experimental results demonstrate that for a 13B model using an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence inference by up to 11.1$\times$, compared to existing SSD-based solutions such as FlexGen.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-08T06:06:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.04992v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.04992v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 Training-Free Style Consistent Image Synthesis with Condition and Mask
  Guidance in E-Commerce</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guandong Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Generating style-consistent images is a common task in the e-commerce field, and current methods are largely based on diffusion models, which have achieved excellent results. This paper introduces the concept of the QKV (query/key/value) level, referring to modifications in the attention maps (self-attention and cross-attention) when integrating UNet with image conditions. Without disrupting the product's main composition in e-commerce images, we aim to use a train-free method guided by pre-set conditions. This involves using shared KV to enhance similarity in cross-attention and generating mask guidance from the attention map to cleverly direct the generation of style-consistent images. Our method has shown promising results in practical applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-07T07:50:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.04750v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.04750v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 MiniCache: KV Cache Compression in Depth Dimension for Large Language
  Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Akide Liu, Jing Liu, Zizheng Pan, Yefei He, Gholamreza Haffari, Bohan Zhuang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> A critical approach for efficiently deploying computationally demanding large language models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value states of previously generated tokens, significantly reducing the need for repetitive computations and thereby lowering latency in autoregressive generation. However, the size of the KV cache grows linearly with sequence length, posing challenges for applications requiring long context input and extensive sequence generation. In this paper, we present a simple yet effective approach, called MiniCache, to compress the KV cache across layers from a novel depth perspective, significantly reducing the memory footprint for LLM inference. Our approach is based on the observation that KV cache states exhibit high similarity between the adjacent layers in the middle-to-deep portion of LLMs. To facilitate merging, we propose disentangling the states into the magnitude and direction components, interpolating the directions of the state vectors while preserving their lengths unchanged. Furthermore, we introduce a token retention strategy to keep highly distinct state pairs unmerged, thus preserving the information with minimal additional storage overhead. Our MiniCache is training-free and general, complementing existing KV cache compression strategies, such as quantization and sparsity. We conduct a comprehensive evaluation of MiniCache utilizing various models including LLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks, demonstrating its exceptional performance in achieving superior compression ratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit MiniCache achieves a remarkable compression ratio of up to 5.02x, enhances inference throughput by approximately 5x, and reduces the memory footprint by 41% compared to the FP16 full cache baseline, all while maintaining near-lossless performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-07T02:52:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2405.14366v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.14366v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 QET: Enhancing Quantized LLM Parameters and KV cache Compression through
  Element Substitution and Residual Clustering</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yanshu Wang, Wang Li, Zhaoqian Yao, Tong Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The matrix quantization entails representing matrix elements in a more space-efficient form to reduce storage usage, with dequantization restoring the original matrix for use. We formulate the Quantization Error Minimization (QEM) problem as minimizing the distance between a matrix before and after quantization, under the condition that the quantized matrix occupies the same memory space. Matrix quantization is crucial in various applications, including Large Language Models (LLMs) weight quantization, vector databases, KV cache quantization, graph compression, and image compression. Recent advancements in LLMs, such as GPT-4 and BERT, have highlighted the importance of matrix compression due to the large size of parameters and KV cache, which are stored as matrices.   We propose Quantum Entanglement Trees (QET) to address the QEM problem by leveraging the local orderliness of matrix elements, involving iterative element swapping to form a locally ordered matrix. This matrix is then grouped and quantized by columns. To enhance QET, we introduce two optimizations: further quantizing residuals to reduce MSE, and using masking and batch processing to accelerate the algorithm.   Experimental results demonstrate that QET can effectively reduce MSE to 5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K cache, and V cache, respectively. Our contributions include the abstraction of the QEM problem, the design of the QET algorithm, and the proposal of two optimizations to improve accuracy and speed.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-06T08:28:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.03637v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.03637v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 A First Look At Efficient And Secure On-Device LLM Inference Against KV
  Leakage</h2>
                <div class="authors">
                    <strong>Authors:</strong> Huan Yang, Deyu Zhang, Yudong Zhao, Yuanchun Li, Yunxin Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Running LLMs on end devices has garnered significant attention recently due to their advantages in privacy preservation. With the advent of lightweight LLM models and specially designed GPUs, on-device LLM inference has achieved the necessary accuracy and performance metrics. However, we have identified that LLM inference on GPUs can leak privacy-sensitive intermediate information, specifically the KV pairs. An attacker could exploit these KV pairs to reconstruct the entire user conversation, leading to significant vulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE) and Trusted Execution Environments (TEE), are either too computation-intensive or resource-limited. To address these issues, we designed KV-Shield, which operates in two phases. In the initialization phase, it permutes the weight matrices so that all KV pairs are correspondingly permuted. During the runtime phase, the attention vector is inversely permuted to ensure the correctness of the layer output. All permutation-related operations are executed within the TEE, ensuring that insecure GPUs cannot access the original KV pairs, thus preventing conversation reconstruction. Finally, we theoretically analyze the correctness of KV-Shield, along with its advantages and overhead.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-06T06:16:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.04040v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.04040v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 Potential and Limitation of High-Frequency Cores and Caches</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kunal Pai, Anusheel Nand, Jason Lowe-Power
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper explores the potential of cryogenic semiconductor computing and superconductor electronics as promising alternatives to traditional semiconductor devices. As semiconductor devices face challenges such as increased leakage currents and reduced performance at higher temperatures, these novel technologies offer high performance and low power computation. Conventional semiconductor electronics operating at cryogenic temperatures (below -150{\deg}C or 123.15 K) can benefit from reduced leakage currents and improved electron mobility. On the other hand, superconductor electronics, operating below 10 K, allow electrons to flow without resistance, offering the potential for ultra-low-power, high-speed computation. This study presents a comprehensive performance modeling and analysis of these technologies and provides insights into their potential benefits and limitations. We implement models of in-order and out-of-order cores operating at high clock frequencies associated with superconductor electronics and cryogenic semiconductor computing in gem5. We evaluate the performance of these components using workloads representative of real-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the potential speedups achievable by these components and the limitations posed by cache bandwidth. This work provides valuable insights into the performance implications and design trade-offs associated with cryogenic and superconductor technologies, laying the foundation for future research in this field using gem5.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-05T20:21:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.03308v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.03308v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 Libra: Architectural Support For Principled, Secure And Efficient
  Balanced Execution On High-End Processors (Extended Version)</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hans Winderix, Marton Bognar, Lesly-Ann Daniel, Frank Piessens
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Control-flow leakage (CFL) attacks enable an attacker to expose control-flow decisions of a victim program via side-channel observations. Linearization (i.e., elimination) of secret-dependent control flow is the main countermeasure against these attacks, yet it comes at a non-negligible cost. Conversely, balancing secret-dependent branches often incurs a smaller overhead, but is notoriously insecure on high-end processors. Hence, linearization has been widely believed to be the only effective countermeasure against CFL attacks. In this paper, we challenge this belief and investigate an unexplored alternative: how to securely balance secret-dependent branches on higher-end processors?   We propose Libra, a generic and principled hardware-software codesign to efficiently address CFL on high-end processors. We perform a systematic classification of hardware primitives leaking control flow from the literature, and provide guidelines to handle them with our design. Importantly, Libra enables secure control-flow balancing without the need to disable performance-critical hardware such as the instruction cache and the prefetcher. We formalize the semantics of Libra and propose a code transformation algorithm for securing programs, which we prove correct and secure. Finally, we implement and evaluate Libra on an out-of-order RISC-V processor, showing performance overhead on par with insecure balanced code, and outperforming state-of-the-art linearized code by 19.3%.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-05T17:56:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3658644.3690319' target='_blank'>doi</a><a href='http://arxiv.org/abs/2409.03743v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.03743v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 Enabling Practical and Privacy-Preserving Image Processing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chao Wang, Shubing Yang, Xiaoyan Sun, Jun Dai, Dongfang Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Fully Homomorphic Encryption (FHE) enables computations on encrypted data, preserving confidentiality without the need for decryption. However, FHE is often hindered by significant performance overhead, particularly for high-precision and complex data like images. Due to serious efficiency issues, traditional FHE methods often encrypt images by monolithic data blocks (such as pixel rows), instead of pixels. However, this strategy compromises the advantages of homomorphic operations and disables pixel-level image processing. In this study, we address these challenges by proposing and implementing a pixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS scheme. To enhance computational efficiency, we introduce three novel caching mechanisms to pre-encrypt radix values or frequently occurring pixel values, substantially reducing redundant encryption operations. Extensive experiments demonstrate that our approach achieves up to a 19-fold improvement in encryption speed compared to the original CKKS, while maintaining high image quality. Additionally, real-world image applications such as mean filtering, brightness enhancement, image matching and watermarking are tested based on FHE, showcasing up to a 91.53% speed improvement. We also proved that our method is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure, providing strong encryption security. These results underscore the practicality and efficiency of iCHEETAH, marking a significant advancement in privacy-preserving image processing at scale.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-05T14:22:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>C.2.0; K.6.5</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.03568v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.03568v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 SELCC: Coherent Caching over Compute-Limited Disaggregated Memory</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruihong Wang, Jianguo Wang, Walid G. Aref
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Disaggregating memory from compute offers the opportunity to better utilize stranded memory in data centers. It is important to cache data in the compute nodes and maintain cache coherence across multiple compute nodes to save on round-trip communication cost between the disaggregated memory and the compute nodes. However, the limited computing power on the disaggregated memory servers makes it challenging to maintain cache coherence among multiple compute-side caches over disaggregated shared memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache Coherence protocol that maintains cache coherence without imposing any computational burden on the remote memory side. SELCC builds on a one-sided shared-exclusive latch protocol by introducing lazy latch release and invalidation messages among the compute nodes so that it can guarantee both data access atomicity and cache coherence. SELCC minimizes communication round-trips by embedding the current cache copy holder IDs into RDMA latch words and prioritizes local concurrency control over global concurrency control. We instantiate the SELCC protocol onto compute-sided cache, forming an abstraction layer over disaggregated memory. This abstraction layer provides main-memory-like APIs to upper-level applications, and thus enabling existing data structures and algorithms to function over disaggregated memory with minimal code change. To demonstrate the usability of SELCC, we implement a B-tree and three transaction concurrency control algorithms over SELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves better performance compared to RPC-based cache-coherence protocols. Additionally, YCSB and TPC-C benchmarks indicate that applications over SELCC can achieve comparable or superior performance against competitors over disaggregated memory.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-05T01:12:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.DC</span><span>cs.ET</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.02088v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.02088v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in
  Fine-tuning LLMs for Simultaneous Translation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Matthew Raffel, Victor Agostinelli, Lizhong Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have achieved state-of-the-art performance in various language processing tasks, motivating their adoption in simultaneous translation. Current fine-tuning methods to adapt LLMs for simultaneous translation focus on prompting optimization strategies using either data augmentation or prompt structure modifications. However, these methods suffer from several issues, such as unnecessarily expanded training sets, computational inefficiency from dumping the key and value cache, increased prompt sizes, or restriction to a single decision policy. To eliminate these issues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs for simultaneous translation. It utilizes a novel attention mask approach that models simultaneous translation during fine-tuning by masking attention for a desired decision policy. Applying the proposed SimulMask on a Falcon LLM for the IWSLT 2017 dataset, we have observed a significant translation quality improvement compared to state-of-the-art prompting optimization strategies on five language pairs while reducing the computational cost.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-05T01:06:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2405.10443v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.10443v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 SparQ Attention: Bandwidth-Efficient LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, Douglas Orr
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The computational difficulties of large language model (LLM) inference remain a significant obstacle to their widespread deployment. The need for many applications to support long input sequences and process them in large batches typically causes token-generation to be bottlenecked by data transfer. For this reason, we introduce SparQ Attention, a technique for increasing the inference throughput of LLMs by utilising memory bandwidth more efficiently within the attention layers, through selective fetching of the cached history. Our proposed technique can be applied directly to off-the-shelf LLMs during inference, without requiring any modification to the pre-training setup or additional fine-tuning. We show that SparQ Attention brings up to 8x savings in attention data transfers without substantial drops in accuracy, by evaluating Llama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-04T10:04:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2312.04985v6' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2312.04985v6' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 A brown dwarf orbiting around the planetary-nebula central binary KV Vel</h2>
                <div class="authors">
                    <strong>Authors:</strong> S. -B. Qian, L. -Y. Zhu, F. -X. Li, L. -J. Li, Z. -T. Han, J. -J. He, L. Zang, L. -F. Chang, Q. -B. Sun, M. -Y. Li, H. -T. Zhang, F. -Z. Yan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary containing a very hot subdwarf primary (77000 K) and a cool low-mass secondary star (3400 K) that is located at the center of the planetary nebula DS 1. The changes in the orbital period of the close binary were analyzed based on 262 new times of light maximum together with those compiled from the literature. It is discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic period variation with a period of 29.55 years. The explanation by the solar-type magnetic activity cycles of the cool component is ruled out because the required energies are much larger than the total radiant energy of this component in a whole cycle. Therefore, the cyclic variation was plausibly explained as the light-travel time effect via the presence of a tertiary component, which is supported by the periodic changes of the O-C curve and the rather symmetric and stable light curves obtained by TESS. The mass of the tertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third body is coplanar with the central binary (i.e., i' = 62.5{\deg}), the mass of the tertiary component is computed as M_3 ~ 0.068 M\sun, and thus it would be below the stable hydrogen-burning limit and is a brown dwarf. The orbital separation is shorter than 9.35 astronomical units (AU). KV Vel together with its surrounding planetary nebula and the brown-dwarf companion may be formed through the common-envelope evolution after the primary filled its Roche lobe during the early asymptotic giant branch stage.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-04T07:13:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.SR</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.3847/1538-4357/ad631a' target='_blank'>doi</a><a href='http://arxiv.org/abs/2409.02480v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.02480v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 Contemporary Model Compression on Large Language Models Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dong Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have revolutionized natural language processing by achieving state-of-the-art results across a variety of tasks. However, the computational demands of LLM inference, including high memory consumption and slow processing speeds, pose significant challenges for real-world applications, particularly on resource-constrained devices. Efficient inference is crucial for scaling the deployment of LLMs to a broader range of platforms, including mobile and edge devices.   This survey explores contemporary techniques in model compression that address these challenges by reducing the size and computational requirements of LLMs while maintaining their performance. We focus on model-level compression methods, including quantization, knowledge distillation, and pruning, as well as system-level optimizations like KV cache efficient design. Each of these methodologies offers a unique approach to optimizing LLMs, from reducing numerical precision to transferring knowledge between models and structurally simplifying neural networks. Additionally, we discuss emerging trends in system-level design that further enhance the efficiency of LLM inference. This survey aims to provide a comprehensive overview of current advancements in model compression and their potential to make LLMs more accessible and practical for diverse applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-03T15:35:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.01990v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.01990v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 A Fresh Take on Stale Embeddings: Improving Dense Retriever Training
  with Corrector Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nicholas Monath, Will Grathwohl, Michael Boratko, Rob Fergus, Andrew McCallum, Manzil Zaheer
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In dense retrieval, deep encoders provide embeddings for both inputs and targets, and the softmax function is used to parameterize a distribution over a large number of candidate targets (e.g., textual passages for information retrieval). Significant challenges arise in training such encoders in the increasingly prevalent scenario of (1) a large number of targets, (2) a computationally expensive target encoder model, (3) cached target embeddings that are out-of-date due to ongoing training of target encoder parameters. This paper presents a simple and highly scalable response to these challenges by training a small parametric corrector network that adjusts stale cached target embeddings, enabling an accurate softmax approximation and thereby sampling of up-to-date high scoring "hard negatives." We theoretically investigate the generalization properties of our proposed target corrector, relating the complexity of the network, staleness of cached representations, and the amount of training data. We present experimental results on large benchmark dense retrieval datasets as well as on QA with retrieval augmented language models. Our approach matches state-of-the-art results even when no target embedding updates are made during training beyond an initial cache from the unsupervised pre-trained model, providing a 4-80x reduction in re-embedding computational cost.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-03T13:29:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.01890v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.01890v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 Reward Augmentation in Reinforcement Learning for Testing Distributed
  Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Andrea Borgarelli, Constantin Enea, Rupak Majumdar, Srinidhi Nagendra
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Bugs in popular distributed protocol implementations have been the source of many downtimes in popular internet services. We describe a randomized testing approach for distributed protocol implementations based on reinforcement learning. Since the natural reward structure is very sparse, the key to successful exploration in reinforcement learning is reward augmentation. We show two different techniques that build on one another. First, we provide a decaying exploration bonus based on the discovery of new states -- the reward decays as the same state is visited multiple times. The exploration bonus captures the intuition from coverage-guided fuzzing of prioritizing new coverage points; in contrast to other schemes, we show that taking the maximum of the bonus and the Q-value leads to more effective exploration. Second, we provide waypoints to the algorithm as a sequence of predicates that capture interesting semantic scenarios. Waypoints exploit designer insight about the protocol and guide the exploration to ``interesting'' parts of the state space. Our reward structure ensures that new episodes can reliably get to deep interesting states even without execution caching. We have implemented our algorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and RSL) shows that our algorithm can significantly outperform baseline approaches in terms of coverage and bug finding.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-02T15:07:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.DC</span><span>cs.LG</span><span>cs.PL</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3689779' target='_blank'>doi</a><a href='http://arxiv.org/abs/2409.02137v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.02137v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 Learning in Hybrid Active Inference Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Poppy Collis, Ryan Singh, Paul F Kinghorn, Christopher L Buckley
                </div>
                <div class="summary">
                    <strong>Summary:</strong> An open problem in artificial intelligence is how systems can flexibly learn discrete abstractions that are useful for solving inherently continuous problems. Previous work in computational neuroscience has considered this functional integration of discrete and continuous variables during decision-making under the formalism of active inference (Parr, Friston & de Vries, 2017; Parr & Friston, 2018). However, their focus is on the expressive physical implementation of categorical decisions and the hierarchical mixed generative model is assumed to be known. As a consequence, it is unclear how this framework might be extended to learning. We therefore present a novel hierarchical hybrid active inference agent in which a high-level discrete active inference planner sits above a low-level continuous active inference controller. We make use of recent work in recurrent switching linear dynamical systems (rSLDS) which implement end-to-end learning of meaningful discrete representations via the piecewise linear decomposition of complex continuous dynamics (Linderman et al., 2016). The representations learned by the rSLDS inform the structure of the hybrid decision-making agent and allow us to (1) specify temporally-abstracted sub-goals in a method reminiscent of the options framework, (2) lift the exploration into discrete space allowing us to exploit information-theoretic exploration bonuses and (3) `cache' the approximate solutions to low-level problems in the discrete planner. We apply our model to the sparse Continuous Mountain Car task, demonstrating fast system identification via enhanced exploration and successful planning through the delineation of abstract sub-goals.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-02T08:41:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.01066v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.01066v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 Throughput Optimization in Cache-aided Networks: An Opportunistic
  Probing and Scheduling Approach</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhou Zhang, Saman Atapattu, Yizhu Wang, Marco Di Renzo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper addresses the challenges of throughput optimization in wireless cache-aided cooperative networks. We propose an opportunistic cooperative probing and scheduling strategy for efficient content delivery. The strategy involves the base station probing the relaying channels and cache states of multiple cooperative nodes, thereby enabling opportunistic user scheduling for content delivery. Leveraging the theory of Sequentially Planned Decision (SPD) optimization, we dynamically formulate decisions on cooperative probing and stopping time. Our proposed Reward Expected Thresholds (RET)-based strategy optimizes opportunistic probing and scheduling. This approach significantly enhances system throughput by exploiting gains from local caching, cooperative transmission and time diversity. Simulations confirm the effectiveness and practicality of the proposed Media Access Control (MAC) strategy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-02T02:36:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.00905v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.00905v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 Rapid GPU-Based Pangenome Graph Layout</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiajie Li, Jan-Niklas Schmelzle, Yixiao Du, Simon Heumos, Andrea Guarracino, Giulia Guidi, Pjotr Prins, Erik Garrison, Zhiru Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Computational Pangenomics is an emerging field that studies genetic variation using a graph structure encompassing multiple genomes. Visualizing pangenome graphs is vital for understanding genome diversity. Yet, handling large graphs can be challenging due to the high computational demands of the graph layout process.   In this work, we conduct a thorough performance characterization of a state-of-the-art pangenome graph layout algorithm, revealing significant data-level parallelism, which makes GPUs a promising option for compute acceleration. However, irregular data access and the algorithm's memory-bound nature present significant hurdles. To overcome these challenges, we develop a solution implementing three key optimizations: a cache-friendly data layout, coalesced random states, and warp merging. Additionally, we propose a quantitative metric for scalable evaluation of pangenome layout quality.   Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution achieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline without layout quality loss, reducing execution time from hours to minutes.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-02T00:05:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.CE</span><span>cs.DS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.00876v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.00876v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 Towards 3D AI Hardware: Fine-Grain Hardware Characterization of 3D
  Stacks for Heterogeneous System Integration & AI Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Eren Kurshan, Paul Franzon
                </div>
                <div class="summary">
                    <strong>Summary:</strong> 3D integration offers key advantages in improving system performance and efficiency for the End-of-Scaling era. It enables the incorporation of heterogeneous system components and disparate technologies, eliminates off-chip communication constraints, reduces on-chip latency and total power dissipation. Moreover, AIs demand for increased computational power, larger GPU cache capacity, energy efficiency and low power custom AI hardware integration all serve as drivers for 3D integration. Although 3D advantages such as enhanced interconnectivity and increased performance have been demonstrated through numerous technology sites, heterogeneous 3D system design raises numerous unanswered questions. Among the primary challenges are the temperature and lifetime reliability issues caused by the complex interaction patterns among system components. Such interactions are harder to model with current modeling tools and require detailed hardware characterization. This study presents the latest drivers for 3D integration and the resulting need for hardware emulation frameworks. It then presents a design to profile power, temperature, noise, inter-layer bandwidth and lifetime reliability characterization that can emulate a wide range of stacking alternatives. This framework allows for controlling activity levels at the macro-level, along with customized sensor infrastructure to characterize heat propagation, inter-layer noise, power delivery, reliability and inter-connectivity as well as the interactions among critical design objectives.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-31T15:45:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.ET</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10539v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10539v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,
  Communication and Computing Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wanming Hao, Xue Wu, Xingwang Li, Gangcan Sun, Qingqing Wu, Liang Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper, we investigate an intelligent reflecting surface (IRS) assisted full-duplex (FD) integrated sensing, communication and computing system. Specifically, an FD base station (BS) provides service for uplink and downlink transmission, and a local cache is connected to the BS through a backhaul link to store data. Meanwhile, active sensing elements are deployed on the IRS to receive target echo signals. On this basis, in order to evaluate the overall performance of the system under consideration, we propose a system utility maximization problem while ensuring the sensing quality, expressed as the difference between the sum of communication throughput, total computation bits (offloading bits and local computation bits) and the total backhaul cost for content delivery. This makes the problem difficult to solve due to the highly non-convex coupling of the optimization variables. To effectively solve this problem, we first design the most effective caching strategy. Then, we develop an algorithm based on weighted minimum mean square error, alternative direction method of multipliers, majorization-minimization framework, semi-definite relaxation techniques, and several complex transformations to jointly solve the optimization variables. Finally, simulation results are provided to verify the utility performance of the proposed algorithm and demonstrate the advantages of the proposed scheme compared with the baseline scheme.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-31T06:33:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>eess.SP</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.00364v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.00364v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 >3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction
  Termination Extension and Sub-1V Turn-on</h2>
                <div class="authors">
                    <strong>Authors:</strong> Advait Gilankar, Abishek Katta, Nabasindhu Das, Nidhin Kurian Kalarickal
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction diodes (HJDs) with a 2-step space-modulated junction termination extension. Distinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown voltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a forward current density (IF) of 1 A-cm-2. The measured devices exhibit excellent turn-on characteristics achieving 100 A-cm-2 current density at a forward bias of 1.5V along with a low differential specific on-resistance (Ron,sp) of 4.4 m{\Omega}-cm2. The SM-JTE was realized using concentric NiO rings with varying widths and spacing that approximates a gradual reduction in JTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and is among the best reported for devices with a sub-1V turn-on. The fabricated devices also displayed minimal change in forward I-V characteristics post reverse bias stress of 3 kV applied during breakdown voltage testing.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-31T04:20:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.app-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.00344v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.00344v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume
  Visualization through Functional Approximation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jianxin Sun, David Lenz, Hongfeng Yu, Tom Peterka
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Functional approximation as a high-order continuous representation provides a more accurate value and gradient query compared to the traditional discrete volume representation. Volume visualization directly rendered from functional approximation generates high-quality rendering results without high-order artifacts caused by trilinear interpolations. However, querying an encoded functional approximation is computationally expensive, especially when the input dataset is large, making functional approximation impractical for interactive visualization. In this paper, we proposed a novel functional approximation multi-resolution representation, Adaptive-FAM, which is lightweight and fast to query. We also design a GPU-accelerated out-of-core multi-resolution volume visualization framework that directly utilizes the Adaptive-FAM representation to generate high-quality rendering with interactive responsiveness. Our method can not only dramatically decrease the caching time, one of the main contributors to input latency, but also effectively improve the cache hit rate through prefetching. Our approach significantly outperforms the traditional function approximation method in terms of input latency while maintaining comparable rendering quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-30T18:04:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.GR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.00184v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.00184v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 Modelling the High-Voltage Grid Using Open Data for Europe and Beyond</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bobby Xiong, Davide Fioriti, Fabian Neumann, Iegor Riepin, Tom Brown
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper provides the background, methodology and validation for constructing a representation of the European high-voltage grid, including and above 200 kV, based on public data provided by OpenStreetMap. The model-independent grid dataset is published under the Open Data Commons Open Database (ODbL 1.0) licence and can be used for large-scale electricity as well as energy system modelling. The dataset and workflow are provided as part of PyPSA-Eur -- an open-source, sector-coupled optimisation model of the European energy system. By integrating with the codebase for initiatives such as PyPSA-Earth, the value of open and maintainable high-voltage grid data extends to the global context. By accessing the latest data through the the Overpass turbo API, the dataset can be easily reconstructed and updated within minutes. To assess the data quality, this paper further compares the dataset with official statistics and representative model runs using PyPSA-Eur based on different electricity grid representations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-30T10:26:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.soc-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.17178v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.17178v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 MemLong: Memory-Augmented Retrieval for Long Text Modeling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weijie Liu, Zecheng Tang, Juntao Li, Kehai Chen, Min Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in Large Language Models (LLMs) have yielded remarkable success across diverse fields. However, handling long contexts remains a significant challenge for LLMs due to the quadratic time and space complexity of attention mechanisms and the growing memory consumption of the key-value cache during generation. This work introduces MemLong: Memory-Augmented Retrieval for Long Text Generation, a method designed to enhance the capabilities of long-context language modeling by utilizing an external retriever for historical information retrieval. MemLong combines a non-differentiable ``ret-mem'' module with a partially trainable decoder-only language model and introduces a fine-grained, controllable retrieval attention mechanism that leverages semantic-level relevant chunks. Comprehensive evaluations on multiple long-context language modeling benchmarks demonstrate that MemLong consistently outperforms other state-of-the-art LLMs. More importantly, MemLong can extend the context length on a single 3090 GPU from 4k up to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-30T02:01:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.16967v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.16967v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hesameddin Mokhtarzadeh, Mohammed S. Al-Abiad, Md Jahangir Hossain, Julian Cheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio heads (eRRHs) are connected to a macro base station (MBS) through fronthaul links. Deploying a massive number of eRRHs is not always feasible due to site constraints and the cost of fronthaul links. This paper introduces an innovative concept of using smart helpers (SHs) in F-RANs. These SHs do not require fronthaul links and listen to the nearby eRRHs' communications. Then, they smartly select and cache popular content. This capability enables SHs to serve users with frequent on-demand service requests potentially. As such, network operators have the flexibility to easily deploy SHs in various scenarios, such as dense urban areas and temporary public events, to expand their F-RANs and improve the quality of service (QoS). To study the performance of the proposed SH-aided F-RAN, we formulate an optimization problem of minimizing the average transmission delay that jointly optimizes cache resources and user scheduling. To tackle the formulated problem, we develop an innovative multi-stage algorithm that uses a reinforcement learning (RL) framework. Various performance measures, e.g., the average transmission delay, fronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated numerically and compared with those of traditional F-RANs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-29T17:43:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2309.07975v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2309.07975v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths
  Vision Computation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shiwei Wu, Joya Chen, Kevin Qinghong Lin, Qimeng Wang, Yan Gao, Qianli Xu, Tong Xu, Yao Hu, Enhong Chen, Mike Zheng Shou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is that while increasing the number of vision tokens generally enhances visual understanding, it also significantly raises memory and computational costs, especially in long-term, dense video frame streaming scenarios. Although learnable approaches like Q-Former and Perceiver Resampler have been developed to reduce the vision token burden, they overlook the context causally modeled by LLMs (i.e., key-value cache), potentially leading to missed visual cues when addressing user queries. In this paper, we introduce a novel approach to reduce vision compute by leveraging redundant vision tokens "skipping layers" rather than decreasing the number of vision tokens. Our method, VideoLLM-MoD, is inspired by mixture-of-depths LLMs and addresses the challenge of numerous vision tokens in long-term or streaming video. Specifically, for each transformer layer, we learn to skip the computation for a high proportion (e.g., 80\%) of vision tokens, passing them directly to the next layer. This approach significantly enhances model efficiency, achieving approximately \textasciitilde42\% time and \textasciitilde30\% memory savings for the entire training. Moreover, our method reduces the computation in the context and avoid decreasing the vision tokens, thus preserving or even improving performance compared to the vanilla model. We conduct extensive experiments to demonstrate the effectiveness of VideoLLM-MoD, showing its state-of-the-art results on multiple benchmarks, including narration, forecasting, and summarization tasks in COIN, Ego4D, and Ego-Exo4D datasets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-29T17:21:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.16730v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.16730v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless
  Generative Inference of LLM</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, Tuo Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Key-value (KV) caching has become the de-facto to accelerate generation speed for large language models (LLMs) inference. However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing all entries uniformly. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient KV cache compression framework that achieves near-lossless high-ratio compression. GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision. It then employs a low rank matrix to approximate the quantization error, and a sparse matrix to remedy individual errors from outlier entries. By adeptly integrating three techniques, GEAR is able to fully exploit their synergistic potentials. Our experiments demonstrate that compared to alternatives, GEAR achieves near-lossless 4-bit KV cache compression with up to 2.38x throughput improvement, while reducing peak-memory size up to 2.29x. Our code is publicly available at https://github.com/HaoKang-Timmy/GEAR.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-29T16:48:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2403.05527v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2403.05527v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through
  Targeted Instruction Hardening</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yiming Zhu, Wenchao Huang, Yan Xiong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Several software mitigations have been proposed to defend against Spectre vulnerabilities. However, these countermeasures often suffer from high performance overhead, largely due to unnecessary protections. We propose LightSLH, designed to mitigate this overhead by hardening instructions only when they are under threat from Spectre vulnerabilities. LightSLH leverages program analysis techniques based on abstract interpretation to identify all instructions that could potentially lead to Spectre vulnerabilities and provides provable protection. To enhance analysis efficiency and precision, LightSLH employs novel taint and value domains. The taint domain enables bit-level taint tracking, while the value domain allows LightSLH to analyze complex program structures such as pointers and structures. Furthermore, LightSLH uses a two-stage abstract interpretation approach to circumvent potential analysis paralysis issues.   We demonstrate the security guarantees of LightSLH and evaluate its performance on cryptographic algorithm implementations from OpenSSL. LightSLH significantly reduces the overhead associated with speculative-load-hardening techniques. Our results show that LightSLH introduces no protection and thus no overhead on 4 out of the 7 studied algorithms, which contrasts with existing countermeasures that introduce additional overhead due to unnecessary hardening. Additionally, LightSLH performs, for the first time, a rigorous analysis of the security guarantees of RSA against Spectre v1, highlighting that the memory access patterns generated by the scatter-gather algorithm depend on secrets, even for observers at the cache line granularity, necessitating protection for such accesses.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-29T02:31:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.16220v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.16220v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 On the Impact of ISA Extension on Energy Consumption of I-Cache in
  Extensible Processors</h2>
                <div class="authors">
                    <strong>Authors:</strong> Noushin Behboudi, Mehdi Kamal, Ali Afzali-Kusha
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As is widely known, the computational speed and power consumption are two critical parameters in microprocessor design. A solution for these issues is the application specific instruction set processor (ASIP) methodology, which can improve speed and reduce power consumption of the general purpose processor (GPP) technique. In ASIP, changing the instruction set architecture (ISA) of the processor will lead to alter the number and the mean time of accesses to the cache memory. This issue has a direct impact on the processor energy consumption. In this work, we study the impacts of extended ISA on the energy consumption of the extended ISA processor. Also, we demonstrate the extended ISA let the designer to reduce the cache size in order to minimize the energy consumption while meeting performance constraint.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-28T17:28:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.08286v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.08286v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 RIP Linked List</h2>
                <div class="authors">
                    <strong>Authors:</strong> Benoît Sonntag, Dominique Colnet
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Linked lists have long served as a valuable teaching tool in programming. However, the question arises: Are they truly practical for everyday program use? In most cases, it appears that array-based data structures offer distinct advantages, particularly in terms of memory efficiency and,more importantly, execution speed. While it's relatively straightforward to calculate the complexity of operations, gauging actual execution efficiency remains a challenge. This paper addresses this question by introducing a new benchmark. Our study compares various linked list implementations with several array-based alternatives. We also demonstrate the ease of incorporating memory caching for linked lists, enhancing their performance. Additionally, we introduce a new array-based data structure designed to excel in a wide range of operations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-28T08:41:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2306.06942v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2306.06942v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 Efficient LLM Training and Serving with Heterogeneous Context Sharding
  among Attention Heads</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xihui Lin, Yunan Zhang, Suyu Ge, Barun Patra, Vishrav Chaudhary, Hao Peng, Xia Song
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Existing LLM training and inference frameworks struggle in boosting efficiency with sparsity while maintaining the integrity of context and model architecture. Inspired by the sharding concept in database and the fact that attention parallelizes over heads on accelerators, we propose Sparsely-Sharded (S2) Attention, an attention algorithm that allocates heterogeneous context partitions for different attention heads to divide and conquer. S2-Attention enforces each attention head to only attend to a partition of contexts following a strided sparsity pattern, while the full context is preserved as the union of all the shards. As attention heads are processed in separate thread blocks, the context reduction for each head can thus produce end-to-end speed-up and memory reduction. At inference, LLMs trained with S2-Attention can then take the KV cache reduction as free meals with guaranteed model quality preserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X wall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction in end-to-end training time and 10X inference latency, (2) on-par model training quality compared to default attention, (3)perfect needle retrieval accuracy over 32K context window. On top of the algorithm, we build DKernel, an LLM training and inference kernel library that allows users to customize sparsity patterns for their own models. We open-sourced DKerneland make it compatible with Megatron, Pytorch, and vLLM.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-27T22:06:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.17678v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.17678v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 Styx: Transactional Stateful Functions on Streaming Dataflows</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kyriakos Psarakis, George Siachamis, George Christodoulou, Marios Fragkoulis, Asterios Katsifodimos
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Developing stateful cloud applications, such as low-latency workflows and microservices with strict consistency requirements, remains arduous for programmers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve these use cases. However, existing approaches either provide serializable transactional guarantees at the level of individual functions, or separate application logic from the state and use inefficient transactional protocols. These design choices increase the execution latency, limiting the adoption of SFaaS systems.   In this paper, we present Styx, a novel SFaaS runtime that executes serializable transactions across functions with exactly-once guarantees. Styx extends a deterministic transactional protocol to support an arbitrary call graph of stateful functions. It introduces a transaction-execution acknowledgment scheme that allows tracking a transactional workflow's SFaaS calls, guaranteeing atomicity and exactly-once processing. Finally, Styx features a function-execution caching mechanism and early transactional commit replies for optimized performance. Experiments with the YCSB-T, TPC-C, and Deathstar benchmarks show that Styx outperforms state-of-the-art approaches by achieving at least one order of magnitude higher throughput while exhibiting near-linear scalability and low latency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-27T17:30:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2312.06893v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2312.06893v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 Writing in the Margins: Better Inference Pattern for Long Context
  Retrieval</h2>
                <div class="authors">
                    <strong>Authors:</strong> Melisa Russak, Umar Jamil, Christopher Bryant, Kiran Kamble, Axel Magnuson, Mateusz Russak, Waseem AlShikh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper, we introduce Writing in the Margins (WiM), a new inference pattern for Large Language Models designed to optimize the handling of long input sequences in retrieval-oriented tasks. This approach leverages the chunked prefill of the key-value cache to perform segment-wise inference, which enables efficient processing of extensive contexts along with the generation and classification of intermediate information ("margins") that guide the model towards specific tasks. This method increases computational overhead marginally while significantly enhancing the performance of off-the-shelf models without the need for fine-tuning. Specifically, we observe that WiM provides an average enhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG) and more than a 30.0% increase in the F1-score for aggregation tasks (CWE). Additionally, we show how the proposed pattern fits into an interactive retrieval design that provides end-users with ongoing updates about the progress of context processing, and pinpoints the integration of relevant information into the final response. We release our implementation of WiM using Hugging Face Transformers library at https://github.com/writer/writing-in-the-margins.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-27T09:34:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.14906v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.14906v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework
  with Correlated Differential Privacy</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xianzhi Zhang, Yipeng Zhou, Di Wu, Quan Z. Sheng, Miao Hu, Linchang Xiao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Online video streaming has evolved into an integral component of the contemporary Internet landscape. Yet, the disclosure of user requests presents formidable privacy challenges. As users stream their preferred online videos, their requests are automatically seized by video content providers, potentially leaking users' privacy.   Unfortunately, current protection methods are not well-suited to preserving user request privacy from content providers while maintaining high-quality online video services. To tackle this challenge, we introduce a novel Privacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge devices to pre-fetch and cache videos, ensuring the privacy of users' requests while optimizing the efficiency of edge caching. More specifically, we design PPVF with three core components: (1) \textit{Online privacy budget scheduler}, which employs a theoretically guaranteed online algorithm to select non-requested videos as candidates with assigned privacy budgets. Alternative videos are chosen by an online algorithm that is theoretically guaranteed to consider both video utilities and available privacy budgets. (2) \textit{Noisy video request generator}, which generates redundant video requests (in addition to original ones) utilizing correlated differential privacy to obfuscate request privacy. (3) \textit{Online video utility predictor}, which leverages federated learning to collaboratively evaluate video utility in an online fashion, aiding in video selection in (1) and noise generation in (2). Finally, we conduct extensive experiments using real-world video request traces from Tencent Video. The results demonstrate that PPVF effectively safeguards user request privacy while upholding high video caching performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-27T02:03:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MM</span><span>cs.CR</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.14735v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.14735v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, Song Han
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As the demand for long-context large language models (LLMs) increases, models with context windows of up to 128K or 1M tokens are becoming increasingly prevalent. However, long-context LLM inference is challenging since the inference speed decreases significantly as the sequence length grows. This slowdown is primarily caused by loading a large KV cache during self-attention. Previous works have shown that a small portion of critical tokens will dominate the attention outcomes. However, we observe the criticality of a token highly depends on the query. To this end, we propose Quest, a query-aware KV cache selection algorithm. Quest keeps track of the minimal and maximal Key values in KV cache pages and estimates the criticality of a given page using Query vectors. By only loading the Top-K critical KV cache pages for attention, Quest significantly speeds up self-attention without sacrificing accuracy. We show that Quest can achieve up to 2.23x self-attention speedup, which reduces inference latency by 7.03x while performing well on tasks with long dependencies with negligible accuracy loss. Code is available at http://github.com/mit-han-lab/Quest .
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-26T21:01:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.10774v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.10774v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 Employing Artificial Intelligence to Steer Exascale Workflows with
  Colmena</h2>
                <div class="authors">
                    <strong>Authors:</strong> Logan Ward, J. Gregory Pauloski, Valerie Hayot-Sasson, Yadu Babuji, Alexander Brace, Ryan Chard, Kyle Chard, Rajeev Thakur, Ian Foster
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Computational workflows are a common class of application on supercomputers, yet the loosely coupled and heterogeneous nature of workflows often fails to take full advantage of their capabilities. We created Colmena to leverage the massive parallelism of a supercomputer by using Artificial Intelligence (AI) to learn from and adapt a workflow as it executes. Colmena allows scientists to define how their application should respond to events (e.g., task completion) as a series of cooperative agents. In this paper, we describe the design of Colmena, the challenges we overcame while deploying applications on exascale systems, and the science workflows we have enhanced through interweaving AI. The scaling challenges we discuss include developing steering strategies that maximize node utilization, introducing data fabrics that reduce communication overhead of data-intensive tasks, and implementing workflow tasks that cache costly operations between invocations. These innovations coupled with a variety of application patterns accessible through our agent-based steering model have enabled science advances in chemistry, biophysics, and materials science using different types of AI. Our vision is that Colmena will spur creative solutions that harness AI across many domains of scientific computing.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-26T17:21:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.14434v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.14434v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 Decision-Focused Learning to Predict Action Costs for Planning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jayanta Mandi, Marco Foschini, Daniel Holler, Sylvie Thiebaux, Jorg Hoffmann, Tias Guns
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In many automated planning applications, action costs can be hard to specify. An example is the time needed to travel through a certain road segment, which depends on many factors, such as the current weather conditions. A natural way to address this issue is to learn to predict these parameters based on input features (e.g., weather forecasts) and use the predicted action costs in automated planning afterward. Decision-Focused Learning (DFL) has been successful in learning to predict the parameters of combinatorial optimization problems in a way that optimizes solution quality rather than prediction quality. This approach yields better results than treating prediction and optimization as separate tasks. In this paper, we investigate for the first time the challenges of implementing DFL for automated planning in order to learn to predict the action costs. There are two main challenges to overcome: (1) planning systems are called during gradient descent learning, to solve planning problems with negative action costs, which are not supported in planning. We propose novel methods for gradient computation to avoid this issue. (2) DFL requires repeated planner calls during training, which can limit the scalability of the method. We experiment with different methods approximating the optimal plan as well as an easy-to-implement caching mechanism to speed up the learning process. As the first work that addresses DFL for automated planning, we demonstrate that the proposed gradient computation consistently yields significantly better plans than predictions aimed at minimizing prediction error; and that caching can temper the computation requirements.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-26T11:29:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.06876v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.06876v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yiwei Li, Boyu Tian, Mingyu Gao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Hybrid main memory systems combine both performance and capacity advantages from heterogeneous memory technologies. With larger capacities, higher associativities, and finer granularities, hybrid memory systems currently exhibit significant metadata storage and lookup overheads for flexibly remapping data blocks between the two memory tiers. To alleviate the inefficiencies of existing designs, we propose Trimma, the combination of a multi-level metadata structure and an efficient metadata cache design. Trimma uses a multi-level metadata table to only track truly necessary address remap entries. The saved memory space is effectively utilized as extra DRAM cache capacity to improve performance. Trimma also uses separate formats to store the entries with non-identity and identity address mappings. This improves the overall remap cache hit rate, further boosting the performance. Trimma is transparent to software and compatible with various types of hybrid memory systems. When evaluated on a representative hybrid memory system with HBM3 and DDR5, Trimma achieves up to 1.68$\times$ and on average 1.33$\times$ speedup benefits, compared to state-of-the-art hybrid memory designs. These results show that Trimma effectively addresses metadata management overheads, especially for future scalable large-scale hybrid memory architectures.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-26T07:26:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.16343v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.16343v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 RollingCache: Using Runtime Behavior to Defend Against Cache Side
  Channel Attacks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Divya Ojha, Sandhya Dwarkadas
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Shared caches are vulnerable to side channel attacks through contention in cache sets. Besides being a simple source of information leak, these side channels form useful gadgets for more sophisticated attacks that compromise the security of shared systems.   The fundamental design aspect that contention attacks exploit is the deterministic nature of the set of addresses contending for a cache set. In this paper, we present RollingCache, a cache design that defends against contention attacks by dynamically changing the set of addresses contending for cache sets. Unlike prior defenses, RollingCache does not rely on address encryption/decryption, data relocation, or cache partitioning. We use one level of indirection to implement dynamic mapping controlled by the whole-cache runtime behavior. Our solution does not depend on having defined security domains, and can defend against an attacker running on the same or another core.   We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our security evaluation shows that our dynamic mapping removes the deterministic ability to identify the source of contention. The performance evaluation shows an impact of 1.67\% over a mix of workloads, with a corresponding
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-26T04:32:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.08795v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.08795v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 Decentralized Federated Learning with Model Caching on Mobile Agents</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaoyu Wang, Guojun Xiong, Houwei Cao, Jian Li, Yong Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Federated Learning (FL) aims to train a shared model using data and computation power on distributed agents coordinated by a central server. Decentralized FL (DFL) utilizes local model exchange and aggregation between agents to reduce the communication and computation overheads on the central server. However, when agents are mobile, the communication opportunity between agents can be sporadic, largely hindering the convergence and accuracy of DFL. In this paper, we study delay-tolerant model spreading and aggregation enabled by model caching on mobile agents. Each agent stores not only its own model, but also models of agents encountered in the recent past. When two agents meet, they exchange their own models as well as the cached models. Local model aggregation works on all models in the cache. We theoretically analyze the convergence of DFL with cached models, explicitly taking into account the model staleness introduced by caching. We design and compare different model caching algorithms for different DFL and mobility scenarios. We conduct detailed case studies in a vehicular network to systematically investigate the interplay between agent mobility, cache staleness, and model convergence. In our experiments, cached DFL converges quickly, and significantly outperforms DFL without caching.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-26T03:58:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.14001v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.14001v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 Mobile Edge Computing Networks: Online Low-Latency and Fresh Service
  Provisioning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuhan Yi, Guanglin Zhang, Hai Jiang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Edge service caching can significantly mitigate latency and reduce communication and computing overhead by fetching and initializing services (applications) from clouds. The freshness of cached service data is critical when providing satisfactory services to users, but has been overlooked in existing research efforts. In this paper, we study the online low-latency and fresh service provisioning in mobile edge computing (MEC) networks. Specifically, we jointly optimize the service caching, task offloading, and resource allocation without knowledge of future system information, which is formulated as a joint online long-term optimization problem. This problem is NP-hard. To solve the problem, we design a Lyapunov-based online framework that decouples the problem at temporal level into a series of per-time-slot subproblems. For each subproblem, we propose an online integrated optimization-deep reinforcement learning (OIODRL) method, which contains an optimization stage including a quadratically constrained quadratic program (QCQP) transformation and a semidefinite relaxation (SDR) method, and a learning stage including a deep reinforcement learning (DRL) algorithm. Extensive simulations show that the proposed OIODRL method achieves a near-optimal solution and outperforms other benchmark methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-24T15:23:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.13605v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.13605v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context
  Generation with Speculative Decoding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jian Chen, Vashisth Tiwari, Ranajoy Sadhukhan, Zhuoming Chen, Jinyuan Shi, Ian En-Hsu Yen, Beidi Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have become more prevalent in long-context applications such as interactive chatbots, document analysis, and agent workflows, but it is challenging to serve long-context requests with low latency and high throughput. Speculative decoding (SD) is a widely used technique to reduce latency without sacrificing performance but the conventional wisdom suggests that its efficacy is limited to small batch sizes. In MagicDec, we show that surprisingly SD can achieve speedup even for a high throughput inference regime for moderate to long sequences. More interestingly, an intelligent drafting strategy can achieve better speedup with increasing batch size based on our rigorous analysis. MagicDec first identifies the bottleneck shifts with increasing batch size and sequence length, and uses these insights to deploy speculative decoding more effectively for high throughput inference. Then, it leverages draft models with sparse KV cache to address the KV bottleneck that scales with both sequence length and batch size. This finding underscores the broad applicability of speculative decoding in long-context serving, as it can enhance throughput and reduce latency without compromising accuracy. For moderate to long sequences, we demonstrate up to 2x speedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving batch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available at https://github.com/Infini-AI-Lab/MagicDec/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-23T17:54:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.11049v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.11049v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dhruv Pratap Singh, Anjana A. Mahesh, B. Sundar Rajan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We consider a variant of the coded caching problem where users connect to two types of caches, called private caches and access caches. The problem setting consists of a server having a library of files and a set of access caches. Every user, equipped with a private cache, connects to $L$ neighboring access caches in a cyclic wrap-around fashion. The server populates the private and access caches with file contents in either coded or uncoded format. For this setting, we derive a lower bound on the optimal worst-case transmission rate using cut-set arguments. This lower bound applies to both coded and uncoded placements. We then provide an achievable scheme with uncoded placement and show that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for the dedicated cache network in the absence of access caches. Finally, we show that the proposed scheme achieves optimality in large memory regimes and provide numerical plots comparing the rate of the proposed scheme with the derived lower bound, demonstrating the optimality of our scheme.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-23T15:39:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.13165v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.13165v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 Fundamental Limits of Multi-Message Private Computation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ali Gholami, Kai Wan, Tayyebeh Jahani-Nezhad, Hua Sun, Mingyue Ji, Giuseppe Caire
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In a typical formulation of the private information retrieval (PIR) problem, a single user wishes to retrieve one out of $ K$ files from $N$ servers without revealing the demanded file index to any server. This paper formulates an extended model of PIR, referred to as multi-message private computation (MM-PC), where instead of retrieving a single file, the user wishes to retrieve $P>1$ linear combinations of files while preserving the privacy of the demand information. The MM-PC problem is a generalization of the private computation (PC) problem (where the user requests one linear combination of the files), and the multi-message private information retrieval (MM-PIR) problem (where the user requests $P>1$ files). A baseline achievable scheme repeats the optimal PC scheme by Sun and Jafar $P$ times, or treats each possible demanded linear combination as an independent file and then uses the near optimal MM-PIR scheme by Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that significantly improves upon the baseline schemes. In doing so, we design the queries inspired by the structure in the cache-aided scalar linear function retrieval scheme by Wan {\it et al.}, which leverages the dependency between linear functions to reduce the amount of communications. To ensure the decodability of our scheme, we propose a new method to benefit from the existing dependency, referred to as the sign assignment step. In the end, we use Maximum Distance Separable matrices to code the queries, which allows the reduction of download from the servers, while preserving privacy. By the proposed schemes, we characterize the capacity within a multiplicative factor of $2$.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-23T13:25:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2305.05332v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2305.05332v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 Which Part of the Heap is Useful? Improving Heap Liveness Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Vini Kanvar, Uday P. Khedker
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the growing sizes of data structures allocated in heap, understanding the actual use of heap memory is critically important for minimizing cache misses and reclaiming unused memory. A static analysis aimed at this is difficult because the heap locations are unnamed. Using allocation sites to name them creates very few distinctions making it difficult to identify allocated heap locations that are not used. Heap liveness analysis using access graphs solves this problem by (a) using a storeless model of heap memory by naming the locations with access paths, and (b) representing the unbounded sets of access paths (which are regular languages) as finite automata.   We improve the scalability and efficiency of heap liveness analysis, and reduce the amount of computed heap liveness information by using deterministic automata and by minimizing the inclusion of aliased access paths in the language. Practically, our field-, flow-, context-sensitive liveness analysis on SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5 kLoC) and improves efficiency even up to 99%. For some of the benchmarks, our technique shows multifold reduction in the computed liveness information, ranging from 2 to 100 times (in terms of the number of live access paths), without compromising on soundness.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-23T09:54:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.12947v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.12947v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 Exposing Shadow Branches</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chrysanthos Pepi, Bhargav Reddy Godala, Krishnam Tibrewala, Gino Chacon, Paul V. Gratz, Daniel A. Jiménez, Gilles A. Pokam, David I. August
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modern processors implement a decoupled front-end in the form of Fetch Directed Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is driven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and branch target tracking structures to speculatively fetch instructions into the Instruction Cache (L1I). As data center applications become more complex, their code footprints also grow, resulting in an increase in Branch Target Buffer (BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB miss, the BPU may not identify the current instruction as a branch to FDIP. This can prevent FDIP from prefetching or cause it to speculate down the wrong path, further polluting the L1I cache. We observe that the vast majority, 75%, of BTB-missing, unidentified branches are actually present in instruction cache lines that FDIP has previously fetched but, these missing branches have not yet been decoded and inserted into the BTB. This is because the instruction line is decoded from an entry point (which is the target of the previous taken branch) till an exit point (the taken branch). Branch instructions present in the ignored portion of the cache line we call them "Shadow Branches". Here we present Skeia, a novel shadow branch decoding technique that identifies and decodes unused bytes in cache lines fetched by FDIP, inserting them into a Shadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB, allowing FDIP to speculate despite a BTB miss. With a minimal storage state of 12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB) and ~2% versus adding an equal amount of state to the BTB across 16 front-end bound applications. Since many branches stored in the SBB are unique compared to those in a similarly sized BTB, we consistently observe greater performance gains with Skeia across all examined sizes until saturation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-22T17:56:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.12592v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.12592v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties</h2>
                <div class="authors">
                    <strong>Authors:</strong> Simon Hettler, Kankona Singha Roy, Raul Arenal, Leela S. Panchakarla
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Layered CoO$_2$ is of great interest for its promising properties but is meta-stable in its bulk form. CoO$_2$ was synthesized by converting the quasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a hydrothermal treatment. The resulting nanostructures were predominantly nanoscrolls with very thin walls, which exhibit long-term stability. A detailed structural investigation reveals that the CoO$_2$ is found to crystallize in monoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure. Individual nanoscrolls are characterized electrically and show a p-type semiconducting nature with a high current-carrying capacity of 4$\cdot$10$^5$ A cm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The results demonstrate the possibility to stabilize meta-stable materials in low-dimensional forms and a promising application of the nanoscrolls as interconnect in high-voltage electronic circuitry.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-22T17:47:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.mtrl-sci</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1002/admi.202400317' target='_blank'>doi</a><a href='http://arxiv.org/abs/2309.14533v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2309.14533v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 Rheological behavior of molybdenum disulfide (MoS2) inks under electric
  fields: influence of concentration and voltage</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pedro C Rijo, Francisco J. Galindo-Rosales
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This work provides a complete rheological characterization of molybdenum disulfide (MoS2) inks in the presence of electric fields. Several concentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The lubrication effects are present in the ink when the MoS2 concentration is higher than 0.10% w/w. The dielectric properties show the impossibility of a positive electrorheological effect for all MoS2-inks studied. The formation of vortices and electromigration of MoS2 particles occur under the influence of an external electric field. These two phenomena affect the rheological behavior of MoS2-inks under shear flow condition. Relatively to the extensional rheology experiments, the particle migration and the vortex formation promote anisotropy on the rheological properties of the inks which affects the relaxation time, the formation of beads-on-a-string and the uniaxial elongational flow condition is no longer valid. When the electric field strength is 1.5 kV/mm, the formation of Taylor's cone is observed and independent of MoS2 concentration.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-21T10:26:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.flu-dyn</span><span>cond-mat.soft</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.11506v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.11506v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 Towards End-to-End GPS Localization with Neural Pseudorange Correction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xu Weng, KV Ling, Haochen Liu, Kun Cao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The pseudorange error is one of the root causes of localization inaccuracy in GPS. Previous data-driven methods regress and eliminate pseudorange errors using handcrafted intermediate labels. Unlike them, we propose an end-to-end GPS localization framework, E2E-PrNet, to train a neural network for pseudorange correction (PrNet) directly using the final task loss calculated with the ground truth of GPS receiver states. The gradients of the loss with respect to learnable parameters are backpropagated through a Differentiable Nonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing the data-driven neural network and the model-based DNLS module is verified with GPS data collected by Android phones, showing that E2E-PrNet outperforms the baseline weighted least squares method and the state-of-the-art end-to-end data-driven approach. Finally, we discuss the explainability of E2E-PrNet.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-21T06:10:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2401.10685v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2401.10685v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 Telepathic Datacenters: Fast RPCs using Shared CXL Memory</h2>
                <div class="authors">
                    <strong>Authors:</strong> Suyash Mahar, Ehsan Hajyjasini, Seungjin Lee, Zifeng Zhang, Mingyao Shen, Steven Swanson
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Datacenter applications often rely on remote procedure calls (RPCs) for fast, efficient, and secure communication. However, RPCs are slow, inefficient, and hard to use as they require expensive serialization and compression to communicate over a packetized serial network link. Compute Express Link 3.0 (CXL) offers an alternative solution, allowing applications to share data using a cache-coherent, shared-memory interface across clusters of machines.   RPCool is a new framework that exploits CXL's shared memory capabilities. RPCool avoids serialization by passing pointers to data structures in shared memory. While avoiding serialization is useful, directly sharing pointer-rich data eliminates the isolation that copying data over traditional networks provides, leaving the receiver vulnerable to invalid pointers and concurrent updates to shared data by the sender. RPCool restores this safety with careful and efficient management of memory permissions. Another significant challenge with CXL shared memory capabilities is that they are unlikely to scale to an entire datacenter. RPCool addresses this by falling back to RDMA-based communication.   Overall, RPCool reduces the round-trip latency by 1.93$\times$ and 7.2$\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms, respectively. Moreover, RPCool performs either comparably or better than other RPC mechanisms across a range of workloads.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-21T04:16:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.OS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.11325v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.11325v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical
  Planning and Control</h2>
                <div class="authors">
                    <strong>Authors:</strong> Poppy Collis, Ryan Singh, Paul F Kinghorn, Christopher L Buckley
                </div>
                <div class="summary">
                    <strong>Summary:</strong> An open problem in artificial intelligence is how systems can flexibly learn discrete abstractions that are useful for solving inherently continuous problems. Previous work has demonstrated that a class of hybrid state-space model known as recurrent switching linear dynamical systems (rSLDS) discover meaningful behavioural units via the piecewise linear decomposition of complex continuous dynamics (Linderman et al., 2016). Furthermore, they model how the underlying continuous states drive these discrete mode switches. We propose that the rich representations formed by an rSLDS can provide useful abstractions for planning and control. We present a novel hierarchical model-based algorithm inspired by Active Inference in which a discrete MDP sits above a low-level linear-quadratic controller. The recurrent transition dynamics learned by the rSLDS allow us to (1) specify temporally-abstracted sub-goals in a method reminiscent of the options framework, (2) lift the exploration into discrete space allowing us to exploit information-theoretic exploration bonuses and (3) `cache' the approximate solutions to low-level problems in the discrete planner. We successfully apply our model to the sparse Continuous Mountain Car task, demonstrating fast system identification via enhanced exploration and non-trivial planning through the delineation of abstract sub-goals.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-20T16:02:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.10970v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.10970v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI
  Framework for Personal LLMs Fine-Tuning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bei Ouyang, Shengyuan Ye, Liekang Zeng, Tianyi Qian, Jingyi Li, Xu Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have unlocked a plethora of powerful applications at the network edge, such as intelligent personal assistants. Data privacy and security concerns have prompted a shift towards edge-based fine-tuning of personal LLMs, away from cloud reliance. However, this raises issues of computational intensity and resource scarcity, hindering training efficiency and feasibility. While current studies investigate parameter-efficient fine-tuning (PEFT) techniques to mitigate resource constraints, our analysis indicates that these techniques are not sufficiently resource-efficient for edge devices. To tackle these challenges, we propose Pluto and Charon (PAC), a time and memory efficient collaborative edge AI framework for personal LLMs fine-tuning. PAC breaks the resource wall of personal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1) Algorithmically, PAC implements a personal LLMs fine-tuning technique that is efficient in terms of parameters, time, and memory. It utilizes Parallel Adapters to circumvent the need for a full backward pass through the LLM backbone. Additionally, an activation cache mechanism further streamlining the process by negating the necessity for repeated forward passes across multiple epochs. (2) Systematically, PAC leverages edge devices in close proximity, pooling them as a collective resource for in-situ personal LLMs fine-tuning, utilizing a hybrid data and pipeline parallelism to orchestrate distributed training. The use of the activation cache eliminates the need for forward pass through the LLM backbone,enabling exclusive fine-tuning of the Parallel Adapters using data parallelism. Extensive evaluation based on prototype implementation demonstrates that PAC remarkably outperforms state-of-the-art approaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction in memory footprint.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-20T11:30:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span><span>cs.LG</span><span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.10746v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.10746v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 Heta: Distributed Training of Heterogeneous Graph Neural Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuchen Zhong, Junwei Su, Chuan Wu, Minjie Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic relationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable learning performance in various applications. However, current distributed GNN training systems often overlook unique characteristics of HetGs, such as varying feature dimensions and the prevalence of missing features among nodes, leading to suboptimal performance or even incompatibility with distributed HGNN training. We introduce Heta, a framework designed to address the communication bottleneck in distributed HGNN training. Heta leverages the inherent structure of HGNNs - independent relation-specific aggregations for each relation, followed by a cross-relation aggregation - and advocates for a novel Relation-Aggregation-First computation paradigm. It performs relation-specific aggregations within graph partitions and then exchanges partial aggregations. This design, coupled with a new graph partitioning method that divides a HetG based on its graph schema and HGNN computation dependency, substantially reduces communication overhead. Heta further incorporates an innovative GPU feature caching strategy that accounts for the different cache miss-penalties associated with diverse node types. Comprehensive evaluations of various HGNN models and large heterogeneous graph datasets demonstrate that Heta outperforms state-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in end-to-end epoch time, respectively.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-20T04:46:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.09697v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.09697v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory</h2>
                <div class="authors">
                    <strong>Authors:</strong> Olena Tkach, Gerd Schoenhense
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The strong electric field between the sample and the extractor is the core of cathode lenses and a pivotal determinant of high resolution. Nevertheless, fields in the range of 3-8 kV/mm can be a source of complications. Local field enhancement at sharp edges or microscopic protrusions of cleaved samples may result in field emission or flashovers. Moreover, slow background electrons are drawn into the microscope column, where they contribute to space charge effects. A novel front lens configuration, optimized through ray-tracing simulations, significantly reduces the field at the sample and allows even for zero field or retarding field, which serves to suppress space charge effects. One or several annular electrodes, situated in a concentric position relative to the extractor, serve to form an additional lens within the gap between the sample and the extractor. The refractory power of this lens, and consequently the field at the sample surface, can be modified by adjusting the potentials of the annular electrodes. The imaging properties and aberrations of this gap lens have been investigated with regard to momentum imaging and XPEEM. The study encompasses the energy range from the few-eV level for laser-ARPES to 6 keV, for hard X-ray ARPES. The additional converging lens situated in close proximity to the sample exhibits a reduced field curvature of the k-image in the backfocal plane. This allows for the acquisition of larger fields of view in both momentum and real-space imaging.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-19T15:47:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.app-ph</span><span>cond-mat.mtrl-sci</span><span>physics.ins-det</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.10104v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.10104v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 Abstract Environment Trimming</h2>
                <div class="authors">
                    <strong>Authors:</strong> Daniel Jurjo-Rivas, Jose F. Morales, Pedro López-García, Manuel V. Hermenegildo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Variable sharing is a fundamental property in the static analysis of logic programs, since it is instrumental for ensuring correctness and increasing precision while inferring many useful program properties. Such properties include modes, determinacy, non-failure, cost, etc. This has motivated significant work on developing abstract domains to improve the precision and performance of sharing analyses. Much of this work has centered around the family of set-sharing domains, because of the high precision they offer. However, this comes at a price: their scalability to a wide set of realistic programs remains challenging and this hinders their wider adoption. In this work, rather than defining new sharing abstract domains, we focus instead on developing techniques which can be incorporated in the analyzers to address aspects that are known to affect the efficiency of these domains, such as the number of variables, without affecting precision. These techniques are inspired in others used in the context of compiler optimizations, such as expression reassociation and variable trimming. We present several such techniques and provide an extensive experimental evaluation of over 1100 program modules taken from both production code and classical benchmarks. This includes the Spectector cache analyzer, the s(CASP) system, the libraries of the Ciao system, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental results are quite encouraging: we have obtained significant speed-ups, and, more importantly, the number of modules that require a timeout was cut in half. As a result, many more programs can be analyzed precisely in reasonable times.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-19T09:50:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.09848v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.09848v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for
  Efficient MoE Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shuzhang Zhong, Ling Liang, Yuan Wang, Runsheng Wang, Ru Huang, Meng Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Mixture-of-Experts (MoE) models are designed to enhance the efficiency of large language models (LLMs) without proportionally increasing the computational demands. However, their deployment on edge devices still faces significant challenges due to high on-demand loading overheads from managing sparsely activated experts. This paper introduces AdapMoE, an algorithm-system co-design framework for efficient MoE inference. AdapMoE features adaptive expert gating and management to reduce the on-demand loading overheads. We observe the heterogeneity of experts loading across layers and tokens, based on which we propose a sensitivity-based strategy to adjust the number of activated experts dynamically. Meanwhile, we also integrate advanced prefetching and cache management techniques to further reduce the loading latency. Through comprehensive evaluations on various platforms, we demonstrate AdapMoE consistently outperforms existing techniques, reducing the average number of activated experts by 25% and achieving a 1.35x speedup without accuracy degradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-19T03:27:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3676536.3676741' target='_blank'>doi</a><a href='http://arxiv.org/abs/2408.10284v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.10284v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 Post-Training Sparse Attention with Double Sparsity</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shuo Yang, Ying Sheng, Joseph E. Gonzalez, Ion Stoica, Lianmin Zheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The inference process for large language models is slow and memory-intensive, with one of the most critical bottlenecks being excessive Key-Value (KV) cache accesses. This paper introduces "Double Sparsity," a novel post-training sparse attention technique designed to alleviate this bottleneck by reducing KV cache access. Double Sparsity combines token sparsity, which focuses on utilizing only the important tokens for computing self-attention, with channel sparsity, an approach that uses important feature channels for identifying important tokens. Our key insight is that the pattern of channel sparsity is relatively static, allowing us to use offline calibration to make it efficient at runtime, thereby enabling accurate and efficient identification of important tokens. Moreover, this method can be combined with offloading to achieve significant memory usage reduction. Experimental results demonstrate that Double Sparsity can achieve $\frac{1}{16}$ token and channel sparsity with minimal impact on accuracy across various tasks, including wiki-2 perplexity, key-value retrieval, and long context benchmarks with models including Llama-2-7B, Llama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\times$ acceleration in attention operations and a 1.9$\times$ improvement in end-to-end inference on GPUs. With offloading, it achieves a decoding speed acceleration of 16.3$\times$ compared to state-of-the-art solutions at a sequence length of 256K. Our code is publicly available at https://github.com/andy-yang-1/DoubleSparse.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-18T17:27:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07092v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07092v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 CMD: A Cache-assisted GPU Memory Deduplication Architecture</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wei Zhao, Dan Feng, Wei Tong, Xueliang Wei, Bing Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Massive off-chip accesses in GPUs are the main performance bottleneck, and we divided these accesses into three types: (1) Write, (2) Data-Read, and (3) Read-Only. Besides, We find that many writes are duplicate, and the duplication can be inter-dup and intra-dup. While inter-dup means different memory blocks are identical, and intra-dup means all the 4B elements in a line are the same. In this work, we propose a cache-assisted GPU memory deduplication architecture named CMD to reduce the off-chip accesses via utilizing the data duplication in GPU applications. CMD includes three key design contributions which aim to reduce the three kinds of accesses: (1) A novel GPU memory deduplication architecture that removes the inter-dup and inter-dup lines. As for the inter-dup detection, we reduce the extra read requests caused by the traditional read-verify hash process. Besides, we design several techniques to manage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce the reads to duplicate data. When an L2 cache miss wants to read the duplicate block, if the reference block has been fetched to L2 and it is clean, we can copy it to the L2 missed block without accessing off-chip DRAM. As for the reads to intra-dup data, CMD uses the on-chip metadata cache to get the data. (3) When a cache line is evicted, the clean sectors in the line are invalidated while the dirty sectors are written back. However, most read-only victims are re-referenced from DRAM more than twice. Therefore, we add a full-associate FIFO to accommodate the read-only (it is also clean) victims to reduce the re-reference counts. Experiments show that CMD can decrease the off-chip accesses by 31.01%, reduce the energy by 32.78% and improve performance by 37.79%. Besides, CMD can improve the performance of memory-intensive workloads by 50.18%.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-18T13:54:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.09483v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.09483v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for
  Efficient LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, S. Kevin Zhou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models have excelled in various fields but encounter challenges in memory and time efficiency due to the expanding Key-Value (KV) cache required for long-sequence inference. Recent efforts try to reduce KV cache size to a given memory budget by evicting vast non-critical cache elements during runtime, while preserving generation quality. Our revisiting of current eviction methods reveals that they fundamentally minimize an upper bound of the $L_1$ eviction loss between the pre- and post-eviction outputs of multi-head self-attention mechanisms. Moreover, our analysis indicates that the common practices of uniformly assigning budgets across attention heads harm their post-eviction generation quality. In light of these findings, we propose a simple yet effective adaptive budget allocation algorithm. This algorithm not only optimizes the theoretical loss upper bound but also reduces the $L_1$ eviction loss in practice by aligning with the varied characteristics across different heads. By integrating this algorithm into two state-of-the-art methods, we demonstrate the effectiveness of using adaptive budget allocation to optimize KV cache eviction. Extensive evaluations on 16 datasets and the Needle-in-a-Haystack test confirm significant performance improvements across various tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-16T08:46:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.11550v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.11550v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language
  Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kaushal Kumar Maurya, KV Aditya Srivatsa, Ekaterina Kochmar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have gained increased popularity due to their remarkable success across various tasks, which has led to the active development of a large set of diverse LLMs. However, individual LLMs have limitations when applied to complex tasks because of such factors as training biases, model sizes, and the datasets used. A promising approach is to efficiently harness the diverse capabilities of LLMs to overcome these individual limitations. Towards this goal, we introduce a novel LLM selection algorithm called SelectLLM. This algorithm directs input queries to the most suitable subset of LLMs from a large pool, ensuring they collectively provide the correct response efficiently. SelectLLM uses a multi-label classifier, utilizing the classifier's predictions and confidence scores to design optimal policies for selecting an optimal, query-aware, and lightweight subset of LLMs. Our findings show that the proposed model outperforms individual LLMs and achieves competitive performance compared to similarly sized, computationally expensive top-performing LLM subsets. Specifically, with a similarly sized top-performing LLM subset, we achieve a significant reduction in latency on two standard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower latency for MMLU. Additionally, we conduct comprehensive analyses and ablation studies, which validate the robustness of the proposed model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-16T06:11:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.08545v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.08545v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 Symmetric Locality: Definition and Initial Results</h2>
                <div class="authors">
                    <strong>Authors:</strong> Giordan Escalona, Dylan McKellips, Chen Ding
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this short paper, we characterize symmetric locality. In designing algorithms, compilers, and systems, data movement is a common bottleneck in high-performance computation, in which we improve cache and memory performance. We study a special type of data reuse in the form of repeated traversals, or re-traversals, which are based on the symmetric group. The cyclic and sawtooth traces are previously known results in symmetric locality, and in this work, we would like to generalize this result for any re-traversal. Then, we also provide an abstract framework for applications in compiler design and machine learning models to improve the memory performance of certain programs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-16T04:12:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SY</span><span>cs.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.19291v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.19291v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 ConfusedPilot: Confused Deputy Risks in RAG-based LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ayush RoyChowdhury, Mulong Luo, Prateek Sahu, Sarbartha Banerjee, Mohit Tiwari
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Retrieval augmented generation (RAG) is a process where a large language model (LLM) retrieves useful information from a database and then generates the responses. It is becoming popular in enterprise settings for daily business operations. For example, Copilot for Microsoft 365 has accumulated millions of businesses. However, the security implications of adopting such RAG-based systems are unclear.   In this paper, we introduce ConfusedPilot, a class of security vulnerabilities of RAG systems that confuse Copilot and cause integrity and confidentiality violations in its responses. First, we investigate a vulnerability that embeds malicious text in the modified prompt in RAG, corrupting the responses generated by the LLM. Second, we demonstrate a vulnerability that leaks secret data, which leverages the caching mechanism during retrieval. Third, we investigate how both vulnerabilities can be exploited to propagate misinformation within the enterprise and ultimately impact its operations, such as sales and manufacturing. We also discuss the root cause of these attacks by investigating the architecture of a RAG-based system. This study highlights the security vulnerabilities in today's RAG-based systems and proposes design guidelines to secure future RAG-based systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-15T05:24:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.04870v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.04870v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 A Case for Enabling Delegation of 5G Core Decisions to the RAN</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lucas Vancina, Geoffrey Xie
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Under conventional 5G system design, the authentication and continuous monitoring of user equipment (UE) demands a reliable backhaul connection between the radio access network (RAN) and the core network functions (AMF, AUSF, UDM, etc.). This is not a given, especially in disaster response and military operations. We propose that, in these scenarios, decisions made by core functions can be effectively delegated to the RAN by leveraging the RAN's computing resources and the micro-service programmability of the O-RAN system architecture. This paper presents several concrete designs of core-RAN decision delegation, including caching of core decisions and replicating some of the core decision logic. Each design has revealed interesting performance and security trade-offs that warrant further investigation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T23:42:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07853v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07853v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 The Bicameral Cache: a split cache for vector architectures</h2>
                <div class="authors">
                    <strong>Authors:</strong> Susana Rebolledo, Borja Perez, Jose Luis Bosque, Peter Hsu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Bicameral Cache is a cache organization proposal for a vector architecture that segregates data according to their access type, distinguishing scalar from vector references. Its aim is to avoid both types of references from interfering in each other's data locality, with a special focus on prioritizing the performance on vector references. The proposed system incorporates an additional, non-polluting prefetching mechanism to help populate the long vector cache lines in advance to increase the hit rate by further exploiting the spatial locality on vector data. Its evaluation was conducted on the Cavatools simulator, comparing the performance to a standard conventional cache, over different typical vector benchmarks for several vector lengths. The results proved the proposed cache speeds up performance on stride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition, the prefetching feature consistently provided an additional value.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T09:18:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.15440v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.15440v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 At Least Factor-of-Two Optimization for RWLE-Based Homomorphic
  Encryption</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jonathan Ly
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Many modern applications that deal with sensitive data, such as healthcare and government services, outsource computation to cloud platforms. In such untrusted environments, privacy is of vital importance. One solution to this problem is homomorphic encryption (HE), a family of cryptographic schemes that support certain algebraic operations on encrypted data without the need for decryption. However, despite major advancements, encryption in modern HE schemes still comes with a non-trivial computational overhead that can hamper data-intensive workloads. To resolve this, recent research has shown that leveraging caching techniques, such as Rache, can significantly enhance the performance of HE schemes while maintaining security. Rache unfortunately displays a key limitation in the time complexity of its caching procedure, which scales with the size of the plaintext space. Smuche is another caching scheme that simultaneously improves the scalability of the caching procedure and turns the encryption process into a constant-time operation, utilizing only a single scalar multiplication. Even still, more can be done. In this paper, we present an encryption method we call ``Zinc" which entirely forgoes the multiple caching process, replacing it with a single scalar addition, and then injecting randomness that takes constant time with respect to the plaintext space. This injection of randomness is similar to Smuche, and a great improvement from Rache, allowing Zinc to achieve efficiency without compromising security. We implement the scheme using Microsoft SEAL and compare its performance to vanilla CKKS.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-14T05:42:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07304v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07304v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 Cache-Aided MIMO Communications: DoF Analysis and Transmitter
  Optimization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mohammad NaseriTehrani, MohammadJavad Salehi, Antti Tölli
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Cache-aided MIMO communications aims to jointly exploit both coded caching~(CC) and spatial multiplexing gains to enhance communication efficiency. In this paper, we first analyze the achievable degrees of freedom~(DoF) in a MIMO-CC system with CC gain \(t\), where a server with \(L\) transmit antennas communicates with \(K\) users, each equipped with \(G\) receive antennas. We demonstrate that the enhanced achievable DoF is \(\max_{\beta, \Omega} \Omega \beta\), where the number of users \(\Omega\) served in each transmission is fine-tuned to maximize DoF, and \(\beta \le \min\big(G, \nicefrac{L \binom{\Omega-1}{t}}{1 + (\Omega - t - 1)\binom{\Omega-1}{t}}\big)\) represents the number of parallel streams decoded by each user. Second, we introduce an effective transmit covariance matrix design aimed at maximizing the symmetric rate, solved iteratively via successive convex approximation. Third, we propose a new class of MIMO-CC schemes using a novel scheduling mechanism leveraging maximal multicasting opportunities to maximize delivery rates at given SNR levels while adhering to linear processing constraints. Lastly, we devise linear multicast beamforming strategies tailored for the flexible scheduling schemes in MIMO-CC systems and present an iterative solution for the efficient design of beamformers. Extensive numerical simulations are used to verify the results of the paper.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T13:56:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>eess.SP</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.15743v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.15743v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 Ownership in low-level intermediate representation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Siddharth Priya, Arie Gurfinkel
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The concept of ownership in high level languages can aid both the programmer and the compiler to reason about the validity of memory operations. Previously, ownership semantics has been used successfully in high level automatic program verification to model a reference to data by a first order logic (FOL) representation of data instead of maintaining an address map. However, ownership semantics is not used in low level program verification. We have identified two challenges. First, ownership information is lost when a program is compiled to a low level intermediate representation (e.g., in LLVM IR). Second, pointers in low level programs point to bytes using an address map (e.g., in unsafe Rust) and thus the verification condition (VC) cannot always replace a pointer by its FOL abstraction. To remedy the situation, we develop ownership semantics for an LLVM like low level intermediate representation. Using these semantics, the VC can opportunistically model some memory accesses by a direct access of a pointer cache that stores byte representation of data. This scheme reduces instances where an address map must be maintained, especially for mostly safe programs that follow ownership semantics. For unsafe functionality, memory accesses are modelled by operations on an address map and we provide mechanisms to keep the address map and pointer cache in sync. We implement these semantics in SEABMC, a bit precise bounded model checker for LLVM. For evaluation, the source programs are assumed to be written in C. Since C does not have ownership built in, suitable macros are added that introduce and preserve ownership during translation to LLVM like IR for verification. This approach is evaluated on mature open source C code. For both handcrafted benchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT solving.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T13:31:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PL</span><span>cs.SE</span><span>D.2.4</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.04043v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.04043v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache
  Consumption</h2>
                <div class="authors">
                    <strong>Authors:</strong> Luohe Shi, Hongyi Zhang, Yao Yao, Zuchao Li, Hai Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022, have revolutionized various industries with their advanced language comprehension. However, their efficiency is challenged by the Transformer architecture' s struggle with handling long texts. KV-Cache has emerged as a pivotal solution to this issue, converting the time complexity of token generation from quadratic to linear, albeit with increased GPU memory overhead proportional to conversation length. With the development of the LLM community and academia, various KV-Cache compression methods have been proposed. In this review, we dissect the various properties of KV-Cache and elaborate on various methods currently used to optimize the KV-Cache space usage of LLMs. These methods span the pre-training phase, deployment phase, and inference phase, and we summarize the commonalities and differences among these methods. Additionally, we list some metrics for evaluating the long-text capabilities of large language models, from both efficiency and capability perspectives. Our review thus sheds light on the evolving landscape of LLM optimization, offering insights into future advancements in this dynamic field.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T09:55:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.18003v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.18003v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 Finch: Prompt-guided Key-Value Cache Compression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Giulio Corallo, Paolo Papotti
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent large language model applications, such as Retrieval-Augmented Generation and chatbots, have led to an increased need to process longer input contexts. However, this requirement is hampered by inherent limitations. Architecturally, models are constrained by a context window defined during training. Additionally, processing extensive texts requires substantial GPU memory. We propose a novel approach, Finch, to compress the input context by leveraging the pre-trained model weights of the self-attention. Given a prompt and a long text, Finch iteratively identifies the most relevant Key (K) and Value (V) pairs over chunks of the text conditioned on the prompt. Only such pairs are stored in the KV cache, which, within the space constrained by the context window, ultimately contains a compressed version of the long text. Our proposal enables models to consume large inputs even with high compression (up to 93x) while preserving semantic integrity without the need for fine-tuning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-13T09:08:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.00167v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.00167v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 Value-based Proactive Caching for Sensing Data in Internet of Vehicles</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yantong Wang, Ke Liu, Hui Ji, Jiande Sun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Sensing data (SD) plays an important role in safe-related applications for Internet of Vehicles. Proactively caching required sensing data (SD) is a pivotal strategy for alleviating network congestion and improving data accessibility. Despite merits, existing studies predominantly address SD caching within a single time slot, which may not be scalable to scenarios involving multi-slots. Furthermore, the oversight of service capacity at caching nodes could lead to significant queuing delays in SD reception. To tackle these limitations, we jointly consider the problem of anchoring caching placement and requests allocation for SD. A value model incorporating both temporal and spacial characteristics is first proposed to estimate the significance of different caching decisions. Subsequently, a stochastic integer nonlinear programming model is provided to optimize the long-term system performance, which is converted into a series of online optimization problem by leveraging the Lyapunov method and linearized via introducing auxiliary variables. To expedite the solution, we provide a binary quantum particle swarm optimization based algorithm with quadratic time complexity. Numerical investigations demonstrate the superiority of proposed algorithms compared with other schemes in terms of energy consumption, response latency, and cache-hit ratio.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-12T08:46:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.05996v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.05996v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open
  Source RISC-V application processor</h2>
                <div class="authors">
                    <strong>Authors:</strong> Riccardo Tedeschi, Luca Valente, Gianmarco Ottavi, Enrico Zelioli, Nils Wistoff, Massimiliano Giacometti, Abdul Basit Sajjad, Luca Benini, Davide Rossi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Symmetric Multi-Processing (SMP) based on cache coherency is crucial for high-end embedded systems like automotive applications. RISC-V is gaining traction, and open-source hardware (OSH) platforms offer solutions to issues such as IP costs and vendor dependency. Existing multi-core cache-coherent RISC-V platforms are complex and not efficient for small embedded core clusters. We propose an open-source SystemVerilog implementation of a lightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our design uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with Splash-3 benchmarks, our solution shows up to 32.87% faster performance in a dual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized using GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of the system area.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-12T07:47:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SY</span><span>cs.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.19895v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.19895v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 Correct Wrong Path</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bhargav Reddy Godala, Sankara Prasad Ramesh, Krishnam Tibrewala, Chrysanthos Pepi, Gino Chacon, Svilen Kanev, Gilles A. Pokam, Daniel A. Jiménez, Paul V. Gratz, David I. August
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modern OOO CPUs have very deep pipelines with large branch misprediction recovery penalties. Speculatively executed instructions on the wrong path can significantly change cache state, depending on speculation levels. Architects often employ trace-driven simulation models in the design exploration stage, which sacrifice precision for speed. Trace-driven simulators are orders of magnitude faster than execution-driven models, reducing the often hundreds of thousands of simulation hours needed to explore new micro-architectural ideas. Despite this strong benefit of trace-driven simulation, these often fail to adequately model the consequences of wrong path because obtaining them is nontrivial. Prior works consider either a positive or negative impact of wrong path but not both. Here, we examine wrong path execution in simulation results and design a set of infrastructure for enabling wrong-path execution in a trace driven simulator. Our analysis shows the wrong path affects structures on both the instruction and data sides extensively, resulting in performance variations ranging from $-3.05$\% to $20.9$\% when ignoring wrong path. To benefit the research community and enhance the accuracy of simulators, we opened our traces and tracing utility in the hopes that industry can provide wrong-path traces generated by their internal simulators, enabling academic simulation without exposing industry IP.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-08-12T03:53:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.05912v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.05912v1' target='_blank'>pdf</a>
                </div>
            </div>
            <h1>Keyword: LLM Inference</h1>

            <div class="paper">
                <h2>#1 AraDiCE: Benchmarks for Dialectal and Cultural Capabilities in LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Basel Mousi, Nadir Durrani, Fatema Ahmad, Md. Arid Hasan, Maram Hasanain, Tameem Kabbani, Fahim Dalvi, Shammur Absar Chowdhury, Firoj Alam
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Arabic, with its rich diversity of dialects, remains significantly underrepresented in Large Language Models, particularly in dialectal variations. We address this gap by introducing seven synthetic datasets in dialects alongside Modern Standard Arabic (MSA), created using Machine Translation (MT) combined with human post-editing. We present AraDiCE, a benchmark for Arabic Dialect and Cultural Evaluation. We evaluate LLMs on dialect comprehension and generation, focusing specifically on low-resource Arabic dialects. Additionally, we introduce the first-ever fine-grained benchmark designed to evaluate cultural awareness across the Gulf, Egypt, and Levant regions, providing a novel dimension to LLM evaluation. Our findings demonstrate that while Arabic-specific models like Jais and AceGPT outperform multilingual models on dialectal tasks, significant challenges persist in dialect identification, generation, and translation. This work contributes ~45K post-edited samples, a cultural benchmark, and highlights the importance of tailored training to improve LLM performance in capturing the nuances of diverse Arabic dialects and cultural contexts. We will release the dialectal translation models and benchmarks curated in this study.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:59:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>68T50</span><span>F.2.2; I.2.7</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11404v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11404v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 UniLCD: Unified Local-Cloud Decision-Making via Reinforcement Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kathakoli Sengupta, Zhongkai Shagguan, Sandesh Bharadwaj, Sanjay Arora, Eshed Ohn-Bar, Renato Mancuso
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Embodied vision-based real-world systems, such as mobile robots, require a careful balance between energy consumption, compute latency, and safety constraints to optimize operation across dynamic tasks and contexts. As local computation tends to be restricted, offloading the computation, ie, to a remote server, can save local resources while providing access to high-quality predictions from powerful and large models. However, the resulting communication and latency overhead has led to limited usability of cloud models in dynamic, safety-critical, real-time settings. To effectively address this trade-off, we introduce UniLCD, a novel hybrid inference framework for enabling flexible local-cloud collaboration. By efficiently optimizing a flexible routing module via reinforcement learning and a suitable multi-task objective, UniLCD is specifically designed to support the multiple constraints of safety-critical end-to-end mobile systems. We validate the proposed approach using a challenging, crowded navigation task requiring frequent and timely switching between local and cloud operations. UniLCD demonstrates improved overall performance and efficiency, by over 35% compared to state-of-the-art baselines based on various split computing and early exit strategies.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:59:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11403v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11403v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 NVLM: Open Frontier-Class Multimodal LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuoling Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce NVLM 1.0, a family of frontier-class multimodal large language models (LLMs) that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models (e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved text-only performance over its LLM backbone after multimodal training. In terms of model design, we perform a comprehensive comparison between decoder-only multimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g., Flamingo). Based on the strengths and weaknesses of both approaches, we propose a novel architecture that enhances both training efficiency and multimodal reasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for tile-based dynamic high-resolution images, which significantly boosts performance on multimodal reasoning and OCR-related tasks. Regarding training data, we meticulously curate and provide detailed information on our multimodal pretraining and supervised fine-tuning datasets. Our findings indicate that dataset quality and task diversity are more important than scale, even during the pretraining phase, across all architectures. Notably, we develop production-grade multimodality for the NVLM-1.0 models, enabling them to excel in vision-language tasks while maintaining and even improving text-only performance compared to their LLM backbones. To achieve this, we craft and integrate a high-quality text-only dataset into multimodal training, alongside a substantial amount of multimodal math and reasoning data, leading to enhanced math and coding capabilities across modalities. To advance research in the field, we are releasing the model weights and will open-source the code for the community: https://nvlm-project.github.io/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:59:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.CV</span><span>cs.LG</span><span>cs.MM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11402v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11402v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 Teaching dark matter simulations to speak the halo language</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shivam Pandey, Francois Lanusse, Chirag Modi, Benjamin D. Wandelt
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We develop a transformer-based conditional generative model for discrete point objects and their properties. We use it to build a model for populating cosmological simulations with gravitationally collapsed structures called dark matter halos. Specifically, we condition our model with dark matter distribution obtained from fast, approximate simulations to recover the correct three-dimensional positions and masses of individual halos. This leads to a first model that can recover the statistical properties of the halos at small scales to better than 3% level using an accelerated dark matter simulation. This trained model can then be applied to simulations with significantly larger volumes which would otherwise be computationally prohibitive with traditional simulations, and also provides a crucial missing link in making end-to-end differentiable cosmological simulations. The code, named GOTHAM (Generative cOnditional Transformer for Halo's Auto-regressive Modeling) is publicly available at \url{https://github.com/shivampcosmo/GOTHAM}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:59:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.CO</span><span>astro-ph.IM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11401v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11401v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless
  Integration of Multi Active/Passive Core-Agents</h2>
                <div class="authors">
                    <strong>Authors:</strong> Amine B. Hassouna, Hana Chaari, Ines Belhaj
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The integration of tools in LLM-based agents overcame the difficulties of standalone LLMs and traditional agents' limited capabilities. However, the conjunction of these technologies and the proposed enhancements in several state-of-the-art works followed a non-unified software architecture resulting in a lack of modularity. Indeed, they focused mainly on functionalities and overlooked the definition of the component's boundaries within the agent. This caused terminological and architectural ambiguities between researchers which we addressed in this paper by proposing a unified framework that establishes a clear foundation for LLM-based agents' development from both functional and software architectural perspectives.   Our framework, LLM-Agent-UMF (LLM-based Agent Unified Modeling Framework), clearly distinguishes between the different components of an agent, setting LLMs, and tools apart from a newly introduced element: the core-agent, playing the role of the central coordinator of the agent which comprises five modules: planning, memory, profile, action, and security, the latter often neglected in previous works. Differences in the internal structure of core-agents led us to classify them into a taxonomy of passive and active types. Based on this, we proposed different multi-core agent architectures combining unique characteristics of various individual agents.   For evaluation purposes, we applied this framework to a selection of state-of-the-art agents, thereby demonstrating its alignment with their functionalities and clarifying the overlooked architectural aspects. Moreover, we thoroughly assessed four of our proposed architectures by integrating distinctive agents into hybrid active/passive core-agents' systems. This analysis provided clear insights into potential improvements and highlighted the challenges involved in the combination of specific agents.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:54:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span><span>cs.CR</span><span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11393v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11393v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 Says Who? Effective Zero-Shot Annotation of Focalization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rebecca M. M. Hicke, Yuri Bizzoni, Pascale Feldkamp, Ross Deans Kristensen-McLachlan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Focalization, the perspective through which narrative is presented, is encoded via a wide range of lexico-grammatical features and is subject to reader interpretation. Moreover, trained readers regularly disagree on interpretations, suggesting that this problem may be computationally intractable. In this paper, we provide experiments to test how well contemporary Large Language Models (LLMs) perform when annotating literary texts for focalization mode. Despite the challenging nature of the task, LLMs show comparable performance to trained human annotators in our experiments. We provide a case study working with the novels of Stephen King to demonstrate the usefulness of this approach for computational literary studies, illustrating how focalization can be studied at scale.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:50:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11390v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11390v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 Larger Language Models Don't Care How You Think: Why Chain-of-Thought
  Prompting Fails in Subjective Tasks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Georgios Chochlakis, Niyantha Maruthu Pandiyan, Kristina Lerman, Shrikanth Narayanan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the dominant technique for performing natural language tasks, as it does not require updating the model parameters with gradient-based methods. ICL promises to "adapt" the LLM to perform the present task at a competitive or state-of-the-art level at a fraction of the computational cost. ICL can be augmented by incorporating the reasoning process to arrive at the final label explicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting. However, recent work has found that ICL relies mostly on the retrieval of task priors and less so on "learning" to perform tasks, especially for complex subjective domains like emotion and morality, where priors ossify posterior predictions. In this work, we examine whether "enabling" reasoning also creates the same behavior in LLMs, wherein the format of CoT retrieves reasoning priors that remain relatively unchanged despite the evidence in the prompt. We find that, surprisingly, CoT indeed suffers from the same posterior collapse as ICL for larger language models. Code is avalaible at https://github.com/gchochla/cot-priors.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:42:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.06173v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.06173v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 Cooperative Online Learning for Multi-Agent System Control via Gaussian
  Processes with Event-Triggered Mechanism: Extended Version</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaobing Dai, Zewen Yang, Sihua Zhang, Di-Hua Zhai, Yuanqing Xia, Sandra Hirche
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the realm of the cooperative control of multi-agent systems (MASs) with unknown dynamics, Gaussian process (GP) regression is widely used to infer the uncertainties due to its modeling flexibility of nonlinear functions and the existence of a theoretical prediction error bound. Online learning, which involves incorporating newly acquired training data into Gaussian process models, promises to improve control performance by enhancing predictions during the operation. Therefore, this paper investigates the online cooperative learning algorithm for MAS control. Moreover, an event-triggered data selection mechanism, inspired by the analysis of a centralized event-trigger, is introduced to reduce the model update frequency and enhance the data efficiency. With the proposed learning-based control, the practical convergence of the MAS is validated with guaranteed tracking performance via the Lynaponve theory. Furthermore, the exclusion of the Zeno behavior for individual agents is shown. Finally, the effectiveness of the proposed event-triggered online learning method is demonstrated in simulations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:31:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SY</span><span>cs.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2304.05138v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2304.05138v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 Dated Data: Tracing Knowledge Cutoffs in Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jeffrey Cheng, Marc Marone, Orion Weller, Dawn Lawrie, Daniel Khashabi, Benjamin Van Durme
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Released Large Language Models (LLMs) are often paired with a claimed knowledge cutoff date, or the dates at which training data was gathered. Such information is crucial for applications where the LLM must provide up to date information. However, this statement only scratches the surface: do all resources in the training data share the same knowledge cutoff date? Does the model's demonstrated knowledge for these subsets closely align to their cutoff dates? In this work, we define the notion of an effective cutoff. This is distinct from the LLM designer reported cutoff and applies separately to sub-resources and topics. We propose a simple approach to estimate effective cutoffs on the resource-level temporal alignment of an LLM by probing across versions of the data. Using this analysis, we find that effective cutoffs often differ from reported cutoffs. To understand the root cause of this observation, we conduct a direct large-scale analysis on open pre-training datasets. Our analysis reveals two reasons for these inconsistencies: (1) temporal biases of CommonCrawl data due to non-trivial amounts of old data in new dumps and (2) complications in LLM deduplication schemes involving semantic duplicates and lexical near-duplicates. Overall, our results show that knowledge cutoffs are not as simple as they have seemed and that care must be taken both by LLM dataset curators as well as practitioners who seek to use information from these models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:25:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2403.12958v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2403.12958v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 Diversify and Conquer: Diversity-Centric Data Selection with Iterative
  Refinement</h2>
                <div class="authors">
                    <strong>Authors:</strong> Simon Yu, Liangyu Chen, Sara Ahmadian, Marzieh Fadaee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Finetuning large language models on instruction data is crucial for enhancing pre-trained knowledge and improving instruction-following capabilities. As instruction datasets proliferate, selecting optimal data for effective training becomes increasingly important. This work addresses the question: How can we determine the optimal subset of data for effective training? While existing research often emphasizes local criteria like instance quality for subset selection, we argue that a global approach focused on data diversity is more critical. Our method employs k-means clustering to ensure the selected subset effectively represents the full dataset. We propose an iterative refinement method inspired by active learning techniques to resample instances from clusters, reassessing each cluster's importance and sampling weight in every training iteration. This approach reduces the effect of outliers and automatically filters out clusters containing low-quality data. Through extensive evaluation across natural language reasoning, general world knowledge, code and math reasoning tasks, and by fine-tuning models from various families, we observe consistent improvements, achieving a 7% increase over random selection and a 3.8% improvement over state-of-the-art sampling methods. Our work highlights the significance of diversity-first sampling when finetuning LLMs to enhance performance across a broad array of evaluation tasks. Our code is available at https://github.com/for-ai/iterative-data-selection.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:25:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11378v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11378v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 Towards Time Series Reasoning with LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Winnie Chow, Lauren Gardiner, Haraldur T. Hallgrímsson, Maxwell A. Xu, Shirley You Ren
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multi-modal large language models (MLLMs) have enabled numerous advances in understanding and reasoning in domains like vision, but we have not yet seen this broad success for time-series. Although prior works on time-series MLLMs have shown promising performance in time-series forecasting, very few works show how an LLM could be used for time-series reasoning in natural language. We propose a novel multi-modal time-series LLM approach that learns generalizable information across various domains with powerful zero-shot performance. First, we train a lightweight time-series encoder on top of an LLM to directly extract time-series information. Then, we fine-tune our model with chain-of-thought augmented time-series tasks to encourage the model to generate reasoning paths. We show that our model learns a latent representation that reflects specific time-series features (e.g. slope, frequency), as well as outperforming GPT-4o on a set of zero-shot reasoning tasks on a variety of domains.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:23:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11376v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11376v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 Multi-OCT-SelfNet: Integrating Self-Supervised Learning with
  Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease
  Classification</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fatema-E- Jannat, Sina Gholami, Jennifer I. Lim, Theodore Leng, Minhaj Nur Alam, Hamed Tabkhi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the medical domain, acquiring large datasets poses significant challenges due to privacy concerns. Nonetheless, the development of a robust deep-learning model for retinal disease diagnosis necessitates a substantial dataset for training. The capacity to generalize effectively on smaller datasets remains a persistent challenge. The scarcity of data presents a significant barrier to the practical implementation of scalable medical AI solutions. To address this issue, we've combined a wide range of data sources to improve performance and generalization to new data by giving it a deeper understanding of the data representation from multi-modal datasets and developed a self-supervised framework based on large language models (LLMs), SwinV2 to gain a deeper understanding of multi-modal dataset representations, enhancing the model's ability to extrapolate to new data for the detection of eye diseases using optical coherence tomography (OCT) images. We adopt a two-phase training methodology, self-supervised pre-training, and fine-tuning on a downstream supervised classifier. An ablation study conducted across three datasets employing various encoder backbones, without data fusion, with low data availability setting, and without self-supervised pre-training scenarios, highlights the robustness of our method. Our findings demonstrate consistent performance across these diverse conditions, showcasing superior generalization capabilities compared to the baseline model, ResNet-50.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:22:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11375v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11375v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 Faster algorithms for the alignment of sparse correlated Erdös-Rényi
  random graphs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Andrea Muratori, Guilhem Semerjian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The correlated Erd\"os-R\'enyi random graph ensemble is a probability law on pairs of graphs with $n$ vertices, parametrized by their average degree $\lambda$ and their correlation coefficient $s$. It can be used as a benchmark for the graph alignment problem, in which the labels of the vertices of one of the graphs are reshuffled by an unknown permutation; the goal is to infer this permutation and thus properly match the pairs of vertices in both graphs. A series of recent works has unveiled the role of Otter's constant $\alpha$ (that controls the exponential rate of growth of the number of unlabeled rooted trees as a function of their sizes) in this problem: for $s>\sqrt{\alpha}$ and $\lambda$ large enough it is possible to recover in a time polynomial in $n$ a positive fraction of the hidden permutation. The exponent of this polynomial growth is however quite large and depends on the other parameters, which limits the range of applications of the algorithm. In this work we present a family of faster algorithms for this task, show through numerical simulations that their accuracy is only slightly reduced with respect to the original one, and conjecture that they undergo, in the large $\lambda$ limit, phase transitions at modified Otter's thresholds $\sqrt{\widehat{\alpha}}>\sqrt{\alpha}$, with $\widehat{\alpha}$ related to the enumeration of a restricted family of trees.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:20:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cond-mat.dis-nn</span><span>cs.DS</span><span>math.PR</span><span>math.ST</span><span>stat.TH</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2405.08421v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.08421v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 OSV: One Step is Enough for High-Quality Image to Video Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaofeng Mao, Zhengkai Jiang, Fu-Yun Wang, Wenbing Zhu, Jiangning Zhang, Hao Chen, Mingmin Chi, Yabiao Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Video diffusion models have shown great potential in generating high-quality videos, making them an increasingly popular focus. However, their inherent iterative nature leads to substantial computational and time costs. While efforts have been made to accelerate video diffusion by reducing inference steps (through techniques like consistency distillation) and GAN training (these approaches often fall short in either performance or training stability). In this work, we introduce a two-stage training framework that effectively combines consistency distillation with GAN training to address these challenges. Additionally, we propose a novel video discriminator design, which eliminates the need for decoding the video latents and improves the final performance. Our model is capable of producing high-quality videos in merely one-step, with the flexibility to perform multi-step refinement for further performance enhancement. Our quantitative evaluation on the OpenWebVid-1M benchmark shows that our model significantly outperforms existing methods. Notably, our 1-step performance(FVD 171.15) exceeds the 8-step performance of the consistency distillation based method, AnimateLCM (FVD 184.79), and approaches the 25-step performance of advanced Stable Video Diffusion (FVD 156.94).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:16:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11367v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11367v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 CoCA: Regaining Safety-awareness of Multimodal Large Language Models
  with Constitutional Calibration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiahui Gao, Renjie Pi, Tianyang Han, Han Wu, Lanqing Hong, Lingpeng Kong, Xin Jiang, Zhenguo Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The deployment of multimodal large language models (MLLMs) has demonstrated remarkable success in engaging in conversations involving visual inputs, thanks to the superior power of large language models (LLMs). Those MLLMs are typically built based on the LLMs, with an image encoder to process images into the token embedding space of the LLMs. However, the integration of visual modality has introduced a unique vulnerability: the MLLM becomes susceptible to malicious visual inputs and prone to generating sensitive or harmful responses, even though the LLM has been trained on textual dataset to align with human value. In this paper, we first raise the question: ``Do the MLLMs possess safety-awareness against malicious image inputs?". We find that after adding a principle that specifies the safety requirement into the input of the MLLM, the model's safety awareness becomes boosted. This phenomenon verifies the existence of MLLM's safety-awareness against image inputs, it is only weakened by the modality gap. We then introduce a simple yet effective technique termed CoCA, which amplifies the safety-awareness of the MLLM by calibrating its output distribution. Our proposed strategy helps the model reclaim its original safety awareness without losing its original capabilities. We verify the effectiveness of our approach on both multimodal safety and understanding benchmarks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:14:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11365v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11365v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 On the number of elements beyond the ones actually observed</h2>
                <div class="authors">
                    <strong>Authors:</strong> Eugenio Regazzini
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this work, a variant of the birth and death chain with constant intensities, originally introduced by   Bruno de Finetti way back in 1957, is revisited. This fact is also underlined by the choice of the title,   which is clearly a literal translation of the original one. Characteristic of the variant is that it   allows negative jumps of any magnitude. And this, as explained in the paper, might be useful in offering   some insight into the issue, arising in numerous situations, of inferring the number of the undetected   elements of a given population. One thinks, for example, of problems concerning abundance or richness of   species.   The author's purpose is twofold: to align the original de Finetti's construction with the modern,   well-established theory of the continuous-time Markov chains with discrete state space and show how it   could be used to make probabilistic previsions on the number of the unseen elements of a population.   With the aim of enhancing the possible practical applications of the model, one discusses the statistical   point estimation of the rates which characterize its infinitesimal description.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:13:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.PR</span><span>60J27 (Primary) 62F10 (Secondary)</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11364v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11364v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 AI Suggestions Homogenize Writing Toward Western Styles and Diminish
  Cultural Nuances</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dhruv Agarwal, Mor Naaman, Aditya Vashistha
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) are being increasingly integrated into everyday products and services, such as coding tools and writing assistants. As these embedded AI applications are deployed globally, there is a growing concern that the AI models underlying these applications prioritize Western values. This paper investigates what happens when a Western-centric AI model provides writing suggestions to users from a different cultural background. We conducted a cross-cultural controlled experiment with 118 participants from India and the United States who completed culturally grounded writing tasks with and without AI suggestions. Our analysis reveals that AI provided greater efficiency gains for Americans compared to Indians. Moreover, AI suggestions led Indian participants to adopt Western writing styles, altering not just what is written but also how it is written. These findings show that Western-centric AI models homogenize writing toward Western norms, diminishing nuances that differentiate cultural expression.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:07:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11360v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11360v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 Fine-Tuning Image-Conditional Diffusion Models is Easier than You Think</h2>
                <div class="authors">
                    <strong>Authors:</strong> Gonzalo Martin Garcia, Karim Abou Zeid, Christian Schmidt, Daan de Geus, Alexander Hermans, Bastian Leibe
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent work showed that large diffusion models can be reused as highly precise monocular depth estimators by casting depth estimation as an image-conditional image generation task. While the proposed model achieved state-of-the-art results, high computational demands due to multi-step inference limited its use in many scenarios. In this paper, we show that the perceived inefficiency was caused by a flaw in the inference pipeline that has so far gone unnoticed. The fixed model performs comparably to the best previously reported configuration while being more than 200$\times$ faster. To optimize for downstream task performance, we perform end-to-end fine-tuning on top of the single-step model with task-specific losses and get a deterministic model that outperforms all other diffusion-based depth and normal estimation models on common zero-shot benchmarks. We surprisingly find that this fine-tuning protocol also works directly on Stable Diffusion and achieves comparable performance to current state-of-the-art diffusion-based depth and normal estimation models, calling into question some of the conclusions drawn from prior works.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T16:58:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11355v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11355v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation
  in Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mengfei Liang, Archish Arun, Zekun Wu, Cristian Munoz, Jonathan Lutch, Emre Kazim, Adriano Koshiyama, Philip Treleaven
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Hallucination, the generation of factually incorrect content, is a growing challenge in Large Language Models (LLMs). Existing detection and mitigation methods are often isolated and insufficient for domain-specific needs, lacking a standardized pipeline. This paper introduces THaMES (Tool for Hallucination Mitigations and EvaluationS), an integrated framework and library addressing this gap. THaMES offers an end-to-end solution for evaluating and mitigating hallucinations in LLMs, featuring automated test set generation, multifaceted benchmarking, and adaptable mitigation strategies. It automates test set creation from any corpus, ensuring high data quality, diversity, and cost-efficiency through techniques like batch processing, weighted sampling, and counterfactual validation. THaMES assesses a model's ability to detect and reduce hallucinations across various tasks, including text generation and binary classification, applying optimal mitigation strategies like In-Context Learning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient Fine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge base of academic papers, political news, and Wikipedia reveal that commercial models like GPT-4o benefit more from RAG than ICL, while open-weight models like Llama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFT significantly enhances the performance of Llama-3.1-8B-Instruct in both evaluation tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T16:55:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11353v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11353v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 Clinical Validation of a Real-Time Machine Learning-based System for the
  Detection of Acute Myeloid Leukemia by Flow Cytometry</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lauren M. Zuromski, Jacob Durtschi, Aimal Aziz, Jeffrey Chumley, Mark Dewey, Paul English, Muir Morrison, Keith Simmon, Blaine Whipple, Brendan O'Fallon, David P. Ng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Machine-learning (ML) models in flow cytometry have the potential to reduce error rates, increase reproducibility, and boost the efficiency of clinical labs. While numerous ML models for flow cytometry data have been proposed, few studies have described the clinical deployment of such models. Realizing the potential gains of ML models in clinical labs requires not only an accurate model, but infrastructure for automated inference, error detection, analytics and monitoring, and structured data extraction. Here, we describe an ML model for detection of Acute Myeloid Leukemia (AML), along with the infrastructure supporting clinical implementation. Our infrastructure leverages the resilience and scalability of the cloud for model inference, a Kubernetes-based workflow system that provides model reproducibility and resource management, and a system for extracting structured diagnoses from full-text reports. We also describe our model monitoring and visualization platform, an essential element for ensuring continued model accuracy. Finally, we present a post-deployment analysis of impacts on turn-around time and compare production accuracy to the original validation statistics.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T16:53:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>q-bio.TO</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11350v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11350v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 Integrating ytopt and libEnsemble to Autotune OpenMC</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xingfu Wu, John R. Tramm, Jeffrey Larson, John-Luke Navarro, Prasanna Balaprakash, Brice Videau, Michael Kruse, Paul Hovland, Valerie Taylor, Mary Hall
                </div>
                <div class="summary">
                    <strong>Summary:</strong> ytopt is a Python machine-learning-based autotuning software package developed within the ECP PROTEAS-TUNE project. The ytopt software adopts an asynchronous search framework that consists of sampling a small number of input parameter configurations and progressively fitting a surrogate model over the input-output space until exhausting the user-defined maximum number of evaluations or the wall-clock time. libEnsemble is a Python toolkit for coordinating workflows of asynchronous and dynamic ensembles of calculations across massively parallel resources developed within the ECP PETSc/TAO project. libEnsemble helps users take advantage of massively parallel resources to solve design, decision, and inference problems and expands the class of problems that can benefit from increased parallelism. In this paper we present our methodology and framework to integrate ytopt and libEnsemble to take advantage of massively parallel resources to accelerate the autotuning process. Specifically, we focus on using the proposed framework to autotune the ECP ExaSMR application OpenMC, an open source Monte Carlo particle transport code. OpenMC has seven tunable parameters some of which have large ranges such as the number of particles in-flight, which is in the range of 100,000 to 8 million, with its default setting of 1 million. Setting the proper combination of these parameter values to achieve the best performance is extremely time-consuming. Therefore, we apply the proposed framework to autotune the MPI/OpenMP offload version of OpenMC based on a user-defined metric such as the figure of merit (FoM) (particles/s) or energy efficiency energy-delay product (EDP) on Crusher at Oak Ridge Leadership Computing Facility. The experimental results show that we achieve improvement up to 29.49\% in FoM and up to 30.44\% in EDP.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T16:52:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.09222v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.09222v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 MonoCoder: Domain-Specific Code Language Model for HPC Codes and Tasks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tal Kadosh, Niranjan Hasabnis, Vy A. Vo, Nadav Schneider, Neva Krien, Mihai Capota, Abdul Wasay, Nesreen Ahmed, Ted Willke, Guy Tamir, Yuval Pinter, Timothy Mattson, Gal Oren
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With easier access to powerful compute resources, there is a growing trend in AI for software development to develop large language models (LLMs) to address a variety of programming tasks. Even LLMs applied to tasks from the high-performance computing (HPC) domain are huge in size and demand expensive compute resources for training. This is partly because LLMs for HPC tasks are obtained by finetuning existing LLMs that support several natural and/or programming languages. We found this design choice confusing - why do we need LLMs trained on natural languages and programming languages unrelated to HPC for HPC-specific tasks? In this line of work, we aim to question choices made by existing LLMs by developing smaller language models (LMs) for specific domains - we call them domain-specific LMs. Specifically, we start with HPC as a domain and build an HPC-specific LM, named MonoCoder, which is orders of magnitude smaller than existing LMs but delivers better performance on non-HPC and HPC codes. Specifically, we pre-trained MonoCoder on an HPC-specific dataset (named HPCorpus) of C and C++ programs mined from GitHub. We evaluated the performance of MonoCoder against state-of-the-art multi-lingual LLMs. Results demonstrate that MonoCoder, although much smaller than existing LMs, outperforms other LLMs on normalized-perplexity tests (in relation to model size) while also delivering competing CodeBLEU scores for high-performance and parallel code generations. In other words, results suggest that MonoCoder understands HPC code better than state-of-the-art LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T16:29:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PL</span><span>cs.AI</span><span>cs.LG</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2312.13322v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2312.13322v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 CodePori: Large-Scale System for Autonomous Software Development Using
  Multi-Agent Technology</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zeeshan Rasheed, Malik Abdul Sami, Kai-Kristian Kemell, Muhammad Waseem, Mika Saari, Kari Systä, Pekka Abrahamsson
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Context: Large Language Models (LLMs) and Generative Pre-trained Transformers (GPTs) have transformed the field of Software Engineering (SE). Existing LLM-based multi-agent models have successfully addressed basic dialogue tasks. However, the potential of LLMs for more challenging tasks, such as automated code generation for large and complex projects, has been investigated in only a few existing works. Objective: This paper aims to investigate the potential of LLM-based agents in the software industry, particularly in enhancing productivity and reducing time-to-market for complex software solutions. Our primary objective is to gain insights into how these agents can fundamentally transform the development of large-scale software. Methods: We introduce CodePori, a novel system designed to automate code generation for large and complex software projects based on functional and non-functional requirements defined by stakeholders. To assess the proposed system performance, we utilized the HumanEval benchmark and manually tested the CodePori model, providing 20 different project descriptions as input and then evaluated the code accuracy by manually executing the code. Results: CodePori is able to generate running code for large-scale projects, aligned with the typical software development process. The HumanEval benchmark results indicate that CodePori improves code accuracy by 89%. A manual assessment conducted by the first author shows that the CodePori system achieved an accuracy rate of 85%. Conclusion: Based on the results, our conclusion is that proposed system demonstrates the transformative potential of LLM-based agents in SE, highlighting their practical applications and opening new opportunities for broader adoption in both industry and academia. Our project is publicly available at https://github.com/GPT-Laboratory/CodePori.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T15:57:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.01411v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.01411v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 Zero-resource Hallucination Detection for Text Generation via
  Graph-based Contextual Knowledge Triples Modeling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xinyue Fang, Zhen Huang, Zhiliang Tian, Minghui Fang, Ziyi Pan, Quntian Fang, Zhihua Wen, Hengyue Pan, Dongsheng Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> LLMs obtain remarkable performance but suffer from hallucinations. Most research on detecting hallucination focuses on the questions with short and concrete correct answers that are easy to check the faithfulness. Hallucination detections for text generation with open-ended answers are more challenging. Some researchers use external knowledge to detect hallucinations in generated texts, but external resources for specific scenarios are hard to access. Recent studies on detecting hallucinations in long text without external resources conduct consistency comparison among multiple sampled outputs. To handle long texts, researchers split long texts into multiple facts and individually compare the consistency of each pairs of facts. However, these methods (1) hardly achieve alignment among multiple facts; (2) overlook dependencies between multiple contextual facts. In this paper, we propose a graph-based context-aware (GCA) hallucination detection for text generations, which aligns knowledge facts and considers the dependencies between contextual knowledge triples in consistency comparison. Particularly, to align multiple facts, we conduct a triple-oriented response segmentation to extract multiple knowledge triples. To model dependencies among contextual knowledge triple (facts), we construct contextual triple into a graph and enhance triples' interactions via message passing and aggregating via RGCN. To avoid the omission of knowledge triples in long text, we conduct a LLM-based reverse verification via reconstructing the knowledge triples. Experiments show that our model enhances hallucination detection and excels all baselines.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-18T05:42:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11283v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11283v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 Leveraging Distillation Techniques for Document Understanding: A Case
  Study with FLAN-T5</h2>
                <div class="authors">
                    <strong>Authors:</strong> Marcel Lamott, Muhammad Armaghan Shakir
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The surge of digital documents in various formats, including less standardized documents such as business reports and environmental assessments, underscores the growing importance of Document Understanding. While Large Language Models (LLMs) have showcased prowess across diverse natural language processing tasks, their direct application to Document Understanding remains a challenge. Previous research has demonstrated the utility of LLMs in this domain, yet their significant computational demands make them challenging to deploy effectively. Additionally, proprietary Blackbox LLMs often outperform their open-source counterparts, posing a barrier to widespread accessibility. In this paper, we delve into the realm of document understanding, leveraging distillation methods to harness the power of large LLMs while accommodating computational limitations. Specifically, we present a novel approach wherein we distill document understanding knowledge from the proprietary LLM ChatGPT into FLAN-T5. Our methodology integrates labeling and curriculum-learning mechanisms to facilitate efficient knowledge transfer. This work contributes to the advancement of document understanding methodologies by offering a scalable solution that bridges the gap between resource-intensive LLMs and practical applications. Our findings underscore the potential of distillation techniques in facilitating the deployment of sophisticated language models in real-world scenarios, thereby fostering advancements in natural language processing and document comprehension domains.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T15:37:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11282v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11282v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 P-RAG: Progressive Retrieval Augmented Generation For Planning on
  Embodied Everyday Task</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weiye Xu, Min Wang, Wengang Zhou, Houqiang Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Embodied Everyday Task is a popular task in the embodied AI community, requiring agents to make a sequence of actions based on natural language instructions and visual observations. Traditional learning-based approaches face two challenges. Firstly, natural language instructions often lack explicit task planning. Secondly, extensive training is required to equip models with knowledge of the task environment. Previous works based on Large Language Model (LLM) either suffer from poor performance due to the lack of task-specific knowledge or rely on ground truth as few-shot samples. To address the above limitations, we propose a novel approach called Progressive Retrieval Augmented Generation (P-RAG), which not only effectively leverages the powerful language processing capabilities of LLMs but also progressively accumulates task-specific knowledge without ground-truth. Compared to the conventional RAG methods, which retrieve relevant information from the database in a one-shot manner to assist generation, P-RAG introduces an iterative approach to progressively update the database. In each iteration, P-RAG retrieves the latest database and obtains historical information from the previous interaction as experiential references for the current interaction. Moreover, we also introduce a more granular retrieval scheme that not only retrieves similar tasks but also incorporates retrieval of similar situations to provide more valuable reference experiences. Extensive experiments reveal that P-RAG achieves competitive results without utilizing ground truth and can even further improve performance through self-iterations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T15:29:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.CL</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11279v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11279v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 Hackphyr: A Local Fine-Tuned LLM Agent for Network Security Environments</h2>
                <div class="authors">
                    <strong>Authors:</strong> Maria Rigaki, Carlos Catania, Sebastian Garcia
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have shown remarkable potential across various domains, including cybersecurity. Using commercial cloud-based LLMs may be undesirable due to privacy concerns, costs, and network connectivity constraints. In this paper, we present Hackphyr, a locally fine-tuned LLM to be used as a red-team agent within network security environments. Our fine-tuned 7 billion parameter model can run on a single GPU card and achieves performance comparable with much larger and more powerful commercial models such as GPT-4. Hackphyr clearly outperforms other models, including GPT-3.5-turbo, and baselines, such as Q-learning agents in complex, previously unseen scenarios. To achieve this performance, we generated a new task-specific cybersecurity dataset to enhance the base model's capabilities. Finally, we conducted a comprehensive analysis of the agents' behaviors that provides insights into the planning abilities and potential shortcomings of such agents, contributing to the broader understanding of LLM-based agents in cybersecurity contexts
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T15:28:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11276v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11276v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 Task Arithmetic for Language Expansion in Speech Translation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yao-Fei Cheng, Hayato Futami, Yosuke Kashiwagi, Emiru Tsunoo, Wen Shen Teo, Siddhant Arora, Shinji Watanabe
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in large language models (LLMs) have gained interest in speech-text multimodal foundation models, achieving strong performance on instruction-based speech translation (ST). However, expanding language pairs from an existing instruction-tuned ST system is costly due to the necessity of re-training on a combination of new and previous datasets. We propose to expand new language pairs by merging the model trained on new language pairs and the existing model, using task arithmetic. We find that the direct application of task arithmetic for ST causes the merged model to fail to follow instructions; thus, generating translation in incorrect languages. To eliminate language confusion, we propose an augmented task arithmetic method that merges an additional language control model. It is trained to generate the correct target language token following the instructions. Our experiments demonstrate that our proposed language control model can achieve language expansion by eliminating language confusion. In our MuST-C and CoVoST-2 experiments, it shows up to 4.66 and 4.92 BLEU scores improvement, respectively. In addition, we demonstrate the use of our task arithmetic framework can expand to a language pair where neither paired ST training data nor a pre-trained ST model is available. We first synthesize the ST system from machine translation (MT) systems via task analogy, then merge the synthesized ST system to the existing ST model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T15:25:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11274v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11274v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 Performance of Cross-Validated Targeted Maximum Likelihood Estimation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Matthew J. Smith, Rachael V. Phillips, Camille Maringe, Miguel Angel Luque-Fernandez
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Background: Advanced methods for causal inference, such as targeted maximum likelihood estimation (TMLE), require certain conditions for statistical inference. However, in situations where there is not differentiability due to data sparsity or near-positivity violations, the Donsker class condition is violated. In such situations, TMLE variance can suffer from inflation of the type I error and poor coverage, leading to conservative confidence intervals. Cross-validation of the TMLE algorithm (CVTMLE) has been suggested to improve on performance compared to TMLE in settings of positivity or Donsker class violations. We aim to investigate the performance of CVTMLE compared to TMLE in various settings.   Methods: We utilised the data-generating mechanism as described in Leger et al. (2022) to run a Monte Carlo experiment under different Donsker class violations. Then, we evaluated the respective statistical performances of TMLE and CVTMLE with different super learner libraries, with and without regression tree methods.   Results: We found that CVTMLE vastly improves confidence interval coverage without adversely affecting bias, particularly in settings with small sample sizes and near-positivity violations. Furthermore, incorporating regression trees using standard TMLE with ensemble super learner-based initial estimates increases bias and variance leading to invalid statistical inference.   Conclusions: It has been shown that when using CVTMLE the Donsker class condition is no longer necessary to obtain valid statistical inference when using regression trees and under either data sparsity or near-positivity violations. We show through simulations that CVTMLE is much less sensitive to the choice of the super learner library and thereby provides better estimation and inference in cases where the super learner library uses more flexible candidates and is prone to overfitting.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-18T07:26:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>stat.AP</span><span>stat.ML</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11265v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11265v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 A Dynamical System View of Langevin-Based Non-Convex Sampling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mohammad Reza Karimi, Ya-Ping Hsieh, Andreas Krause
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Non-convex sampling is a key challenge in machine learning, central to non-convex optimization in deep learning as well as to approximate probabilistic inference. Despite its significance, theoretically there remain many important challenges: Existing guarantees (1) typically only hold for the averaged iterates rather than the more desirable last iterates, (2) lack convergence metrics that capture the scales of the variables such as Wasserstein distances, and (3) mainly apply to elementary schemes such as stochastic gradient Langevin dynamics. In this paper, we develop a new framework that lifts the above issues by harnessing several tools from the theory of dynamical systems. Our key result is that, for a large class of state-of-the-art sampling schemes, their last-iterate convergence in Wasserstein distances can be reduced to the study of their continuous-time counterparts, which is much better understood. Coupled with standard assumptions of MCMC sampling, our theory immediately yields the last-iterate Wasserstein convergence of many advanced sampling schemes such as proximal, randomized mid-point, and Runge-Kutta integrators. Beyond existing methods, our framework also motivates more efficient schemes that enjoy the same rigorous guarantees.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T15:03:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>math.PR</span><span>math.ST</span><span>stat.TH</span><span>62D05</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2210.13867v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2210.13867v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 Towards Novel Malicious Packet Recognition: A Few-Shot Learning Approach</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kyle Stein, Andrew A. Mahyari, Guillermo Francia III, Eman El-Sheikh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As the complexity and connectivity of networks increase, the need for novel malware detection approaches becomes imperative. Traditional security defenses are becoming less effective against the advanced tactics of today's cyberattacks. Deep Packet Inspection (DPI) has emerged as a key technology in strengthening network security, offering detailed analysis of network traffic that goes beyond simple metadata analysis. DPI examines not only the packet headers but also the payload content within, offering a thorough insight into the data traversing the network. This study proposes a novel approach that leverages a large language model (LLM) and few-shot learning to accurately recognizes novel, unseen malware types with few labels samples. Our proposed approach uses a pretrained LLM on known malware types to extract the embeddings from packets. The embeddings are then used alongside few labeled samples of an unseen malware type. This technique is designed to acclimate the model to different malware representations, further enabling it to generate robust embeddings for each trained and unseen classes. Following the extraction of embeddings from the LLM, few-shot learning is utilized to enhance performance with minimal labeled data. Our evaluation, which utilized two renowned datasets, focused on identifying malware types within network traffic and Internet of Things (IoT) environments. Our approach shows promising results with an average accuracy of 86.35% and F1-Score of 86.40% on different malware types across the two datasets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T15:02:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11254v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11254v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded
  Attributions and Learning to Refuse</h2>
                <div class="authors">
                    <strong>Authors:</strong> Maojia Song, Shang Hong Sim, Rishabh Bhardwaj, Hai Leong Chieu, Navonil Majumder, Soujanya Poria
                </div>
                <div class="summary">
                    <strong>Summary:</strong> LLMs are an integral part of retrieval-augmented generation (RAG) systems. While many studies focus on evaluating the quality of end-to-end RAG systems, there is a lack of research on understanding the appropriateness of an LLM for the RAG task. Thus, we introduce a new metric, Trust-Score, that provides a holistic evaluation of the trustworthiness of LLMs in an RAG framework. We show that various prompting methods, such as in-context learning, fail to adapt LLMs effectively to the RAG task. Thus, we propose Trust-Align, a framework to align LLMs for higher Trust-Score. LLaMA-3-8b, aligned with our method, significantly outperforms open-source LLMs of comparable sizes on ASQA (up 10.7), QAMPARI (up 29.2) and ELI5 (up 14.9). We release our code at: https://github.com/declare-lab/trust-align.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T14:47:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11242v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11242v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 ExploreSelf: Fostering User-driven Exploration and Reflection on
  Personal Challenges with Adaptive Guidance by Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Inhwa Song, SoHyun Park, Sachin R. Pendse, Jessica Lee Schleider, Munmun De Choudhury, Young-Ho Kim
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Expressing stressful experiences in words is proven to improve mental and physical health, but individuals often disengage with writing interventions as they struggle to organize their thoughts and emotions. Reflective prompts have been used to provide direction, and large language models (LLMs) have demonstrated the potential to provide tailored guidance. Current systems often limit users' flexibility to direct their reflections. We thus present ExploreSelf, an LLM-driven application designed to empower users to control their reflective journey. ExploreSelf allows users to receive adaptive support through dynamically generated questions. Through an exploratory study with 19 participants, we examine how participants explore and reflect on personal challenges using ExploreSelf. Our findings demonstrate that participants valued the balance between guided support and freedom to control their reflective journey, leading to deeper engagement and insight. Building on our findings, we discuss implications for designing LLM-driven tools that promote user empowerment through effective reflective practices.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T14:44:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span><span>cs.CL</span><span>H.5.2; I.2.7</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.09662v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.09662v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 LLM-as-a-Judge & Reward Model: What They Can and Cannot Do</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guijin Son, Hyunwoo Ko, Hoyoung Lee, Yewon Kim, Seunghyeok Hong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> LLM-as-a-Judge and reward models are widely used alternatives of multiple-choice questions or human annotators for large language model (LLM) evaluation. Their efficacy shines in evaluating long-form responses, serving a critical role as evaluators of leaderboards and as proxies to align LLMs via reinforcement learning. However, despite their popularity, their effectiveness outside of English remains largely unexplored. In this paper, we conduct a comprehensive analysis on automated evaluators, reporting key findings on their behavior in a non-English environment. First, we discover that English evaluation capabilities significantly influence language-specific capabilities, often more than the language proficiency itself, enabling evaluators trained in English to easily transfer their skills to other languages. Second, we identify critical shortcomings, where LLMs fail to detect and penalize errors, such as factual inaccuracies, cultural misrepresentations, and the presence of unwanted language. Finally, we release Kudge, the first non-English meta-evaluation dataset containing 5,012 human annotations in Korean.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T14:40:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11239v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11239v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 Evaluating the Impact of Compression Techniques on Task-Specific
  Performance of Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bishwash Khanal, Jeffery M. Capone
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) offer powerful capabilities but incur substantial computational costs, driving the need for efficient compression techniques. This study evaluates the impact of popular compression methods - Magnitude Pruning, SparseGPT, and Wanda - on the LLaMA-2-7B model, focusing on the trade-offs between model size reduction, downstream task performance, and the role of calibration data. Our findings reveal that while SparseGPT and Wanda preserve perplexity even at 50% sparsity, they suffer significant degradation on downstream tasks, highlighting the inadequacy of perplexity as the sole evaluation metric. To address this, we introduce Jensen-Shannon (JS) Divergence as a more comprehensive metric that captures nuanced changes in model behavior post-compression. We further demonstrate that task-specific calibration data significantly enhances the downstream performance of compressed models compared to general calibration data. This research underscores the necessity for diverse evaluation metrics and careful calibration data selection to fully understand the complexities of LLM compression and its implications for practical applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T14:34:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11233v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11233v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 Fast Analysis of the OpenAI O1-Preview Model in Solving Random K-SAT
  Problem: Does the LLM Solve the Problem Itself or Call an External SAT
  Solver?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Raffaele Marino
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this manuscript I present an analysis on the performance of OpenAI O1-preview model in solving random K-SAT instances for K$\in {2,3,4}$ as a function of $\alpha=M/N$ where $M$ is the number of clauses and $N$ is the number of variables of the satisfiable problem. I show that the model can call an external SAT solver to solve the instances, rather than solving them directly. Despite using external solvers, the model reports incorrect assignments as output. Moreover, I propose and present an analysis to quantify whether the OpenAI O1-preview model demonstrates a spark of intelligence or merely makes random guesses when outputting an assignment for a Boolean satisfiability problem.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T14:29:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cond-mat.dis-nn</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11232v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11232v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 EmPO: Emotion Grounding for Empathetic Response Generation through
  Preference Optimization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ondrej Sotolar, Vojtech Formanek, Alok Debnath, Allison Lahnala, Charles Welch, Lucie FLek
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Empathetic response generation is a desirable aspect of conversational agents, crucial for facilitating engaging and emotionally intelligent multi-turn conversations between humans and machines. Leveraging large language models for this task has shown promising results, yet challenges persist in ensuring both the empathetic quality of the responses and retention of the generalization performance of the models. We propose a novel approach where we construct theory-driven preference datasets based on emotion grounding and use them to align LLMs with preference optimization algorithms to address these challenges. To evaluate empathetic response generation, we employ the EmpatheticDialogues dataset, assessing empathy with the diff-Epitome and BERTscore metrics and with multi-dimensional human evaluation. Additionally, we measure diversity and emotional valence using feature-based methods. We also evaluate the impact of training on the generalization performance using the MMLU benchmark and tasks from the Open LLM Leaderboard. The results show that LLMs can be aligned for empathetic response generation by preference optimization while retaining their general performance and that emotion grounding can guide preference dataset creation. We make all datasets, source code, and models publicly available. https://github.com/justtherightsize/empo
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T14:24:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>68T50</span><span>I.2.7</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.19071v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.19071v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 Exploring ChatGPT-based Augmentation Strategies for Contrastive
  Aspect-based Sentiment Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lingling Xu, Haoran Xie, S. Joe Qin, Fu Lee Wang, Xiaohui Tao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Aspect-based sentiment analysis (ABSA) involves identifying sentiment towards specific aspect terms in a sentence and allows us to uncover nuanced perspectives and attitudes on particular aspects of a product, service, or topic. However, the scarcity of labeled data poses a significant challenge to training high-quality models. To address this issue, we explore the potential of data augmentation using ChatGPT, a well-performing large language model (LLM), to enhance the sentiment classification performance towards aspect terms. Specifically, we explore three data augmentation strategies based on ChatGPT: context-focused, aspect-focused, and context-aspect data augmentation techniques. Context-focused data augmentation focuses on changing the word expression of context words in the sentence while keeping aspect terms unchanged. In contrast, aspect-focused data augmentation aims to change aspect terms but keep context words unchanged. Context-Aspect data augmentation integrates the above two data augmentations to generate augmented samples. Furthermore, we incorporate contrastive learning into the ABSA tasks to improve performance. Extensive experiments show that all three data augmentation techniques lead to performance improvements, with the context-aspect data augmentation strategy performing best and surpassing the performance of the baseline models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T14:12:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11218v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11218v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 Ideal-LLM: Integrating Dual Encoders and Language-Adapted LLM for
  Multilingual Speech-to-Text</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hongfei Xue, Wei Ren, Xuelong Geng, Kun Wei, Longhao Li, Qijie Shao, Linju Yang, Kai Diao, Lei Xie
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Integrating audio encoders with LLMs through connectors has enabled these models to process and comprehend audio modalities, significantly enhancing speech-to-text tasks, including automatic speech recognition (ASR) and automatic speech translation (AST). However, these methods often overlook the critical aspect of language adaptation in multilingual settings, relying instead on multilingual data without adequately addressing language differences. To address this gap, we propose the Ideal-LLM model, which employs dual multilingual encoders to enrich language feature information and utilizes a language-adapted connector to target the adaptation of each language specifically. By leveraging the complementary strengths of Whisper and MMS encoders, our approach ensures richer multilingual representations. Additionally, the language-adapted connector enhances modal transformation via a language weight selector tailored for each language. Experimental results demonstrate that Ideal-LLM significantly improves ASR performance, achieving a 32.6% relative reduction in average word error rates compared to the standard speech encoder integrated with LLMs and yields an average BLEU score of 36.78 for AST task.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T14:10:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.AS</span><span>cs.SD</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11214v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11214v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 Self-Evolutionary Large Language Models through Uncertainty-Enhanced
  Preference Optimization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jianing Wang, Yang Zhou, Xiaocheng Zhang, Mengjiao Bao, Peng Yan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Iterative preference optimization has recently become one of the de-facto training paradigms for large language models (LLMs), but the performance is still underwhelming due to too much noisy preference data yielded in the loop. To combat this issue, we present an \textbf{U}ncertainty-enhanced \textbf{P}reference \textbf{O}ptimization (UPO) framework to make the LLM self-evolve with reliable feedback. The key idea is mitigating the noisy preference data derived from the current policy and reward models by performing pair-wise uncertainty estimation and judiciously reliable feedback sampling. To reach this goal, we thus introduce an estimator model, which incorporates Monte Carlo (MC) dropout in Bayesian neural network (BNN) to perform uncertainty estimation for the preference data derived from the LLM policy. Compared to the existing methods that directly filter generated responses based on the reward score, the estimator focuses on the model uncertainty in a pair-wise manner and effectively bypasses the confirmation bias problem of the reward model. Additionally, we also propose an uncertainty-enhanced self-evolution algorithm to improve the robustness of preference optimization and encourage the LLM to generate responses with both high reward and certainty. Extensive experiments over multiple benchmarks demonstrate that our framework substantially alleviates the noisy problem and improves the performance of iterative preference optimization.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T14:05:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11212v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11212v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 Can Many-Shot In-Context Learning Help LLMs as Evaluators? A Preliminary
  Empirical Study</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mingyang Song, Mao Zheng, Xuan Luo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Utilizing Large Language Models (LLMs) as evaluators for evaluating the performance of LLMs has recently garnered attention. However, this kind of evaluation approach is affected by potential biases in LLMs, raising concerns about the accuracy and reliability of the evaluation results. To mitigate this issue, we propose and study two many-shot ICL prompts, which rely on two versions of many-shot ICL prompt templates for helping LLM evaluators to mitigate the potential biases in LLMs, \textbf{M}any-\textbf{S}hot \textbf{w}ith \textbf{R}eference (\textbf{MSwR}) and \textbf{M}any-\textbf{S}hot with\textbf{o}ut \textbf{R}eference (\textbf{MSoR}). Concretely, the former utilizes in-context examples with model-generated rationales as guidance, and the latter without. Based on the designed prompts, we investigate the impact of scaling the number of in-context examples on the consistency and quality of the evaluation results. Experimental results show that advanced LLMs, such as GPT-4o, perform better in the many-shot regime than in the zero-shot regime. Furthermore, we reveal the symbol bias hidden in the selection bias of LLMs and propose a simple yet effective approach to mitigate the bias. Experimental results further verify the effectiveness of the symbol bias mitigation approach.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T14:04:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.11629v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.11629v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 A Review of Prominent Paradigms for LLM-Based Agents: Tool Use
  (Including RAG), Planning, and Feedback Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xinzhe Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Tool use, planning, and feedback learning are currently three prominent paradigms for developing Large Language Model (LLM)-based agents across various tasks. Although numerous frameworks have been devised for each paradigm, their intricate workflows and inconsistent taxonomy create challenges in understanding and reviewing the frameworks across different paradigms. This survey introduces a unified taxonomy to systematically review and discuss these frameworks. Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on LMPR implementations and workflow usage across different agent paradigms.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T14:02:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.05804v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.05804v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 Diversifying the Expert Knowledge for Task-Agnostic Pruning in Sparse
  Mixture-of-Experts</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zeliang Zhang, Xiaodong Liu, Hao Cheng, Chenliang Xu, Jianfeng Gao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> By increasing model parameters but activating them sparsely when performing a task, the use of Mixture-of-Experts (MoE) architecture significantly improves the performance of Large Language Models (LLMs) without increasing the inference cost. However, the memory consumption due to the growing number of experts presents a challenge to the deployment of these models in many real world settings. Our empirical study reveals that some experts encode redundant knowledge during pre-training. We thus propose a method of grouping and pruning similar experts to improve the model's parameter efficiency. We validate the effectiveness of our method by pruning three state-of-the-art MoE architectures, including Mixtral, Deepseek-MoE, and Qwen. The evaluation shows that our method outperforms other model pruning methods on a range of natural language tasks. We will release our code to facilitate future research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T13:48:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.09590v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.09590v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 SuperCoder2.0: Technical Report on Exploring the feasibility of LLMs as
  Autonomous Programmer</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anmol Gautam, Kishore Kumar, Adarsh Jha, Mukunda NS, Ishaan Bhola
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present SuperCoder2.0, an advanced autonomous system designed to enhance software development through artificial intelligence. The system combines an AI-native development approach with intelligent agents to enable fully autonomous coding. Key focus areas include a retry mechanism with error output traceback, comprehensive code rewriting and replacement using Abstract Syntax Tree (ast) parsing to minimize linting issues, code embedding technique for retrieval-augmented generation, and a focus on localizing methods for problem-solving rather than identifying specific line numbers. The methodology employs a three-step hierarchical search space reduction approach for code base navigation and bug localization:utilizing Retrieval Augmented Generation (RAG) and a Repository File Level Map to identify candidate files, (2) narrowing down to the most relevant files using a File Level Schematic Map, and (3) extracting 'relevant locations' within these files. Code editing is performed through a two-part module comprising CodeGeneration and CodeEditing, which generates multiple solutions at different temperature values and replaces entire methods or classes to maintain code integrity. A feedback loop executes repository-level test cases to validate and refine solutions. Experiments conducted on the SWE-bench Lite dataset demonstrate SuperCoder2.0's effectiveness, achieving correct file localization in 84.33% of cases within the top 5 candidates and successfully resolving 34% of test instances. This performance places SuperCoder2.0 fourth globally on the SWE-bench leaderboard. The system's ability to handle diverse repositories and problem types highlights its potential as a versatile tool for autonomous software development. Future work will focus on refining the code editing process and exploring advanced embedding models for improved natural language to code mapping.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T13:44:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11190v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11190v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 Annealed Winner-Takes-All for Motion Forecasting</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yihong Xu, Victor Letzelter, Mickaël Chen, Éloi Zablocki, Matthieu Cord
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In autonomous driving, motion prediction aims at forecasting the future trajectories of nearby agents, helping the ego vehicle to anticipate behaviors and drive safely. A key challenge is generating a diverse set of future predictions, commonly addressed using data-driven models with Multiple Choice Learning (MCL) architectures and Winner-Takes-All (WTA) training objectives. However, these methods face initialization sensitivity and training instabilities. Additionally, to compensate for limited performance, some approaches rely on training with a large set of hypotheses, requiring a post-selection step during inference to significantly reduce the number of predictions. To tackle these issues, we take inspiration from annealed MCL, a recently introduced technique that improves the convergence properties of MCL methods through an annealed Winner-Takes-All loss (aWTA). In this paper, we demonstrate how the aWTA loss can be integrated with state-of-the-art motion forecasting models to enhance their performance using only a minimal set of hypotheses, eliminating the need for the cumbersome post-selection step. Our approach can be easily incorporated into any trajectory prediction model normally trained using WTA and yields significant improvements. To facilitate the application of our approach to future motion forecasting models, the code will be made publicly available upon acceptance: https://github.com/valeoai/MF_aWTA.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-18T09:35:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11172v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11172v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 Fast radio bursts as a probe of gravity on cosmological scales</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dennis Neumann, Robert Reischke, Steffen Hagstotz, Hendrik Hildebrandt
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We explore the potential for improving constraints on gravity by leveraging correlations in the dispersion measure derived from Fast Radio Bursts (FRBs) in combination with cosmic shear. Specifically, we focus on Horndeski gravity, inferring the kinetic braiding and Planck mass run rate from a stage-4 cosmic shear mock survey alongside a survey comprising $10^4$ FRBs. For the inference pipeline, we utilise hi_class to predict the linear matter power spectrum in modified gravity scenarios, while non-linear corrections are modelled with HMcode, including feedback mechanisms. Our findings indicate that FRBs can disentangle degeneracies between baryonic feedback and cosmological parameters, as well as the mass of massive neutrinos. Since these parameters are also degenerate with modified gravity parameters, the inclusion of FRBs can enhance constraints on Horndeski parameters by up to $40$ percent, despite being a less significant measurement. Additionally, we apply our model to current FRB data and use the uncertainty in the $\mathrm{DM}-z$ relation to impose limits on gravity. However, due to the limited sample size of current data, constraints are predominantly influenced by theoretical priors. Despite this, our study demonstrates that FRBs will significantly augment the limited set of cosmological probes available, playing a critical role in providing alternative tests of feedback, cosmology, and gravity. All codes used in this work are made publically available.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T13:17:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.CO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11163v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11163v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 Chasing Shadows: How Implausible Assumptions Skew Our Understanding of
  Causal Estimands</h2>
                <div class="authors">
                    <strong>Authors:</strong> Stijn Vansteelandt, Kelly Van Lancker
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The ICH E9 (R1) addendum on estimands, coupled with recent advancements in causal inference, has prompted a shift towards using model-free treatment effect estimands that are more closely aligned with the underlying scientific question. This represents a departure from traditional, model-dependent approaches where the statistical model often overshadows the inquiry itself. While this shift is a positive development, it has unintentionally led to the prioritization of an estimand's theoretical appeal over its practical learnability from data under plausible assumptions. We illustrate this by scrutinizing assumptions in the recent clinical trials literature on principal stratum estimands, demonstrating that some popular assumptions are not only implausible but often inevitably violated. We advocate for a more balanced approach to estimand formulation, one that carefully considers both the scientific relevance and the practical feasibility of estimation under realistic conditions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T13:16:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11162v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11162v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 Reasoning Graph Enhanced Exemplars Retrieval for In-Context Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yukang Lin, Bingchen Zhong, Shuoran Jiang, Joanna Siebert, Qingcai Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models(LLMs) have exhibited remarkable few-shot learning capabilities and unified the paradigm of NLP tasks through the in-context learning(ICL) technique. Despite the success of ICL, the quality of the exemplar demonstrations can significantly influence the LLM's performance. Existing exemplar selection methods mainly focus on the semantic similarity between queries and candidate exemplars. On the other hand, the logical connections between reasoning steps can be beneficial to depict the problem-solving process as well. In this paper, we proposes a novel method named Reasoning Graph-enhanced Exemplar Retrieval(RGER). RGER first quires LLM to generate an initial response, then expresses intermediate problem-solving steps to a graph structure. After that, it employs graph kernel to select exemplars with semantic and structural similarity. Extensive experiments demonstrate the structural relationship is helpful to the alignment of queries and candidate exemplars. The efficacy of RGER on math and logit reasoning tasks showcases its superiority over state-of-the-art retrieval-based approaches. Our code is released at https://github.com/Yukang-Lin/RGER.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T12:58:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11147v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11147v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 E-Values for Exponential Families: the General Case</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yunda Hao, Peter Grünwald
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We analyze common types of e-variables and e-processes for composite exponential family nulls: the optimal e-variable based on the reverse information projection (RIPr), the conditional (COND) e-variable, and the universal inference (UI) and sequen\-tialized RIPr e-processes. We characterize the RIPr prior for simple and Bayes-mixture based alternatives, either precisely (for Gaussian nulls and alternatives) or in an approximate sense (general exponential families). We provide conditions under which the RIPr e-variable is (again exactly vs. approximately) equal to the COND e-variable. Based on these and other interrelations which we establish, we determine the e-power of the four e-statistics as a function of sample size, exactly for Gaussian and up to $o(1)$ in general. For $d$-dimensional null and alternative, the e-power of UI tends to be smaller by a term of $(d/2) \log n + O(1)$ than that of the COND e-variable, which is the clear winner.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T12:37:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11134v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11134v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 The Invalsi Benchmarks: measuring Linguistic and Mathematical
  understanding of Large Language Models in Italian</h2>
                <div class="authors">
                    <strong>Authors:</strong> Giovanni Puccetti, Maria Cassese, Andrea Esuli
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While Italian is a high-resource language, there are few Italian-native benchmarks to evaluate generative Large Language Models (LLMs) in this language. This work presents three new benchmarks: Invalsi MATE to evaluate models performance on mathematical understanding in Italian, Invalsi ITA to evaluate language understanding in Italian and Olimpiadi MATE for more complex mathematical understanding.   The first two benchmarks are based on the Invalsi tests, which are administered to students of age between 6 and 18 within the Italian school system and have been validated by several experts in teaching and pedagogy, the third one comes from the Italian high school math Olympics.   We evaluate 10 powerful language models on these benchmarks and find that they are bound by 71% accuracy on Invasli MATE, achieved by Llama 3.1 70b instruct and by 88% on Invalsi ITA. For both Invalsi MATE and Invalsi ITA we compare LLMs with the average performance of Italian students to show that Llama 3.1 is the only one to outperform them on Invalsi MATE while most models do so on Invalsi ITA, we then show that Olimpiadi MATE is more challenging than Invalsi MATE and the highest accuracy, achieved by Llama 3.1 405b instruct is 45%.   We will make data and evaluation code openly available upon acceptance of the paper.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T12:27:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2403.18697v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2403.18697v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 Schrodinger's Memory: Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wei Wang, Qing Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Memory is the foundation of all human activities; without memory, it would be nearly impossible for people to perform any task in daily life. With the development of Large Language Models (LLMs), their language capabilities are becoming increasingly comparable to those of humans. But do LLMs have memory? Based on current performance, LLMs do appear to exhibit memory. So, what is the underlying mechanism of this memory? Previous research has lacked a deep exploration of LLMs' memory capabilities and the underlying theory. In this paper, we use Universal Approximation Theorem (UAT) to explain the memory mechanism in LLMs. We also conduct experiments to verify the memory capabilities of various LLMs, proposing a new method to assess their abilities based on these memory ability. We argue that LLM memory operates like Schr\"odinger's memory, meaning that it only becomes observable when a specific memory is queried. We can only determine if the model retains a memory based on its output in response to the query; otherwise, it remains indeterminate. Finally, we expand on this concept by comparing the memory capabilities of the human brain and LLMs, highlighting the similarities and differences in their operational mechanisms.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T12:10:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10482v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10482v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 Diversity-grounded Channel Prototypical Learning for Out-of-Distribution
  Intent Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bo Liu, Liming Zhan, Yujie Feng, Zexin Lu, Chengqiang Xie, Lei Xue, Xiao-Ming Wu, Albert Y. S. Lam
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the realm of task-oriented dialogue systems, a robust intent detection mechanism must effectively handle malformed utterances encountered in real-world scenarios. This study presents a novel fine-tuning framework for large language models (LLMs) aimed at enhancing in-distribution (ID) intent classification and out-of-distribution (OOD) intent detection, which utilizes semantic matching with prototypes derived from ID class names. By harnessing the highly distinguishable representations of LLMs, we construct semantic prototypes for each ID class using a diversity-grounded prompt tuning approach. We rigorously test our framework in a challenging OOD context, where ID and OOD classes are semantically close yet distinct, referred to as \emph{near} OOD detection. For a thorough assessment, we benchmark our method against the prevalent fine-tuning approaches. The experimental findings reveal that our method demonstrates superior performance in both few-shot ID intent classification and near-OOD intent detection tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T12:07:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11114v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11114v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 Situated Understanding of Errors in Older Adults' Interactions with
  Voice Assistants: A Month-Long, In-Home Study</h2>
                <div class="authors">
                    <strong>Authors:</strong> Amama Mahmood, Junxiang Wang, Chien-Ming Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Our work addresses the challenges older adults face with commercial Voice Assistants (VAs), notably in conversation breakdowns and error handling. Traditional methods of collecting user experiences-usage logs and post-hoc interviews-do not fully capture the intricacies of older adults' interactions with VAs, particularly regarding their reactions to errors. To bridge this gap, we equipped 15 older adults' homes with smart speakers integrated with custom audio recorders to collect ``in-the-wild'' audio interaction data for detailed error analysis. Recognizing the conversational limitations of current VAs, our study also explored the capabilities of Large Language Models (LLMs) to handle natural and imperfect text for improving VAs. Midway through our study, we deployed ChatGPT-powered VA to investigate its efficacy for older adults. Our research suggests leveraging vocal and verbal responses combined with LLMs' contextual capabilities for enhanced error prevention and management in VAs, while proposing design considerations to align VA capabilities with older adults' expectations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T12:07:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2403.02421v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2403.02421v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 Unveiling Entity-Level Unlearning for Large Language Models: A
  Comprehensive Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weitao Ma, Xiaocheng Feng, Weihong Zhong, Lei Huang, Yangfan Ye, Xiachong Feng, Bing Qin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language model unlearning has garnered increasing attention due to its potential to address security and privacy concerns, leading to extensive research in the field. However, much of this research has concentrated on instance-level unlearning, specifically targeting the removal of predefined instances containing sensitive content. This focus has left a significant gap in the exploration of full entity-level unlearning, which is critical in real-world scenarios such as copyright protection. To this end, we propose a novel task of Entity-level unlearning, which aims to erase entity-related knowledge from the target model completely. To thoroughly investigate this task, we systematically evaluate trending unlearning algorithms, revealing that current methods struggle to achieve effective entity-level unlearning. Then, we further explore the factors that influence the performance of the unlearning algorithms, identifying that knowledge coverage and the size of the forget set play pivotal roles. Notably, our analysis also uncovers that entities introduced through fine-tuning are more vulnerable to unlearning than pre-trained entities. These findings collectively offer valuable insights for advancing entity-level unlearning for LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T12:00:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.15796v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.15796v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 Depth-based Privileged Information for Boosting 3D Human Pose Estimation
  on RGB</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alessandro Simoni, Francesco Marchetti, Guido Borghi, Federico Becattini, Davide Davoli, Lorenzo Garattoni, Gianpiero Francesca, Lorenzo Seidenari, Roberto Vezzani
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Despite the recent advances in computer vision research, estimating the 3D human pose from single RGB images remains a challenging task, as multiple 3D poses can correspond to the same 2D projection on the image. In this context, depth data could help to disambiguate the 2D information by providing additional constraints about the distance between objects in the scene and the camera. Unfortunately, the acquisition of accurate depth data is limited to indoor spaces and usually is tied to specific depth technologies and devices, thus limiting generalization capabilities. In this paper, we propose a method able to leverage the benefits of depth information without compromising its broader applicability and adaptability in a predominantly RGB-camera-centric landscape. Our approach consists of a heatmap-based 3D pose estimator that, leveraging the paradigm of Privileged Information, is able to hallucinate depth information from the RGB frames given at inference time. More precisely, depth information is used exclusively during training by enforcing our RGB-based hallucination network to learn similar features to a backbone pre-trained only on depth data. This approach proves to be effective even when dealing with limited and small datasets. Experimental results reveal that the paradigm of Privileged Information significantly enhances the model's performance, enabling efficient extraction of depth information by using only RGB images.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T11:59:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11104v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11104v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 Vetting quark-star models with gravitational waves in the hierarchical
  Bayesian framework</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ziming Wang, Yong Gao, Dicong Liang, Junjie Zhao, Lijing Shao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The recent discovery of gravitational waves (GWs) has opened a new avenue for investigating the equation of state (EOS) of dense matter in compact stars, which is an outstanding problem in astronomy and nuclear physics. In the future, next-generation (XG) GW detectors will be constructed, deemed to provide a large number of high-precision observations. We investigate the potential of constraining the EOS of quark stars (QSs) with high-precision measurements of mass $m$ and tidal deformability $\Lambda$ from the XG GW observatories. We adopt the widely-used bag model for QSs, consisting of four microscopic parameters: the effective bag constant $B_{\rm eff}$, the perturbative quantum chromodynamics correction parameter $a_4$, the strange quark mass $m_s$, and the pairing energy gap $\Delta$. With the help of hierarchical Bayesian inference, for the first time we are able to infer the EOS of QSs combining multiple GW observations. Using the top 25 loudest GW events in our simulation, we find that, the constraints on $B_{\rm eff}$ and $\Delta$ are tightened by several times, while $a_4$ and $m_s$ are still poorly constrained. We also study a simplified 2-dimensional (2-d) EOS model which was recently proposed in literature. The 2-d model is found to exhibit significant parameter-estimation biases as more GW events are analyzed, while the predicted $m$-$\Lambda$ relation remains consistent with the full model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T11:56:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.HE</span><span>gr-qc</span><span>nucl-th</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11103v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11103v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 Beyond principlism: Practical strategies for ethical AI use in research
  practices</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhicheng Lin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid adoption of generative artificial intelligence (AI) in scientific research, particularly large language models (LLMs), has outpaced the development of ethical guidelines, leading to a Triple-Too problem: too many high-level ethical initiatives, too abstract principles lacking contextual and practical relevance, and too much focus on restrictions and risks over benefits and utilities. Existing approaches, including principlism (reliance on abstract ethical principles), formalism (rigid application of rules), and technical solutionism (overemphasis on technological fixes), offer little practical guidance for addressing ethical challenges of AI in scientific research practices. To bridge the gap between abstract principles and day-to-day research practices, a user-centered, realism-inspired approach is proposed here. It outlines five specific goals for ethical AI use: 1) understanding model training and output, including bias mitigation strategies; 2) respecting privacy, confidentiality, and copyright; 3) avoiding plagiarism and policy violations; 4) applying AI beneficially compared to alternatives; and 5) using AI transparently and reproducibly. Each goal is accompanied by actionable strategies and realistic cases of misuse and corrective measures. I argue that ethical AI application requires evaluating its utility against existing alternatives rather than isolated performance metrics. Additionally, I propose documentation guidelines to enhance transparency and reproducibility in AI-assisted research. Moving forward, we need targeted professional development, training programs, and balanced enforcement mechanisms to promote responsible AI use while fostering innovation. By refining these ethical guidelines and adapting them to emerging AI capabilities, we can accelerate scientific progress without compromising research integrity.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-18T02:03:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2401.15284v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2401.15284v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 Reactive Environments for Active Inference Agents with RxEnvironments.jl</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wouter W. L. Nuijten, Bert de Vries
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Active Inference is a framework that emphasizes the interaction between agents and their environment. While the framework has seen significant advancements in the development of agents, the environmental models are often borrowed from reinforcement learning problems, which may not fully capture the complexity of multi-agent interactions or allow complex, conditional communication. This paper introduces Reactive Environments, a comprehensive paradigm that facilitates complex multi-agent communication. In this paradigm, both agents and environments are defined as entities encapsulated by boundaries with interfaces. This setup facilitates a robust framework for communication in nonequilibrium-Steady-State systems, allowing for complex interactions and information exchange. We present a Julia package RxEnvironments.jl, which is a specific implementation of Reactive Environments, where we utilize a Reactive Programming style for efficient implementation. The flexibility of this paradigm is demonstrated through its application to several complex, multi-agent environments. These case studies highlight the potential of Reactive Environments in modeling sophisticated systems of interacting agents.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T11:34:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SY</span><span>cs.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11087v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11087v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 Counterfactual contrastive learning: robust representations via causal
  image synthesis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Melanie Roschewitz, Fabio De Sousa Ribeiro, Tian Xia, Galvin Khara, Ben Glocker
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Contrastive pretraining is well-known to improve downstream task performance and model generalisation, especially in limited label settings. However, it is sensitive to the choice of augmentation pipeline. Positive pairs should preserve semantic information while destroying domain-specific information. Standard augmentation pipelines emulate domain-specific changes with pre-defined photometric transformations, but what if we could simulate realistic domain changes instead? In this work, we show how to utilise recent progress in counterfactual image generation to this effect. We propose CF-SimCLR, a counterfactual contrastive learning approach which leverages approximate counterfactual inference for positive pair creation. Comprehensive evaluation across five datasets, on chest radiography and mammography, demonstrates that CF-SimCLR substantially improves robustness to acquisition shift with higher downstream performance on both in- and out-of-distribution data, particularly for domains which are under-represented during training.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T11:25:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2403.09605v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2403.09605v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 Large language models can replicate cross-cultural differences in
  personality</h2>
                <div class="authors">
                    <strong>Authors:</strong> Paweł Niszczota, Mateusz Janczak, Michał Misiak
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We use a large-scale experiment (N=8000) to determine whether GPT-4 can replicate cross-cultural differences in the Big Five, measured using the Ten-Item Personality Inventory. We used the US and South Korea as the cultural pair, given that prior research suggests substantial personality differences between people from these two countries. We manipulated the target of the simulation (US vs. Korean), the language of the inventory (English vs. Korean), and the language model (GPT-4 vs. GPT-3.5). Our results show that GPT-4 replicated the cross-cultural differences for each factor. However, mean ratings had an upward bias and exhibited lower variation than in the human samples, as well as lower structural validity. We provide preliminary evidence that LLMs can aid cross-cultural researchers and practitioners.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T10:47:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.CY</span><span>I.2.7; K.4.2; J.4</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2310.10679v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2310.10679v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 KVPruner: Structural Pruning for Faster and Memory-Efficient Large
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bo Lv, Quan Zhou, Xuanang Ding, Yan Wang, Zeming Ma
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The bottleneck associated with the key-value(KV) cache presents a significant challenge during the inference processes of large language models. While depth pruning accelerates inference, it requires extensive recovery training, which can take up to two weeks. On the other hand, width pruning retains much of the performance but offers slight speed gains. To tackle these challenges, we propose KVPruner to improve model efficiency while maintaining performance. Our method uses global perplexity-based analysis to determine the importance ratio for each block and provides multiple strategies to prune non-essential KV channels within blocks. Compared to the original model, KVPruner reduces runtime memory usage by 50% and boosts throughput by over 35%. Additionally, our method requires only two hours of LoRA fine-tuning on small datasets to recover most of the performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T10:35:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11057v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11057v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 Large Language Models are Good Multi-lingual Learners : When LLMs Meet
  Cross-lingual Prompts</h2>
                <div class="authors">
                    <strong>Authors:</strong> Teng Wang, Zhenqi He, Wing-Yin Yu, Xiaojin Fu, Xiongwei Han
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the advent of Large Language Models (LLMs), generating rule-based data for real-world applications has become more accessible. Due to the inherent ambiguity of natural language and the complexity of rule sets, especially in long contexts, LLMs often struggle to follow all specified rules, frequently omitting at least one. To enhance the reasoning and understanding of LLMs on long and complex contexts, we propose a novel prompting strategy Multi-Lingual Prompt, namely MLPrompt, which automatically translates the error-prone rule that an LLM struggles to follow into another language, thus drawing greater attention to it. Experimental results on public datasets across various tasks have shown MLPrompt can outperform state-of-the-art prompting methods such as Chain of Thought, Tree of Thought, and Self-Consistency. Additionally, we introduce a framework integrating MLPrompt with an auto-checking mechanism for structured data generation, with a specific case study in text-to-MIP instances. Further, we extend the proposed framework for text-to-SQL to demonstrate its generation ability towards structured data synthesis.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T10:33:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11056v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11056v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language
  Models: An Experimental Analysis up to 405B</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jemin Lee, Sihyeong Park, Jinse Kwon, Jihun Oh, Yongin Kwon
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Prior research works have evaluated quantized LLMs using limited metrics such as perplexity or a few basic knowledge tasks and old datasets. Additionally, recent large-scale models such as Llama 3.1 with up to 405B have not been thoroughly examined. This paper evaluates the performance of instruction-tuned LLMs across various quantization methods (GPTQ, AWQ, SmoothQuant, and FP8) on models ranging from 7B to 405B. Using 13 benchmarks, we assess performance across six task types: commonsense Q\&A, knowledge and language understanding, instruction following, hallucination detection, mathematics, and dialogue. Our key findings reveal that (1) quantizing a larger LLM to a similar size as a smaller FP16 LLM generally performs better across most benchmarks, except for hallucination detection and instruction following; (2) performance varies significantly with different quantization methods, model size, and bit-width, with weight-only methods often yielding better results in larger models; (3) task difficulty does not significantly impact accuracy degradation due to quantization; and (4) the MT-Bench evaluation method has limited discriminatory power among recent high-performing LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T10:31:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11055v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11055v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 Advancing Cyber Incident Timeline Analysis Through Rule Based AI and
  Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fatma Yasmine Loumachi, Mohamed Chahine Ghanem
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Timeline Analysis (TA) plays a crucial role in Timeline Forensics (TF) within the field of Digital Forensics (DF). It focuses on examining and analyzing time-based digital artefacts, such as timestamps derived from event logs, file metadata, and other relevant data, to correlate events linked to cyber incidents and reconstruct their chronological sequence. Traditional tools often struggle to efficiently handle the large volume and variety of data generated during DF investigations and Incident Response (IR) processes. This paper introduces a novel framework, GenDFIR, which combines Rule-Based Artificial Intelligence (R-BAI) algorithms with Large Language Models (LLMs) to enhance and automate the TA process. The proposed approach consists of two key stages: (1) R-BAI is used to identify and select anomalous digital artefacts based on predefined rules. (2) The selected artefacts are then transformed into embeddings for processing by an LLM with the assistance of a Retrieval-Augmented Generation (RAG) agent. The LLM uses its capabilities to perform automated TA on the artefacts and predict potential incident outcomes. To validate the framework, we evaluated its performance, efficiency, and reliability. Several metrics were applied to simulated cyber incident scenarios, which were presented as forensic case documents. Our findings demonstrate the significant potential of integrating R-BAI and LLMs for TA. This innovative approach underscores the power of Generative AI (GenAI), particularly LLMs, and opens up new possibilities for advanced threat detection and incident reconstruction, marking a significant advancement in the field.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T10:16:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span><span>cs.ET</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.02572v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.02572v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 Visual grounding for desktop graphical user interfaces</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tassnim Dardouri, Laura Minkova, Jessica López Espejel, Walid Dahhane, El Hassane Ettifouri
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Most instance perception and image understanding solutions focus mainly on natural images. However, applications for synthetic images, and more specifically, images of Graphical User Interfaces (GUI) remain limited. This hinders the development of autonomous computer-vision-powered Artificial Intelligence (AI) agents. In this work, we present Instruction Visual Grounding or IVG, a multi-modal solution for object identification in a GUI. More precisely, given a natural language instruction and GUI screen, IVG locates the coordinates of the element on the screen where the instruction would be executed. To this end, we develop two methods. The first method is a three-part architecture that relies on a combination of a Large Language Model (LLM) and an object detection model. The second approach uses a multi-modal foundation model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T10:15:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.01558v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.01558v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 TacDiffusion: Force-domain Diffusion Policy for Precise Tactile
  Manipulation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yansong Wu, Zongxie Chen, Fan Wu, Lingyun Chen, Liding Zhang, Zhenshan Bing, Abdalla Swikir, Alois Knoll, Sami Haddadin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Assembly is a crucial skill for robots in both modern manufacturing and service robotics. However, mastering transferable insertion skills that can handle a variety of high-precision assembly tasks remains a significant challenge. This paper presents a novel framework that utilizes diffusion models to generate 6D wrench for high-precision tactile robotic insertion tasks. It learns from demonstrations performed on a single task and achieves a zero-shot transfer success rate of 95.7% across various novel high-precision tasks. Our method effectively inherits the self-adaptability demonstrated by our previous work. In this framework, we address the frequency misalignment between the diffusion policy and the real-time control loop with a dynamic system-based filter, significantly improving the task success rate by 9.15%. Furthermore, we provide a practical guideline regarding the trade-off between diffusion models' inference ability and speed.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T10:13:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11047v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11047v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 Computation and Complexity of Preference Inference Based on Hierarchical
  Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Nic Wilson, Anne-Marie George, Barry O'Sullivan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Preference Inference involves inferring additional user preferences from elicited or observed preferences, based on assumptions regarding the form of the user's preference relation. In this paper we consider a situation in which alternatives have an associated vector of costs, each component corresponding to a different criterion, and are compared using a kind of lexicographic order, similar to the way alternatives are compared in a Hierarchical Constraint Logic Programming model. It is assumed that the user has some (unknown) importance ordering on criteria, and that to compare two alternatives, firstly, the combined cost of each alternative with respect to the most important criteria are compared; only if these combined costs are equal, are the next most important criteria considered. The preference inference problem then consists of determining whether a preference statement can be inferred from a set of input preferences. We show that this problem is coNP-complete, even if one restricts the cardinality of the equal-importance sets to have at most two elements, and one only considers non-strict preferences. However, it is polynomial if it is assumed that the user's ordering of criteria is a total ordering; it is also polynomial if the sets of equally important criteria are all equivalence classes of a given fixed equivalence relation. We give an efficient polynomial algorithm for these cases, which also throws light on the structure of the inference.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T10:08:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11044v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11044v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 Towards No-Code Programming of Cobots: Experiments with Code Synthesis
  by Large Code Models for Conversational Programming</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While there has been a lot of research recently on robots in household environments, at the present time, most robots in existence can be found on shop floors, and most interactions between humans and robots happen there. ``Collaborative robots'' (cobots) designed to work alongside humans on assembly lines traditionally require expert programming, limiting ability to make changes, or manual guidance, limiting expressivity of the resulting programs. To address these limitations, we explore using Large Language Models (LLMs), and in particular, their abilities of doing in-context learning, for conversational code generation. As a first step, we define RATS, the ``Repetitive Assembly Task'', a 2D building task designed to lay the foundation for simulating industry assembly scenarios. In this task, a `programmer' instructs a cobot, using natural language, on how a certain assembly is to be built; that is, the programmer induces a program, through natural language. We create a dataset that pairs target structures with various example instructions (human-authored, template-based, and model-generated) and example code. With this, we systematically evaluate the capabilities of state-of-the-art LLMs for synthesising this kind of code, given in-context examples. Evaluating in a simulated environment, we find that LLMs are capable of generating accurate `first order code' (instruction sequences), but have problems producing `higher-order code' (abstractions such as functions, or use of loops).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-18T07:17:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11041v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11041v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 Self-Translate-Train: Enhancing Cross-Lingual Transfer of Large Language
  Models via Inherent Capability</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ryokan Ri, Shun Kiyono, Sho Takase
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Zero-shot cross-lingual transfer by fine-tuning multilingual pretrained models shows promise for low-resource languages, but often suffers from misalignment of internal representations between languages. We hypothesize that even when the model cannot generalize across languages effectively in fine-tuning, it still captures cross-lingual correspondence useful for cross-lingual transfer. We explore this hypothesis with Self-Translate-Train, a method that lets large language models (LLMs) to translate training data into the target language and fine-tunes the model on its own generated data. By demonstrating that Self-Translate-Train outperforms zero-shot transfer, we encourage further exploration of better methods to elicit cross-lingual capabilities of LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T10:04:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.00454v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.00454v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 Hierarchical Narrative Analysis: Unraveling Perceptions of Generative AI</h2>
                <div class="authors">
                    <strong>Authors:</strong> Riona Matsuoka, Hiroki Matsumoto, Takahiro Yoshida, Tomohiro Watanabe, Ryoma Kondo, Ryohei Hisano
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Written texts reflect an author's perspective, making the thorough analysis of literature a key research method in fields such as the humanities and social sciences. However, conventional text mining techniques like sentiment analysis and topic modeling are limited in their ability to capture the hierarchical narrative structures that reveal deeper argumentative patterns. To address this gap, we propose a method that leverages large language models (LLMs) to extract and organize these structures into a hierarchical framework. We validate this approach by analyzing public opinions on generative AI collected by Japan's Agency for Cultural Affairs, comparing the narratives of supporters and critics. Our analysis provides clearer visualization of the factors influencing divergent opinions on generative AI, offering deeper insights into the structures of agreement and disagreement.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T09:56:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11032v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11032v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 Prompt Obfuscation for Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> David Pape, Thorsten Eisenhofer, Lea Schönherr
                </div>
                <div class="summary">
                    <strong>Summary:</strong> System prompts that include detailed instructions to describe the task performed by the underlying large language model (LLM) can easily transform foundation models into tools and services with minimal overhead. Because of their crucial impact on the utility, they are often considered intellectual property, similar to the code of a software product. However, extracting system prompts is easily possible by using prompt injection. As of today, there is no effective countermeasure to prevent the stealing of system prompts and all safeguarding efforts could be evaded with carefully crafted prompt injections that bypass all protection mechanisms.In this work, we propose an alternative to conventional system prompts. We introduce prompt obfuscation to prevent the extraction of the system prompt while maintaining the utility of the system itself with only little overhead. The core idea is to find a representation of the original system prompt that leads to the same functionality, while the obfuscated system prompt does not contain any information that allows conclusions to be drawn about the original system prompt. We implement an optimization-based method to find an obfuscated prompt representation while maintaining the functionality. To evaluate our approach, we investigate eight different metrics to compare the performance of a system using the original and the obfuscated system prompts, and we show that the obfuscated version is constantly on par with the original one. We further perform three different deobfuscation attacks and show that with access to the obfuscated prompt and the LLM itself, we are not able to consistently extract meaningful information. Overall, we showed that prompt obfuscation can be an effective method to protect intellectual property while maintaining the same utility as the original system prompt.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T09:43:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11026v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11026v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 GEIC: Universal and Multilingual Named Entity Recognition with Large
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hanjun Luo, Yingbin Jin, Xuecheng Liu, Tong Shang, Ruizhe Chen, Zuozhu Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have supplanted traditional methods in numerous natural language processing tasks. Nonetheless, in Named Entity Recognition (NER), existing LLM-based methods underperform compared to baselines and require significantly more computational resources, limiting their application. In this paper, we introduce the task of generation-based extraction and in-context classification (GEIC), designed to leverage LLMs' prior knowledge and self-attention mechanisms for NER tasks. We then propose CascadeNER, a universal and multilingual GEIC framework for few-shot and zero-shot NER. CascadeNER employs model cascading to utilize two small-parameter LLMs to extract and classify independently, reducing resource consumption while enhancing accuracy. We also introduce AnythingNER, the first NER dataset specifically designed for LLMs, including 8 languages, 155 entity types and a novel dynamic categorization system. Experiments show that CascadeNER achieves state-of-the-art performance on low-resource and fine-grained scenarios, including CrossNER and FewNERD. Our work is openly accessible.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-18T10:05:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11022v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11022v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 Large Language Model Based Generative Error Correction: A Challenge and
  Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chao-Han Huck Yang, Taejin Park, Yuan Gong, Yuanchao Li, Zhehuai Chen, Yen-Ting Lin, Chen Chen, Yuchen Hu, Kunal Dhawan, Piotr Żelasko, Chao Zhang, Yun-Nung Chen, Yu Tsao, Jagadeesh Balam, Boris Ginsburg, Sabato Marco Siniscalchi, Eng Siong Chng, Peter Bell, Catherine Lai, Shinji Watanabe, Andreas Stolcke
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Given recent advances in generative AI technology, a key question is how large language models (LLMs) can enhance acoustic modeling tasks using text decoding results from a frozen, pretrained automatic speech recognition (ASR) model. To explore new capabilities in language modeling for speech processing, we introduce the generative speech transcription error correction (GenSEC) challenge. This challenge comprises three post-ASR language modeling tasks: (i) post-ASR transcription correction, (ii) speaker tagging, and (iii) emotion recognition. These tasks aim to emulate future LLM-based agents handling voice-based interfaces while remaining accessible to a broad audience by utilizing open pretrained language models or agent-based APIs. We also discuss insights from baseline evaluations, as well as lessons learned for designing future evaluations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T09:32:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span><span>cs.SD</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.09785v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.09785v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 COBIAS: Contextual Reliability in Bias Assessment</h2>
                <div class="authors">
                    <strong>Authors:</strong> Priyanshul Govil, Hemang Jain, Vamshi Krishna Bonagiri, Aman Chadha, Ponnurangam Kumaraguru, Manas Gaur, Sanorita Dey
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) often inherit biases from the web data they are trained on, which contains stereotypes and prejudices. Current methods for evaluating and mitigating these biases rely on bias-benchmark datasets. These benchmarks measure bias by observing an LLM's behavior on biased statements. However, these statements lack contextual considerations of the situations they try to present. To address this, we introduce a contextual reliability framework, which evaluates model robustness to biased statements by considering the various contexts in which they may appear. We develop the Context-Oriented Bias Indicator and Assessment Score (COBIAS) to measure a biased statement's reliability in detecting bias based on the variance in model behavior across different contexts. To evaluate the metric, we augment 2,291 stereotyped statements from two existing benchmark datasets by adding contextual information. We show that COBIAS aligns with human judgment on the contextual reliability of biased statements (Spearman's $\rho = 0.65$, $p = 3.4 * 10^{-60}$) and can be used to create reliable datasets, which would assist bias mitigation works.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T09:24:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.14889v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.14889v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 MM2Latent: Text-to-facial image generation and editing in GANs with
  multimodal assistance</h2>
                <div class="authors">
                    <strong>Authors:</strong> Debin Meng, Christos Tzelepis, Ioannis Patras, Georgios Tzimiropoulos
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Generating human portraits is a hot topic in the image generation area, e.g. mask-to-face generation and text-to-face generation. However, these unimodal generation methods lack controllability in image generation. Controllability can be enhanced by exploring the advantages and complementarities of various modalities. For instance, we can utilize the advantages of text in controlling diverse attributes and masks in controlling spatial locations. Current state-of-the-art methods in multimodal generation face limitations due to their reliance on extensive hyperparameters, manual operations during the inference stage, substantial computational demands during training and inference, or inability to edit real images. In this paper, we propose a practical framework - MM2Latent - for multimodal image generation and editing. We use StyleGAN2 as our image generator, FaRL for text encoding, and train an autoencoders for spatial modalities like mask, sketch and 3DMM. We propose a strategy that involves training a mapping network to map the multimodal input into the w latent space of StyleGAN. The proposed framework 1) eliminates hyperparameters and manual operations in the inference stage, 2) ensures fast inference speeds, and 3) enables the editing of real images. Extensive experiments demonstrate that our method exhibits superior performance in multimodal image generation, surpassing recent GAN- and diffusion-based methods. Also, it proves effective in multimodal image editing and is faster than GAN- and diffusion-based methods. We make the code publicly available at: https://github.com/Open-Debin/MM2Latent
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T09:21:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11010v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11010v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 Latent mixed-effect models for high-dimensional longitudinal data</h2>
                <div class="authors">
                    <strong>Authors:</strong> Priscilla Ong, Manuel Haußmann, Otto Lönnroth, Harri Lähdesmäki
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modelling longitudinal data is an important yet challenging task. These datasets can be high-dimensional, contain non-linear effects and time-varying covariates. Gaussian process (GP) prior-based variational autoencoders (VAEs) have emerged as a promising approach due to their ability to model time-series data. However, they are costly to train and struggle to fully exploit the rich covariates characteristic of longitudinal data, making them difficult for practitioners to use effectively. In this work, we leverage linear mixed models (LMMs) and amortized variational inference to provide conditional priors for VAEs, and propose LMM-VAE, a scalable, interpretable and identifiable model. We highlight theoretical connections between it and GP-based techniques, providing a unified framework for this class of methods. Our proposal performs competitively compared to existing approaches across simulated and real-world datasets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T09:16:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>stat.AP</span><span>stat.ML</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11008v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11008v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 Enhancing Low-Resource Language and Instruction Following Capabilities
  of Audio Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Potsawee Manakul, Guangzhi Sun, Warit Sirichotedumrong, Kasima Tharnpipitchai, Kunat Pipatanakul
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Audio language models can understand audio inputs and perform a range of audio-related tasks based on instructions, such as speech recognition and audio captioning, where the instructions are usually textual prompts. Audio language models are mostly initialized from pre-trained audio encoders and large language models (LLMs). Although these pre-trained components were developed to support multiple languages, audio-language models are trained predominantly on English data, which may limit their usability to only English instructions or English speech inputs. First, this paper examines the performance of existing audio language models in an underserved language using Thai as an example. This paper demonstrates that, despite being built on multilingual backbones, audio language models do not exhibit cross-lingual emergent abilities to low-resource languages. Second, this paper studies data mixture for developing audio language models that are optimized for a target language as well as English. In addition. this paper integrates audio comprehension and speech instruction-following capabilities into a single unified model. Our experiments provide insights into data mixture for enhancing instruction-following capabilities in both a low-resource language and English. Our model, Typhoon-Audio, outperforms existing open-source audio language models by a considerable margin, and it is comparable to state-of-the-art Gemini-1.5-Pro in both English and Thai languages.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T09:04:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.SD</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10999v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10999v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 Less is More: A Simple yet Effective Token Reduction Method for
  Efficient Multi-modal LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dingjie Song, Wenjun Wang, Shunian Chen, Xidong Wang, Michael Guan, Benyou Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid advancement of Multimodal Large Language Models (MLLMs) has led to remarkable performances across various domains. However, this progress is accompanied by a substantial surge in the resource consumption of these models. We address this pressing issue by introducing a new approach, Token Reduction using CLIP Metric (TRIM), aimed at improving the efficiency of MLLMs without sacrificing their performance. Inspired by human attention patterns in Visual Question Answering (VQA) tasks, TRIM presents a fresh perspective on the selection and reduction of image tokens. The TRIM method has been extensively tested across 12 datasets, and the results demonstrate a significant reduction in computational overhead while maintaining a consistent level of performance. This research marks a critical stride in efficient MLLM development, promoting greater accessibility and sustainability of high-performing models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T08:56:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.MM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10994v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10994v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 Statistical Jump Model for Mixed-Type Data with Missing Data Imputation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Federico P. Cortese, Antonio Pievatolo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper, we address the challenge of clustering mixed-type data with temporal evolution by introducing the statistical jump model for mixed-type data. This novel framework incorporates regime persistence, enhancing interpretability and reducing the frequency of state switches, and efficiently handles missing data. The model is easily interpretable through its state-conditional means and modes, making it accessible to practitioners and policymakers. We validate our approach through extensive simulation studies and an empirical application to air quality data, demonstrating its superiority in inferring persistent air quality regimes compared to the traditional air quality index. Our contributions include a robust method for mixed-type temporal clustering, effective missing data management, and practical insights for environmental monitoring.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T08:42:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>stat.AP</span><span>stat.ML</span><span>37M10, 62D10, 62H30</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.01208v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.01208v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 MoDex: Planning High-Dimensional Dexterous Control via Learning Neural
  Hand Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tong Wu, Shoujie Li, Chuqiao Lyu, Kit-Wa Sou, Wang-Sing Chan, Wenbo Ding
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Controlling hands in the high-dimensional action space has been a longstanding challenge, yet humans naturally perform dexterous tasks with ease. In this paper, we draw inspiration from the human embodied cognition and reconsider dexterous hands as learnable systems. Specifically, we introduce MoDex, a framework which employs a neural hand model to capture the dynamical characteristics of hand movements. Based on the model, a bidirectional planning method is developed, which demonstrates efficiency in both training and inference. The method is further integrated with a large language model to generate various gestures such as ``Scissorshand" and ``Rock\&Roll." Moreover, we show that decomposing the system dynamics into a pretrained hand model and an external model improves data efficiency, as supported by both theoretical analysis and empirical experiments. Additional visualization results are available at https://tongwu19.github.io/MoDex.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T08:34:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10983v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10983v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 StuGPTViz: A Visual Analytics Approach to Understand Student-ChatGPT
  Interactions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zixin Chen, Jiachen Wang, Meng Xia, Kento Shigyo, Dingdong Liu, Rong Zhang, Huamin Qu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The integration of Large Language Models (LLMs), especially ChatGPT, into education is poised to revolutionize students' learning experiences by introducing innovative conversational learning methodologies. To empower students to fully leverage the capabilities of ChatGPT in educational scenarios, understanding students' interaction patterns with ChatGPT is crucial for instructors. However, this endeavor is challenging due to the absence of datasets focused on student-ChatGPT conversations and the complexities in identifying and analyzing the evolutional interaction patterns within conversations. To address these challenges, we collected conversational data from 48 students interacting with ChatGPT in a master's level data visualization course over one semester. We then developed a coding scheme, grounded in the literature on cognitive levels and thematic analysis, to categorize students' interaction patterns with ChatGPT. Furthermore, we present a visual analytics system, StuGPTViz, that tracks and compares temporal patterns in student prompts and the quality of ChatGPT's responses at multiple scales, revealing significant pedagogical insights for instructors. We validated the system's effectiveness through expert interviews with six data visualization instructors and three case studies. The results confirmed StuGPTViz's capacity to enhance educators' insights into the pedagogical value of ChatGPT. We also discussed the potential research opportunities of applying visual analytics in education and developing AI-driven personalized learning solutions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T08:32:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.12423v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.12423v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 Does Biomedical Training Lead to Better Medical Performance?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Amin Dada, Marie Bauer, Amanda Butler Contreras, Osman Alperen Koraş, Constantin Marc Seibold, Kaleb E Smith, Jens Kleesiek
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are expected to significantly contribute to patient care, diagnostics, and administrative processes. Emerging biomedical LLMs aim to address healthcare-specific challenges, including privacy demands and computational constraints. Assessing the models' suitability for this sensitive application area is of the utmost importance. However, biomedical training has not been systematically evaluated on medical tasks. This study investigates the effect of biomedical training in the context of six practical medical tasks evaluating $25$ models. In contrast to previous evaluations, our results reveal a performance decline in nine out of twelve biomedical models after fine-tuning, particularly on tasks involving hallucinations, ICD10 coding, and instruction adherence. General-domain models like Meta-Llama-3.1-70B-Instruct outperformed their biomedical counterparts, indicating a trade-off between domain-specific fine-tuning and general medical task performance. We open-source all evaluation scripts and datasets at https://github.com/TIO-IKIM/CLUE to support further research in this critical area.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T08:19:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.04067v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.04067v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 Enhancing Multilingual Speech Generation and Recognition Abilities in
  LLMs with Constructed Code-switched Data</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jing Xu, Daxin Tan, Jiaqi Wang, Xiao Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While large language models (LLMs) have been explored in the speech domain for both generation and recognition tasks, their applications are predominantly confined to the monolingual scenario, with limited exploration in multilingual and code-switched (CS) contexts. Additionally, speech generation and recognition tasks are often handled separately, such as VALL-E and Qwen-Audio. In this paper, we propose a MutltiLingual MultiTask (MLMT) model, integrating multilingual speech generation and recognition tasks within the single LLM. Furthermore, we develop an effective data construction approach that splits and concatenates words from different languages to equip LLMs with CS synthesis ability without relying on CS data. The experimental results demonstrate that our model outperforms other baselines with a comparable data scale. Furthermore, our data construction approach not only equips LLMs with CS speech synthesis capability with comparable speaker consistency and similarity to any given speaker, but also improves the performance of LLMs in multilingual speech generation and recognition tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T08:11:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.AS</span><span>cs.CL</span><span>cs.SD</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10969v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10969v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 Open-Source Web Service with Morphological Dictionary-Supplemented Deep
  Learning for Morphosyntactic Analysis of Czech</h2>
                <div class="authors">
                    <strong>Authors:</strong> Milan Straka, Jana Straková
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present an open-source web service for Czech morphosyntactic analysis. The system combines a deep learning model with rescoring by a high-precision morphological dictionary at inference time. We show that our hybrid method surpasses two competitive baselines: While the deep learning model ensures generalization for out-of-vocabulary words and better disambiguation, an improvement over an existing morphological analyser MorphoDiTa, at the same time, the deep learning model benefits from inference-time guidance of a manually curated morphological dictionary. We achieve 50% error reduction in lemmatization and 58% error reduction in POS tagging over MorphoDiTa, while also offering dependency parsing. The model is trained on one of the currently largest Czech morphosyntactic corpora, the PDT-C 1.0, with the trained models available at https://hdl.handle.net/11234/1-5293. We provide the tool as a web service deployed at https://lindat.mff.cuni.cz/services/udpipe/. The source code is available at GitHub (https://github.com/ufal/udpipe/tree/udpipe-2), along with a Python client for a simple use. The documentation for the models can be found at https://ufal.mff.cuni.cz/udpipe/2/models#czech_pdtc1.0_model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T07:58:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1007/978-3-031-70563-2_22' target='_blank'>doi</a><a href='http://arxiv.org/abs/2406.12422v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.12422v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 Investigating Context-Faithfulness in Large Language Models: The Roles
  of Memory Strength and Evidence Style</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuepei Li, Kang Zhou, Qiao Qiao, Bach Nguyen, Qing Wang, Qi Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Retrieval-augmented generation (RAG) improves Large Language Models (LLMs) by incorporating external information into the response generation process. However, how context-faithful LLMs are and what factors influence LLMs' context-faithfulness remain largely unexplored. In this study, we investigate the impact of memory strength and evidence presentation on LLMs' receptiveness to external evidence. We introduce a method to quantify the memory strength of LLMs by measuring the divergence in LLMs' responses to different paraphrases of the same question, which is not considered by previous works. We also generate evidence in various styles to evaluate the effects of evidence in different styles. Two datasets are used for evaluation: Natural Questions (NQ) with popular questions and popQA featuring long-tail questions. Our results show that for questions with high memory strength, LLMs are more likely to rely on internal memory, particularly for larger LLMs such as GPT-4. On the other hand, presenting paraphrased evidence significantly increases LLMs' receptiveness compared to simple repetition or adding details.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T07:44:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10955v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10955v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 Infer metabolic velocities from moment differences of molecular weight
  distributions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Li Tuobang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Metabolic pathways are fundamental maps in biochemistry that detail how molecules are transformed through various reactions. The complexity of metabolic network, where a single compound can play a part in multiple pathways, poses a challenge in inferring metabolic balance changes over time or after different treatments. Isotopic labeling experiment is the standard method to infer metabolic flux, which is currently defined as the flow of a single metabolite through a given pathway over time. However, there is still no way to accurately infer the metabolic balance changes after different treatments in an experiment. This study introduces a different concept: molecular weight distribution, which is the empirical distribution of the molecular weights of all metabolites of interest. By estimating the differences of the location and scale estimates of these distributions, it becomes possible to quantitatively infer the metabolic balance changes even without requiring knowledge of the exact chemical structures of these compounds and their related pathways. This research article provides a mathematical framing for a classic biological concept.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T07:42:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>q-bio.QM</span><span>q-bio.BM</span><span>q-bio.CB</span><span>q-bio.MN</span><span>q-bio.SC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.14887v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.14887v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 Optimizing TinyML: The Impact of Reduced Data Acquisition Rates for Time
  Series Classification on Microcontrollers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Riya Samanta, Bidyut Saha, Soumya K. Ghosh, Ram Babu Roy
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Tiny Machine Learning (TinyML) enables efficient, lowcost, and privacy preserving machine learning inference directly on microcontroller units (MCUs) connected to sensors. Optimizing models for these constrained environments is crucial. This paper investigates how reducing data acquisition rates affects TinyML models for time series classification, focusing on resource-constrained, battery operated IoT devices. By lowering data sampling frequency, we aim to reduce computational demands RAM usage, energy consumption, latency, and MAC operations by approximately fourfold while maintaining similar classification accuracies. Our experiments with six benchmark datasets (UCIHAR, WISDM, PAMAP2, MHEALTH, MITBIH, and PTB) showed that reducing data acquisition rates significantly cut energy consumption and computational load, with minimal accuracy loss. For example, a 75\% reduction in acquisition rate for MITBIH and PTB datasets led to a 60\% decrease in RAM usage, 75\% reduction in MAC operations, 74\% decrease in latency, and 70\% reduction in energy consumption, without accuracy loss. These results offer valuable insights for deploying efficient TinyML models in constrained environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T07:21:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10942v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10942v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 Fast, Accurate and Perturbative Forward Modeling of Galaxy Clustering
  Part I: Galaxies in the Restframe</h2>
                <div class="authors">
                    <strong>Authors:</strong> Julia Stadler, Fabian Schmidt, Martin Reinecke
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Forward models of the galaxy density field enable simulation based inference as well as field level inference of galaxy clustering. However, these analysis techniques require forward models that are both computationally fast and robust to modeling uncertainties in the relation between galaxies and matter. Both requirements can be addressed with the Effective Field Theory of Large Scale Structure. Here, we focus on the physical and numerical convergence of the LEFTfield model. Based on the perturbative nature of the forward model, we derive an analytic understanding of the leading numerical errors, and we compare our estimates to high-resolution and N-body references. This allows us to derive a set of best-practice recommendations for the numerical accuracy parameters, which are completely specified by the desired order of the perturbative solution and the cut-off scale. We verify these recommendations by an extended set of parameter recovery tests from fully nonlinear mock data and find very consistent results. A single evaluation of the forward model takes seconds, making cosmological analyses of galaxy clustering data based on forward models computationally feasible.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T07:15:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.CO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10937v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10937v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 Propulsion: Steering LLM with Tiny Fine-Tuning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Md Kowsher, Nusrat Jahan Prottasha, Prakash Bhat
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid advancements in Large Language Models (LLMs) have revolutionized natural language processing (NLP) and related fields. However, fine-tuning these models for specific tasks remains computationally expensive and risks degrading pre-learned features. To address these challenges, we propose Propulsion, a novel parameter efficient fine-tuning (PEFT) method designed to optimize task-specific performance while drastically reducing computational overhead. Inspired by the concept of controlled adjustments in physical motion, Propulsion selectively re-scales specific dimensions of a pre-trained model, guiding output predictions toward task objectives without modifying the model's parameters. By introducing lightweight, trainable Propulsion parameters at the pre-trained layer, we minimize the number of parameters updated during fine-tuning, preventing overfitting or overwriting of existing knowledge. Our theoretical analysis, supported by Neural Tangent Kernel (NTK) theory, shows that Propulsion approximates the performance of full fine-tuning with far fewer trainable parameters. Empirically, Propulsion reduces the parameter count from 355.3 million to just 0.086 million, achieving over a 10x reduction compared to standard approaches like LoRA while maintaining competitive performance across benchmarks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-18T07:23:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10927v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10927v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 Remote Keylogging Attacks in Multi-user VR Applications</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zihao Su, Kunlin Cai, Reuben Beeler, Lukas Dresel, Allan Garcia, Ilya Grishchenko, Yuan Tian, Christopher Kruegel, Giovanni Vigna
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As Virtual Reality (VR) applications grow in popularity, they have bridged distances and brought users closer together. However, with this growth, there have been increasing concerns about security and privacy, especially related to the motion data used to create immersive experiences. In this study, we highlight a significant security threat in multi-user VR applications, which are applications that allow multiple users to interact with each other in the same virtual space. Specifically, we propose a remote attack that utilizes the avatar rendering information collected from an adversary's game clients to extract user-typed secrets like credit card information, passwords, or private conversations. We do this by (1) extracting motion data from network packets, and (2) mapping motion data to keystroke entries. We conducted a user study to verify the attack's effectiveness, in which our attack successfully inferred 97.62% of the keystrokes. Besides, we performed an additional experiment to underline that our attack is practical, confirming its effectiveness even when (1) there are multiple users in a room, and (2) the attacker cannot see the victims. Moreover, we replicated our proposed attack on four applications to demonstrate the generalizability of the attack. Lastly, we proposed a defense against the attack, which has been implemented by major players in the VR industry. These results underscore the severity of the vulnerability and its potential impact on millions of VR social platform users.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T06:33:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2405.14036v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.14036v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 Augmenting Automatic Speech Recognition Models with Disfluency Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Robin Amann, Zhaolin Li, Barbara Bruno, Jan Niehues
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Speech disfluency commonly occurs in conversational and spontaneous speech. However, standard Automatic Speech Recognition (ASR) models struggle to accurately recognize these disfluencies because they are typically trained on fluent transcripts. Current research mainly focuses on detecting disfluencies within transcripts, overlooking their exact location and duration in the speech. Additionally, previous work often requires model fine-tuning and addresses limited types of disfluencies.   In this work, we present an inference-only approach to augment any ASR model with the ability to detect open-set disfluencies. We first demonstrate that ASR models have difficulty transcribing speech disfluencies. Next, this work proposes a modified Connectionist Temporal Classification(CTC)-based forced alignment algorithm from \cite{kurzinger2020ctc} to predict word-level timestamps while effectively capturing disfluent speech. Additionally, we develop a model to classify alignment gaps between timestamps as either containing disfluent speech or silence. This model achieves an accuracy of 81.62% and an F1-score of 80.07%. We test the augmentation pipeline of alignment gap detection and classification on a disfluent dataset. Our results show that we captured 74.13% of the words that were initially missed by the transcription, demonstrating the potential of this pipeline for downstream tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T06:30:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10177v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10177v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 JARViS: Detecting Actions in Video Using Unified Actor-Scene Context
  Relation Modeling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Seok Hwan Lee, Taein Son, Soo Won Seo, Jisong Kim, Jun Won Choi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Video action detection (VAD) is a formidable vision task that involves the localization and classification of actions within the spatial and temporal dimensions of a video clip. Among the myriad VAD architectures, two-stage VAD methods utilize a pre-trained person detector to extract the region of interest features, subsequently employing these features for action detection. However, the performance of two-stage VAD methods has been limited as they depend solely on localized actor features to infer action semantics. In this study, we propose a new two-stage VAD framework called Joint Actor-scene context Relation modeling based on Visual Semantics (JARViS), which effectively consolidates cross-modal action semantics distributed globally across spatial and temporal dimensions using Transformer attention. JARViS employs a person detector to produce densely sampled actor features from a keyframe. Concurrently, it uses a video backbone to create spatio-temporal scene features from a video clip. Finally, the fine-grained interactions between actors and scenes are modeled through a Unified Action-Scene Context Transformer to directly output the final set of actions in parallel. Our experimental results demonstrate that JARViS outperforms existing methods by significant margins and achieves state-of-the-art performance on three popular VAD datasets, including AVA, UCF101-24, and JHMDB51-21.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T06:25:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.03612v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.03612v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 ASHABot: An LLM-Powered Chatbot to Support the Informational Needs of
  Community Health Workers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pragnya Ramjee, Mehak Chhokar, Bhuvan Sachdeva, Mahendra Meena, Hamid Abdullah, Aditya Vashistha, Ruchit Nagar, Mohit Jain
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Community health workers (CHWs) provide last-mile healthcare services but face challenges due to limited medical knowledge and training. This paper describes the design, deployment, and evaluation of ASHABot, an LLM-powered, experts-in-the-loop, WhatsApp-based chatbot to address the information needs of CHWs in India. Through interviews with CHWs and their supervisors and log analysis, we examine factors affecting their engagement with ASHABot, and ASHABot's role in addressing CHWs' informational needs. We found that ASHABot provided a private channel for CHWs to ask rudimentary and sensitive questions they hesitated to ask supervisors. CHWs trusted the information they received on ASHABot and treated it as an authoritative resource. CHWs' supervisors expanded their knowledge by contributing answers to questions ASHABot failed to answer, but were concerned about demands on their workload and increased accountability. We emphasize positioning LLMs as supplemental fallible resources within the community healthcare ecosystem, instead of as replacements for supervisor support.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T06:11:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10913v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10913v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 GenCRF: Generative Clustering and Reformulation Framework for Enhanced
  Intent-Driven Information Retrieval</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wonduk Seo, Haojie Zhang, Yueyang Zhang, Changhao Zhang, Songyao Duan, Lixin Su, Daiting Shi, Jiashu Zhao, Dawei Yin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Query reformulation is a well-known problem in Information Retrieval (IR) aimed at enhancing single search successful completion rate by automatically modifying user's input query. Recent methods leverage Large Language Models (LLMs) to improve query reformulation, but often generate limited and redundant expansions, potentially constraining their effectiveness in capturing diverse intents. In this paper, we propose GenCRF: a Generative Clustering and Reformulation Framework to capture diverse intentions adaptively based on multiple differentiated, well-generated queries in the retrieval phase for the first time. GenCRF leverages LLMs to generate variable queries from the initial query using customized prompts, then clusters them into groups to distinctly represent diverse intents. Furthermore, the framework explores to combine diverse intents query with innovative weighted aggregation strategies to optimize retrieval performance and crucially integrates a novel Query Evaluation Rewarding Model (QERM) to refine the process through feedback loops. Empirical experiments on the BEIR benchmark demonstrate that GenCRF achieves state-of-the-art performance, surpassing previous query reformulation SOTAs by up to 12% on nDCG@10. These techniques can be adapted to various LLMs, significantly boosting retriever performance and advancing the field of Information Retrieval.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T05:59:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10909v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10909v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 Towards Goal-oriented Prompt Engineering for Large Language Models: A
  Survey</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haochen Li, Jonathan Leung, Zhiqi Shen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have shown prominent performance in various downstream tasks and prompt engineering plays a pivotal role in optimizing LLMs' performance. This paper, not only as an overview of current prompt engineering methods, but also aims to highlight the limitation of designing prompts based on an anthropomorphic assumption that expects LLMs to think like humans. From our review of 50 representative studies, we demonstrate that a goal-oriented prompt formulation, which guides LLMs to follow established human logical thinking, significantly improves the performance of LLMs. Furthermore, We introduce a novel taxonomy that categorizes goal-oriented prompting methods into five interconnected stages and we demonstrate the broad applicability of our framework. With four future directions proposed, we hope to further emphasize the power and potential of goal-oriented prompt engineering in all fields.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T04:56:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2401.14043v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2401.14043v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 CREAM: Comparison-Based Reference-Free ELO-Ranked Automatic Evaluation
  for Meeting Summarization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ziwei Gong, Lin Ai, Harshsaiprasad Deshpande, Alexander Johnson, Emmy Phung, Zehui Wu, Ahmad Emami, Julia Hirschberg
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have spurred interest in automatic evaluation methods for summarization, offering a faster, more cost-effective alternative to human evaluation. However, existing methods often fall short when applied to complex tasks like long-context summarizations and dialogue-based meeting summarizations. In this paper, we introduce CREAM (Comparison-Based Reference-Free Elo-Ranked Automatic Evaluation for Meeting Summarization), a novel framework that addresses the unique challenges of evaluating meeting summaries. CREAM leverages a combination of chain-of-thought reasoning and key facts alignment to assess conciseness and completeness of model-generated summaries without requiring reference. By employing an ELO ranking system, our approach provides a robust mechanism for comparing the quality of different models or prompt configurations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T04:39:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10883v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10883v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 Adaptive Large Language Models By Layerwise Attention Shortcuts</h2>
                <div class="authors">
                    <strong>Authors:</strong> Prateek Verma, Mert Pilanci
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformer architectures are the backbone of the modern AI revolution. However, they are based on simply stacking the same blocks in dozens of layers and processing information sequentially from one block to another. In this paper, we propose to challenge this and introduce adaptive computations for LLM-like setups, which allow the final layer to attend to all of the intermediate layers as it deems fit through the attention mechanism, thereby introducing computational \textbf{attention shortcuts}. These shortcuts can thus make the architecture depth and context adaptive. We showcase four different datasets, namely acoustic tokens, natural language, and symbolic music, and we achieve superior performance for GPT-like architecture. We give evidence via attention maps that the models learn complex dependencies across layers that are adaptive in context and depth depending on the input tokens.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T03:46:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span><span>cs.SD</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10870v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10870v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 Stable Case BB/BC Mass Transfer to Form GW190425-like Massive Binary
  Neutron Star Mergers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ying Qin, Jin-Ping Zhu, Georges Meynet, Bing Zhang, Fa-Yin Wang, Xin-Wen Shu, Han-Feng Song, Yuan-Zhu Wang, Liang Yuan, Zhen-Han-Tao Wang, Rui-Chong Hu, Dong-Hong Wu, Shuang-Xi Yi, Qing-Wen Tang, Jun-Jie Wei, Xue-Feng Wu, En-Wei Liang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> On April 25th, 2019, the LIGO-Virgo Collaboration discovered a Gravitational-wave (GW) signal from a binary neutron star (BNS) merger, i.e., GW190425. Due to the inferred large total mass, the origin of GW190425 remains unclear. We perform detailed stellar structure and binary evolution calculations that take into account mass-loss, internal differential rotation, and tidal interactions between a He-rich star and a NS companion. We explore the parameter space of the initial binary properties, including initial NS and He-rich masses and initial orbital period. We find that the immediate post-common-envelope progenitor system, consisting of a primary $\sim2.0\,M_\odot$ ($\sim1.7\,M_\odot$) NS and a secondary He-rich star with an initial mass of $\sim3.0-5.5\,M_\odot$ ($\sim5.5-6.0\,M_\odot$) in a close binary with an initial period of $\sim0.08-0.5\,{\rm{days}}$ ($\sim 0.08-0.4\,{\rm{days}}$), that experiences stable Case BB/BC mass transfer (MT) during binary evolution, can reproduce the formation of GW190425-like BNS events. Our studies reveal that the secondary He-rich star of the GW190425's progenitor before its core collapse can be efficiently spun up through tidal interaction, finally remaining as a NS with rotational energy even reaching $\sim10^{52}\,{\rm{erg}}$, which is always much higher than the neutrino-driven energy of the supernova (SN) explosion. If the newborn secondary NS is a magnetar, we expect that GW190425 can be the remnant of a magnetar-driven SN, e.g., a magnetar-driven ultra-stripped SN, a superluminous SN, or a broad-line Type Ic SN. Our results show that GW190425 could be formed through the isolated binary evolution, which involves a stable Case BB/BC MT just after the common envelope phase. On top of that, we show the He-rich star can be tidally spun up, potentially forming a spinning magnetized NS (magnetar) during the second SN explosion.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T03:44:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.SR</span><span>astro-ph.HE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10869v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10869v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 Learnings from a Large-Scale Deployment of an LLM-Powered
  Expert-in-the-Loop Healthcare Chatbot</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bhuvan Sachdeva, Pragnya Ramjee, Geeta Fulari, Kaushik Murali, Mohit Jain
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are widely used in healthcare, but limitations like hallucinations, incomplete information, and bias hinder their reliability. To address these, researchers released the Build Your Own expert Bot (BYOeB) platform, enabling developers to create LLM-powered chatbots with integrated expert verification. CataractBot, its first implementation, provides expert-verified responses to cataract surgery questions. A pilot evaluation showed its potential; however the study had a small sample size and was primarily qualitative. In this work, we conducted a large-scale 24-week deployment of CataractBot involving 318 patients and attendants who sent 1,992 messages, with 91.71% of responses verified by seven experts. Analysis of interaction logs revealed that medical questions significantly outnumbered logistical ones, hallucinations were negligible, and experts rated 84.52% of medical answers as accurate. As the knowledge base expanded with expert corrections, system performance improved by 19.02%, reducing expert workload. These insights guide the design of future LLM-powered chatbots.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T03:22:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10354v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10354v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 X-ray view of dissipative warm corona in active galactic nuclei</h2>
                <div class="authors">
                    <strong>Authors:</strong> B. Palit, A. Rozanska, P. O. Petrucci, D. Gronkiewicz, S. Barnier, S. Bianchi, D. R. Ballantyne, V. E. Gianolli, R. Middei, R. Belmont, F. Ursini
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the X-ray spectra of AGNs, a noticeable excess of soft X-rays is typically detected beyond the extrapolation of the power-law trend observed between 2-10 keV. In the scenario of warm Comptonization, observations propose a warm corona temperature ranging from 0.1-1 keV and an optical depth of approximately 10-20. Furthermore, according to radiative constraints derived from spectral analyses employing Comptonization models, it is suggested that the majority of the accretion power is released within the warm corona, while the disk beneath it is largely non-dissipative, emitting mainly the reprocessed radiation from the corona. We test the dissipative warm corona model using the radiative transfer code-TITAN/NOAR on a sample of 82 XMM-Newton observations of AGNs. Through spectral modeling of the X-ray data, we aim to estimate the total amount of internal heating inside the warm corona situated on top of the accretion disk. By modeling the 0.3-10 keV EPIC-pn spectra, we estimate the internal heating and optical depth of the warm corona and check their correlations with global parameters blackhole parameters. From model normalization, we compute the radial extent of warm corona on top of cold accretion disk. Our model infers the presence of dissipative warm corona, with optical depths distributed in the range 6-30 and total internal heating in the range 1-29 x 1e-23 erg/s-cm3. The extent of warm corona is spread across a large range from 7-408 gravitational radii, and we find that warm corona is more extended for larger accretion rates. Soft excess emission is ubiquitous in AGNs across wide mass range and accretion rate. We confirm that warm corona responsible for producing the soft-excess is highly dissipative in nature with larger optical depths being associated with lower internal heating and vice versa. The cold standard accretion disk regulates the extent of warm corona.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T02:48:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.HE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.14378v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.14378v3' target='_blank'>pdf</a>
                </div>
            </div>
            <h1>Keyword: LLM Deployment</h1>

            <div class="paper">
                <h2>#1 AraDiCE: Benchmarks for Dialectal and Cultural Capabilities in LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Basel Mousi, Nadir Durrani, Fatema Ahmad, Md. Arid Hasan, Maram Hasanain, Tameem Kabbani, Fahim Dalvi, Shammur Absar Chowdhury, Firoj Alam
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Arabic, with its rich diversity of dialects, remains significantly underrepresented in Large Language Models, particularly in dialectal variations. We address this gap by introducing seven synthetic datasets in dialects alongside Modern Standard Arabic (MSA), created using Machine Translation (MT) combined with human post-editing. We present AraDiCE, a benchmark for Arabic Dialect and Cultural Evaluation. We evaluate LLMs on dialect comprehension and generation, focusing specifically on low-resource Arabic dialects. Additionally, we introduce the first-ever fine-grained benchmark designed to evaluate cultural awareness across the Gulf, Egypt, and Levant regions, providing a novel dimension to LLM evaluation. Our findings demonstrate that while Arabic-specific models like Jais and AceGPT outperform multilingual models on dialectal tasks, significant challenges persist in dialect identification, generation, and translation. This work contributes ~45K post-edited samples, a cultural benchmark, and highlights the importance of tailored training to improve LLM performance in capturing the nuances of diverse Arabic dialects and cultural contexts. We will release the dialectal translation models and benchmarks curated in this study.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:59:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>68T50</span><span>F.2.2; I.2.7</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11404v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11404v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 NVLM: Open Frontier-Class Multimodal LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuoling Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce NVLM 1.0, a family of frontier-class multimodal large language models (LLMs) that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models (e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved text-only performance over its LLM backbone after multimodal training. In terms of model design, we perform a comprehensive comparison between decoder-only multimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g., Flamingo). Based on the strengths and weaknesses of both approaches, we propose a novel architecture that enhances both training efficiency and multimodal reasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for tile-based dynamic high-resolution images, which significantly boosts performance on multimodal reasoning and OCR-related tasks. Regarding training data, we meticulously curate and provide detailed information on our multimodal pretraining and supervised fine-tuning datasets. Our findings indicate that dataset quality and task diversity are more important than scale, even during the pretraining phase, across all architectures. Notably, we develop production-grade multimodality for the NVLM-1.0 models, enabling them to excel in vision-language tasks while maintaining and even improving text-only performance compared to their LLM backbones. To achieve this, we craft and integrate a high-quality text-only dataset into multimodal training, alongside a substantial amount of multimodal math and reasoning data, leading to enhanced math and coding capabilities across modalities. To advance research in the field, we are releasing the model weights and will open-source the code for the community: https://nvlm-project.github.io/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:59:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.CV</span><span>cs.LG</span><span>cs.MM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11402v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11402v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless
  Integration of Multi Active/Passive Core-Agents</h2>
                <div class="authors">
                    <strong>Authors:</strong> Amine B. Hassouna, Hana Chaari, Ines Belhaj
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The integration of tools in LLM-based agents overcame the difficulties of standalone LLMs and traditional agents' limited capabilities. However, the conjunction of these technologies and the proposed enhancements in several state-of-the-art works followed a non-unified software architecture resulting in a lack of modularity. Indeed, they focused mainly on functionalities and overlooked the definition of the component's boundaries within the agent. This caused terminological and architectural ambiguities between researchers which we addressed in this paper by proposing a unified framework that establishes a clear foundation for LLM-based agents' development from both functional and software architectural perspectives.   Our framework, LLM-Agent-UMF (LLM-based Agent Unified Modeling Framework), clearly distinguishes between the different components of an agent, setting LLMs, and tools apart from a newly introduced element: the core-agent, playing the role of the central coordinator of the agent which comprises five modules: planning, memory, profile, action, and security, the latter often neglected in previous works. Differences in the internal structure of core-agents led us to classify them into a taxonomy of passive and active types. Based on this, we proposed different multi-core agent architectures combining unique characteristics of various individual agents.   For evaluation purposes, we applied this framework to a selection of state-of-the-art agents, thereby demonstrating its alignment with their functionalities and clarifying the overlooked architectural aspects. Moreover, we thoroughly assessed four of our proposed architectures by integrating distinctive agents into hybrid active/passive core-agents' systems. This analysis provided clear insights into potential improvements and highlighted the challenges involved in the combination of specific agents.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:54:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span><span>cs.CR</span><span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11393v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11393v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 Jäger: Automated Telephone Call Traceback</h2>
                <div class="authors">
                    <strong>Authors:</strong> David Adei, Varun Madathil, Sathvik Prasad, Bradley Reaves, Alessandra Scafuro
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Unsolicited telephone calls that facilitate fraud or unlawful telemarketing continue to overwhelm network users and the regulators who prosecute them. The first step in prosecuting phone abuse is traceback -- identifying the call originator. This fundamental investigative task currently requires hours of manual effort per call. In this paper, we introduce J\"ager, a distributed secure call traceback system. J\"ager can trace a call in a few seconds, even with partial deployment, while cryptographically preserving the privacy of call parties, carrier trade secrets like peers and call volume, and limiting the threat of bulk analysis. We establish definitions and requirements of secure traceback, then develop a suite of protocols that meet these requirements using witness encryption, oblivious pseudorandom functions, and group signatures. We prove these protocols secure in the universal composibility framework. We then demonstrate that J\"ager has low compute and bandwidth costs per call, and these costs scale linearly with call volume. J\"ager provides an efficient, secure, privacy-preserving system to revolutionize telephone abuse investigation with minimal costs to operators.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:51:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.CY</span><span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3658644.3690290' target='_blank'>doi</a><a href='http://arxiv.org/abs/2409.02839v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.02839v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 Says Who? Effective Zero-Shot Annotation of Focalization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rebecca M. M. Hicke, Yuri Bizzoni, Pascale Feldkamp, Ross Deans Kristensen-McLachlan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Focalization, the perspective through which narrative is presented, is encoded via a wide range of lexico-grammatical features and is subject to reader interpretation. Moreover, trained readers regularly disagree on interpretations, suggesting that this problem may be computationally intractable. In this paper, we provide experiments to test how well contemporary Large Language Models (LLMs) perform when annotating literary texts for focalization mode. Despite the challenging nature of the task, LLMs show comparable performance to trained human annotators in our experiments. We provide a case study working with the novels of Stephen King to demonstrate the usefulness of this approach for computational literary studies, illustrating how focalization can be studied at scale.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:50:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11390v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11390v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 Larger Language Models Don't Care How You Think: Why Chain-of-Thought
  Prompting Fails in Subjective Tasks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Georgios Chochlakis, Niyantha Maruthu Pandiyan, Kristina Lerman, Shrikanth Narayanan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the dominant technique for performing natural language tasks, as it does not require updating the model parameters with gradient-based methods. ICL promises to "adapt" the LLM to perform the present task at a competitive or state-of-the-art level at a fraction of the computational cost. ICL can be augmented by incorporating the reasoning process to arrive at the final label explicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting. However, recent work has found that ICL relies mostly on the retrieval of task priors and less so on "learning" to perform tasks, especially for complex subjective domains like emotion and morality, where priors ossify posterior predictions. In this work, we examine whether "enabling" reasoning also creates the same behavior in LLMs, wherein the format of CoT retrieves reasoning priors that remain relatively unchanged despite the evidence in the prompt. We find that, surprisingly, CoT indeed suffers from the same posterior collapse as ICL for larger language models. Code is avalaible at https://github.com/gchochla/cot-priors.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:42:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.06173v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.06173v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 Dated Data: Tracing Knowledge Cutoffs in Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jeffrey Cheng, Marc Marone, Orion Weller, Dawn Lawrie, Daniel Khashabi, Benjamin Van Durme
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Released Large Language Models (LLMs) are often paired with a claimed knowledge cutoff date, or the dates at which training data was gathered. Such information is crucial for applications where the LLM must provide up to date information. However, this statement only scratches the surface: do all resources in the training data share the same knowledge cutoff date? Does the model's demonstrated knowledge for these subsets closely align to their cutoff dates? In this work, we define the notion of an effective cutoff. This is distinct from the LLM designer reported cutoff and applies separately to sub-resources and topics. We propose a simple approach to estimate effective cutoffs on the resource-level temporal alignment of an LLM by probing across versions of the data. Using this analysis, we find that effective cutoffs often differ from reported cutoffs. To understand the root cause of this observation, we conduct a direct large-scale analysis on open pre-training datasets. Our analysis reveals two reasons for these inconsistencies: (1) temporal biases of CommonCrawl data due to non-trivial amounts of old data in new dumps and (2) complications in LLM deduplication schemes involving semantic duplicates and lexical near-duplicates. Overall, our results show that knowledge cutoffs are not as simple as they have seemed and that care must be taken both by LLM dataset curators as well as practitioners who seek to use information from these models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:25:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2403.12958v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2403.12958v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 Diversify and Conquer: Diversity-Centric Data Selection with Iterative
  Refinement</h2>
                <div class="authors">
                    <strong>Authors:</strong> Simon Yu, Liangyu Chen, Sara Ahmadian, Marzieh Fadaee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Finetuning large language models on instruction data is crucial for enhancing pre-trained knowledge and improving instruction-following capabilities. As instruction datasets proliferate, selecting optimal data for effective training becomes increasingly important. This work addresses the question: How can we determine the optimal subset of data for effective training? While existing research often emphasizes local criteria like instance quality for subset selection, we argue that a global approach focused on data diversity is more critical. Our method employs k-means clustering to ensure the selected subset effectively represents the full dataset. We propose an iterative refinement method inspired by active learning techniques to resample instances from clusters, reassessing each cluster's importance and sampling weight in every training iteration. This approach reduces the effect of outliers and automatically filters out clusters containing low-quality data. Through extensive evaluation across natural language reasoning, general world knowledge, code and math reasoning tasks, and by fine-tuning models from various families, we observe consistent improvements, achieving a 7% increase over random selection and a 3.8% improvement over state-of-the-art sampling methods. Our work highlights the significance of diversity-first sampling when finetuning LLMs to enhance performance across a broad array of evaluation tasks. Our code is available at https://github.com/for-ai/iterative-data-selection.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:25:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11378v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11378v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 Towards Time Series Reasoning with LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Winnie Chow, Lauren Gardiner, Haraldur T. Hallgrímsson, Maxwell A. Xu, Shirley You Ren
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multi-modal large language models (MLLMs) have enabled numerous advances in understanding and reasoning in domains like vision, but we have not yet seen this broad success for time-series. Although prior works on time-series MLLMs have shown promising performance in time-series forecasting, very few works show how an LLM could be used for time-series reasoning in natural language. We propose a novel multi-modal time-series LLM approach that learns generalizable information across various domains with powerful zero-shot performance. First, we train a lightweight time-series encoder on top of an LLM to directly extract time-series information. Then, we fine-tune our model with chain-of-thought augmented time-series tasks to encourage the model to generate reasoning paths. We show that our model learns a latent representation that reflects specific time-series features (e.g. slope, frequency), as well as outperforming GPT-4o on a set of zero-shot reasoning tasks on a variety of domains.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:23:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11376v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11376v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 Multi-OCT-SelfNet: Integrating Self-Supervised Learning with
  Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease
  Classification</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fatema-E- Jannat, Sina Gholami, Jennifer I. Lim, Theodore Leng, Minhaj Nur Alam, Hamed Tabkhi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the medical domain, acquiring large datasets poses significant challenges due to privacy concerns. Nonetheless, the development of a robust deep-learning model for retinal disease diagnosis necessitates a substantial dataset for training. The capacity to generalize effectively on smaller datasets remains a persistent challenge. The scarcity of data presents a significant barrier to the practical implementation of scalable medical AI solutions. To address this issue, we've combined a wide range of data sources to improve performance and generalization to new data by giving it a deeper understanding of the data representation from multi-modal datasets and developed a self-supervised framework based on large language models (LLMs), SwinV2 to gain a deeper understanding of multi-modal dataset representations, enhancing the model's ability to extrapolate to new data for the detection of eye diseases using optical coherence tomography (OCT) images. We adopt a two-phase training methodology, self-supervised pre-training, and fine-tuning on a downstream supervised classifier. An ablation study conducted across three datasets employing various encoder backbones, without data fusion, with low data availability setting, and without self-supervised pre-training scenarios, highlights the robustness of our method. Our findings demonstrate consistent performance across these diverse conditions, showcasing superior generalization capabilities compared to the baseline model, ResNet-50.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:22:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11375v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11375v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 CoCA: Regaining Safety-awareness of Multimodal Large Language Models
  with Constitutional Calibration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiahui Gao, Renjie Pi, Tianyang Han, Han Wu, Lanqing Hong, Lingpeng Kong, Xin Jiang, Zhenguo Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The deployment of multimodal large language models (MLLMs) has demonstrated remarkable success in engaging in conversations involving visual inputs, thanks to the superior power of large language models (LLMs). Those MLLMs are typically built based on the LLMs, with an image encoder to process images into the token embedding space of the LLMs. However, the integration of visual modality has introduced a unique vulnerability: the MLLM becomes susceptible to malicious visual inputs and prone to generating sensitive or harmful responses, even though the LLM has been trained on textual dataset to align with human value. In this paper, we first raise the question: ``Do the MLLMs possess safety-awareness against malicious image inputs?". We find that after adding a principle that specifies the safety requirement into the input of the MLLM, the model's safety awareness becomes boosted. This phenomenon verifies the existence of MLLM's safety-awareness against image inputs, it is only weakened by the modality gap. We then introduce a simple yet effective technique termed CoCA, which amplifies the safety-awareness of the MLLM by calibrating its output distribution. Our proposed strategy helps the model reclaim its original safety awareness without losing its original capabilities. We verify the effectiveness of our approach on both multimodal safety and understanding benchmarks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:14:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11365v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11365v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 AI Suggestions Homogenize Writing Toward Western Styles and Diminish
  Cultural Nuances</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dhruv Agarwal, Mor Naaman, Aditya Vashistha
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) are being increasingly integrated into everyday products and services, such as coding tools and writing assistants. As these embedded AI applications are deployed globally, there is a growing concern that the AI models underlying these applications prioritize Western values. This paper investigates what happens when a Western-centric AI model provides writing suggestions to users from a different cultural background. We conducted a cross-cultural controlled experiment with 118 participants from India and the United States who completed culturally grounded writing tasks with and without AI suggestions. Our analysis reveals that AI provided greater efficiency gains for Americans compared to Indians. Moreover, AI suggestions led Indian participants to adopt Western writing styles, altering not just what is written but also how it is written. These findings show that Western-centric AI models homogenize writing toward Western norms, diminishing nuances that differentiate cultural expression.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T17:07:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11360v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11360v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation
  in Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mengfei Liang, Archish Arun, Zekun Wu, Cristian Munoz, Jonathan Lutch, Emre Kazim, Adriano Koshiyama, Philip Treleaven
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Hallucination, the generation of factually incorrect content, is a growing challenge in Large Language Models (LLMs). Existing detection and mitigation methods are often isolated and insufficient for domain-specific needs, lacking a standardized pipeline. This paper introduces THaMES (Tool for Hallucination Mitigations and EvaluationS), an integrated framework and library addressing this gap. THaMES offers an end-to-end solution for evaluating and mitigating hallucinations in LLMs, featuring automated test set generation, multifaceted benchmarking, and adaptable mitigation strategies. It automates test set creation from any corpus, ensuring high data quality, diversity, and cost-efficiency through techniques like batch processing, weighted sampling, and counterfactual validation. THaMES assesses a model's ability to detect and reduce hallucinations across various tasks, including text generation and binary classification, applying optimal mitigation strategies like In-Context Learning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient Fine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge base of academic papers, political news, and Wikipedia reveal that commercial models like GPT-4o benefit more from RAG than ICL, while open-weight models like Llama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFT significantly enhances the performance of Llama-3.1-8B-Instruct in both evaluation tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T16:55:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11353v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11353v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 Clinical Validation of a Real-Time Machine Learning-based System for the
  Detection of Acute Myeloid Leukemia by Flow Cytometry</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lauren M. Zuromski, Jacob Durtschi, Aimal Aziz, Jeffrey Chumley, Mark Dewey, Paul English, Muir Morrison, Keith Simmon, Blaine Whipple, Brendan O'Fallon, David P. Ng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Machine-learning (ML) models in flow cytometry have the potential to reduce error rates, increase reproducibility, and boost the efficiency of clinical labs. While numerous ML models for flow cytometry data have been proposed, few studies have described the clinical deployment of such models. Realizing the potential gains of ML models in clinical labs requires not only an accurate model, but infrastructure for automated inference, error detection, analytics and monitoring, and structured data extraction. Here, we describe an ML model for detection of Acute Myeloid Leukemia (AML), along with the infrastructure supporting clinical implementation. Our infrastructure leverages the resilience and scalability of the cloud for model inference, a Kubernetes-based workflow system that provides model reproducibility and resource management, and a system for extracting structured diagnoses from full-text reports. We also describe our model monitoring and visualization platform, an essential element for ensuring continued model accuracy. Finally, we present a post-deployment analysis of impacts on turn-around time and compare production accuracy to the original validation statistics.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T16:53:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>q-bio.TO</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11350v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11350v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 Meijer-G Function with Continued Product and Integer Exponent:
  Performance of Multi-Aperture UOWC System over EGG Turbulence</h2>
                <div class="authors">
                    <strong>Authors:</strong> Arvind Kumar, Nikumani Choudhury, Jayendra N. Bandyopadhyay, S. M. Zafaruddin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Signal transmission over underwater optical wireless communication (UOWC) experiences the combined effect of oceanic turbulence and pointing errors statistically modeled using the sum of two Meijer-G functions. There is a research gap in the exact statistical analysis of multi-aperture UOWC systems that use selection combining diversity techniques to enhance performance compared to single-aperture systems. In this paper, we develop a general framework for the continued product and positive integer exponent for the sum of Meijer-G functions to analyze the exact statistical performance of the UOWC system in terms of multivariate Fox-H function for both independent and non-identically distributed (i.ni.d.) and independent and identically distributed (i.i.d.) channels. We also approximate the performance of a multi-aperture UOWC system with i.i.d. channels using the single-variate Fox-H function. Using the generalized approach, we present analytical expressions for average bit-error rate (BER) and ergodic capacity for the considered system operating over exponential generalized gamma (EGG) oceanic turbulence combined with zero-boresight pointing errors. We also develop asymptotic expressions for the average BER at a high signal-to-noise (SNR) to capture insights into the system's performance. Our simulation findings confirm the accuracy of our derived expressions and illustrate the impact of turbulence parameters for i.ni.d. and i.i.d. models for the average BER and ergodic capacity, which may provide a better estimate for the efficient deployment of UOWC.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T16:39:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span><span>cs.IT</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11337v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11337v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 Strengthening Solidity Invariant Generation: From Post- to
  Pre-Deployment</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kartik Kaushik, Raju Halder, Samrat Mondal
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Invariants are essential for ensuring the security and correctness of Solidity smart contracts, particularly in the context of blockchain's immutability and decentralized execution. This paper introduces InvSol, a novel framework for pre-deployment invariant generation tailored specifically for Solidity smart contracts. Unlike existing solutions, namely InvCon, InvCon+, and Trace2Inv, that rely on post-deployment transaction histories on Ethereum mainnet, InvSol identifies invariants before deployment and offers comprehensive coverage of Solidity language constructs, including loops. Additionally, InvSol incorporates custom templates to effectively prevent critical issues such as reentrancy, out-of-gas errors, and exceptions during invariant generation. We rigorously evaluate InvSol using a benchmark set of smart contracts and compare its performance with state-of-the-art solutions. Our findings reveal that InvSol significantly outperforms these tools, demonstrating its effectiveness in handling new contracts with limited transaction histories. Notably, InvSol achieves a 15% improvement in identifying common vulnerabilities compared to InvCon+ and is able to address certain crucial vulnerabilities using specific invariant templates, better than Trace2Inv.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T16:31:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.PL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.01804v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.01804v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 MonoCoder: Domain-Specific Code Language Model for HPC Codes and Tasks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tal Kadosh, Niranjan Hasabnis, Vy A. Vo, Nadav Schneider, Neva Krien, Mihai Capota, Abdul Wasay, Nesreen Ahmed, Ted Willke, Guy Tamir, Yuval Pinter, Timothy Mattson, Gal Oren
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With easier access to powerful compute resources, there is a growing trend in AI for software development to develop large language models (LLMs) to address a variety of programming tasks. Even LLMs applied to tasks from the high-performance computing (HPC) domain are huge in size and demand expensive compute resources for training. This is partly because LLMs for HPC tasks are obtained by finetuning existing LLMs that support several natural and/or programming languages. We found this design choice confusing - why do we need LLMs trained on natural languages and programming languages unrelated to HPC for HPC-specific tasks? In this line of work, we aim to question choices made by existing LLMs by developing smaller language models (LMs) for specific domains - we call them domain-specific LMs. Specifically, we start with HPC as a domain and build an HPC-specific LM, named MonoCoder, which is orders of magnitude smaller than existing LMs but delivers better performance on non-HPC and HPC codes. Specifically, we pre-trained MonoCoder on an HPC-specific dataset (named HPCorpus) of C and C++ programs mined from GitHub. We evaluated the performance of MonoCoder against state-of-the-art multi-lingual LLMs. Results demonstrate that MonoCoder, although much smaller than existing LMs, outperforms other LLMs on normalized-perplexity tests (in relation to model size) while also delivering competing CodeBLEU scores for high-performance and parallel code generations. In other words, results suggest that MonoCoder understands HPC code better than state-of-the-art LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T16:29:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PL</span><span>cs.AI</span><span>cs.LG</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2312.13322v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2312.13322v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 CodePori: Large-Scale System for Autonomous Software Development Using
  Multi-Agent Technology</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zeeshan Rasheed, Malik Abdul Sami, Kai-Kristian Kemell, Muhammad Waseem, Mika Saari, Kari Systä, Pekka Abrahamsson
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Context: Large Language Models (LLMs) and Generative Pre-trained Transformers (GPTs) have transformed the field of Software Engineering (SE). Existing LLM-based multi-agent models have successfully addressed basic dialogue tasks. However, the potential of LLMs for more challenging tasks, such as automated code generation for large and complex projects, has been investigated in only a few existing works. Objective: This paper aims to investigate the potential of LLM-based agents in the software industry, particularly in enhancing productivity and reducing time-to-market for complex software solutions. Our primary objective is to gain insights into how these agents can fundamentally transform the development of large-scale software. Methods: We introduce CodePori, a novel system designed to automate code generation for large and complex software projects based on functional and non-functional requirements defined by stakeholders. To assess the proposed system performance, we utilized the HumanEval benchmark and manually tested the CodePori model, providing 20 different project descriptions as input and then evaluated the code accuracy by manually executing the code. Results: CodePori is able to generate running code for large-scale projects, aligned with the typical software development process. The HumanEval benchmark results indicate that CodePori improves code accuracy by 89%. A manual assessment conducted by the first author shows that the CodePori system achieved an accuracy rate of 85%. Conclusion: Based on the results, our conclusion is that proposed system demonstrates the transformative potential of LLM-based agents in SE, highlighting their practical applications and opening new opportunities for broader adoption in both industry and academia. Our project is publicly available at https://github.com/GPT-Laboratory/CodePori.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T15:57:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.01411v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.01411v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 Zero-resource Hallucination Detection for Text Generation via
  Graph-based Contextual Knowledge Triples Modeling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xinyue Fang, Zhen Huang, Zhiliang Tian, Minghui Fang, Ziyi Pan, Quntian Fang, Zhihua Wen, Hengyue Pan, Dongsheng Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> LLMs obtain remarkable performance but suffer from hallucinations. Most research on detecting hallucination focuses on the questions with short and concrete correct answers that are easy to check the faithfulness. Hallucination detections for text generation with open-ended answers are more challenging. Some researchers use external knowledge to detect hallucinations in generated texts, but external resources for specific scenarios are hard to access. Recent studies on detecting hallucinations in long text without external resources conduct consistency comparison among multiple sampled outputs. To handle long texts, researchers split long texts into multiple facts and individually compare the consistency of each pairs of facts. However, these methods (1) hardly achieve alignment among multiple facts; (2) overlook dependencies between multiple contextual facts. In this paper, we propose a graph-based context-aware (GCA) hallucination detection for text generations, which aligns knowledge facts and considers the dependencies between contextual knowledge triples in consistency comparison. Particularly, to align multiple facts, we conduct a triple-oriented response segmentation to extract multiple knowledge triples. To model dependencies among contextual knowledge triple (facts), we construct contextual triple into a graph and enhance triples' interactions via message passing and aggregating via RGCN. To avoid the omission of knowledge triples in long text, we conduct a LLM-based reverse verification via reconstructing the knowledge triples. Experiments show that our model enhances hallucination detection and excels all baselines.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-18T05:42:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11283v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11283v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 Leveraging Distillation Techniques for Document Understanding: A Case
  Study with FLAN-T5</h2>
                <div class="authors">
                    <strong>Authors:</strong> Marcel Lamott, Muhammad Armaghan Shakir
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The surge of digital documents in various formats, including less standardized documents such as business reports and environmental assessments, underscores the growing importance of Document Understanding. While Large Language Models (LLMs) have showcased prowess across diverse natural language processing tasks, their direct application to Document Understanding remains a challenge. Previous research has demonstrated the utility of LLMs in this domain, yet their significant computational demands make them challenging to deploy effectively. Additionally, proprietary Blackbox LLMs often outperform their open-source counterparts, posing a barrier to widespread accessibility. In this paper, we delve into the realm of document understanding, leveraging distillation methods to harness the power of large LLMs while accommodating computational limitations. Specifically, we present a novel approach wherein we distill document understanding knowledge from the proprietary LLM ChatGPT into FLAN-T5. Our methodology integrates labeling and curriculum-learning mechanisms to facilitate efficient knowledge transfer. This work contributes to the advancement of document understanding methodologies by offering a scalable solution that bridges the gap between resource-intensive LLMs and practical applications. Our findings underscore the potential of distillation techniques in facilitating the deployment of sophisticated language models in real-world scenarios, thereby fostering advancements in natural language processing and document comprehension domains.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T15:37:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11282v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11282v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 P-RAG: Progressive Retrieval Augmented Generation For Planning on
  Embodied Everyday Task</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weiye Xu, Min Wang, Wengang Zhou, Houqiang Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Embodied Everyday Task is a popular task in the embodied AI community, requiring agents to make a sequence of actions based on natural language instructions and visual observations. Traditional learning-based approaches face two challenges. Firstly, natural language instructions often lack explicit task planning. Secondly, extensive training is required to equip models with knowledge of the task environment. Previous works based on Large Language Model (LLM) either suffer from poor performance due to the lack of task-specific knowledge or rely on ground truth as few-shot samples. To address the above limitations, we propose a novel approach called Progressive Retrieval Augmented Generation (P-RAG), which not only effectively leverages the powerful language processing capabilities of LLMs but also progressively accumulates task-specific knowledge without ground-truth. Compared to the conventional RAG methods, which retrieve relevant information from the database in a one-shot manner to assist generation, P-RAG introduces an iterative approach to progressively update the database. In each iteration, P-RAG retrieves the latest database and obtains historical information from the previous interaction as experiential references for the current interaction. Moreover, we also introduce a more granular retrieval scheme that not only retrieves similar tasks but also incorporates retrieval of similar situations to provide more valuable reference experiences. Extensive experiments reveal that P-RAG achieves competitive results without utilizing ground truth and can even further improve performance through self-iterations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T15:29:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.CL</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11279v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11279v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 Hackphyr: A Local Fine-Tuned LLM Agent for Network Security Environments</h2>
                <div class="authors">
                    <strong>Authors:</strong> Maria Rigaki, Carlos Catania, Sebastian Garcia
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have shown remarkable potential across various domains, including cybersecurity. Using commercial cloud-based LLMs may be undesirable due to privacy concerns, costs, and network connectivity constraints. In this paper, we present Hackphyr, a locally fine-tuned LLM to be used as a red-team agent within network security environments. Our fine-tuned 7 billion parameter model can run on a single GPU card and achieves performance comparable with much larger and more powerful commercial models such as GPT-4. Hackphyr clearly outperforms other models, including GPT-3.5-turbo, and baselines, such as Q-learning agents in complex, previously unseen scenarios. To achieve this performance, we generated a new task-specific cybersecurity dataset to enhance the base model's capabilities. Finally, we conducted a comprehensive analysis of the agents' behaviors that provides insights into the planning abilities and potential shortcomings of such agents, contributing to the broader understanding of LLM-based agents in cybersecurity contexts
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T15:28:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11276v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11276v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 Task Arithmetic for Language Expansion in Speech Translation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yao-Fei Cheng, Hayato Futami, Yosuke Kashiwagi, Emiru Tsunoo, Wen Shen Teo, Siddhant Arora, Shinji Watanabe
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in large language models (LLMs) have gained interest in speech-text multimodal foundation models, achieving strong performance on instruction-based speech translation (ST). However, expanding language pairs from an existing instruction-tuned ST system is costly due to the necessity of re-training on a combination of new and previous datasets. We propose to expand new language pairs by merging the model trained on new language pairs and the existing model, using task arithmetic. We find that the direct application of task arithmetic for ST causes the merged model to fail to follow instructions; thus, generating translation in incorrect languages. To eliminate language confusion, we propose an augmented task arithmetic method that merges an additional language control model. It is trained to generate the correct target language token following the instructions. Our experiments demonstrate that our proposed language control model can achieve language expansion by eliminating language confusion. In our MuST-C and CoVoST-2 experiments, it shows up to 4.66 and 4.92 BLEU scores improvement, respectively. In addition, we demonstrate the use of our task arithmetic framework can expand to a language pair where neither paired ST training data nor a pre-trained ST model is available. We first synthesize the ST system from machine translation (MT) systems via task analogy, then merge the synthesized ST system to the existing ST model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T15:25:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11274v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11274v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 The Sounds of Home: A Speech-Removed Residential Audio Dataset for Sound
  Event Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Gabriel Bibbó, Thomas Deacon, Arshdeep Singh, Mark D. Plumbley
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper presents a residential audio dataset to support sound event detection research for smart home applications aimed at promoting wellbeing for older adults. The dataset is constructed by deploying audio recording systems in the homes of 8 participants aged 55-80 years for a 7-day period. Acoustic characteristics are documented through detailed floor plans and construction material information to enable replication of the recording environments for AI model deployment. A novel automated speech removal pipeline is developed, using pre-trained audio neural networks to detect and remove segments containing spoken voice, while preserving segments containing other sound events. The resulting dataset consists of privacy-compliant audio recordings that accurately capture the soundscapes and activities of daily living within residential spaces. The paper details the dataset creation methodology, the speech removal pipeline utilizing cascaded model architectures, and an analysis of the vocal label distribution to validate the speech removal process. This dataset enables the development and benchmarking of sound event detection models tailored specifically for in-home applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T15:10:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SD</span><span>cs.AI</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11262v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11262v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 Temporal As a Plugin: Unsupervised Video Denoising with Pre-Trained
  Image Denoisers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zixuan Fu, Lanqing Guo, Chong Wang, Yufei Wang, Zhihao Li, Bihan Wen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in deep learning have shown impressive results in image and video denoising, leveraging extensive pairs of noisy and noise-free data for supervision. However, the challenge of acquiring paired videos for dynamic scenes hampers the practical deployment of deep video denoising techniques. In contrast, this obstacle is less pronounced in image denoising, where paired data is more readily available. Thus, a well-trained image denoiser could serve as a reliable spatial prior for video denoising. In this paper, we propose a novel unsupervised video denoising framework, named ``Temporal As a Plugin'' (TAP), which integrates tunable temporal modules into a pre-trained image denoiser. By incorporating temporal modules, our method can harness temporal information across noisy frames, complementing its power of spatial denoising. Furthermore, we introduce a progressive fine-tuning strategy that refines each temporal module using the generated pseudo clean video frames, progressively enhancing the network's denoising performance. Compared to other unsupervised video denoising methods, our framework demonstrates superior performance on both sRGB and raw video denoising datasets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T15:05:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>eess.IV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11256v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11256v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 Towards Novel Malicious Packet Recognition: A Few-Shot Learning Approach</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kyle Stein, Andrew A. Mahyari, Guillermo Francia III, Eman El-Sheikh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As the complexity and connectivity of networks increase, the need for novel malware detection approaches becomes imperative. Traditional security defenses are becoming less effective against the advanced tactics of today's cyberattacks. Deep Packet Inspection (DPI) has emerged as a key technology in strengthening network security, offering detailed analysis of network traffic that goes beyond simple metadata analysis. DPI examines not only the packet headers but also the payload content within, offering a thorough insight into the data traversing the network. This study proposes a novel approach that leverages a large language model (LLM) and few-shot learning to accurately recognizes novel, unseen malware types with few labels samples. Our proposed approach uses a pretrained LLM on known malware types to extract the embeddings from packets. The embeddings are then used alongside few labeled samples of an unseen malware type. This technique is designed to acclimate the model to different malware representations, further enabling it to generate robust embeddings for each trained and unseen classes. Following the extraction of embeddings from the LLM, few-shot learning is utilized to enhance performance with minimal labeled data. Our evaluation, which utilized two renowned datasets, focused on identifying malware types within network traffic and Internet of Things (IoT) environments. Our approach shows promising results with an average accuracy of 86.35% and F1-Score of 86.40% on different malware types across the two datasets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T15:02:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11254v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11254v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 GenQ: Quantization in Low Data Regimes with Generative Synthetic Data</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuhang Li, Youngeun Kim, Donghyun Lee, Souvik Kundu, Priyadarshini Panda
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the realm of deep neural network deployment, low-bit quantization presents a promising avenue for enhancing computational efficiency. However, it often hinges on the availability of training data to mitigate quantization errors, a significant challenge when data availability is scarce or restricted due to privacy or copyright concerns. Addressing this, we introduce GenQ, a novel approach employing an advanced Generative AI model to generate photorealistic, high-resolution synthetic data, overcoming the limitations of traditional methods that struggle to accurately mimic complex objects in extensive datasets like ImageNet. Our methodology is underscored by two robust filtering mechanisms designed to ensure the synthetic data closely aligns with the intrinsic characteristics of the actual training data. In case of limited data availability, the actual data is used to guide the synthetic data generation process, enhancing fidelity through the inversion of learnable token embeddings. Through rigorous experimentation, GenQ establishes new benchmarks in data-free and data-scarce quantization, significantly outperforming existing methods in accuracy and efficiency, thereby setting a new standard for quantization in low data regimes. Code is released at \url{https://github.com/Intelligent-Computing-Lab-Yale/GenQ}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T14:49:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2312.05272v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2312.05272v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded
  Attributions and Learning to Refuse</h2>
                <div class="authors">
                    <strong>Authors:</strong> Maojia Song, Shang Hong Sim, Rishabh Bhardwaj, Hai Leong Chieu, Navonil Majumder, Soujanya Poria
                </div>
                <div class="summary">
                    <strong>Summary:</strong> LLMs are an integral part of retrieval-augmented generation (RAG) systems. While many studies focus on evaluating the quality of end-to-end RAG systems, there is a lack of research on understanding the appropriateness of an LLM for the RAG task. Thus, we introduce a new metric, Trust-Score, that provides a holistic evaluation of the trustworthiness of LLMs in an RAG framework. We show that various prompting methods, such as in-context learning, fail to adapt LLMs effectively to the RAG task. Thus, we propose Trust-Align, a framework to align LLMs for higher Trust-Score. LLaMA-3-8b, aligned with our method, significantly outperforms open-source LLMs of comparable sizes on ASQA (up 10.7), QAMPARI (up 29.2) and ELI5 (up 14.9). We release our code at: https://github.com/declare-lab/trust-align.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T14:47:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11242v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11242v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 ExploreSelf: Fostering User-driven Exploration and Reflection on
  Personal Challenges with Adaptive Guidance by Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Inhwa Song, SoHyun Park, Sachin R. Pendse, Jessica Lee Schleider, Munmun De Choudhury, Young-Ho Kim
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Expressing stressful experiences in words is proven to improve mental and physical health, but individuals often disengage with writing interventions as they struggle to organize their thoughts and emotions. Reflective prompts have been used to provide direction, and large language models (LLMs) have demonstrated the potential to provide tailored guidance. Current systems often limit users' flexibility to direct their reflections. We thus present ExploreSelf, an LLM-driven application designed to empower users to control their reflective journey. ExploreSelf allows users to receive adaptive support through dynamically generated questions. Through an exploratory study with 19 participants, we examine how participants explore and reflect on personal challenges using ExploreSelf. Our findings demonstrate that participants valued the balance between guided support and freedom to control their reflective journey, leading to deeper engagement and insight. Building on our findings, we discuss implications for designing LLM-driven tools that promote user empowerment through effective reflective practices.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T14:44:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span><span>cs.CL</span><span>H.5.2; I.2.7</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.09662v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.09662v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 LLM-as-a-Judge & Reward Model: What They Can and Cannot Do</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guijin Son, Hyunwoo Ko, Hoyoung Lee, Yewon Kim, Seunghyeok Hong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> LLM-as-a-Judge and reward models are widely used alternatives of multiple-choice questions or human annotators for large language model (LLM) evaluation. Their efficacy shines in evaluating long-form responses, serving a critical role as evaluators of leaderboards and as proxies to align LLMs via reinforcement learning. However, despite their popularity, their effectiveness outside of English remains largely unexplored. In this paper, we conduct a comprehensive analysis on automated evaluators, reporting key findings on their behavior in a non-English environment. First, we discover that English evaluation capabilities significantly influence language-specific capabilities, often more than the language proficiency itself, enabling evaluators trained in English to easily transfer their skills to other languages. Second, we identify critical shortcomings, where LLMs fail to detect and penalize errors, such as factual inaccuracies, cultural misrepresentations, and the presence of unwanted language. Finally, we release Kudge, the first non-English meta-evaluation dataset containing 5,012 human annotations in Korean.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T14:40:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11239v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11239v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 Evaluating the Impact of Compression Techniques on Task-Specific
  Performance of Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bishwash Khanal, Jeffery M. Capone
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) offer powerful capabilities but incur substantial computational costs, driving the need for efficient compression techniques. This study evaluates the impact of popular compression methods - Magnitude Pruning, SparseGPT, and Wanda - on the LLaMA-2-7B model, focusing on the trade-offs between model size reduction, downstream task performance, and the role of calibration data. Our findings reveal that while SparseGPT and Wanda preserve perplexity even at 50% sparsity, they suffer significant degradation on downstream tasks, highlighting the inadequacy of perplexity as the sole evaluation metric. To address this, we introduce Jensen-Shannon (JS) Divergence as a more comprehensive metric that captures nuanced changes in model behavior post-compression. We further demonstrate that task-specific calibration data significantly enhances the downstream performance of compressed models compared to general calibration data. This research underscores the necessity for diverse evaluation metrics and careful calibration data selection to fully understand the complexities of LLM compression and its implications for practical applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T14:34:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11233v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11233v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 Fast Analysis of the OpenAI O1-Preview Model in Solving Random K-SAT
  Problem: Does the LLM Solve the Problem Itself or Call an External SAT
  Solver?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Raffaele Marino
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this manuscript I present an analysis on the performance of OpenAI O1-preview model in solving random K-SAT instances for K$\in {2,3,4}$ as a function of $\alpha=M/N$ where $M$ is the number of clauses and $N$ is the number of variables of the satisfiable problem. I show that the model can call an external SAT solver to solve the instances, rather than solving them directly. Despite using external solvers, the model reports incorrect assignments as output. Moreover, I propose and present an analysis to quantify whether the OpenAI O1-preview model demonstrates a spark of intelligence or merely makes random guesses when outputting an assignment for a Boolean satisfiability problem.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T14:29:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cond-mat.dis-nn</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11232v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11232v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 EmPO: Emotion Grounding for Empathetic Response Generation through
  Preference Optimization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ondrej Sotolar, Vojtech Formanek, Alok Debnath, Allison Lahnala, Charles Welch, Lucie FLek
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Empathetic response generation is a desirable aspect of conversational agents, crucial for facilitating engaging and emotionally intelligent multi-turn conversations between humans and machines. Leveraging large language models for this task has shown promising results, yet challenges persist in ensuring both the empathetic quality of the responses and retention of the generalization performance of the models. We propose a novel approach where we construct theory-driven preference datasets based on emotion grounding and use them to align LLMs with preference optimization algorithms to address these challenges. To evaluate empathetic response generation, we employ the EmpatheticDialogues dataset, assessing empathy with the diff-Epitome and BERTscore metrics and with multi-dimensional human evaluation. Additionally, we measure diversity and emotional valence using feature-based methods. We also evaluate the impact of training on the generalization performance using the MMLU benchmark and tasks from the Open LLM Leaderboard. The results show that LLMs can be aligned for empathetic response generation by preference optimization while retaining their general performance and that emotion grounding can guide preference dataset creation. We make all datasets, source code, and models publicly available. https://github.com/justtherightsize/empo
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T14:24:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>68T50</span><span>I.2.7</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.19071v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.19071v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 A Human-Centered Risk Evaluation of Biometric Systems Using Conjoint
  Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tetsushi Ohki, Narishige Abe, Hidetsugu Uchida, Shigefumi Yamada
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Biometric recognition systems, known for their convenience, are widely adopted across various fields. However, their security faces risks depending on the authentication algorithm and deployment environment. Current risk assessment methods faces significant challenges in incorporating the crucial factor of attacker's motivation, leading to incomplete evaluations. This paper presents a novel human-centered risk evaluation framework using conjoint analysis to quantify the impact of risk factors, such as surveillance cameras, on attacker's motivation. Our framework calculates risk values incorporating the False Acceptance Rate (FAR) and attack probability, allowing comprehensive comparisons across use cases. A survey of 600 Japanese participants demonstrates our method's effectiveness, showing how security measures influence attacker's motivation. This approach helps decision-makers customize biometric systems to enhance security while maintaining usability.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T14:18:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.CR</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11224v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11224v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 AACessTalk: Fostering Communication between Minimally Verbal Autistic
  Children and Parents with Contextual Guidance and Card Recommendation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dasom Choi, SoHyun Park, Kyungah Lee, Hwajung Hong, Young-Ho Kim
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As minimally verbal autistic (MVA) children communicate with parents through few words and nonverbal cues, parents often struggle to encourage their children to express subtle emotions and needs and to grasp their nuanced signals. We present AACessTalk, a tablet-based, AI-mediated communication system that facilitates meaningful exchanges between an MVA child and a parent. AACessTalk provides real-time guides to the parent to engage the child in conversation and, in turn, recommends contextual vocabulary cards to the child. Through a two-week deployment study with 11 MVA child-parent dyads, we examine how AACessTalk fosters everyday conversation practice and mutual engagement. Our findings show high engagement from all dyads, leading to increased frequency of conversation and turn-taking. AACessTalk also encouraged parents to explore their own interaction strategies and empowered the children to have more agency in communication. We discuss the implications of designing technologies for balanced communication dynamics in parent-MVA child interaction.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T14:16:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span><span>H.5.2; I.2.7</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.09641v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.09641v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 Exploring ChatGPT-based Augmentation Strategies for Contrastive
  Aspect-based Sentiment Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lingling Xu, Haoran Xie, S. Joe Qin, Fu Lee Wang, Xiaohui Tao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Aspect-based sentiment analysis (ABSA) involves identifying sentiment towards specific aspect terms in a sentence and allows us to uncover nuanced perspectives and attitudes on particular aspects of a product, service, or topic. However, the scarcity of labeled data poses a significant challenge to training high-quality models. To address this issue, we explore the potential of data augmentation using ChatGPT, a well-performing large language model (LLM), to enhance the sentiment classification performance towards aspect terms. Specifically, we explore three data augmentation strategies based on ChatGPT: context-focused, aspect-focused, and context-aspect data augmentation techniques. Context-focused data augmentation focuses on changing the word expression of context words in the sentence while keeping aspect terms unchanged. In contrast, aspect-focused data augmentation aims to change aspect terms but keep context words unchanged. Context-Aspect data augmentation integrates the above two data augmentations to generate augmented samples. Furthermore, we incorporate contrastive learning into the ABSA tasks to improve performance. Extensive experiments show that all three data augmentation techniques lead to performance improvements, with the context-aspect data augmentation strategy performing best and surpassing the performance of the baseline models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T14:12:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11218v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11218v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 Ideal-LLM: Integrating Dual Encoders and Language-Adapted LLM for
  Multilingual Speech-to-Text</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hongfei Xue, Wei Ren, Xuelong Geng, Kun Wei, Longhao Li, Qijie Shao, Linju Yang, Kai Diao, Lei Xie
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Integrating audio encoders with LLMs through connectors has enabled these models to process and comprehend audio modalities, significantly enhancing speech-to-text tasks, including automatic speech recognition (ASR) and automatic speech translation (AST). However, these methods often overlook the critical aspect of language adaptation in multilingual settings, relying instead on multilingual data without adequately addressing language differences. To address this gap, we propose the Ideal-LLM model, which employs dual multilingual encoders to enrich language feature information and utilizes a language-adapted connector to target the adaptation of each language specifically. By leveraging the complementary strengths of Whisper and MMS encoders, our approach ensures richer multilingual representations. Additionally, the language-adapted connector enhances modal transformation via a language weight selector tailored for each language. Experimental results demonstrate that Ideal-LLM significantly improves ASR performance, achieving a 32.6% relative reduction in average word error rates compared to the standard speech encoder integrated with LLMs and yields an average BLEU score of 36.78 for AST task.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T14:10:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.AS</span><span>cs.SD</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11214v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11214v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 Self-Evolutionary Large Language Models through Uncertainty-Enhanced
  Preference Optimization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jianing Wang, Yang Zhou, Xiaocheng Zhang, Mengjiao Bao, Peng Yan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Iterative preference optimization has recently become one of the de-facto training paradigms for large language models (LLMs), but the performance is still underwhelming due to too much noisy preference data yielded in the loop. To combat this issue, we present an \textbf{U}ncertainty-enhanced \textbf{P}reference \textbf{O}ptimization (UPO) framework to make the LLM self-evolve with reliable feedback. The key idea is mitigating the noisy preference data derived from the current policy and reward models by performing pair-wise uncertainty estimation and judiciously reliable feedback sampling. To reach this goal, we thus introduce an estimator model, which incorporates Monte Carlo (MC) dropout in Bayesian neural network (BNN) to perform uncertainty estimation for the preference data derived from the LLM policy. Compared to the existing methods that directly filter generated responses based on the reward score, the estimator focuses on the model uncertainty in a pair-wise manner and effectively bypasses the confirmation bias problem of the reward model. Additionally, we also propose an uncertainty-enhanced self-evolution algorithm to improve the robustness of preference optimization and encourage the LLM to generate responses with both high reward and certainty. Extensive experiments over multiple benchmarks demonstrate that our framework substantially alleviates the noisy problem and improves the performance of iterative preference optimization.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T14:05:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11212v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11212v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 Can Many-Shot In-Context Learning Help LLMs as Evaluators? A Preliminary
  Empirical Study</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mingyang Song, Mao Zheng, Xuan Luo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Utilizing Large Language Models (LLMs) as evaluators for evaluating the performance of LLMs has recently garnered attention. However, this kind of evaluation approach is affected by potential biases in LLMs, raising concerns about the accuracy and reliability of the evaluation results. To mitigate this issue, we propose and study two many-shot ICL prompts, which rely on two versions of many-shot ICL prompt templates for helping LLM evaluators to mitigate the potential biases in LLMs, \textbf{M}any-\textbf{S}hot \textbf{w}ith \textbf{R}eference (\textbf{MSwR}) and \textbf{M}any-\textbf{S}hot with\textbf{o}ut \textbf{R}eference (\textbf{MSoR}). Concretely, the former utilizes in-context examples with model-generated rationales as guidance, and the latter without. Based on the designed prompts, we investigate the impact of scaling the number of in-context examples on the consistency and quality of the evaluation results. Experimental results show that advanced LLMs, such as GPT-4o, perform better in the many-shot regime than in the zero-shot regime. Furthermore, we reveal the symbol bias hidden in the selection bias of LLMs and propose a simple yet effective approach to mitigate the bias. Experimental results further verify the effectiveness of the symbol bias mitigation approach.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T14:04:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.11629v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.11629v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 Energy Efficiency Support for Software Defined Networks: a Serverless
  Computing Approach</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fatemeh Banaie, Karim Djemame, Abdulaziz Alhindi, Vasilios Kelefouras
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Automatic network management strategies have become paramount for meeting the needs of innovative real-time and data-intensive applications, such as in the Internet of Things. However, meeting the ever-growing and fluctuating demands for data and services in such applications requires more than ever an efficient and scalable network resource management approach. Such approach should enable the automated provisioning of services while incentivising energy-efficient resource usage that expands throughout the edge-to-cloud continuum. This paper is the first to realise the concept of modular Software-Defined Networks based on serverless functions in an energy-aware environment. By adopting Function as a Service, the approach enables on-demand deployment of network functions, resulting in cost reduction through fine resource provisioning granularity. An analytical model is presented to approximate the service delivery time and power consumption, as well as an open-source prototype implementation supported by an extensive experimental evaluation. The experiments demonstrate not only the practical applicability of the proposed approach but significant improvement in terms of energy efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T14:02:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11208v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11208v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 A Review of Prominent Paradigms for LLM-Based Agents: Tool Use
  (Including RAG), Planning, and Feedback Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xinzhe Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Tool use, planning, and feedback learning are currently three prominent paradigms for developing Large Language Model (LLM)-based agents across various tasks. Although numerous frameworks have been devised for each paradigm, their intricate workflows and inconsistent taxonomy create challenges in understanding and reviewing the frameworks across different paradigms. This survey introduces a unified taxonomy to systematically review and discuss these frameworks. Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on LMPR implementations and workflow usage across different agent paradigms.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T14:02:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.05804v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.05804v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 LoRa Communication for Agriculture 4.0: Opportunities, Challenges, and
  Future Directions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lameya Aldhaheri, Noor Alshehhi, Irfana Ilyas Jameela Manzil, Ruhul Amin Khalil, Shumaila Javaid, Nasir Saeed, Mohamed-Slim Alouini
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The emerging field of smart agriculture leverages the Internet of Things (IoT) to revolutionize farming practices. This paper investigates the transformative potential of Long Range (LoRa) technology as a key enabler of long-range wireless communication for agricultural IoT systems. By reviewing existing literature, we identify a gap in research specifically focused on LoRa's prospects and challenges from a communication perspective in smart agriculture. We delve into the details of LoRa-based agricultural networks, covering network architecture design, Physical Layer (PHY) considerations tailored to the agricultural environment, and channel modeling techniques that account for soil characteristics. The paper further explores relaying and routing mechanisms that address the challenges of extending network coverage and optimizing data transmission in vast agricultural landscapes. Transitioning to practical aspects, we discuss sensor deployment strategies and energy management techniques, offering insights for real-world deployments. A comparative analysis of LoRa with other wireless communication technologies employed in agricultural IoT applications highlights its strengths and weaknesses in this context. Furthermore, the paper outlines several future research directions to leverage the potential of LoRa-based agriculture 4.0. These include advancements in channel modeling for diverse farming environments, novel relay routing algorithms, integrating emerging sensor technologies like hyper-spectral imaging and drone-based sensing, on-device Artificial Intelligence (AI) models, and sustainable solutions. This survey can guide researchers, technologists, and practitioners to understand, implement, and propel smart agriculture initiatives using LoRa technology.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T13:55:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span><span>cs.ET</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11200v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11200v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 Diversifying the Expert Knowledge for Task-Agnostic Pruning in Sparse
  Mixture-of-Experts</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zeliang Zhang, Xiaodong Liu, Hao Cheng, Chenliang Xu, Jianfeng Gao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> By increasing model parameters but activating them sparsely when performing a task, the use of Mixture-of-Experts (MoE) architecture significantly improves the performance of Large Language Models (LLMs) without increasing the inference cost. However, the memory consumption due to the growing number of experts presents a challenge to the deployment of these models in many real world settings. Our empirical study reveals that some experts encode redundant knowledge during pre-training. We thus propose a method of grouping and pruning similar experts to improve the model's parameter efficiency. We validate the effectiveness of our method by pruning three state-of-the-art MoE architectures, including Mixtral, Deepseek-MoE, and Qwen. The evaluation shows that our method outperforms other model pruning methods on a range of natural language tasks. We will release our code to facilitate future research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T13:48:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.09590v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.09590v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 Towards Ethical Personal AI Applications: Practical Considerations for
  AI Assistants with Long-Term Memory</h2>
                <div class="authors">
                    <strong>Authors:</strong> Eunhae Lee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> One application area of long-term memory (LTM) capabilities with increasing traction is personal AI companions and assistants. With the ability to retain and contextualize past interactions and adapt to user preferences, personal AI companions and assistants promise a profound shift in how we interact with AI and are on track to become indispensable in personal and professional settings. However, this advancement introduces new challenges and vulnerabilities that require careful consideration regarding the deployment and widespread use of these systems. The goal of this paper is to explore the broader implications of building and deploying personal AI applications with LTM capabilities using a holistic evaluation approach. This will be done in three ways: 1) reviewing the technological underpinnings of LTM in Large Language Models, 2) surveying current personal AI companions and assistants, and 3) analyzing critical considerations and implications of deploying and using these applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T13:48:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span><span>cs.AI</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11192v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11192v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 SuperCoder2.0: Technical Report on Exploring the feasibility of LLMs as
  Autonomous Programmer</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anmol Gautam, Kishore Kumar, Adarsh Jha, Mukunda NS, Ishaan Bhola
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present SuperCoder2.0, an advanced autonomous system designed to enhance software development through artificial intelligence. The system combines an AI-native development approach with intelligent agents to enable fully autonomous coding. Key focus areas include a retry mechanism with error output traceback, comprehensive code rewriting and replacement using Abstract Syntax Tree (ast) parsing to minimize linting issues, code embedding technique for retrieval-augmented generation, and a focus on localizing methods for problem-solving rather than identifying specific line numbers. The methodology employs a three-step hierarchical search space reduction approach for code base navigation and bug localization:utilizing Retrieval Augmented Generation (RAG) and a Repository File Level Map to identify candidate files, (2) narrowing down to the most relevant files using a File Level Schematic Map, and (3) extracting 'relevant locations' within these files. Code editing is performed through a two-part module comprising CodeGeneration and CodeEditing, which generates multiple solutions at different temperature values and replaces entire methods or classes to maintain code integrity. A feedback loop executes repository-level test cases to validate and refine solutions. Experiments conducted on the SWE-bench Lite dataset demonstrate SuperCoder2.0's effectiveness, achieving correct file localization in 84.33% of cases within the top 5 candidates and successfully resolving 34% of test instances. This performance places SuperCoder2.0 fourth globally on the SWE-bench leaderboard. The system's ability to handle diverse repositories and problem types highlights its potential as a versatile tool for autonomous software development. Future work will focus on refining the code editing process and exploring advanced embedding models for improved natural language to code mapping.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T13:44:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11190v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11190v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 Digital Ecosystem for FAIR Time Series Data Management in Environmental
  System Science</h2>
                <div class="authors">
                    <strong>Authors:</strong> J. Bumberger, M. Abbrent, N. Brinckmann, J. Hemmen, R. Kunkel, C. Lorenz, P. Lünenschloß, B. Palm, T. Schnicke, C. Schulz, H. van der Schaaf, D. Schäfer
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Addressing the challenges posed by climate change, biodiversity loss, and environmental pollution requires comprehensive monitoring and effective data management strategies that are applicable across various scales in environmental system science. This paper introduces a versatile and transferable digital ecosystem for managing time series data, designed to adhere to the FAIR principles (Findable, Accessible, Interoperable, and Reusable). The system is highly adaptable, cloud-ready, and suitable for deployment in a wide range of settings, from small-scale projects to large-scale monitoring initiatives. The ecosystem comprises three core components: the Sensor Management System (SMS) for detailed metadata registration and management; timeIO, a platform for efficient time series data storage, transfer, and real-time visualization; and the System for Automated Quality Control (SaQC), which ensures data integrity through real-time analysis and quality assurance. The modular architecture, combined with standardized protocols and interfaces, ensures that the ecosystem can be easily transferred and deployed across different environments and institutions. This approach enhances data accessibility for a broad spectrum of stakeholders, including researchers, policymakers, and the public, while fostering collaboration and advancing scientific research in environmental monitoring.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T13:25:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.03351v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.03351v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 UltimateDO: An Efficient Framework to Marry Occupancy Prediction with 3D
  Object Detection via Channel2height</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zichen Yu, Changyong Shu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Occupancy and 3D object detection are characterized as two standard tasks in modern autonomous driving system. In order to deploy them on a series of edge chips with better precision and time-consuming trade-off, contemporary approaches either deploy standalone models for individual tasks, or design a multi-task paradigm with separate heads. However, they might suffer from deployment difficulties (i.e., 3D convolution, transformer and so on) or deficiencies in task coordination. Instead, we argue that a favorable framework should be devised in pursuit of ease deployment on diverse chips and high precision with little time-consuming. Oriented at this, we revisit the paradigm for interaction between 3D object detection and occupancy prediction, reformulate the model with 2D convolution and prioritize the tasks such that each contributes to other. Thus, we propose a method to achieve fast 3D object detection and occupancy prediction (UltimateDO), wherein the light occupancy prediction head in FlashOcc is married to 3D object detection network, with negligible additional timeconsuming of only 1.1ms while facilitating each other. We instantiate UltimateDO on the challenging nuScenes-series benchmarks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T13:14:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11160v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11160v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 What Lies Beneath? Exploring the Impact of Underlying AI Model Updates
  in AI-Infused Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Vikram Mohanty, Jude Lim, Kurt Luther
                </div>
                <div class="summary">
                    <strong>Summary:</strong> AI models are constantly evolving, with new versions released frequently. This raises a key question: how should AI-infused systems integrate updates when the downstream impact on user experience and performance is unclear? Human-AI interaction guidelines encourage notifying users about (changes in) model capabilities, ideally supported by thorough benchmarking. Yet, as AI models integrate into domain-specific workflows, exhaustive benchmarking can become impractical or expensive, often leading to invisible or minimally communicated updates. In this work, we explore the impact of such updates through two complementary studies on facial recognition for historical person identification. First, we conducted an online experiment to understand how users distinguish between models, followed by a diary study examining user perceptions in a real-world deployment. Our findings reveal how model changes impact human-AI performance, downstream user behavior, and the folk theories they develop. Based on these insights, we discuss implications for updating models in AI-infused systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T13:14:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2311.10652v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2311.10652v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 On Performance of Distributed RIS-aided Communication in Random Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jindan Xu, Wei Xu, Chau Yuen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper evaluates the geometrically averaged performance of a wireless communication network assisted by a multitude of distributed reconfigurable intelligent surfaces (RISs), where the RIS locations are randomly dropped obeying a homogeneous Poisson point process. By exploiting stochastic geometry and then averaging over the random locations of RISs as well as the serving user, we first derive a closed-form expression for the spatially ergodic rate in the presence of phase errors at the RISs in practice. Armed with this closed-form characterization, we then optimize the RIS deployment under a reasonable and fair constraint of a total number of RIS elements per unit area. The optimal configurations in terms of key network parameters, including the RIS deployment density and the array sizes of RISs, are disclosed for the spatially ergodic rate maximization. Our findings suggest that deploying larger-size RISs with reduced deployment density is theoretically preferred to support extended RIS coverages, under the cases of bounded phase shift errors. However, when dealing with random phase shifts, the reflecting elements are recommended to spread out as much as possible, disregarding the deployment cost. Furthermore, the spatially ergodic rate loss due to the phase shift errors is quantitatively characterized. For bounded phase shift errors, the rate loss is eventually upper bounded by a constant as $N\rightarrow\infty$, where $N$ is the number of reflecting elements at each RIS. While for random phase shifts, this rate loss scales up in the order of $\log N$. These analytical observations are validated through numerical results.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T13:10:08Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11156v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11156v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 Reasoning Graph Enhanced Exemplars Retrieval for In-Context Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yukang Lin, Bingchen Zhong, Shuoran Jiang, Joanna Siebert, Qingcai Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models(LLMs) have exhibited remarkable few-shot learning capabilities and unified the paradigm of NLP tasks through the in-context learning(ICL) technique. Despite the success of ICL, the quality of the exemplar demonstrations can significantly influence the LLM's performance. Existing exemplar selection methods mainly focus on the semantic similarity between queries and candidate exemplars. On the other hand, the logical connections between reasoning steps can be beneficial to depict the problem-solving process as well. In this paper, we proposes a novel method named Reasoning Graph-enhanced Exemplar Retrieval(RGER). RGER first quires LLM to generate an initial response, then expresses intermediate problem-solving steps to a graph structure. After that, it employs graph kernel to select exemplars with semantic and structural similarity. Extensive experiments demonstrate the structural relationship is helpful to the alignment of queries and candidate exemplars. The efficacy of RGER on math and logit reasoning tasks showcases its superiority over state-of-the-art retrieval-based approaches. Our code is released at https://github.com/Yukang-Lin/RGER.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T12:58:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11147v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11147v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 Privacy-Preserving Billing for Local Energy Markets</h2>
                <div class="authors">
                    <strong>Authors:</strong> Eman Alqahtani, Mustafa A. Mustafa
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We propose a privacy-preserving billing protocol for local energy markets (PBP-LEM) that takes into account market participants' energy volume deviations from their bids. PBP-LEM enables a group of market entities to jointly compute participants' bills in a decentralized and privacy-preserving manner without sacrificing correctness. It also mitigates risks on individuals' privacy arising from any potential internal collusion. We first propose an efficient and privacy-preserving individual billing scheme, achieving information-theoretic security, which serves as a building block. PBP-LEM utilizes this scheme, along with other techniques such as multiparty computation, inner product functional encryption and Pedersen commitments to ensure data confidentiality and accuracy. Additionally, we present three approaches, resulting in different levels of privacy protection and performance. We prove that the protocol meets its security and privacy requirements and is feasible for deployment in real LEMs: bills can be computed in less than five minutes for 4,000 users using the most computationally intensive approach, and in just 0.18 seconds using the least intensive one.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T12:42:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.15886v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.15886v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 The Invalsi Benchmarks: measuring Linguistic and Mathematical
  understanding of Large Language Models in Italian</h2>
                <div class="authors">
                    <strong>Authors:</strong> Giovanni Puccetti, Maria Cassese, Andrea Esuli
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While Italian is a high-resource language, there are few Italian-native benchmarks to evaluate generative Large Language Models (LLMs) in this language. This work presents three new benchmarks: Invalsi MATE to evaluate models performance on mathematical understanding in Italian, Invalsi ITA to evaluate language understanding in Italian and Olimpiadi MATE for more complex mathematical understanding.   The first two benchmarks are based on the Invalsi tests, which are administered to students of age between 6 and 18 within the Italian school system and have been validated by several experts in teaching and pedagogy, the third one comes from the Italian high school math Olympics.   We evaluate 10 powerful language models on these benchmarks and find that they are bound by 71% accuracy on Invasli MATE, achieved by Llama 3.1 70b instruct and by 88% on Invalsi ITA. For both Invalsi MATE and Invalsi ITA we compare LLMs with the average performance of Italian students to show that Llama 3.1 is the only one to outperform them on Invalsi MATE while most models do so on Invalsi ITA, we then show that Olimpiadi MATE is more challenging than Invalsi MATE and the highest accuracy, achieved by Llama 3.1 405b instruct is 45%.   We will make data and evaluation code openly available upon acceptance of the paper.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T12:27:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2403.18697v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2403.18697v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 Schrodinger's Memory: Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wei Wang, Qing Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Memory is the foundation of all human activities; without memory, it would be nearly impossible for people to perform any task in daily life. With the development of Large Language Models (LLMs), their language capabilities are becoming increasingly comparable to those of humans. But do LLMs have memory? Based on current performance, LLMs do appear to exhibit memory. So, what is the underlying mechanism of this memory? Previous research has lacked a deep exploration of LLMs' memory capabilities and the underlying theory. In this paper, we use Universal Approximation Theorem (UAT) to explain the memory mechanism in LLMs. We also conduct experiments to verify the memory capabilities of various LLMs, proposing a new method to assess their abilities based on these memory ability. We argue that LLM memory operates like Schr\"odinger's memory, meaning that it only becomes observable when a specific memory is queried. We can only determine if the model retains a memory based on its output in response to the query; otherwise, it remains indeterminate. Finally, we expand on this concept by comparing the memory capabilities of the human brain and LLMs, highlighting the similarities and differences in their operational mechanisms.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T12:10:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10482v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10482v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 Diversity-grounded Channel Prototypical Learning for Out-of-Distribution
  Intent Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bo Liu, Liming Zhan, Yujie Feng, Zexin Lu, Chengqiang Xie, Lei Xue, Xiao-Ming Wu, Albert Y. S. Lam
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the realm of task-oriented dialogue systems, a robust intent detection mechanism must effectively handle malformed utterances encountered in real-world scenarios. This study presents a novel fine-tuning framework for large language models (LLMs) aimed at enhancing in-distribution (ID) intent classification and out-of-distribution (OOD) intent detection, which utilizes semantic matching with prototypes derived from ID class names. By harnessing the highly distinguishable representations of LLMs, we construct semantic prototypes for each ID class using a diversity-grounded prompt tuning approach. We rigorously test our framework in a challenging OOD context, where ID and OOD classes are semantically close yet distinct, referred to as \emph{near} OOD detection. For a thorough assessment, we benchmark our method against the prevalent fine-tuning approaches. The experimental findings reveal that our method demonstrates superior performance in both few-shot ID intent classification and near-OOD intent detection tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T12:07:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11114v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11114v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 Situated Understanding of Errors in Older Adults' Interactions with
  Voice Assistants: A Month-Long, In-Home Study</h2>
                <div class="authors">
                    <strong>Authors:</strong> Amama Mahmood, Junxiang Wang, Chien-Ming Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Our work addresses the challenges older adults face with commercial Voice Assistants (VAs), notably in conversation breakdowns and error handling. Traditional methods of collecting user experiences-usage logs and post-hoc interviews-do not fully capture the intricacies of older adults' interactions with VAs, particularly regarding their reactions to errors. To bridge this gap, we equipped 15 older adults' homes with smart speakers integrated with custom audio recorders to collect ``in-the-wild'' audio interaction data for detailed error analysis. Recognizing the conversational limitations of current VAs, our study also explored the capabilities of Large Language Models (LLMs) to handle natural and imperfect text for improving VAs. Midway through our study, we deployed ChatGPT-powered VA to investigate its efficacy for older adults. Our research suggests leveraging vocal and verbal responses combined with LLMs' contextual capabilities for enhanced error prevention and management in VAs, while proposing design considerations to align VA capabilities with older adults' expectations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T12:07:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2403.02421v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2403.02421v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 Unveiling Entity-Level Unlearning for Large Language Models: A
  Comprehensive Analysis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weitao Ma, Xiaocheng Feng, Weihong Zhong, Lei Huang, Yangfan Ye, Xiachong Feng, Bing Qin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language model unlearning has garnered increasing attention due to its potential to address security and privacy concerns, leading to extensive research in the field. However, much of this research has concentrated on instance-level unlearning, specifically targeting the removal of predefined instances containing sensitive content. This focus has left a significant gap in the exploration of full entity-level unlearning, which is critical in real-world scenarios such as copyright protection. To this end, we propose a novel task of Entity-level unlearning, which aims to erase entity-related knowledge from the target model completely. To thoroughly investigate this task, we systematically evaluate trending unlearning algorithms, revealing that current methods struggle to achieve effective entity-level unlearning. Then, we further explore the factors that influence the performance of the unlearning algorithms, identifying that knowledge coverage and the size of the forget set play pivotal roles. Notably, our analysis also uncovers that entities introduced through fine-tuning are more vulnerable to unlearning than pre-trained entities. These findings collectively offer valuable insights for advancing entity-level unlearning for LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T12:00:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.15796v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.15796v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 Beyond principlism: Practical strategies for ethical AI use in research
  practices</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhicheng Lin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid adoption of generative artificial intelligence (AI) in scientific research, particularly large language models (LLMs), has outpaced the development of ethical guidelines, leading to a Triple-Too problem: too many high-level ethical initiatives, too abstract principles lacking contextual and practical relevance, and too much focus on restrictions and risks over benefits and utilities. Existing approaches, including principlism (reliance on abstract ethical principles), formalism (rigid application of rules), and technical solutionism (overemphasis on technological fixes), offer little practical guidance for addressing ethical challenges of AI in scientific research practices. To bridge the gap between abstract principles and day-to-day research practices, a user-centered, realism-inspired approach is proposed here. It outlines five specific goals for ethical AI use: 1) understanding model training and output, including bias mitigation strategies; 2) respecting privacy, confidentiality, and copyright; 3) avoiding plagiarism and policy violations; 4) applying AI beneficially compared to alternatives; and 5) using AI transparently and reproducibly. Each goal is accompanied by actionable strategies and realistic cases of misuse and corrective measures. I argue that ethical AI application requires evaluating its utility against existing alternatives rather than isolated performance metrics. Additionally, I propose documentation guidelines to enhance transparency and reproducibility in AI-assisted research. Moving forward, we need targeted professional development, training programs, and balanced enforcement mechanisms to promote responsible AI use while fostering innovation. By refining these ethical guidelines and adapting them to emerging AI capabilities, we can accelerate scientific progress without compromising research integrity.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-18T02:03:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2401.15284v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2401.15284v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 Large language models can replicate cross-cultural differences in
  personality</h2>
                <div class="authors">
                    <strong>Authors:</strong> Paweł Niszczota, Mateusz Janczak, Michał Misiak
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We use a large-scale experiment (N=8000) to determine whether GPT-4 can replicate cross-cultural differences in the Big Five, measured using the Ten-Item Personality Inventory. We used the US and South Korea as the cultural pair, given that prior research suggests substantial personality differences between people from these two countries. We manipulated the target of the simulation (US vs. Korean), the language of the inventory (English vs. Korean), and the language model (GPT-4 vs. GPT-3.5). Our results show that GPT-4 replicated the cross-cultural differences for each factor. However, mean ratings had an upward bias and exhibited lower variation than in the human samples, as well as lower structural validity. We provide preliminary evidence that LLMs can aid cross-cultural researchers and practitioners.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T10:47:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.CY</span><span>I.2.7; K.4.2; J.4</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2310.10679v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2310.10679v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 Large Language Models are Good Multi-lingual Learners : When LLMs Meet
  Cross-lingual Prompts</h2>
                <div class="authors">
                    <strong>Authors:</strong> Teng Wang, Zhenqi He, Wing-Yin Yu, Xiaojin Fu, Xiongwei Han
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the advent of Large Language Models (LLMs), generating rule-based data for real-world applications has become more accessible. Due to the inherent ambiguity of natural language and the complexity of rule sets, especially in long contexts, LLMs often struggle to follow all specified rules, frequently omitting at least one. To enhance the reasoning and understanding of LLMs on long and complex contexts, we propose a novel prompting strategy Multi-Lingual Prompt, namely MLPrompt, which automatically translates the error-prone rule that an LLM struggles to follow into another language, thus drawing greater attention to it. Experimental results on public datasets across various tasks have shown MLPrompt can outperform state-of-the-art prompting methods such as Chain of Thought, Tree of Thought, and Self-Consistency. Additionally, we introduce a framework integrating MLPrompt with an auto-checking mechanism for structured data generation, with a specific case study in text-to-MIP instances. Further, we extend the proposed framework for text-to-SQL to demonstrate its generation ability towards structured data synthesis.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T10:33:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11056v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11056v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language
  Models: An Experimental Analysis up to 405B</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jemin Lee, Sihyeong Park, Jinse Kwon, Jihun Oh, Yongin Kwon
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Prior research works have evaluated quantized LLMs using limited metrics such as perplexity or a few basic knowledge tasks and old datasets. Additionally, recent large-scale models such as Llama 3.1 with up to 405B have not been thoroughly examined. This paper evaluates the performance of instruction-tuned LLMs across various quantization methods (GPTQ, AWQ, SmoothQuant, and FP8) on models ranging from 7B to 405B. Using 13 benchmarks, we assess performance across six task types: commonsense Q\&A, knowledge and language understanding, instruction following, hallucination detection, mathematics, and dialogue. Our key findings reveal that (1) quantizing a larger LLM to a similar size as a smaller FP16 LLM generally performs better across most benchmarks, except for hallucination detection and instruction following; (2) performance varies significantly with different quantization methods, model size, and bit-width, with weight-only methods often yielding better results in larger models; (3) task difficulty does not significantly impact accuracy degradation due to quantization; and (4) the MT-Bench evaluation method has limited discriminatory power among recent high-performing LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T10:31:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11055v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11055v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 Advancing Cyber Incident Timeline Analysis Through Rule Based AI and
  Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fatma Yasmine Loumachi, Mohamed Chahine Ghanem
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Timeline Analysis (TA) plays a crucial role in Timeline Forensics (TF) within the field of Digital Forensics (DF). It focuses on examining and analyzing time-based digital artefacts, such as timestamps derived from event logs, file metadata, and other relevant data, to correlate events linked to cyber incidents and reconstruct their chronological sequence. Traditional tools often struggle to efficiently handle the large volume and variety of data generated during DF investigations and Incident Response (IR) processes. This paper introduces a novel framework, GenDFIR, which combines Rule-Based Artificial Intelligence (R-BAI) algorithms with Large Language Models (LLMs) to enhance and automate the TA process. The proposed approach consists of two key stages: (1) R-BAI is used to identify and select anomalous digital artefacts based on predefined rules. (2) The selected artefacts are then transformed into embeddings for processing by an LLM with the assistance of a Retrieval-Augmented Generation (RAG) agent. The LLM uses its capabilities to perform automated TA on the artefacts and predict potential incident outcomes. To validate the framework, we evaluated its performance, efficiency, and reliability. Several metrics were applied to simulated cyber incident scenarios, which were presented as forensic case documents. Our findings demonstrate the significant potential of integrating R-BAI and LLMs for TA. This innovative approach underscores the power of Generative AI (GenAI), particularly LLMs, and opens up new possibilities for advanced threat detection and incident reconstruction, marking a significant advancement in the field.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T10:16:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span><span>cs.ET</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.02572v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.02572v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 Visual grounding for desktop graphical user interfaces</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tassnim Dardouri, Laura Minkova, Jessica López Espejel, Walid Dahhane, El Hassane Ettifouri
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Most instance perception and image understanding solutions focus mainly on natural images. However, applications for synthetic images, and more specifically, images of Graphical User Interfaces (GUI) remain limited. This hinders the development of autonomous computer-vision-powered Artificial Intelligence (AI) agents. In this work, we present Instruction Visual Grounding or IVG, a multi-modal solution for object identification in a GUI. More precisely, given a natural language instruction and GUI screen, IVG locates the coordinates of the element on the screen where the instruction would be executed. To this end, we develop two methods. The first method is a three-part architecture that relies on a combination of a Large Language Model (LLM) and an object detection model. The second approach uses a multi-modal foundation model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T10:15:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.01558v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.01558v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 Towards No-Code Programming of Cobots: Experiments with Code Synthesis
  by Large Code Models for Conversational Programming</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While there has been a lot of research recently on robots in household environments, at the present time, most robots in existence can be found on shop floors, and most interactions between humans and robots happen there. ``Collaborative robots'' (cobots) designed to work alongside humans on assembly lines traditionally require expert programming, limiting ability to make changes, or manual guidance, limiting expressivity of the resulting programs. To address these limitations, we explore using Large Language Models (LLMs), and in particular, their abilities of doing in-context learning, for conversational code generation. As a first step, we define RATS, the ``Repetitive Assembly Task'', a 2D building task designed to lay the foundation for simulating industry assembly scenarios. In this task, a `programmer' instructs a cobot, using natural language, on how a certain assembly is to be built; that is, the programmer induces a program, through natural language. We create a dataset that pairs target structures with various example instructions (human-authored, template-based, and model-generated) and example code. With this, we systematically evaluate the capabilities of state-of-the-art LLMs for synthesising this kind of code, given in-context examples. Evaluating in a simulated environment, we find that LLMs are capable of generating accurate `first order code' (instruction sequences), but have problems producing `higher-order code' (abstractions such as functions, or use of loops).
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-18T07:17:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11041v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11041v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 Self-Translate-Train: Enhancing Cross-Lingual Transfer of Large Language
  Models via Inherent Capability</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ryokan Ri, Shun Kiyono, Sho Takase
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Zero-shot cross-lingual transfer by fine-tuning multilingual pretrained models shows promise for low-resource languages, but often suffers from misalignment of internal representations between languages. We hypothesize that even when the model cannot generalize across languages effectively in fine-tuning, it still captures cross-lingual correspondence useful for cross-lingual transfer. We explore this hypothesis with Self-Translate-Train, a method that lets large language models (LLMs) to translate training data into the target language and fine-tunes the model on its own generated data. By demonstrating that Self-Translate-Train outperforms zero-shot transfer, we encourage further exploration of better methods to elicit cross-lingual capabilities of LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T10:04:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.00454v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.00454v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 Hierarchical Narrative Analysis: Unraveling Perceptions of Generative AI</h2>
                <div class="authors">
                    <strong>Authors:</strong> Riona Matsuoka, Hiroki Matsumoto, Takahiro Yoshida, Tomohiro Watanabe, Ryoma Kondo, Ryohei Hisano
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Written texts reflect an author's perspective, making the thorough analysis of literature a key research method in fields such as the humanities and social sciences. However, conventional text mining techniques like sentiment analysis and topic modeling are limited in their ability to capture the hierarchical narrative structures that reveal deeper argumentative patterns. To address this gap, we propose a method that leverages large language models (LLMs) to extract and organize these structures into a hierarchical framework. We validate this approach by analyzing public opinions on generative AI collected by Japan's Agency for Cultural Affairs, comparing the narratives of supporters and critics. Our analysis provides clearer visualization of the factors influencing divergent opinions on generative AI, offering deeper insights into the structures of agreement and disagreement.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T09:56:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11032v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11032v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 Prompt Obfuscation for Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> David Pape, Thorsten Eisenhofer, Lea Schönherr
                </div>
                <div class="summary">
                    <strong>Summary:</strong> System prompts that include detailed instructions to describe the task performed by the underlying large language model (LLM) can easily transform foundation models into tools and services with minimal overhead. Because of their crucial impact on the utility, they are often considered intellectual property, similar to the code of a software product. However, extracting system prompts is easily possible by using prompt injection. As of today, there is no effective countermeasure to prevent the stealing of system prompts and all safeguarding efforts could be evaded with carefully crafted prompt injections that bypass all protection mechanisms.In this work, we propose an alternative to conventional system prompts. We introduce prompt obfuscation to prevent the extraction of the system prompt while maintaining the utility of the system itself with only little overhead. The core idea is to find a representation of the original system prompt that leads to the same functionality, while the obfuscated system prompt does not contain any information that allows conclusions to be drawn about the original system prompt. We implement an optimization-based method to find an obfuscated prompt representation while maintaining the functionality. To evaluate our approach, we investigate eight different metrics to compare the performance of a system using the original and the obfuscated system prompts, and we show that the obfuscated version is constantly on par with the original one. We further perform three different deobfuscation attacks and show that with access to the obfuscated prompt and the LLM itself, we are not able to consistently extract meaningful information. Overall, we showed that prompt obfuscation can be an effective method to protect intellectual property while maintaining the same utility as the original system prompt.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T09:43:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11026v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11026v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 GEIC: Universal and Multilingual Named Entity Recognition with Large
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hanjun Luo, Yingbin Jin, Xuecheng Liu, Tong Shang, Ruizhe Chen, Zuozhu Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have supplanted traditional methods in numerous natural language processing tasks. Nonetheless, in Named Entity Recognition (NER), existing LLM-based methods underperform compared to baselines and require significantly more computational resources, limiting their application. In this paper, we introduce the task of generation-based extraction and in-context classification (GEIC), designed to leverage LLMs' prior knowledge and self-attention mechanisms for NER tasks. We then propose CascadeNER, a universal and multilingual GEIC framework for few-shot and zero-shot NER. CascadeNER employs model cascading to utilize two small-parameter LLMs to extract and classify independently, reducing resource consumption while enhancing accuracy. We also introduce AnythingNER, the first NER dataset specifically designed for LLMs, including 8 languages, 155 entity types and a novel dynamic categorization system. Experiments show that CascadeNER achieves state-of-the-art performance on low-resource and fine-grained scenarios, including CrossNER and FewNERD. Our work is openly accessible.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-18T10:05:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.11022v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.11022v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 Large Language Model Based Generative Error Correction: A Challenge and
  Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chao-Han Huck Yang, Taejin Park, Yuan Gong, Yuanchao Li, Zhehuai Chen, Yen-Ting Lin, Chen Chen, Yuchen Hu, Kunal Dhawan, Piotr Żelasko, Chao Zhang, Yun-Nung Chen, Yu Tsao, Jagadeesh Balam, Boris Ginsburg, Sabato Marco Siniscalchi, Eng Siong Chng, Peter Bell, Catherine Lai, Shinji Watanabe, Andreas Stolcke
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Given recent advances in generative AI technology, a key question is how large language models (LLMs) can enhance acoustic modeling tasks using text decoding results from a frozen, pretrained automatic speech recognition (ASR) model. To explore new capabilities in language modeling for speech processing, we introduce the generative speech transcription error correction (GenSEC) challenge. This challenge comprises three post-ASR language modeling tasks: (i) post-ASR transcription correction, (ii) speaker tagging, and (iii) emotion recognition. These tasks aim to emulate future LLM-based agents handling voice-based interfaces while remaining accessible to a broad audience by utilizing open pretrained language models or agent-based APIs. We also discuss insights from baseline evaluations, as well as lessons learned for designing future evaluations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T09:32:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span><span>cs.SD</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.09785v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.09785v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 COBIAS: Contextual Reliability in Bias Assessment</h2>
                <div class="authors">
                    <strong>Authors:</strong> Priyanshul Govil, Hemang Jain, Vamshi Krishna Bonagiri, Aman Chadha, Ponnurangam Kumaraguru, Manas Gaur, Sanorita Dey
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) often inherit biases from the web data they are trained on, which contains stereotypes and prejudices. Current methods for evaluating and mitigating these biases rely on bias-benchmark datasets. These benchmarks measure bias by observing an LLM's behavior on biased statements. However, these statements lack contextual considerations of the situations they try to present. To address this, we introduce a contextual reliability framework, which evaluates model robustness to biased statements by considering the various contexts in which they may appear. We develop the Context-Oriented Bias Indicator and Assessment Score (COBIAS) to measure a biased statement's reliability in detecting bias based on the variance in model behavior across different contexts. To evaluate the metric, we augment 2,291 stereotyped statements from two existing benchmark datasets by adding contextual information. We show that COBIAS aligns with human judgment on the contextual reliability of biased statements (Spearman's $\rho = 0.65$, $p = 3.4 * 10^{-60}$) and can be used to create reliable datasets, which would assist bias mitigation works.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T09:24:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.14889v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.14889v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 Enhancing Low-Resource Language and Instruction Following Capabilities
  of Audio Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Potsawee Manakul, Guangzhi Sun, Warit Sirichotedumrong, Kasima Tharnpipitchai, Kunat Pipatanakul
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Audio language models can understand audio inputs and perform a range of audio-related tasks based on instructions, such as speech recognition and audio captioning, where the instructions are usually textual prompts. Audio language models are mostly initialized from pre-trained audio encoders and large language models (LLMs). Although these pre-trained components were developed to support multiple languages, audio-language models are trained predominantly on English data, which may limit their usability to only English instructions or English speech inputs. First, this paper examines the performance of existing audio language models in an underserved language using Thai as an example. This paper demonstrates that, despite being built on multilingual backbones, audio language models do not exhibit cross-lingual emergent abilities to low-resource languages. Second, this paper studies data mixture for developing audio language models that are optimized for a target language as well as English. In addition. this paper integrates audio comprehension and speech instruction-following capabilities into a single unified model. Our experiments provide insights into data mixture for enhancing instruction-following capabilities in both a low-resource language and English. Our model, Typhoon-Audio, outperforms existing open-source audio language models by a considerable margin, and it is comparable to state-of-the-art Gemini-1.5-Pro in both English and Thai languages.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T09:04:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.SD</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10999v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10999v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 PIP-Loco: A Proprioceptive Infinite Horizon Planning Framework for
  Quadrupedal Robot Locomotion</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aditya Shirwatkar, Naman Saxena, Kishore Chandra, Shishir Kolathaya
                </div>
                <div class="summary">
                    <strong>Summary:</strong> A core strength of Model Predictive Control (MPC) for quadrupedal locomotion has been its ability to enforce constraints and provide interpretability of the sequence of commands over the horizon. However, despite being able to plan, MPC struggles to scale with task complexity, often failing to achieve robust behavior on rapidly changing surfaces. On the other hand, model-free Reinforcement Learning (RL) methods have outperformed MPC on multiple terrains, showing emergent motions but inherently lack any ability to handle constraints or perform planning. To address these limitations, we propose a framework that integrates proprioceptive planning with RL, allowing for agile and safe locomotion behaviors through the horizon. Inspired by MPC, we incorporate an internal model that includes a velocity estimator and a Dreamer module. During training, the framework learns an expert policy and an internal model that are co-dependent, facilitating exploration for improved locomotion behaviors. During deployment, the Dreamer module solves an infinite-horizon MPC problem, adapting actions and velocity commands to respect the constraints. We validate the robustness of our training framework through ablation studies on internal model components and demonstrate improved robustness to training noise. Finally, we evaluate our approach across multi-terrain scenarios in both simulation and hardware.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T08:57:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.09441v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.09441v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 Less is More: A Simple yet Effective Token Reduction Method for
  Efficient Multi-modal LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dingjie Song, Wenjun Wang, Shunian Chen, Xidong Wang, Michael Guan, Benyou Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid advancement of Multimodal Large Language Models (MLLMs) has led to remarkable performances across various domains. However, this progress is accompanied by a substantial surge in the resource consumption of these models. We address this pressing issue by introducing a new approach, Token Reduction using CLIP Metric (TRIM), aimed at improving the efficiency of MLLMs without sacrificing their performance. Inspired by human attention patterns in Visual Question Answering (VQA) tasks, TRIM presents a fresh perspective on the selection and reduction of image tokens. The TRIM method has been extensively tested across 12 datasets, and the results demonstrate a significant reduction in computational overhead while maintaining a consistent level of performance. This research marks a critical stride in efficient MLLM development, promoting greater accessibility and sustainability of high-performing models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T08:56:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.MM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10994v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10994v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 StuGPTViz: A Visual Analytics Approach to Understand Student-ChatGPT
  Interactions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zixin Chen, Jiachen Wang, Meng Xia, Kento Shigyo, Dingdong Liu, Rong Zhang, Huamin Qu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The integration of Large Language Models (LLMs), especially ChatGPT, into education is poised to revolutionize students' learning experiences by introducing innovative conversational learning methodologies. To empower students to fully leverage the capabilities of ChatGPT in educational scenarios, understanding students' interaction patterns with ChatGPT is crucial for instructors. However, this endeavor is challenging due to the absence of datasets focused on student-ChatGPT conversations and the complexities in identifying and analyzing the evolutional interaction patterns within conversations. To address these challenges, we collected conversational data from 48 students interacting with ChatGPT in a master's level data visualization course over one semester. We then developed a coding scheme, grounded in the literature on cognitive levels and thematic analysis, to categorize students' interaction patterns with ChatGPT. Furthermore, we present a visual analytics system, StuGPTViz, that tracks and compares temporal patterns in student prompts and the quality of ChatGPT's responses at multiple scales, revealing significant pedagogical insights for instructors. We validated the system's effectiveness through expert interviews with six data visualization instructors and three case studies. The results confirmed StuGPTViz's capacity to enhance educators' insights into the pedagogical value of ChatGPT. We also discussed the potential research opportunities of applying visual analytics in education and developing AI-driven personalized learning solutions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T08:32:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.12423v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.12423v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 Does Biomedical Training Lead to Better Medical Performance?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Amin Dada, Marie Bauer, Amanda Butler Contreras, Osman Alperen Koraş, Constantin Marc Seibold, Kaleb E Smith, Jens Kleesiek
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are expected to significantly contribute to patient care, diagnostics, and administrative processes. Emerging biomedical LLMs aim to address healthcare-specific challenges, including privacy demands and computational constraints. Assessing the models' suitability for this sensitive application area is of the utmost importance. However, biomedical training has not been systematically evaluated on medical tasks. This study investigates the effect of biomedical training in the context of six practical medical tasks evaluating $25$ models. In contrast to previous evaluations, our results reveal a performance decline in nine out of twelve biomedical models after fine-tuning, particularly on tasks involving hallucinations, ICD10 coding, and instruction adherence. General-domain models like Meta-Llama-3.1-70B-Instruct outperformed their biomedical counterparts, indicating a trade-off between domain-specific fine-tuning and general medical task performance. We open-source all evaluation scripts and datasets at https://github.com/TIO-IKIM/CLUE to support further research in this critical area.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T08:19:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.04067v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.04067v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 Enhancing Multilingual Speech Generation and Recognition Abilities in
  LLMs with Constructed Code-switched Data</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jing Xu, Daxin Tan, Jiaqi Wang, Xiao Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While large language models (LLMs) have been explored in the speech domain for both generation and recognition tasks, their applications are predominantly confined to the monolingual scenario, with limited exploration in multilingual and code-switched (CS) contexts. Additionally, speech generation and recognition tasks are often handled separately, such as VALL-E and Qwen-Audio. In this paper, we propose a MutltiLingual MultiTask (MLMT) model, integrating multilingual speech generation and recognition tasks within the single LLM. Furthermore, we develop an effective data construction approach that splits and concatenates words from different languages to equip LLMs with CS synthesis ability without relying on CS data. The experimental results demonstrate that our model outperforms other baselines with a comparable data scale. Furthermore, our data construction approach not only equips LLMs with CS speech synthesis capability with comparable speaker consistency and similarity to any given speaker, but also improves the performance of LLMs in multilingual speech generation and recognition tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T08:11:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.AS</span><span>cs.CL</span><span>cs.SD</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10969v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10969v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 Information That Matters: Exploring Information Needs of People Affected
  by Algorithmic Decisions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Timothée Schmude, Laura Koesten, Torsten Möller, Sebastian Tschiatschek
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Every AI system that makes decisions about people has a group of stakeholders that are personally affected by these decisions. However, explanations of AI systems rarely address the information needs of this stakeholder group, who often are AI novices. This creates a gap between conveyed information and information that matters to those who are impacted by the system's decisions, such as domain experts and decision subjects. To address this, we present the "XAI Novice Question Bank," an extension of the XAI Question Bank containing a catalog of information needs from AI novices in two use cases: employment prediction and health monitoring. The catalog covers the categories of data, system context, system usage, and system specifications. We gathered information needs through task-based interviews where participants asked questions about two AI systems to decide on their adoption and received verbal explanations in response. Our analysis showed that participants' confidence increased after receiving explanations but that their understanding faced challenges. These included difficulties in locating information and in assessing their own understanding, as well as attempts to outsource understanding. Additionally, participants' prior perceptions of the systems' risks and benefits influenced their information needs. Participants who perceived high risks sought explanations about the intentions behind a system's deployment, while those who perceived low risks rather asked about the system's operation. Our work aims to support the inclusion of AI novices in explainability efforts by highlighting their information needs, aims, and challenges. We summarize our findings as five key implications that can inform the design of future explanations for lay stakeholder audiences.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T08:08:00Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2401.13324v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2401.13324v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 Investigating Context-Faithfulness in Large Language Models: The Roles
  of Memory Strength and Evidence Style</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuepei Li, Kang Zhou, Qiao Qiao, Bach Nguyen, Qing Wang, Qi Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Retrieval-augmented generation (RAG) improves Large Language Models (LLMs) by incorporating external information into the response generation process. However, how context-faithful LLMs are and what factors influence LLMs' context-faithfulness remain largely unexplored. In this study, we investigate the impact of memory strength and evidence presentation on LLMs' receptiveness to external evidence. We introduce a method to quantify the memory strength of LLMs by measuring the divergence in LLMs' responses to different paraphrases of the same question, which is not considered by previous works. We also generate evidence in various styles to evaluate the effects of evidence in different styles. Two datasets are used for evaluation: Natural Questions (NQ) with popular questions and popQA featuring long-tail questions. Our results show that for questions with high memory strength, LLMs are more likely to rely on internal memory, particularly for larger LLMs such as GPT-4. On the other hand, presenting paraphrased evidence significantly increases LLMs' receptiveness compared to simple repetition or adding details.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T07:44:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10955v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10955v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 Propulsion: Steering LLM with Tiny Fine-Tuning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Md Kowsher, Nusrat Jahan Prottasha, Prakash Bhat
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid advancements in Large Language Models (LLMs) have revolutionized natural language processing (NLP) and related fields. However, fine-tuning these models for specific tasks remains computationally expensive and risks degrading pre-learned features. To address these challenges, we propose Propulsion, a novel parameter efficient fine-tuning (PEFT) method designed to optimize task-specific performance while drastically reducing computational overhead. Inspired by the concept of controlled adjustments in physical motion, Propulsion selectively re-scales specific dimensions of a pre-trained model, guiding output predictions toward task objectives without modifying the model's parameters. By introducing lightweight, trainable Propulsion parameters at the pre-trained layer, we minimize the number of parameters updated during fine-tuning, preventing overfitting or overwriting of existing knowledge. Our theoretical analysis, supported by Neural Tangent Kernel (NTK) theory, shows that Propulsion approximates the performance of full fine-tuning with far fewer trainable parameters. Empirically, Propulsion reduces the parameter count from 355.3 million to just 0.086 million, achieving over a 10x reduction compared to standard approaches like LoRA while maintaining competitive performance across benchmarks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-18T07:23:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10927v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10927v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 ASHABot: An LLM-Powered Chatbot to Support the Informational Needs of
  Community Health Workers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pragnya Ramjee, Mehak Chhokar, Bhuvan Sachdeva, Mahendra Meena, Hamid Abdullah, Aditya Vashistha, Ruchit Nagar, Mohit Jain
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Community health workers (CHWs) provide last-mile healthcare services but face challenges due to limited medical knowledge and training. This paper describes the design, deployment, and evaluation of ASHABot, an LLM-powered, experts-in-the-loop, WhatsApp-based chatbot to address the information needs of CHWs in India. Through interviews with CHWs and their supervisors and log analysis, we examine factors affecting their engagement with ASHABot, and ASHABot's role in addressing CHWs' informational needs. We found that ASHABot provided a private channel for CHWs to ask rudimentary and sensitive questions they hesitated to ask supervisors. CHWs trusted the information they received on ASHABot and treated it as an authoritative resource. CHWs' supervisors expanded their knowledge by contributing answers to questions ASHABot failed to answer, but were concerned about demands on their workload and increased accountability. We emphasize positioning LLMs as supplemental fallible resources within the community healthcare ecosystem, instead of as replacements for supervisor support.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T06:11:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10913v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10913v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 GenCRF: Generative Clustering and Reformulation Framework for Enhanced
  Intent-Driven Information Retrieval</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wonduk Seo, Haojie Zhang, Yueyang Zhang, Changhao Zhang, Songyao Duan, Lixin Su, Daiting Shi, Jiashu Zhao, Dawei Yin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Query reformulation is a well-known problem in Information Retrieval (IR) aimed at enhancing single search successful completion rate by automatically modifying user's input query. Recent methods leverage Large Language Models (LLMs) to improve query reformulation, but often generate limited and redundant expansions, potentially constraining their effectiveness in capturing diverse intents. In this paper, we propose GenCRF: a Generative Clustering and Reformulation Framework to capture diverse intentions adaptively based on multiple differentiated, well-generated queries in the retrieval phase for the first time. GenCRF leverages LLMs to generate variable queries from the initial query using customized prompts, then clusters them into groups to distinctly represent diverse intents. Furthermore, the framework explores to combine diverse intents query with innovative weighted aggregation strategies to optimize retrieval performance and crucially integrates a novel Query Evaluation Rewarding Model (QERM) to refine the process through feedback loops. Empirical experiments on the BEIR benchmark demonstrate that GenCRF achieves state-of-the-art performance, surpassing previous query reformulation SOTAs by up to 12% on nDCG@10. These techniques can be adapted to various LLMs, significantly boosting retriever performance and advancing the field of Information Retrieval.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T05:59:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10909v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10909v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 Multi-Floor Zero-Shot Object Navigation Policy</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lingfeng Zhang, Hao Wang, Erjia Xiao, Xinyao Zhang, Qiang Zhang, Zixuan Jiang, Renjing Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Object navigation in multi-floor environments presents a formidable challenge in robotics, requiring sophisticated spatial reasoning and adaptive exploration strategies. Traditional approaches have primarily focused on single-floor scenarios, overlooking the complexities introduced by multi-floor structures. To address these challenges, we first propose a Multi-floor Navigation Policy (MFNP) and implement it in Zero-Shot object navigation tasks. Our framework comprises three key components: (i) Multi-floor Navigation Policy, which enables an agent to explore across multiple floors; (ii) Multi-modal Large Language Models (MLLMs) for reasoning in the navigation process; and (iii) Inter-Floor Navigation, ensuring efficient floor transitions. We evaluate MFNP on the Habitat-Matterport 3D (HM3D) and Matterport 3D (MP3D) datasets, both include multi-floor scenes. Our experiment results demonstrate that MFNP significantly outperforms all the existing methods in Zero-Shot object navigation, achieving higher success rates and improved exploration efficiency. Ablation studies further highlight the effectiveness of each component in addressing the unique challenges of multi-floor navigation. Meanwhile, we conducted real-world experiments to evaluate the feasibility of our policy. Upon deployment of MFNP, the Unitree quadruped robot demonstrated successful multi-floor navigation and found the target object in a completely unseen environment. By introducing MFNP, we offer a new paradigm for tackling complex, multi-floor environments in object navigation tasks, opening avenues for future research in visual-based navigation in realistic, multi-floor settings.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T05:53:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10906v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10906v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 Towards Goal-oriented Prompt Engineering for Large Language Models: A
  Survey</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haochen Li, Jonathan Leung, Zhiqi Shen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have shown prominent performance in various downstream tasks and prompt engineering plays a pivotal role in optimizing LLMs' performance. This paper, not only as an overview of current prompt engineering methods, but also aims to highlight the limitation of designing prompts based on an anthropomorphic assumption that expects LLMs to think like humans. From our review of 50 representative studies, we demonstrate that a goal-oriented prompt formulation, which guides LLMs to follow established human logical thinking, significantly improves the performance of LLMs. Furthermore, We introduce a novel taxonomy that categorizes goal-oriented prompting methods into five interconnected stages and we demonstrate the broad applicability of our framework. With four future directions proposed, we hope to further emphasize the power and potential of goal-oriented prompt engineering in all fields.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T04:56:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2401.14043v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2401.14043v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 CREAM: Comparison-Based Reference-Free ELO-Ranked Automatic Evaluation
  for Meeting Summarization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ziwei Gong, Lin Ai, Harshsaiprasad Deshpande, Alexander Johnson, Emmy Phung, Zehui Wu, Ahmad Emami, Julia Hirschberg
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have spurred interest in automatic evaluation methods for summarization, offering a faster, more cost-effective alternative to human evaluation. However, existing methods often fall short when applied to complex tasks like long-context summarizations and dialogue-based meeting summarizations. In this paper, we introduce CREAM (Comparison-Based Reference-Free Elo-Ranked Automatic Evaluation for Meeting Summarization), a novel framework that addresses the unique challenges of evaluating meeting summaries. CREAM leverages a combination of chain-of-thought reasoning and key facts alignment to assess conciseness and completeness of model-generated summaries without requiring reference. By employing an ELO ranking system, our approach provides a robust mechanism for comparing the quality of different models or prompt configurations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T04:39:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10883v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10883v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 Adaptive Large Language Models By Layerwise Attention Shortcuts</h2>
                <div class="authors">
                    <strong>Authors:</strong> Prateek Verma, Mert Pilanci
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformer architectures are the backbone of the modern AI revolution. However, they are based on simply stacking the same blocks in dozens of layers and processing information sequentially from one block to another. In this paper, we propose to challenge this and introduce adaptive computations for LLM-like setups, which allow the final layer to attend to all of the intermediate layers as it deems fit through the attention mechanism, thereby introducing computational \textbf{attention shortcuts}. These shortcuts can thus make the architecture depth and context adaptive. We showcase four different datasets, namely acoustic tokens, natural language, and symbolic music, and we achieve superior performance for GPT-like architecture. We give evidence via attention maps that the models learn complex dependencies across layers that are adaptive in context and depth depending on the input tokens.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T03:46:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span><span>cs.SD</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10870v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10870v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 Learnings from a Large-Scale Deployment of an LLM-Powered
  Expert-in-the-Loop Healthcare Chatbot</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bhuvan Sachdeva, Pragnya Ramjee, Geeta Fulari, Kaushik Murali, Mohit Jain
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are widely used in healthcare, but limitations like hallucinations, incomplete information, and bias hinder their reliability. To address these, researchers released the Build Your Own expert Bot (BYOeB) platform, enabling developers to create LLM-powered chatbots with integrated expert verification. CataractBot, its first implementation, provides expert-verified responses to cataract surgery questions. A pilot evaluation showed its potential; however the study had a small sample size and was primarily qualitative. In this work, we conducted a large-scale 24-week deployment of CataractBot involving 318 patients and attendants who sent 1,992 messages, with 91.71% of responses verified by seven experts. Analysis of interaction logs revealed that medical questions significantly outnumbered logistical ones, hallucinations were negligible, and experts rated 84.52% of medical answers as accurate. As the knowledge base expanded with expert corrections, system performance improved by 19.02%, reducing expert workload. These insights guide the design of future LLM-powered chatbots.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T03:22:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10354v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10354v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 Dynamic Range Reduction via Branch-and-Bound</h2>
                <div class="authors">
                    <strong>Authors:</strong> Thore Gerlach, Nico Piatkowski
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The demand for high-performance computing in machine learning and artificial intelligence has led to the development of specialized hardware accelerators like Tensor Processing Units (TPUs), Graphics Processing Units (GPUs), and Field-Programmable Gate Arrays (FPGAs). A key strategy to enhance these accelerators is the reduction of precision in arithmetic operations, which increases processing speed and lowers latency - crucial for real-time AI applications. Precision reduction minimizes memory bandwidth requirements and energy consumption, essential for large-scale and mobile deployments, and increases throughput by enabling more parallel operations per cycle, maximizing hardware resource utilization. This strategy is equally vital for solving NP-hard quadratic unconstrained binary optimization (QUBO) problems common in machine learning, which often require high precision for accurate representation. Special hardware solvers, such as quantum annealers, benefit significantly from precision reduction. This paper introduces a fully principled Branch-and-Bound algorithm for reducing precision needs in QUBO problems by utilizing dynamic range as a measure of complexity. Experiments validate our algorithm's effectiveness on an actual quantum annealer.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T03:07:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>math.OC</span><span>quant-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10863v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10863v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 Machine Learning for Public Good: Predicting Urban Crime Patterns to
  Enhance Community Safety</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sia Gupta, Simeon Sayer
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In recent years, urban safety has become a paramount concern for city planners and law enforcement agencies. Accurate prediction of likely crime occurrences can significantly enhance preventive measures and resource allocation. However, many law enforcement departments lack the tools to analyze and apply advanced AI and ML techniques that can support city planners, watch programs, and safety leaders to take proactive steps towards overall community safety.   This paper explores the effectiveness of ML techniques to predict spatial and temporal patterns of crimes in urban areas. Leveraging police dispatch call data from San Jose, CA, the research goal is to achieve a high degree of accuracy in categorizing calls into priority levels particularly for more dangerous situations that require an immediate law enforcement response. This categorization is informed by the time, place, and nature of the call. The research steps include data extraction, preprocessing, feature engineering, exploratory data analysis, implementation, optimization and tuning of different supervised machine learning models and neural networks. The accuracy and precision are examined for different models and features at varying granularity of crime categories and location precision.   The results demonstrate that when compared to a variety of other models, Random Forest classification models are most effective in identifying dangerous situations and their corresponding priority levels with high accuracy (Accuracy = 85%, AUC = 0.92) at a local level while ensuring a minimum amount of false negatives. While further research and data gathering is needed to include other social and economic factors, these results provide valuable insights for law enforcement agencies to optimize resources, develop proactive deployment approaches, and adjust response patterns to enhance overall public safety outcomes in an unbiased way.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T02:07:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10838v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10838v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 An Overview of Machine Learning-Enabled Optimization for Reconfigurable
  Intelligent Surfaces-Aided 6G Networks: From Reinforcement Learning to Large
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hao Zhou, Chengming Hu, Xue Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reconfigurable intelligent surface (RIS) becomes a promising technique for 6G networks by reshaping signal propagation in smart radio environments. However, it also leads to significant complexity for network management due to the large number of elements and dedicated phase-shift optimization. In this work, we provide an overview of machine learning (ML)-enabled optimization for RIS-aided 6G networks. In particular, we focus on various reinforcement learning (RL) techniques, e.g., deep Q-learning, multi-agent reinforcement learning, transfer reinforcement learning, hierarchical reinforcement learning, and offline reinforcement learning. Different from existing studies, this work further discusses how large language models (LLMs) can be combined with RL to handle network optimization problems. It shows that LLM offers new opportunities to enhance the capabilities of RL algorithms in terms of generalization, reward function design, multi-modal information processing, etc. Finally, we identify the future challenges and directions of ML-enabled optimization for RIS-aided 6G networks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T01:57:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span><span>cs.LG</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2405.17439v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.17439v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 Can GPT-O1 Kill All Bugs? An Evaluation of GPT-Family LLMs on QuixBugs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haichuan Hu, Ye Shang, Guolin Xu, Congqing He, Quanjun Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> LLMs have long demonstrated remarkable effectiveness in automatic program repair (APR), with OpenAI's ChatGPT being one of the most widely used models in this domain. Through continuous iterations and upgrades of GPT-family models, their performance in fixing bugs has already reached state-of-the-art levels. However, there are few works comparing the effectiveness and variations of different versions of GPT-family models on APR. In this work, inspired by the recent public release of the GPT-o1 models, we conduct the first study to compare the effectiveness of different versions of the GPT-family models in APR. We evaluate the performance of the latest version of the GPT-family models (i.e., O1-preview and O1-mini), GPT-4o, and the historical version of ChatGPT on APR. We conduct an empirical study of the four GPT-family models against other LLMs and APR techniques on the QuixBugs benchmark from multiple evaluation perspectives, including repair success rate, repair cost, response length, and behavior patterns. The results demonstrate that O1's repair capability exceeds that of prior GPT-family models, successfully fixing all 40 bugs in the benchmark. Our work can serve as a foundation for further in-depth exploration of the applications of GPT-family models in APR.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T01:49:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10033v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10033v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 Challenging Fairness: A Comprehensive Exploration of Bias in LLM-Based
  Recommendations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shahnewaz Karim Sakib, Anindya Bijoy Das
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Model (LLM)-based recommendation systems provide more comprehensive recommendations than traditional systems by deeply analyzing content and user behavior. However, these systems often exhibit biases, favoring mainstream content while marginalizing non-traditional options due to skewed training data. This study investigates the intricate relationship between bias and LLM-based recommendation systems, with a focus on music, song, and book recommendations across diverse demographic and cultural groups. Through a comprehensive analysis conducted over different LLM-models, this paper evaluates the impact of bias on recommendation outcomes. Our findings reveal that bias is so deeply ingrained within these systems that even a simpler intervention like prompt engineering can significantly reduce bias, underscoring the pervasive nature of the issue. Moreover, factors like intersecting identities and contextual information, such as socioeconomic status, further amplify these biases, demonstrating the complexity and depth of the challenges faced in creating fair recommendations across different groups.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T01:37:57Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span><span>cs.AI</span><span>cs.ET</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10825v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10825v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 Exploring Concept Depth: How Large Language Models Acquire Knowledge at
  Different Layers?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mingyu Jin, Qinkai Yu, Jingyuan Huang, Qingcheng Zeng, Zhenting Wang, Wenyue Hua, Haiyan Zhao, Kai Mei, Yanda Meng, Kaize Ding, Fan Yang, Mengnan Du, Yongfeng Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have shown remarkable performances across a wide range of tasks. However, the mechanisms by which these models encode tasks of varying complexities remain poorly understood. In this paper, we explore the hypothesis that LLMs process concepts of varying complexities in different layers, introducing the idea of ``Concept Depth'' to suggest that more complex concepts are typically acquired in deeper layers. Specifically, we categorize concepts based on their level of abstraction, defining them in the order of increasing complexity within factual, emotional, and inferential tasks. We conduct extensive probing experiments using layer-wise representations across various LLM families (Gemma, LLaMA, Qwen) on various datasets spanning the three domains of tasks. Our findings reveal that models could efficiently conduct probing for simpler tasks in shallow layers, and more complex tasks typically necessitate deeper layers for accurate understanding. Additionally, we examine how external factors, such as adding noise to the input and quantizing the model weights, might affect layer-wise representations. Our findings suggest that these factors can impede the development of a conceptual understanding of LLMs until deeper layers are explored. We hope that our proposed concept and experimental insights will enhance the understanding of the mechanisms underlying LLMs. Our codes are available at \url{https://github.com/Luckfort/CD}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T01:37:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.07066v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.07066v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 LLM-Powered Ensemble Learning for Paper Source Tracing: A GPU-Free
  Approach</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kunlong Chen, Junjun Wang, Zhaoqun Chen, Kunjin Chen, Yitian Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We participated in the KDD CUP 2024 paper source tracing competition and achieved the 3rd place. This competition tasked participants with identifying the reference sources (i.e., ref-sources, as referred to by the organizers of the competition) of given academic papers. Unlike most teams that addressed this challenge by fine-tuning pre-trained neural language models such as BERT or ChatGLM, our primary approach utilized closed-source large language models (LLMs). With recent advancements in LLM technology, closed-source LLMs have demonstrated the capability to tackle complex reasoning tasks in zero-shot or few-shot scenarios. Consequently, in the absence of GPUs, we employed closed-source LLMs to directly generate predicted reference sources from the provided papers. We further refined these predictions through ensemble learning. Notably, our method was the only one among the award-winning approaches that did not require the use of GPUs for model training. Code available at https://github.com/Cklwanfifa/KDDCUP2024-PST.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T01:35:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.09383v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.09383v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 ArticulatePro: A Comparative Study on a Proactive and Non-Proactive
  Assistant in a Climate Data Exploration Task</h2>
                <div class="authors">
                    <strong>Authors:</strong> Roderick Tabalba, Christopher J. Lee, Giorgio Tran, Nurit Kirshenbaum, Jason Leigh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in Natural Language Interfaces (NLIs) and Large Language Models (LLMs) have transformed our approach to NLP tasks, allowing us to focus more on a Pragmatics-based approach. This shift enables more natural interactions between humans and voice assistants, which have been challenging to achieve. Pragmatics describes how users often talk out of turn, interrupt each other, or provide relevant information without being explicitly asked (maxim of quantity). To explore this, we developed a digital assistant that constantly listens to conversations and proactively generates relevant visualizations during data exploration tasks. In a within-subject study, participants interacted with both proactive and non-proactive versions of a voice assistant while exploring the Hawaii Climate Data Portal (HCDP). Results suggest that the proactive assistant enhanced user engagement and facilitated quicker insights. Our study highlights the potential of Pragmatic, proactive AI in NLIs and identifies key challenges in its implementation, offering insights for future research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-17T00:27:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10797v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10797v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 Model Tells Itself Where to Attend: Faithfulness Meets Automatic
  Attention Steering</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qingru Zhang, Xiaodong Yu, Chandan Singh, Xiaodong Liu, Liyuan Liu, Jianfeng Gao, Tuo Zhao, Dan Roth, Hao Cheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have demonstrated remarkable performance across various real-world tasks. However, they often struggle to fully comprehend and effectively utilize their input contexts, resulting in responses that are unfaithful or hallucinated. This difficulty increases for contexts that are long or contain distracting information, which can divert LLMs from fully capturing essential evidence. To address this issue, many works use prompting to help LLMs utilize contextual information more faithfully. For instance, iterative prompting highlights key information in two steps that first ask the LLM to identify important pieces of context and then derive answers accordingly. However, prompting methods are constrained to highlighting key information implicitly in token space, which is often insufficient to fully steer the model's attention. To improve model faithfulness more reliably, we propose AutoPASTA, a method that automatically identifies key contextual information and explicitly highlights it by steering an LLM's attention scores. Like prompting, AutoPASTA is applied at inference time and does not require changing any model parameters. Our experiments on open-book QA demonstrate that AutoPASTA effectively enables models to grasp essential contextual information, leading to substantially improved model faithfulness and performance, e.g., an average improvement of 7.95% for LLAMA3-70B-Instruct. Code will be publicly available at https://github.com/QingruZhang/AutoPASTA .
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-16T23:52:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10790v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10790v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 Benchmarking Sim2Real Gap: High-fidelity Digital Twinning of Agile
  Manufacturing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sunny Katyara, Suchita Sharma, Praveen Damacharla, Carlos Garcia Santiago, Lubina Dhirani, Bhawani Shankar Chowdhry
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As the manufacturing industry shifts from mass production to mass customization, there is a growing emphasis on adopting agile, resilient, and human-centric methodologies in line with the directives of Industry 5.0. Central to this transformation is the deployment of digital twins, a technology that digitally replicates manufacturing assets to enable enhanced process optimization, predictive maintenance, synthetic data generation, and accelerated customization and prototyping. This chapter delves into the technologies underpinning the creation of digital twins specifically tailored to agile manufacturing scenarios within the realm of robotic automation. It explores the transfer of trained policies and process optimizations from simulated settings to real-world applications through advanced techniques such as domain randomization, domain adaptation, curriculum learning, and model-based system identification. The chapter also examines various industrial manufacturing automation scenarios, including bin-picking, part inspection, and product assembly, under Sim2Real conditions. The performance of digital twin technologies in these scenarios is evaluated using practical metrics including data latency, adaptation rate, simulation fidelity among others reported, providing a comprehensive assessment of their efficacy and potential impact on modern manufacturing processes.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-16T23:36:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10784v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10784v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 Semantics Preserving Emoji Recommendation with Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhongyi Qiu, Kangyi Qiu, Hanjia Lyu, Wei Xiong, Jiebo Luo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Emojis have become an integral part of digital communication, enriching text by conveying emotions, tone, and intent. Existing emoji recommendation methods are primarily evaluated based on their ability to match the exact emoji a user chooses in the original text. However, they ignore the essence of users' behavior on social media in that each text can correspond to multiple reasonable emojis. To better assess a model's ability to align with such real-world emoji usage, we propose a new semantics preserving evaluation framework for emoji recommendation, which measures a model's ability to recommend emojis that maintain the semantic consistency with the user's text. To evaluate how well a model preserves semantics, we assess whether the predicted affective state, demographic profile, and attitudinal stance of the user remain unchanged. If these attributes are preserved, we consider the recommended emojis to have maintained the original semantics. The advanced abilities of Large Language Models (LLMs) in understanding and generating nuanced, contextually relevant output make them well-suited for handling the complexities of semantics preserving emoji recommendation. To this end, we construct a comprehensive benchmark to systematically assess the performance of six proprietary and open-source LLMs using different prompting techniques on our task. Our experiments demonstrate that GPT-4o outperforms other LLMs, achieving a semantics preservation score of 79.23%. Additionally, we conduct case studies to analyze model biases in downstream classification tasks and evaluate the diversity of the recommended emojis.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-16T22:27:46Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.SI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10760v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10760v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 Foundation Models to the Rescue: Deadlock Resolution in Connected
  Multi-Robot Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kunal Garg, Songyuan Zhang, Jacob Arkin, Chuchu Fan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Connected multi-agent robotic systems (MRS) are prone to deadlocks in an obstacle environment where the robots can get stuck away from their desired locations under a smooth low-level control policy. Without an external intervention, often in terms of a high-level command, a low-level control policy cannot resolve such deadlocks. Utilizing the generalizability and low data requirements of foundation models, this paper explores the possibility of using text-based models, i.e., large language models (LLMs), and text-and-image-based models, i.e., vision-language models (VLMs), as high-level planners for deadlock resolution. We propose a hierarchical control framework where a foundation model-based high-level planner helps to resolve deadlocks by assigning a leader to the MRS along with a set of waypoints for the MRS leader. Then, a low-level distributed control policy based on graph neural networks is executed to safely follow these waypoints, thereby evading the deadlock. We conduct extensive experiments on various MRS environments using the best available pre-trained LLMs and VLMs. We compare their performance with a graph-based planner in terms of effectiveness in helping the MRS reach their target locations and computational time. Our results illustrate that, compared to grid-based planners, the foundation models perform better in terms of the goal-reaching rate and computational time for complex environments, which helps us conclude that foundation models can assist MRS operating in complex obstacle-cluttered environments to resolve deadlocks efficiently.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-16T22:05:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.CL</span><span>math.OC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2404.06413v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2404.06413v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 VulnLLMEval: A Framework for Evaluating Large Language Models in
  Software Vulnerability Detection and Patching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Arastoo Zibaeirad, Marco Vieira
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have shown promise in tasks like code translation, prompting interest in their potential for automating software vulnerability detection (SVD) and patching (SVP). To further research in this area, establishing a benchmark is essential for evaluating the strengths and limitations of LLMs in these tasks. Despite their capabilities, questions remain regarding whether LLMs can accurately analyze complex vulnerabilities and generate appropriate patches. This paper introduces VulnLLMEval, a framework designed to assess the performance of LLMs in identifying and patching vulnerabilities in C code. Our study includes 307 real-world vulnerabilities extracted from the Linux kernel, creating a well-curated dataset that includes both vulnerable and patched code. This dataset, based on real-world code, provides a diverse and representative testbed for evaluating LLM performance in SVD and SVP tasks, offering a robust foundation for rigorous assessment. Our results reveal that LLMs often struggle with distinguishing between vulnerable and patched code. Furthermore, in SVP tasks, these models tend to oversimplify the code, producing solutions that may not be directly usable without further refinement.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-16T22:00:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10756v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10756v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 SynDARin: Synthesising Datasets for Automated Reasoning in Low-Resource
  Languages</h2>
                <div class="authors">
                    <strong>Authors:</strong> Gayane Ghazaryan, Erik Arakelyan, Pasquale Minervini, Isabelle Augenstein
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Question Answering (QA) datasets have been instrumental in developing and evaluating Large Language Model (LLM) capabilities. However, such datasets are scarce for languages other than English due to the cost and difficulties of collection and manual annotation. This means that producing novel models and measuring the performance of multilingual LLMs in low-resource languages is challenging. To mitigate this, we propose $\textbf{S}$yn$\textbf{DAR}$in, a method for generating and validating QA datasets for low-resource languages. We utilize parallel content mining to obtain $\textit{human-curated}$ paragraphs between English and the target language. We use the English data as context to $\textit{generate}$ synthetic multiple-choice (MC) question-answer pairs, which are automatically translated and further validated for quality. Combining these with their designated non-English $\textit{human-curated}$ paragraphs form the final QA dataset. The method allows to maintain the content quality, reduces the likelihood of factual errors, and circumvents the need for costly annotation. To test the method, we created a QA dataset with $1.2$K samples for the Armenian language. The human evaluation shows that $98\%$ of the generated English data maintains quality and diversity in the question types and topics, while the translation validation pipeline can filter out $\sim70\%$ of data with poor quality. We use the dataset to benchmark state-of-the-art LLMs, showing their inability to achieve human accuracy with some model performances closer to random chance. This shows that the generated dataset is non-trivial and can be used to evaluate reasoning capabilities in low-resource language.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-16T21:52:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.14425v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.14425v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 GPT takes the SAT: Tracing changes in Test Difficulty and Math
  Performance of Students</h2>
                <div class="authors">
                    <strong>Authors:</strong> Vikram Krishnaveti, Saannidhya Rawat
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Scholastic Aptitude Test (SAT) is crucial for college admissions but its effectiveness and relevance are increasingly questioned. This paper enhances Synthetic Control methods by introducing "Transformed Control", a novel method that employs Large Language Models (LLMs) powered by Artificial Intelligence to generate control groups. We utilize OpenAI's API to generate a control group where GPT-4, or ChatGPT, takes multiple SATs annually from 2008 to 2023. This control group helps analyze shifts in SAT math difficulty over time, starting from the baseline year of 2008. Using parallel trends, we calculate the Average Difference in Scores (ADS) to assess changes in high school students' math performance. Our results indicate a significant decrease in the difficulty of the SAT math section over time, alongside a decline in students' math performance. The analysis shows a 71-point drop in the rigor of SAT math from 2008 to 2023, with student performance decreasing by 36 points, resulting in a 107-point total divergence in average student math performance. We investigate possible mechanisms for this decline in math proficiency, such as changing university selection criteria, increased screen time, grade inflation, and worsening adolescent mental health. Disparities among demographic groups show a 104-point drop for White students, 84 points for Black students, and 53 points for Asian students. Male students saw a 117-point reduction, while female students had a 100-point decrease.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2024-09-16T21:45:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>econ.EM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.10750v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.10750v1' target='_blank'>pdf</a>
                </div>
            </div>
            
        </div>
    </body>
    </html>
    