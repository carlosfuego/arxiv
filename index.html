
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>ArXiv Results</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                margin: 0;
                padding: 0;
                display: flex;
                justify-content: center;
                align-items: center;
                min-height: 100vh;
                background-color: #f9f9f9;
            }
            .container {
                width: 17cm;
                padding: 2cm;
                background-color: #fff;
                box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            }
            h1, h2 {
                color: #333;
            }
            .paper {
                border: 1px solid #ccc;
                padding: 1em;
                margin-bottom: 1em;
                border-radius: 8px;
                box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            }
            .authors, .summary, .updated, .tags, .links {
                margin: 0.5em 0;
            }
            .tags span {
                background-color: #f0f0f0;
                border-radius: 4px;
                padding: 0.2em 0.5em;
                margin-right: 0.5em;
            }
            .links a {
                margin-right: 1em;
                text-decoration: none;
                color: #1a73e8;
            }
            .links a:hover {
                text-decoration: underline;
            }
        </style>
    </head>
    <body>
        <div class="container">
    <h1>Keyword: kv cache</h1>

            <div class="paper">
                <h2>#1 A Survey on Diffusion Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tianyi Li, Mingda Chen, Bowei Guo, Zhiqiang Shen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:47:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10875v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10875v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 Memory-Augmented Transformers: A Systematic Review from Neuroscience
  Principles to Technical Solutions</h2>
                <div class="authors">
                    <strong>Authors:</strong> Parsa Omidi, Xingshuai Huang, Axel Laborieux, Bahareh Nikpour, Tianyu Shi, Armaghan Eshaghi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Memory is fundamental to intelligence, enabling learning, reasoning, and adaptability across biological and artificial systems. While Transformer architectures excel at sequence modeling, they face critical limitations in long-range context retention, continual learning, and knowledge integration. This review presents a unified framework bridging neuroscience principles, including dynamic multi-timescale memory, selective attention, and consolidation, with engineering advances in Memory-Augmented Transformers. We organize recent progress through three taxonomic dimensions: functional objectives (context extension, reasoning, knowledge integration, adaptation), memory representations (parameter-encoded, state-based, explicit, hybrid), and integration mechanisms (attention fusion, gated control, associative retrieval). Our analysis of core memory operations (reading, writing, forgetting, and capacity management) reveals a shift from static caches toward adaptive, test-time learning systems. We identify persistent challenges in scalability and interference, alongside emerging solutions including hierarchical buffering and surprise-gated updates. This synthesis provides a roadmap toward cognitively-inspired, lifelong-learning Transformer architectures.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T16:48:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10824v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10824v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guangda Liu, Chengwei Li, Zhenyu Ning, Minyi Guo, Jieru Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have been widely deployed with rapidly expanding context windows to support increasingly demanding applications. However, long contexts pose significant deployment challenges, primarily due to the KV cache whose size grows proportionally with context length. While KV cache compression methods are proposed to address this issue, KV dropping methods incur considerable accuracy loss, and KV retrieval methods suffer from significant efficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization framework to enhance KV retrieval efficiency while preserving accuracy. On the algorithm side, FreeKV introduces speculative retrieval to shift the KV selection and recall processes out of the critical path, combined with fine-grained correction to ensure accuracy. On the system side, FreeKV employs hybrid KV layouts across CPU and GPU memory to eliminate fragmented data transfers, and leverages double-buffered streamed recall to further improve efficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy across various scenarios and models, delivering up to 13$\times$ speedup compared to SOTA KV retrieval methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T16:12:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.13109v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.13109v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit
  KV Cache</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dayou Du, Shijie Cao, Jianyi Cheng, Luo Mai, Ting Cao, Mao Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rise of long-context Large Language Models (LLMs) amplifies memory and bandwidth demands during autoregressive decoding, as the Key-Value (KV) cache grows with each generated token. Low-bit KV-cache quantization (e.g., 4-bit or 2-bit) can reduce memory footprint while preserving accuracy, but existing systems suffer from slow decoding due to their exclusive reliance on CUDA cores, neglecting Tensor Cores (the primary source of compute on modern GPUs). We present BitDecoding, a new long-context LLM inference system with a low-bit KV cache. BitDecoding enables efficient low-bit KV-cache decoding by cooperatively leveraging CUDA cores and Tensor Cores. It introduces methods for automatically inducing optimized layouts to exploit Tensor Cores, along with warp-level parallelization strategies for dequantization. For unified system support, BitDecoding includes a query transformation module supporting diverse attention variants, a quantization kernel that supports both tensor-wise and channel-wise scaling used in various quantization algorithms with high performance, and a dequantization kernel with a software-defined pipeline to coordinate CUDA and Tensor Cores execution for mixed-precision operations. Evaluated on RTX 4090, A100, and H100, BitDecoding accelerates decoding by up to 7.5x, 4.8x, and 8.9x, respectively, over FP16 FlashDecoding-v2, and surpasses the state-of-the-art low-bit system QServe by up to 4.3x. On LLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding latency by 3x, showing substantial improvements for long-context generation. The code is available at https://github.com/DD-DuDa/BitDecoding.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T15:37:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>cs.AI</span><span>cs.CL</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.18773v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.18773v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 Routing and Wavelength Assignment with Minimal Attack Radius for QKD
  Networks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mengyao Li, Qiaolun Zhang, Zongshuai Yang, Stefano Bregni, Alberto Gatto, Raouf Boutaba, Massimo Tornatore
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Quantum Key Distribution (QKD) can distribute keys with guaranteed security but remains susceptible to key exchange interruption due to physical-layer threats, such as high-power jamming attacks. To address this challenge, we first introduce a novel metric, namely Maximum Number of Affected Requests (maxNAR), to quantify the worst-case impact of a single physical-layer attack, and then we investigate a new problem of Routing and Wavelength Assignment with Minimal Attack Radius (RWA-MAR). We formulate the problem using an Integer Linear Programming (ILP) model and propose a scalable heuristic to efficiently minimize maxNAR. Our approach incorporates key caching through Quantum Key Pools (QKPs) to enhance resilience and optimize resource utilization. Moreover, we model the impact of different QKD network architectures, employing Optical Bypass (OB) for optical switching of quantum channels and Trusted Relay (TR) for secure key forwarding. Moreover, a tunable parameter is designed in the heuristic to guide the preference for OB or TR, offering enhanced adaptability and dynamic control in diverse network scenarios. Simulation results confirm that our method significantly outperforms the baseline in terms of security and scalability.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T13:10:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>quant-ph</span><span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10613v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10613v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 Yan: Foundational Interactive Video Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Deheng Ye, Fangyun Zhou, Jiacheng Lv, Jianqi Ma, Jun Zhang, Junyan Lv, Junyou Li, Minwen Deng, Mingyu Yang, Qiang Fu, Wei Yang, Wenkai Lv, Yangbin Yu, Yewen Wang, Yonghang Guan, Zhihao Hu, Zhongbin Fang, Zhongqian Sun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present Yan, a foundational framework for interactive video generation, covering the entire pipeline from simulation and generation to editing. Specifically, Yan comprises three core modules. AAA-level Simulation: We design a highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based shift-window denoising inference process, achieving real-time 1080P/60FPS interactive simulation. Multi-Modal Generation: We introduce a hierarchical autoregressive caption method that injects game-specific knowledge into open-domain multi-modal video diffusion models (VDMs), then transforming the VDM into a frame-wise, action-controllable, real-time infinite interactive video generator. Notably, when the textual and visual prompts are sourced from different domains, the model demonstrates strong generalization, allowing it to blend and compose the style and mechanics across domains flexibly according to user prompts. Multi-Granularity Editing: We propose a hybrid model that explicitly disentangles interactive mechanics simulation from visual rendering, enabling multi-granularity video content editing during interaction through text. Collectively, Yan offers an integration of these modules, pushing interactive video generation beyond isolated capabilities toward a comprehensive AI-driven interactive creation paradigm, paving the way for the next generation of creative tools, media, and entertainment. The project page is: https://greatx3.github.io/Yan/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T10:26:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.08601v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.08601v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic
  Parallelism in LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Keyu Chen, Zhifeng Shen, Daohai Yu, Haoqian Wu, Wei Wen, Jianfeng He, Ruizhi Qiao, Xing Sun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The increasing scale and complexity of large language models (LLMs) pose significant inference latency challenges, primarily due to their autoregressive decoding paradigm characterized by the sequential nature of next-token prediction. By re-examining the outputs of autoregressive models, we observed that some segments exhibit parallelizable structures, which we term intrinsic parallelism. Decoding each parallelizable branch simultaneously (i.e. parallel decoding) can significantly improve the overall inference speed of LLMs. In this paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which addresses two core challenges: automated construction of parallelizable data and efficient parallel decoding mechanism. More specifically, we introduce a non-invasive pipeline that automatically extracts and validates parallelizable structures from the responses of autoregressive models. To empower efficient adaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which enables seamless transitions between serial and parallel decoding modes while maintaining a reusable KV cache, maximizing computational efficiency. Extensive evaluations across General Tasks, Retrieval-Augmented Generation, Mathematical Reasoning, demonstrate that ASPD achieves unprecedented performance in both effectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up to 3.19x speedup (1.85x on average) while maintaining response quality within 1% difference compared to autoregressive models, realizing significant acceleration without compromising generation quality. Our framework sets a groundbreaking benchmark for efficient LLM parallel inference, paving the way for its deployment in latency-sensitive applications such as AI-powered customer service bots and answer retrieval engines.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T09:04:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.08895v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.08895v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based
  Side-Channel Attacks on Fully Associative Randomized Caches</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chris Cao, Gururaj Saileshwar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent work presented at USENIX Security 2025 claims that occupancy-based attacks can recover AES keys from the MIRAGE randomized cache. In this paper, we examine these claims and find that they arise from fundamental modeling flaws. Most critically, the authors' simulation of MIRAGE uses a constant seed to initialize the random number generator used for global evictions in MIRAGE, causing every AES encryption they trace to evict the same deterministic sequence of cache lines. This artificially creates a highly repeatable timing pattern that is not representative of a realistic implementation of MIRAGE, where eviction sequences vary randomly between encryptions. When we instead randomize the eviction seed for each run, reflecting realistic operation, the correlation between AES T-table accesses and attacker runtimes disappears, and the attack fails. These findings show that the reported leakage is an artifact of incorrect modeling, and not an actual vulnerability in MIRAGE.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T08:04:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10431v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10431v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 NanoControl: A Lightweight Framework for Precise and Efficient Control
  in Diffusion Transformer</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shanyuan Liu, Jian Zhu, Junda Lu, Yue Gong, Liuzhuozheng Li, Bo Cheng, Yuhang Ma, Liebucha Wu, Xiaoyu Wu, Dawei Leng, Yuhui Yin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in text-to-image synthesis. However, in the domain of controllable text-to-image generation using DiTs, most existing methods still rely on the ControlNet paradigm originally designed for UNet-based diffusion models. This paradigm introduces significant parameter overhead and increased computational costs. To address these challenges, we propose the Nano Control Diffusion Transformer (NanoControl), which employs Flux as the backbone network. Our model achieves state-of-the-art controllable text-to-image generation performance while incurring only a 0.024\% increase in parameter count and a 0.029\% increase in GFLOPs, thus enabling highly efficient controllable generation. Specifically, rather than duplicating the DiT backbone for control, we design a LoRA-style (low-rank adaptation) control module that directly learns control signals from raw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation mechanism that integrates condition-specific key-value information into the backbone in a simple yet highly effective manner, facilitating deep fusion of conditional features. Extensive benchmark experiments demonstrate that NanoControl significantly reduces computational overhead compared to conventional control approaches, while maintaining superior generation quality and achieving improved controllability.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T07:54:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10424v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10424v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 XQuant: Breaking the Memory Wall for LLM Inference with KV Cache
  Rematerialization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aditya Tomar, Coleman Hooper, Minjae Lee, Haocheng Xi, Rishabh Tiwari, Wonjun Kang, Luca Manolache, Michael W. Mahoney, Kurt Keutzer, Amir Gholami
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Although LLM inference has emerged as a critical workload for many downstream applications, efficiently inferring LLMs is challenging due to the substantial memory footprint and bandwidth requirements. In parallel, compute capabilities have steadily outpaced both memory capacity and bandwidth over the last few decades, a trend that remains evident in modern GPU hardware and exacerbates the challenge of LLM inference. As such, new algorithms are emerging that trade increased computation for reduced memory operations. To that end, we present XQuant, which takes advantage of this trend, enabling an order-of-magnitude reduction in memory consumption through low-bit quantization with substantial accuracy benefits relative to state-of-the-art KV cache quantization methods. We accomplish this by quantizing and caching the layer input activations X, instead of using standard KV caching, and then rematerializing the Keys and Values on-the-fly during inference. This results in an immediate 2$\times$ memory savings compared to KV caching. By applying XQuant, we achieve up to $\sim 7.7\times$ memory savings with $<0.1$ perplexity degradation compared to the FP16 baseline. Furthermore, our approach leverages the fact that X values are similar across layers. Building on this observation, we introduce XQuant-CL, which exploits the cross-layer similarity in the X embeddings for extreme compression. Across different models, XQuant-CL attains up to 10$\times$ memory savings relative to the FP16 baseline with only 0.01 perplexity degradation, and 12.5$\times$ memory savings with only $0.1$ perplexity degradation. XQuant exploits the rapidly increasing compute capabilities of hardware platforms to eliminate the memory bottleneck, while surpassing state-of-the-art KV cache quantization methods and achieving near-FP16 accuracy across a wide range of models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T06:52:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10395v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10395v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache
  Compression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Payman Behnam, Yaosheng Fu, Ritchie Zhao, Po-An Tsai, Zhiding Yu, Alexey Tumanov
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformer-based Large Language Models rely critically on the KV cache to efficiently handle extended contexts during the decode phase. Yet, the size of the KV cache grows proportionally with the input length, burdening both memory bandwidth and capacity as decoding progresses. To address this challenge, we present RocketKV, a training-free KV cache compression strategy containing two consecutive stages. In the first stage, it performs coarse-grain permanent KV cache eviction on the input sequence tokens. In the second stage, it adopts a hybrid sparse attention method to conduct fine-grain top-k sparse attention, approximating the attention scores by leveraging both head and sequence dimensionality reductions. We show that RocketKV provides a compression ratio of up to 400$\times$, end-to-end speedup of up to 3.7$\times$ as well as peak memory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU compared to the full KV cache baseline, while achieving negligible accuracy loss on a variety of long-context tasks. We also propose a variant of RocketKV for multi-turn scenarios, which consistently outperforms other existing methods and achieves accuracy nearly on par with an oracle top-k attention scheme. The source code is available here: https://github.com/NVlabs/RocketKV.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-13T17:55:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.14051v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.14051v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 Physical Autoregressive Model for Robotic Manipulation without Action
  Pretraining</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zijian Song, Sihan Qin, Tianshui Chen, Liang Lin, Guangrun Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The scarcity of manipulation data has motivated the use of pretrained large models from other modalities in robotics. In this work, we build upon autoregressive video generation models to propose a Physical Autoregressive Model (PAR), where physical tokens combine frames and actions to represent the joint evolution of the robot and its environment. PAR leverages the world knowledge embedded in video pretraining to understand physical dynamics without requiring action pretraining, enabling accurate video prediction and consistent action trajectories. It also adopts a DiT-based de-tokenizer to model frames and actions as continuous tokens, mitigating quantization errors and facilitating mutual enhancement. Furthermore, we incorporate a causal mask with inverse kinematics, parallel training, and the KV-cache mechanism to further improve performance and efficiency. Experiments on the ManiSkill benchmark show that PAR achieves a 100\% success rate on the PushCube task, matches the performance of action-pretrained baselines on other tasks, and accurately predicts future videos with tightly aligned action trajectories. These findings underscore a promising direction for robotic manipulation by transferring world knowledge from autoregressive video pretraining.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-13T13:54:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.09822v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.09822v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 Re-thinking Memory-Bound Limitations in CGRAs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiangfeng Liu, Zhe Jiang, Anzhen Zhu, Xiaomeng Han, Mingsong Lyu, Qingxu Deng, Nan Guan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators commonly employed to boost performance in workloads with iterative structures. Existing research typically focuses on compiler or architecture optimizations aimed at improving CGRA performance, energy efficiency, flexibility, and area utilization, under the idealistic assumption that kernels can access all data from Scratchpad Memory (SPM). However, certain complex workloads-particularly in fields like graph analytics, irregular database operations, and specialized forms of high-performance computing (e.g., unstructured mesh simulations)-exhibit irregular memory access patterns that hinder CGRA utilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To address this challenge, we conduct a thorough analysis of the underlying causes of performance degradation, then propose a redesigned memory subsystem and refine the memory model. With both microarchitectural and theoretical optimization, our solution can effectively manage irregular memory accesses through CGRA-specific runahead execution mechanism and cache reconfiguration techniques. Our results demonstrate that we can achieve performance comparable to the original SPM-only system while requiring only 1.27% of the storage size. The runahead execution mechanism achieves an average 3.04x speedup (up to 6.91x), with cache reconfiguration technique providing an additional 6.02% improvement, significantly enhancing CGRA performance for irregular memory access patterns.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-13T07:40:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>B.3.0; B.6.0</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3760386' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.09570v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.09570v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 Performant Automatic BLAS Offloading on Unified Memory Architecture with
  OpenMP First-Touch Style Data Movement</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junjie Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> BLAS is a fundamental building block of advanced linear algebra libraries and many modern scientific computing applications. GPUs are known for their strong arithmetic computing capabilities and are highly suited for BLAS operations. However, porting code to GPUs often requires significant effort, especially for large, complex codes or legacy codes, even for BLAS-heavy applications. While various tools exist to automatically offload BLAS to GPUs, they are often impractical due to the high costs associated with mandatory data transfers. The advent of unified memory architectures in recent GPU designs, such as the NVIDIA Grace-Hopper, allows cache-coherent memory access across all types of memory for both CPU and GPU, potentially eliminating the bottlenecks faced in conventional architectures. This breakthrough paves the way for innovative application developments and porting strategies. Building on our preliminary work demonstrating the potential of automatic *gemm offload, this paper extends the framework to all level-3 BLAS operations and introduces SCILIB-Accel, a novel tool for automatic BLAS offload. SCILIB-Accel leverages the memory coherency in Grace-Hopper and introduces a Device First-Use data movement policy inspired by the OpenMP First-Touch approach in multi-socket CPU programming, minimizing CPU-GPU data transfers for typical scientific computing codes. Additionally, utilizing dynamic binary instrumentation, the tool intercepts BLAS symbols directly from a CPU binary, requiring no code modifications or recompilation. SCILIB-Accel has been evaluated using multiple quantum physics codes on up to a few hundred GPU nodes, yielding promising speedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was achieved on Grace-Hopper compared to Grace-Grace.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-13T06:13:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.MS</span><span>cs.PF</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.00279v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.00279v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 Advancing Reliable Test-Time Adaptation of Vision-Language Models under
  Visual Variations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yiwen Liang, Hui Chen, Yizhe Xiong, Zihan Zhou, Mengyao Lyu, Zijia Lin, Shuaicheng Niu, Sicheng Zhao, Jungong Han, Guiguang Ding
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but struggle with distribution shifts in downstream tasks when labeled data is unavailable, which has motivated the development of Test-Time Adaptation (TTA) to improve VLMs' performance during inference without annotations. Among various TTA approaches, cache-based methods show promise by preserving historical knowledge from low-entropy samples in a dynamic cache and fostering efficient adaptation. However, these methods face two critical reliability challenges: (1) entropy often becomes unreliable under distribution shifts, causing error accumulation in the cache and degradation in adaptation performance; (2) the final predictions may be unreliable due to inflexible decision boundaries that fail to accommodate large downstream shifts. To address these challenges, we propose a Reliable Test-time Adaptation (ReTA) method that integrates two complementary strategies to enhance reliability from two perspectives. First, to mitigate the unreliability of entropy as a sample selection criterion for cache construction, we introduce Consistency-aware Entropy Reweighting (CER), which incorporates consistency constraints to weight entropy during cache updating. While conventional approaches rely solely on low entropy for cache prioritization and risk introducing noise, our method leverages predictive consistency to maintain a high-quality cache and facilitate more robust adaptation. Second, we present Diversity-driven Distribution Calibration (DDC), which models class-wise text embeddings as multivariate Gaussian distributions, enabling adaptive decision boundaries for more accurate predictions across visually diverse content. Extensive experiments demonstrate that ReTA consistently outperforms state-of-the-art methods, particularly under real-world distribution shifts. Code: https://github.com/Evelyn1ywliang/ReTA.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-13T04:24:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.09500v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.09500v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM
  Serving Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Linke Song, Zixuan Pang, Wenhao Wang, Zihao Wang, XiaoFeng Wang, Hongbo Chen, Wei Song, Yier Jin, Dan Meng, Rui Hou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The wide deployment of Large Language Models (LLMs) has given rise to strong demands for optimizing their inference performance. Today's techniques serving this purpose primarily focus on reducing latency and improving throughput through algorithmic and hardware enhancements, while largely overlooking their privacy side effects, particularly in a multi-user environment. In our research, for the first time, we discovered a set of new timing side channels in LLM systems, arising from shared caches and GPU memory allocations, which can be exploited to infer both confidential system prompts and those issued by other users. These vulnerabilities echo security challenges observed in traditional computing systems, highlighting an urgent need to address potential information leakage in LLM serving infrastructures. In this paper, we report novel attack strategies designed to exploit such timing side channels inherent in LLM deployments, specifically targeting the Key-Value (KV) cache and semantic cache widely used to enhance LLM inference performance. Our approach leverages timing measurements and classification models to detect cache hits, allowing an adversary to infer private prompts with high accuracy. We also propose a token-by-token search algorithm to efficiently recover shared prompt prefixes in the caches, showing the feasibility of stealing system prompts and those produced by peer users. Our experimental studies on black-box testing of popular online LLM services demonstrate that such privacy risks are completely realistic, with significant consequences. Our findings underscore the need for robust mitigation to protect LLM systems against such emerging threats.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-13T04:03:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2409.20002v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2409.20002v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache
  in LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhifan Luo, Shuo Shao, Su Zhang, Lijing Zhou, Yuke Hu, Chenxu Zhao, Zhihao Liu, Zhan Qin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The Key-Value (KV) cache, which stores intermediate attention computations (Key and Value pairs) to avoid redundant calculations, is a fundamental mechanism for accelerating Large Language Model (LLM) inference. However, this efficiency optimization introduces significant yet underexplored privacy risks. This paper provides the first comprehensive analysis of these vulnerabilities, demonstrating that an attacker can reconstruct sensitive user inputs directly from the KV-cache. We design and implement three distinct attack vectors: a direct Inversion Attack, a more broadly applicable and potent Collision Attack, and a semantic-based Injection Attack. These methods demonstrate the practicality and severity of KV-cache privacy leakage issues. To mitigate this, we propose KV-Cloak, a novel, lightweight, and efficient defense mechanism. KV-Cloak uses a reversible matrix-based obfuscation scheme, combined with operator fusion, to secure the KV-cache. Our extensive experiments show that KV-Cloak effectively thwarts all proposed attacks, reducing reconstruction quality to random noise. Crucially, it achieves this robust security with virtually no degradation in model accuracy and minimal performance overhead, offering a practical solution for trustworthy LLM deployment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-13T02:48:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.09442v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.09442v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 Design and Simulation of 6T SRAM Array</h2>
                <div class="authors">
                    <strong>Authors:</strong> Justin London
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Conventional 6T SRAM is used in microprocessors in the cache memory design. The basic 6T SRAM cell and a 6 bit memory array layout are designed in LEdit. The design and analysis of key SRAM components, sense amplifiers, decoders, write drivers and precharge circuits are also provided. The pulse voltage waveforms generated for read and write operations as well as Q and Qbar nodes are simulated in LTSpice. Parasitic capacitances are extracted and their impact on the waveforms analyzed. Static noise margin, propagation delays, and power dissipation are calculated. Comparison of SRAM read and write operational performance using CMOS transistors is made with edge-triggered D flip flops. If certain size area and ratio constraints are satisfied, the 6T cell with CMOS transistors will possess stability, speed, and power efficiency. Both theoretical and simulated results are given.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-13T01:39:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SY</span><span>cs.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.09419v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.09419v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 Scalable Graph Indexing using GPUs for Approximate Nearest Neighbor
  Search</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhonggen Li, Xiangyu Ke, Yifan Zhu, Bocheng Yu, Baihua Zheng, Yunjun Gao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Approximate nearest neighbor search (ANNS) in high-dimensional vector spaces has a wide range of real-world applications. Numerous methods have been proposed to handle ANNS efficiently, while graph-based indexes have gained prominence due to their high accuracy and efficiency. However, the indexing overhead of graph-based indexes remains substantial. With exponential growth in data volume and increasing demands for dynamic index adjustments, this overhead continues to escalate, posing a critical challenge. In this paper, we introduce Tagore, a fast library accelerated by GPUs for graph indexing, which has powerful capabilities of constructing refinement-based graph indexes such as NSG and Vamana. We first introduce GNN-Descent, a GPU-specific algorithm for efficient k-Nearest Neighbor (k-NN) graph initialization. GNN-Descent speeds up the similarity comparison by a two-phase descent procedure and enables highly parallelized neighbor updates. Next, aiming to support various k-NN graph pruning strategies, we formulate a universal computing procedure termed CFS and devise two generalized GPU kernels for parallel processing complex dependencies in neighbor relationships. For large-scale datasets exceeding GPU memory capacity, we propose an asynchronous GPU-CPU-disk indexing framework with a cluster-aware caching mechanism to minimize the I/O pressure on the disk. Extensive experiments on 7 real-world datasets exhibit that Tagore achieves 1.32x-112.79x speedup while maintaining the index quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-13T01:39:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.08744v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.08744v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 Harnessing Input-Adaptive Inference for Efficient VLN</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dongwoo Kang, Akhil Perincherry, Zachary Coalson, Aiden Gabriel, Stefan Lee, Sanghyun Hong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> An emerging paradigm in vision-and-language navigation (VLN) is the use of history-aware multi-modal transformer models. Given a language instruction, these models process observation and navigation history to predict the most appropriate action for an agent. While they have significantly improved performance, the scale of these models can be a bottleneck in practical settings with limited computational resources. In this work, we propose a novel input-adaptive navigation method to enhance VLN model efficiency. We first show that existing input-adaptive mechanisms fail to reduce computations without substantial performance degradation. To address this, we introduce three adaptive algorithms, each deployed at a different level: (1) To improve spatial efficiency, we selectively process panoramic views at each observation of an agent. (2) To improve intra-model efficiency, we propose importance-based adaptive thresholding for the early-exit methods. (3) To improve temporal efficiency, we implement a caching mechanism that prevents reprocessing of views previously seen by the agent. In evaluations on seven VLN benchmarks, we demonstrate over a 2$\times$ reduction in computation across three off-the-shelf agents in both standard and continuous environments. Our code is publicly available at https://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-12T18:05:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.09262v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.09262v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 READER: Retrieval-Assisted Drafter for Efficient LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Maxim Divilkovskiy, Vitaly Malygin, Sergey Zlobin, Sultan Isali, Vasily Kalugin, Stanislav Ilyushin, Nuriza Aitassova, Yi Fei, Zeng Weidi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) generate tokens autoregressively, with each token depending on the preceding context. This sequential nature makes the inference process inherently difficult to accelerate, posing a significant challenge for efficient deployment. In recent years, various methods have been proposed to address this issue, with the most effective approaches often involving the training of additional draft models. In this paper, we introduce READER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel lossless speculative decoding method that enhances model-based approaches by leveraging self-repetitions in the text. Our algorithm expands the speculative decoding tree using tokens obtained through statistical search. This work focuses on large batch sizes (>= 8), an underexplored yet important area for industrial applications. We also analyze the key-value (KV) cache size during speculative decoding and propose an optimization to improve performance for large batches. As a result, READER outperforms existing speculative decoding methods. Notably, READER requires no additional training and can reuse pre-trained speculator models, increasing the speedup by over 40\%. Our method demonstrates particularly strong performance on search-based tasks, such as retrieval-augmented generation, where we achieve more than 10x speedup.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-12T16:47:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.09072v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.09072v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 Retrospective Sparse Attention for Efficient Long-Context Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Seonghwan Choi, Beomseok Kang, Dongwon Jo, Jae-Joon Kim
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are increasingly deployed in long-context tasks such as reasoning, code generation, and multi-turn dialogue. However, inference over extended contexts is bottlenecked by the Key-Value (KV) cache, whose memory footprint grows linearly with sequence length and dominates latency at each decoding step. While recent KV cache compression methods identify and load important tokens, they focus predominantly on input contexts and fail to address the cumulative attention errors that arise during long decoding. In this paper, we introduce RetroAttention, a novel KV cache update technique that retrospectively revises past attention outputs using newly arrived KV entries from subsequent decoding steps. By maintaining a lightweight output cache, RetroAttention enables past queries to efficiently access more relevant context, while incurring minimal latency overhead. This breaks the fixed-attention-output paradigm and allows continual correction of prior approximations. Extensive experiments on long-generation benchmarks show that RetroAttention consistently outperforms state-of-the-art (SOTA) KV compression methods, increasing effective KV exposure by up to 1.6$\times$ and accuracy by up to 21.9\%.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-12T15:11:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.09001v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.09001v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 TaoCache: Structure-Maintained Video Generation Acceleration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhentao Fan, Zongzuo Wang, Weiwei Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Existing cache-based acceleration methods for video diffusion models primarily skip early or mid denoising steps, which often leads to structural discrepancies relative to full-timestep generation and can hinder instruction following and character consistency. We present TaoCache, a training-free, plug-and-play caching strategy that, instead of residual-based caching, adopts a fixed-point perspective to predict the model's noise output and is specifically effective in late denoising stages. By calibrating cosine similarities and norm ratios of consecutive noise deltas, TaoCache preserves high-resolution structure while enabling aggressive skipping. The approach is orthogonal to complementary accelerations such as Pyramid Attention Broadcast (PAB) and TeaCache, and it integrates seamlessly into DiT-based frameworks. Across Latte-1, OpenSora-Plan v110, and Wan2.1, TaoCache attains substantially higher visual quality (LPIPS, SSIM, PSNR) than prior caching methods under the same speedups.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-12T14:40:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.08978v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.08978v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 Keep Your Friends Close: Leveraging Affinity Groups to Accelerate AI
  Inference Workflows</h2>
                <div class="authors">
                    <strong>Authors:</strong> Thiago Garrett, Weijia Song, Roman Vitenberg, Ken Birman
                </div>
                <div class="summary">
                    <strong>Summary:</strong> AI inference workflows are typically structured as a pipeline or graph of AI programs triggered by events. As events occur, the AIs perform inference or classification tasks under time pressure to respond or take some action. Standard techniques that reduce latency in other streaming settings (such as caching and optimization-driven scheduling) are of limited value because AI data access patterns (models, databases) change depending on the triggering event: a significant departure from traditional streaming. In this work, we propose a novel affinity grouping mechanism that makes it easier for developers to express application-specific data access correlations, enabling coordinated management of data objects in server clusters hosting streaming inference tasks. Our proposals are thus complementary to other approaches such as caching and scheduling. Experiments confirm the limitations of standard techniques, while showing that the proposed mechanism is able to maintain significantly lower latency as workload and scale-out increase, and yet requires only minor code changes.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-12T10:43:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2312.11488v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2312.11488v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 From Slow Bidirectional to Fast Autoregressive Video Diffusion Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tianwei Yin, Qiang Zhang, Richard Zhang, William T. Freeman, Fredo Durand, Eli Shechtman, Xun Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Current video diffusion models achieve impressive generation quality but struggle in interactive applications due to bidirectional attention dependencies. The generation of a single frame requires the model to process the entire sequence, including the future. We address this limitation by adapting a pretrained bidirectional diffusion transformer to an autoregressive transformer that generates frames on-the-fly. To further reduce latency, we extend distribution matching distillation (DMD) to videos, distilling 50-step diffusion model into a 4-step generator. To enable stable and high-quality distillation, we introduce a student initialization scheme based on teacher's ODE trajectories, as well as an asymmetric distillation strategy that supervises a causal student model with a bidirectional teacher. This approach effectively mitigates error accumulation in autoregressive generation, allowing long-duration video synthesis despite training on short clips. Our model achieves a total score of 84.27 on the VBench-Long benchmark, surpassing all previous video generation models. It enables fast streaming generation of high-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our approach also enables streaming video-to-video translation, image-to-video, and dynamic prompting in a zero-shot manner.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-12T05:51:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.07772v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.07772v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 Rigorous quantum calculations for atom-molecule chemical reactions in
  electric fields: from single to multiple partial wave regimes</h2>
                <div class="authors">
                    <strong>Authors:</strong> Timur V. Tscherbul, Roman V. Krems
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present an efficient method for rigorous quantum calculations of cross sections for atom-molecule reactive scattering in the presence of a dc electric field. The wavefunction of the reaction complex is expanded in an overcomplete set of arrangement-dependent Fock-Delves hyperspherical basis functions and the interactions of the reactants and products with electric fields are accounted for in the total angular momentum representation. A significant computational challenge affecting our previously developed approach [Phys. Rev. Lett. $\mathbf{115}$, 023201 (2015)] is addressed by an efficient asymptotic frame transformation between the hyperspherical and Jacobi coordinates in the presence of an external field. Using accurate {\it ab initio} potential energy surfaces, we calculate total and state-resolved cross sections for the chemical reactions LiF$(v=1,j=0)$ + H $\to$ Li + HF($v'=0,j'$) and F + HD$(v=0,j=0)$ $\to$ HF + D, DF + H as functions of collision energy and electric field strength. The field dependence of the cross sections for the LiF + H chemical reaction exhibits resonance structure mediated by tunneling-driven interactions between reactants and products. No significant field effects are found for the F + HD $\to$ HF + D, DF + H chemical reaction at 1 Kelvin, even for state-resolved transitions and with field magnitudes reaching 200 kV/cm. Our calculations illustrate the essential role of basis set convergence for the proper interpretation of external field effects on chemical reaction dynamics. While reduced-basis calculations for the F + HD reaction indicate significant effects of electric fields on product state distributions, these effects vanish when the number of total angular momentum basis states is increased.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-12T03:33:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.chem-ph</span><span>physics.atom-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.08600v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.08600v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 Semantic Caching for Low-Cost LLM Serving: From Offline Learning to
  Online Adaptation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xutong Liu, Baran Atalar, Xiangxiang Dai, Jinhang Zuo, Siwei Wang, John C. S. Lui, Wei Chen, Carlee Joe-Wong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are revolutionizing how users interact with information systems, yet their high inference cost poses serious scalability and sustainability challenges. Caching inference responses, allowing them to be retrieved without another forward pass through the LLM, has emerged as one possible solution. Traditional exact-match caching, however, overlooks the semantic similarity between queries, leading to unnecessary recomputation. Semantic caching addresses this by retrieving responses based on semantic similarity, but introduces a fundamentally different cache eviction problem: one must account for mismatch costs between incoming queries and cached responses. Moreover, key system parameters, such as query arrival probabilities and serving costs, are often unknown and must be learned over time. Existing semantic caching methods are largely ad-hoc, lacking theoretical foundations and unable to adapt to real-world uncertainty. In this paper, we present a principled, learning-based framework for semantic cache eviction under unknown query and cost distributions. We formulate both offline optimization and online learning variants of the problem, and develop provably efficient algorithms with state-of-the-art guarantees. We also evaluate our framework on a synthetic dataset, showing that our proposed algorithms perform matching or superior performance compared with baselines.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-12T02:51:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.07675v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.07675v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided
  Region Control</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zeqian Long, Mingzhe Zheng, Kunyu Feng, Xinhua Zhang, Hongyu Liu, Harry Yang, Linfeng Zhang, Qifeng Chen, Yue Ma
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While recent flow-based image editing models demonstrate general-purpose capabilities across diverse tasks, they often struggle to specialize in challenging scenarios -- particularly those involving large-scale shape transformations. When performing such structural edits, these methods either fail to achieve the intended shape change or inadvertently alter non-target regions, resulting in degraded background quality. We propose Follow-Your-Shape, a training-free and mask-free framework that supports precise and controllable editing of object shapes while strictly preserving non-target content. Motivated by the divergence between inversion and editing trajectories, we compute a Trajectory Divergence Map (TDM) by comparing token-wise velocity differences between the inversion and denoising paths. The TDM enables precise localization of editable regions and guides a Scheduled KV Injection mechanism that ensures stable and faithful editing. To facilitate a rigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120 new images and enriched prompt pairs specifically curated for shape-aware editing. Experiments demonstrate that our method achieves superior editability and visual fidelity, particularly in tasks requiring large-scale shape replacement.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-12T02:27:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.08134v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.08134v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 Profiling Large Language Model Inference on Apple Silicon: A
  Quantization Perspective</h2>
                <div class="authors">
                    <strong>Authors:</strong> Afsara Benazir, Felix Xiaozhu Lin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> A systematic understanding of Apple Silicon is lacking in the current landscape of hardware efficiency; research focus is largely centered on accelerating GPUs for large-scale training or inference on CUDA devices. This paper investigates Apple Silicon's unique memory architecture that offers a unified memory integrating CPU and GPU memory and its implications for on-device LLM inference.   We decipher myths about whether Apple Silicon is efficient for on-device inference compared to competitors such as NVIDIA GPUs by directly conducting latency and throughput comparison benchmarks. We explain the performance gap between them through profiling low level hardware metrics - ALU utilization, memory bandwidth, buffer usage, cache residency etc. at runtime. We draw several insights regarding performance bottlenecks such as dequantization overhead, compute throughput and memory bandwidth. We debunk existing false claims regarding large language model inference such as compressing models to lower bit precision is a defacto promise for faster inference across all hardware platforms. We find that the large unified memory enables Apple Silicon to be both cost effective and efficient against NVIDIA GPUs for ultra large language models.   Our large scale evaluation on 5 hardware testbeds incorporating three Apple M-series devices: M2 Ultra, M2 Max and M4 Pro and two NVIDIA GPUs: NVIDIA RTX A6000, a multi GPU setup with 2xNVIDIA RTX A6000, 5 model scales ranging from 8B to 405B parameters and 14 quantization schemes gives an understanding of how Apple Silicon fits within the paradigm of on-device LLM inference. Our analysis reveals multiple resource interdependencies and unexpected findings, while also quantifying established insights. To the best of our knowledge, this study makes the first attempt to present a thorough characterization and analysis of Apple Silicon for on-device inference.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-12T00:06:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.08531v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.08531v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 Architecting Long-Context LLM Acceleration with Packing-Prefetch
  Scheduler and Ultra-Large Capacity On-Chip Memories</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ming-Yen Lee, Faaiq Waqar, Hanchen Yang, Muhammed Ahosan Ul Karim, Harsono Simka, Shimeng Yu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Long-context Large Language Model (LLM) inference faces increasing compute bottlenecks as attention calculations scale with context length, primarily due to the growing KV-cache transfer overhead that saturates High Bandwidth Memory (HBM). While prefetching techniques mitigate cache misses by fetching KV data in advance, their spatial and temporal benefits present new opportunities to exploit. This work proposes a packing-prefetch scheduling architecture with monolithic 3D (M3D) back-end-of-line (BEOL) compatible embedded memories with ultra-large on-chip capacity to accelerate long-context LLM inference. Our optimizations demonstrate 8.06x decode speedup and 1.83x overall latency reduction on Llama3.1-8B using TPUv6e-like hardware with additional 512MB BEOL memories over the serial execution. Evaluations of multi-request workloads on TPU-like architectures show 1.7x-2.4x throughput improvement and 1.5x-2.4x HBM bandwidth reduction compared to packing-only methods on Llama3.1-8B and Llama3.1-70B models. With the co-design of packing, prefetching, and BEOL memories, our approach alleviates HBM constraints and enables efficient long-context LLM inference.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-11T20:30:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>cs.ET</span><span>C.1.3; B.3.1</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.08457v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.08457v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM
  Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kexin Chu, Zecheng Lin, Dawei Xiang, Zixu Shen, Jianchang Su, Cheng Chu, Yiwei Yang, Wenhui Zhang, Wenfei Wu, Wei Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Global KV-cache sharing has emerged as a key optimization for accelerating large language model (LLM) inference. However, it exposes a new class of timing side-channel attacks, enabling adversaries to infer sensitive user inputs via shared cache entries. Existing defenses, such as per-user isolation, eliminate leakage but degrade performance by up to 38.9% in time-to-first-token (TTFT), making them impractical for high-throughput deployment. To address this gap, we introduce SafeKV (Secure and Flexible KV Cache Sharing), a privacy-aware KV-cache management framework that selectively shares non-sensitive entries while confining sensitive content to private caches. SafeKV comprises three components: (i) a hybrid, multi-tier detection pipeline that integrates rule-based pattern matching, a general-purpose privacy detector, and context-aware validation; (ii) a unified radix-tree index that manages public and private entries across heterogeneous memory tiers (HBM, DRAM, SSD); and (iii) entropy-based access monitoring to detect and mitigate residual information leakage. Our evaluation shows that SafeKV mitigates 94% - 97% of timing-based side-channel attacks. Compared to per-user isolation method, SafeKV improves TTFT by up to 40.58% and throughput by up to 2.66X across diverse LLMs and workloads. SafeKV reduces cache-induced TTFT overhead from 50.41% to 11.74% on Qwen3-235B. By combining fine-grained privacy control with high cache reuse efficiency, SafeKV reclaims the performance advantages of global sharing while providing robust runtime privacy guarantees for LLM inference.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-11T19:55:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.LG</span><span>cs.OS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.08438v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.08438v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 Numerical computation of linearized KV and the Deligne-Drinfeld and
  Broadhurst-Kreimer conjectures</h2>
                <div class="authors">
                    <strong>Authors:</strong> Florian Naef, Thomas Willwacher
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We compute numerically the dimensions of the graded quotients of the linearized Kashiwara-Vergne Lie algebra lkv in low weight, confirming a conjecture of Raphael-Schneps in those weights. The Lie algebra lkv appears in a chain of inclusions of Lie algebras, including also the linearized double shuffle Lie algebra and the (depth associated graded of the) Grothendieck-Teichm\"uller Lie algebra. Hence our computations also allow us to check the validity of the Deligne-Drinfeld conjecture on the structure of the Grothendieck-Teichm\"uller group up to weight 29, and (a version of) the the Broadhurst-Kreimer conjecture on the number of multiple zeta values for a range of weight-depth pairs significantly exceeding the previous bounds. Our computations also verify a conjecture by Alekseev-Torossian on the Kashiwara-Vergne Lie algebra up to weight 29.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-11T15:28:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.QA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.08081v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.08081v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 From Reusing to Forecasting: Accelerating Diffusion Models with
  TaylorSeers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiacheng Liu, Chang Zou, Yuanhuiyi Lyu, Junjie Chen, Linfeng Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion Transformers (DiT) have revolutionized high-fidelity image and video synthesis, yet their computational demands remain prohibitive for real-time applications. To solve this problem, feature caching has been proposed to accelerate diffusion models by caching the features in the previous timesteps and then reusing them in the following timesteps. However, at timesteps with significant intervals, the feature similarity in diffusion models decreases substantially, leading to a pronounced increase in errors introduced by feature caching, significantly harming the generation quality. To solve this problem, we propose TaylorSeer, which firstly shows that features of diffusion models at future timesteps can be predicted based on their values at previous timesteps. Based on the fact that features change slowly and continuously across timesteps, TaylorSeer employs a differential method to approximate the higher-order derivatives of features and predict features in future timesteps with Taylor series expansion. Extensive experiments demonstrate its significant effectiveness in both image and video synthesis, especially in high acceleration ratios. For instance, it achieves an almost lossless acceleration of 4.99$\times$ on FLUX and 5.00$\times$ on HunyuanVideo without additional training. On DiT, it achieves $3.41$ lower FID compared with previous SOTA at $4.53$$\times$ acceleration. %Our code is provided in the supplementary materials and will be made publicly available on GitHub. Our codes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-11T14:15:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.06923v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.06923v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 Maximizing GPU Efficiency via Optimal Adapter Caching: An Analytical
  Approach for Multi-Tenant LLM Serving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ferran Agullo, Joan Oliveras, Chen Wang, Alberto Gutierrez-Torre, Olivier Tardieu, Alaa Youssef, Jordi Torres, Josep Ll. Berral
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Serving LLM adapters has gained significant attention as an effective approach to adapt general-purpose language models to diverse, task-specific use cases. However, serving a wide range of adapters introduces several and substantial overheads, leading to performance degradation and challenges in optimal placement. To address these challenges, we present an analytical, AI-driven pipeline that accurately determines the optimal allocation of adapters in single-node setups. This allocation maximizes performance, effectively using GPU resources, while preventing request starvation. Crucially, the proposed allocation is given based on current workload patterns. These insights in single-node setups can be leveraged in multi-replica deployments for overall placement, load balancing and server configuration, ultimately enhancing overall performance and improving resource efficiency. Our approach builds on an in-depth analysis of LLM adapter serving, accounting for overheads and performance variability, and includes the development of the first Digital Twin capable of replicating online LLM-adapter serving systems with matching key performance metrics. The experimental results demonstrate that the Digital Twin achieves a SMAPE difference of no more than 5.5% in throughput compared to real results, and the proposed pipeline accurately predicts the optimal placement with minimal latency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-11T10:47:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.PF</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.08343v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.08343v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 DiTVR: Zero-Shot Diffusion Transformer for Video Restoration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sicheng Gao, Nancy Mehta, Zongwei Wu, Radu Timofte
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Video restoration aims to reconstruct high quality video sequences from low quality inputs, addressing tasks such as super resolution, denoising, and deblurring. Traditional regression based methods often produce unrealistic details and require extensive paired datasets, while recent generative diffusion models face challenges in ensuring temporal consistency. We introduce DiTVR, a zero shot video restoration framework that couples a diffusion transformer with trajectory aware attention and a wavelet guided, flow consistent sampler. Unlike prior 3D convolutional or frame wise diffusion approaches, our attention mechanism aligns tokens along optical flow trajectories, with particular emphasis on vital layers that exhibit the highest sensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically selects relevant tokens based on motion correspondences across frames. The flow guided sampler injects data consistency only into low-frequency bands, preserving high frequency priors while accelerating convergence. DiTVR establishes a new zero shot state of the art on video restoration benchmarks, demonstrating superior temporal consistency and detail preservation while remaining robust to flow noise and occlusions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-11T09:54:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.07811v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.07811v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by
  Exploiting Temporal Continuity</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kunyun Wang, Shuo Yang, Jieru Zhao, Wenchao Ding, Quan Chen, Jingwen Leng, Minyi Guo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Deep learning models have become pivotal in the field of video processing and is increasingly critical in practical applications such as autonomous driving and object detection. Although Vision Transformers (ViTs) have demonstrated their power, Convolutional Neural Networks (CNNs) remain a highly efficient and high-performance choice for feature extraction and encoding. However, the intensive computational demands of convolution operations hinder its broader adoption as a video encoder. Given the inherent temporal continuity in video frames, changes between consecutive frames are minimal, allowing for the skipping of redundant computations. This technique, which we term as Diff Computation, presents two primary challenges. First, Diff Computation requires to cache intermediate feature maps to ensure the correctness of non-linear computations, leading to significant memory consumption. Second, the imbalance of sparsity among layers, introduced by Diff Computation, incurs accuracy degradation. To address these issues, we propose a memory-efficient scheduling method to eliminate memory overhead and an online adjustment mechanism to minimize accuracy degradation. We integrate these techniques into our framework, SparseTem, to seamlessly support various CNN-based video encoders. SparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with minimal accuracy drop and no additional memory overhead. Extensive experimental results demonstrate that SparseTem sets a new state-of-the-art by effectively utilizing temporal continuity to accelerate CNN-based video encoders.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-11T08:10:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2410.20790v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2410.20790v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token
  Selection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Raghavv Goel, Junyoung Park, Mukul Gagrani, Dalton Jones, Matthew Morse, Harper Langston, Mingu Lee, Chris Lott
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While long context support of large language models has extended their abilities, it also incurs challenges in memory and compute which becomes crucial bottlenecks in resource-restricted devices. Token eviction, a widely adopted post-training methodology designed to alleviate the bottlenecks by evicting less important tokens from the cache, typically uses attention scores as proxy metrics for token importance. However, one major limitation of attention score as a token-wise importance metrics is that it lacks the information about contribution of tokens to the attention output. In this paper, we propose a simple eviction criterion based on the contribution of cached tokens to attention outputs. Our method, CAOTE, optimizes for eviction error due to token eviction, by seamlessly integrating attention scores and value vectors. This is the first method which uses value vector information on top of attention-based eviction scores. Additionally, CAOTE can act as a meta-heuristic method with flexible usage with any token eviction method. We show that CAOTE, when combined with the state-of-the-art attention score-based methods, always improves accuracies on the downstream task, indicating the importance of leveraging information from values during token eviction process.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-11T06:16:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.14051v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.14051v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language
  Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Khanh-Binh Nguyen, Phuoc-Nguyen Bui, Hyunseung Choo, Duc Thanh Nguyen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Vision-language models (VLMs) exhibit remarkable zero-shot generalization but suffer performance degradation under distribution shifts in downstream tasks, particularly in the absence of labeled data. Test-Time Adaptation (TTA) addresses this challenge by enabling online optimization of VLMs during inference, eliminating the need for annotated data. Cache-based TTA methods exploit historical knowledge by maintaining a dynamic memory cache of low-entropy or high-confidence samples, promoting efficient adaptation to out-of-distribution data. Nevertheless, these methods face two critical challenges: (1) unreliable confidence metrics under significant distribution shifts, resulting in error accumulation within the cache and degraded adaptation performance; and (2) rigid decision boundaries that fail to accommodate substantial distributional variations, leading to suboptimal predictions. To overcome these limitations, we introduce the Adaptive Cache Enhancement (ACE) framework, which constructs a robust cache by selectively storing high-confidence or low-entropy image embeddings per class, guided by dynamic, class-specific thresholds initialized from zero-shot statistics and iteratively refined using an exponential moving average and exploration-augmented updates. This approach enables adaptive, class-wise decision boundaries, ensuring robust and accurate predictions across diverse visual distributions. Extensive experiments on 15 diverse benchmark datasets demonstrate that ACE achieves state-of-the-art performance, delivering superior robustness and generalization compared to existing TTA methods in challenging out-of-distribution scenarios.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-11T03:03:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.07570v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.07570v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 CoMoE: Collaborative Optimization of Expert Aggregation and Offloading
  for MoE-based LLMs at Edge</h2>
                <div class="authors">
                    <strong>Authors:</strong> Muqing Li, Ning Li, Xin Yuan, Wenchao Xu, Quan Chen, Song Guo, Haijun Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The proliferation of large language models (LLMs) has driven the adoption of Mixture-of-Experts (MoE) architectures as a promising solution to scale model capacity while controlling computational costs. However, deploying MoE models in resource-constrained mobile edge computing environments presents significant challenges due to their large memory footprint and dynamic expert activation patterns. To address these challenges, we propose a novel dynamic resource-aware collaborative optimization framework that jointly optimizes expert aggregation granularity and offloading strategies based on real-time device resource states, network conditions, and input characteristics in mobile edge environments, denoted as CoMoE. In CoMoE, we first systematically analyze existing expert aggregation techniques, including expert parameter merging,knowledge distillation,and parameter sharing decomposition, identifying their limitations in dynamic mobile environments.We then investigate expert offloading strategies encompassing expert prediction and prefetching, expert caching and scheduling, and multi-tier storage architectures, revealing the interdependencies between routing decisions and offloading performance.The CoMoE incorporates adaptive scheduling mechanisms that respond to user mobility and varying network conditions, enabling efficient MoE deployment across heterogeneous edge devices. Extensive experiments on real mobile edge testbeds demonstrate that CoMoE achieves approximately 70% reduction in memory usage compared to baseline methods, 10.5% lower inference latency than existing expert offloading techniques, while maintaining model performance stability. For large-scale MoE models (e.g,7.4B-parameter Switch-Base-128), the CoMoE reduces memory requirements from 15.6GB to 4.7GB, enabling deployment on resource-constrained mobile edge devices that previously could only support much smaller models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-10T14:05:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.09208v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.09208v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal
  Diffusion</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiahua Ma, Yiran Qin, Yixiong Li, Xuanqi Liao, Yulan Guo, Ruimao Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion Policy (DP) enables robots to learn complex behaviors by imitating expert demonstrations through action diffusion. However, in practical applications, hardware limitations often degrade data quality, while real-time constraints restrict model inference to instantaneous state and scene observations. These limitations seriously reduce the efficacy of learning from expert demonstrations, resulting in failures in object localization, grasp planning, and long-horizon task execution. To address these challenges, we propose Causal Diffusion Policy (CDP), a novel transformer-based diffusion model that enhances action prediction by conditioning on historical action sequences, thereby enabling more coherent and context-aware visuomotor policy learning. To further mitigate the computational cost associated with autoregressive inference, a caching mechanism is also introduced to store attention key-value pairs from previous timesteps, substantially reducing redundant computations during execution. Extensive experiments in both simulated and real-world environments, spanning diverse 2D and 3D manipulation tasks, demonstrate that CDP uniquely leverages historical action sequences to achieve significantly higher accuracy than existing methods. Moreover, even when faced with degraded input observation quality, CDP maintains remarkable precision by reasoning through temporal continuity, which highlights its practical robustness for robotic control under realistic, imperfect conditions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-09T11:31:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.14769v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.14769v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 CannyEdit: Selective Canny Control and Dual-Prompt Guidance for
  Training-Free Image Editing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weiyan Xie, Han Gao, Didan Deng, Kaican Li, April Hua Liu, Yongxiang Huang, Nevin L. Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in text-to-image (T2I) models have enabled training-free regional image editing by leveraging the generative priors of foundation models. However, existing methods struggle to balance text adherence in edited regions, context fidelity in unedited areas, and seamless integration of edits. We introduce CannyEdit, a novel training-free framework that addresses these challenges through two key innovations: (1) Selective Canny Control, which masks the structural guidance of Canny ControlNet in user-specified editable regions while strictly preserving details of the source images in unedited areas via inversion-phase ControlNet information retention. This enables precise, text-driven edits without compromising contextual integrity. (2) Dual-Prompt Guidance, which combines local prompts for object-specific edits with a global target prompt to maintain coherent scene interactions. On real-world image editing tasks (addition, replacement, removal), CannyEdit outperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent improvement in the balance of text adherence and context fidelity. In terms of editing seamlessness, user studies reveal only 49.2 percent of general users and 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited when paired with real images without edits, versus 76.08 to 89.09 percent for competitor methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-09T11:06:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.06937v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.06937v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 Managing Data for Scalable and Interactive Event Sequence Visualization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sayef Azad Sakin, Katherine E. Isaacs
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Parallel event sequences, such as those collected in program execution traces and automated manufacturing pipelines, are typically visualized as interactive parallel timelines. As the dataset size grows, these charts frequently experience lag during common interactions such as zooming, panning, and filtering. Summarization approaches can improve interaction performance, but at the cost of accuracy in representation. To address this challenge, we introduce ESeMan (Event Sequence Manager), an event sequence management system designed to support interactive rendering of timeline visualizations with tunable accuracy. ESeMan employs hierarchical data structures and intelligent caching to provide visualizations with only the data necessary to generate accurate summarizations with significantly reduced data fetch time. We evaluate ESeMan's query times against summed area tables, M4 aggregation, and statistical sub-sampling on a variety of program execution traces. Our results demonstrate ESeMan provides better performance, achieving sub-100ms fetch times while maintaining visualization accuracy at the pixel level. We further present our benchmarking harness, enabling future performance evaluations for event sequence visualization.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-09T02:33:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.03974v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.03974v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile
  Device via Additive Side-Tuning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xingke Yang, Liang Li, Zhiyi Wan, Sicong Li, Xiaoqi Qi, Jiang Liu, Tomoaki Ohtsuki, Xin Fu, Miao Pan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> There is a huge gap between numerous intriguing applications fostered by on-device large language model (LLM) fine-tuning (FT) from fresh mobile data and the limited resources of a mobile device. While existing server-assisted methods (e.g., split learning or side-tuning) may enable LLM FT on the local mobile device, they suffer from heavy communication burdens of activation transmissions, and may disclose data and labels to the server. To address those issues, we develop PAE MobiLLM, a a privacy-aware and efficient LLM FT method which can be deployed on the mobile device via server-assisted additive side-tuning. To further accelerate FT convergence and improve computing efficiency, PAE MobiLLM integrates activation caching on the server side, which allows the server to reuse historical activations and saves the mobile device from repeatedly computing forward passes for the recurring data samples. Besides, to reduce communication cost, PAE MobiLLM develops an activation shortcut that transmits only the token involved in the loss calculation instead of full activation matrices to guide the side network tuning. Last but not least, PAE MobiLLM introduces the additive adapter side-network design which makes the server train the adapter modules based on device-defined prediction differences rather than raw ground-truth labels. In this way, the server can only assist device-defined side-network computing, and learn nothing about data and labels. Extensive experimental results demonstrate PAE MobiLLM's superiority.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-09T00:12:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.01216v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.01216v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and
  Extreme KV Compression</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guihong Li, Mehdi Rezagholizadeh, Mingyu Yang, Vikram Appia, Emad Barsoum
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multi-head latent attention (MLA) is designed to optimize KV cache memory through low-rank key-value joint compression. Rather than caching keys and values separately, MLA stores their compressed latent representations, reducing memory overhead while maintaining the performance. While MLA improves memory efficiency without compromising language model accuracy, its major limitation lies in its integration during the pre-training phase, requiring models to be trained from scratch. This raises a key question: can we use MLA's benefits fully or partially in models that have already been pre-trained with different attention mechanisms? In this paper, we propose X-EcoMLA to deploy post training distillation to enable the upcycling of Transformer-based attention into an efficient hybrid MLA variant through lightweight post-training adaptation, bypassing the need for extensive pre-training. We demonstrate that leveraging the dark knowledge of a well-trained model can enhance training accuracy and enable extreme KV cache compression in MLA without compromising model performance. The experimental results show that our proposed method can effectively compress the KV cache while preserving the performance on the benchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression achieves the same average score by using only 3.6B training tokens and 70 GPU hours on AMD MI300, whereas a 10.6x compression have less than 0.1% average score drop with 7B training tokens and 140 GPU hours. The code for this work is available at https://github.com/AMD-AIG-AIMA/AMD-Hybrid-Models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-08T18:16:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.11132v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.11132v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token
  Pruning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lingkun Long, Rubing Yang, Yushi Huang, Desheng Hui, Ao Zhou, Jianlei Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Long-context inference for Large Language Models (LLMs) is heavily limited by high computational demands. While several existing methods optimize attention computation, they still process the full set of hidden states at each layer, limiting overall efficiency. In this work, we propose SlimInfer, an innovative framework that aims to accelerate inference by directly pruning less critical prompt tokens during the forward pass. Our key insight is an information diffusion phenomenon: As information from critical tokens propagates through layers, it becomes distributed across the entire sequence. This diffusion process suggests that LLMs can maintain their semantic integrity when excessive tokens, even including these critical ones, are pruned in hidden states. Motivated by this, SlimInfer introduces a dynamic fine-grained pruning mechanism that accurately removes redundant tokens of hidden state at intermediate layers. This layer-wise pruning naturally enables an asynchronous KV cache manager that prefetches required token blocks without complex predictors, reducing both memory usage and I/O costs. Extensive experiments show that SlimInfer can achieve up to $\mathbf{2.53\times}$ time-to-first-token (TTFT) speedup and $\mathbf{1.88\times}$ end-to-end latency reduction for LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on LongBench. Our code will be released upon acceptance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-08T16:42:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.06447v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.06447v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fatemeh Hadadi, Qinghua Xu, Domenico Bianculli, Lionel Briand
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Most log-based anomaly detectors assume logs are stable, though logs are often unstable due to software or environmental changes. Anomaly detection on unstable logs (ULAD) is therefore a more realistic, yet under-investigated challenge. Current approaches predominantly employ machine learning (ML) models, which often require extensive labeled data for training. To mitigate data insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that combines ML models -- decision tree, k-nearest neighbors, and a feedforward neural network -- with a Large Language Model (Mistral) through ensemble learning. FlexLog also incorporates a cache and retrieval-augmented generation (RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we configured four datasets for \task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and SYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points (pp) in F1 score while using much less labeled data (62.87 pp reduction). When trained on the same amount of data as the baselines, FlexLog achieves up to a 13 pp increase in F1 score on ADFA-U across varying training dataset sizes. Additionally, FlexLog maintains inference time under one second per log sequence, making it suitable for most applications, except latency-sensitive systems. Further analysis reveals the positive impact of FlexLog's key components: cache, RAG and ensemble learning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-08T14:25:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2406.07467v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2406.07467v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 KV Cache Compression for Inference Efficiency in LLMs: A Review</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yanyu Liu, Jingying Fu, Sixiang Liu, Yitian Zou, You Fu, Jiehan Zhou, Shouhua Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Withtherapid advancement of large language models (LLMs), the context length for inference has been continuously increasing, leading to an exponential growth in the demand for Key-Value (KV) caching. This has resulted in a significant memory bottleneck, limiting the inference efficiency and scalability of the models. Therefore, optimizing the KV cache during inference is crucial for enhancing performance and efficiency. This review systematically examines current KV cache optimization techniques, including compression strategies such as selective token strategies, quantization, and attention compression. We evaluate the effectiveness, trade-offs, and application scenarios of these methods, providing a comprehensive analysis of their impact on memory usage and inference speed. We focus on identifying the limitations and challenges of existing methods, such as compatibility issues with different models and tasks. Additionally, this review highlights future research directions, including hybrid optimization techniques, adaptive dynamic strategies, and software-hardware co-design. These approaches aim to improve inference efficiency and promote the practical application of large language models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-08T13:19:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.06297v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.06297v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 Fewer Denoising Steps or Cheaper Per-Step Inference: Towards
  Compute-Optimal Diffusion Model Deployment</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhenbang Du, Yonggan Fu, Lifu Wang, Jiayi Qian, Xiao Luo, Yingyan, Lin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion models have shown remarkable success across generative tasks, yet their high computational demands challenge deployment on resource-limited platforms. This paper investigates a critical question for compute-optimal diffusion model deployment: Under a post-training setting without fine-tuning, is it more effective to reduce the number of denoising steps or to use a cheaper per-step inference? Intuitively, reducing the number of denoising steps increases the variability of the distributions across steps, making the model more sensitive to compression. In contrast, keeping more denoising steps makes the differences smaller, preserving redundancy, and making post-training compression more feasible. To systematically examine this, we propose PostDiff, a training-free framework for accelerating pre-trained diffusion models by reducing redundancy at both the input level and module level in a post-training manner. At the input level, we propose a mixed-resolution denoising scheme based on the insight that reducing generation resolution in early denoising steps can enhance low-frequency components and improve final generation fidelity. At the module level, we employ a hybrid module caching strategy to reuse computations across denoising steps. Extensive experiments and ablation studies demonstrate that (1) PostDiff can significantly improve the fidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to boost efficiency while maintaining decent generation fidelity, reducing per-step inference cost is often more effective than reducing the number of denoising steps. Our code is available at https://github.com/GATECH-EIC/PostDiff.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-08T09:29:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.06160v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.06160v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 LLM Serving Optimization with Variable Prefill and Decode Lengths</h2>
                <div class="authors">
                    <strong>Authors:</strong> Meixuan Wang, Yinyu Ye, Zijie Zhou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We study the problem of serving LLM (Large Language Model) requests where each request has heterogeneous prefill and decode lengths. In LLM serving, the prefill length corresponds to the input prompt length, which determines the initial memory usage in the KV cache. The decode length refers to the number of output tokens generated sequentially, with each additional token increasing the KV cache memory usage by one unit. Given a set of n requests, our goal is to schedule and process them to minimize the total completion time. We show that this problem is NP-hard due to the interplay of batching, placement constraints, precedence relationships, and linearly increasing memory usage. We then analyze commonly used scheduling strategies in practice, such as First-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their competitive ratios scale up sublinearly with the memory limit-a significant drawback in real-world settings where memory demand is large. To address this, we propose a novel algorithm based on a new selection metric that efficiently forms batches over time. We prove that this algorithm achieves a constant competitive ratio. Finally, we develop and evaluate a few algorithm variants inspired by this approach, including dynamic programming variants, local search methods, and an LP-based scheduler, demonstrating through comprehensive simulations that they outperform standard baselines while maintaining computational efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-08T08:54:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.OC</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.06133v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.06133v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 A Generic Complete Anytime Beam Search for Optimal Decision Tree</h2>
                <div class="authors">
                    <strong>Authors:</strong> Harold Silvre Kiossou, Siegfried Nijssen, Pierre Schaus
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Finding an optimal decision tree that minimizes classification error is known to be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic programming guarantee optimality, they often suffer from poor anytime behavior -- meaning they struggle to find high-quality decision trees quickly when the search is stopped before completion -- due to unbalanced search space exploration. To address this, several anytime extensions of exact methods have been proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not been systematically compared, making it difficult to assess their relative effectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and anytime beam search algorithm that extends the DL8.5 framework and unifies some existing anytime strategies. In particular, CA-DL8.5 generalizes previous approaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various heuristics and relaxation mechanisms through a modular design. The algorithm reuses DL8.5's efficient branch-and-bound pruning and trie-based caching, combined with a restart-based beam search that gradually relaxes pruning criteria to improve solution quality over time. Our contributions are twofold: (1) We introduce this new generic framework for exact and anytime decision tree learning, enabling the incorporation of diverse heuristics and search strategies; (2) We conduct a rigorous empirical comparison of several instantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k heuristics -- using an anytime evaluation metric called the primal gap integral. Experimental results on standard classification benchmarks show that CA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime performance, outperforming both other CA-DL8.5 variants and the Blossom algorithm while maintaining completeness and optimality guarantees.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-08T06:53:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.06064v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.06064v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 Affine Springer fibers and depth zero L-packets</h2>
                <div class="authors">
                    <strong>Authors:</strong> Roman Bezrukavnikov, Yakov Varshavsky
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Let $G$ be a connected reductive group over a field $F=\mathbb{F}_q((t))$ splitting over $\overline{\mathbb{F}}_q((t))$. Following [KV,DR], a tamely unramified Langlands parameter $\lambda:W_F\to{}^L G(\overline{\mathbb{Q}}_{\ell})$ in general position gives rise to a finite set $\Pi_{\lambda}$ of irreducible admissible representations of $G(F)$, called the $L$-packet.   The main goal of this work is to provide a geometric description of characters $\chi_{\pi}$ of $\pi\in\Pi_{\lambda}$ and of their endoscopic linear combinations $\chi_{\lambda}^{\kappa}$ in terms of homology of affine Springer fibers, thus establishing an analog of Lusztig conjectures in this case. Furthermore, each $\chi_{\lambda}^{\kappa}$ can be described as the trace of Frobenius function of a conjugation equivariant perverse sheaf on the loop group by the sheaf-function correspondence.   As another application, we prove that the sum $\chi_{\lambda}^{st}:=\sum_{\pi\in\Pi_{\lambda}}\chi_{\pi}$ is stable and show that the $\chi_{\lambda}^{st}$'s are compatible with inner twistings. More generally, we prove that each $\chi_{\lambda}^{\kappa}$ is $\mathcal{E}_{\lambda,\kappa}$-stable.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-08T06:38:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.RT</span><span>math.AG</span><span>22E50, 22E57</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2104.13123v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2104.13123v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion
  Forcing</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xu Wang, Chenkai Xu, Yijie Jin, Jiachun Jin, Hao Zhang, Zhijie Deng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to autoregressive (AR) LLMs for text generation, with the potential to decode multiple tokens in a single iteration. However, none of the existing open-source dLLMs have achieved superior inference speed over AR LLMs of similar size. This paper breaks this barrier based on a simple and effective strategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key capabilities: (1) block-wise autoregressive generation to enable KV cache utilization; (2) prediction of following tokens without requiring completion of prior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs are refurbished into an AR-diffusion hybrid paradigm for efficient inference. D2F can be implemented with an asymmetric distillation process based on pre-trained dLLMs. We further propose a pipelined parallel decoding algorithm, which enables a trade-off between efficiency and efficacy. Empirically, D2F dLLMs achieve more than $\mathbf{2.5\times}$ inference speed than LLaMA3 and Qwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the acceleration can be more than $\mathbf{50\times}$ while maintaining comparable output quality. The code is available at https://github.com/zhijie-group/Discrete-Diffusion-Forcing.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-08T04:51:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.09192v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.09192v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 Snowpark: Performant, Secure, User-Friendly Data Engineering and AI/ML
  Next To Your Data</h2>
                <div class="authors">
                    <strong>Authors:</strong> Brandon Baker, Elliott Brossard, Chenwei Xie, Zihao Ye, Deen Liu, Yijun Xie, Arthur Zwiegincew, Nitya Kumar Sharma, Gaurav Jain, Eugene Retunsky, Mike Halcrow, Derek Denny-Brown, Istvan Cseri, Tyler Akidau, Yuxiong He
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Snowflake revolutionized data analytics with an elastic architecture that decouples compute and storage, enabling scalable solutions supporting data architectures like data lake, data warehouse, data lakehouse, and data mesh. Building on this foundation, Snowflake has advanced its AI Data Cloud vision by introducing Snowpark, a managed turnkey solution that supports data engineering and AI and ML workloads using Python and other programming languages.   This paper outlines Snowpark's design objectives towards high performance, strong security and governance, and ease of use. We detail the architecture of Snowpark, highlighting its elastic scalability and seamless integration with Snowflake core compute infrastructure. This includes leveraging Snowflake control plane for distributed computing and employing a secure sandbox for isolating Snowflake SQL workloads from Snowpark executions. Additionally, we present core innovations in Snowpark that drive further performance enhancements, such as query initialization latency reduction through Python package caching, improved workload scheduling for customized workloads, and data skew management via efficient row redistribution. Finally, we showcase real-world case studies that illustrate Snowpark's efficiency and effectiveness for large-scale data engineering and AI and ML tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-07T23:53:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.05904v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.05904v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 ETTA: Efficient Test-Time Adaptation for Vision-Language Models through
  Dynamic Embedding Updates</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hamidreza Dastmalchi, Aijun An, Ali cheraghian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Pretrained vision-language models (VLMs) like CLIP show strong zero-shot performance but struggle with generalization under distribution shifts. Test-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test data in new domains. While some TTA methods rely on prompt-tuning, training-free cache-based approaches are preferred for efficiency. However, current cache-based TTA models store only a limited set of high-confidence samples, restricting the decision boundary to these samples and ignoring the influence of other incoming test data. To address this, we propose Efficient Test-Time Adaptation (ETTA), introducing a Recursive Updating module that integrates all incoming test samples, progressively refining the decision boundary. This strategy mimics an unbounded cache, dynamically updating contextual embeddings for improved accuracy with minimal memory and computational overhead. ETTA also includes an Adaptive Ensemble module to reduce prompt dependency in image-to-text scores by dynamically selecting optimal prompts for each class. Furthermore, ETTA adaptively combines scores from both modules based on confidence levels, leveraging their complementary strengths. Extensive experiments on two benchmarks confirm that ETTA surpasses the state-of-the-art TTA models in computational complexity and accuracy, setting a new standard for effective, efficient test-time adaptation. The code has been released at https://github.com/hamidreza-dastmalchi/ETTA.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-07T23:11:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.05898v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.05898v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 RTTC: Reward-Guided Collaborative Test-Time Compute</h2>
                <div class="authors">
                    <strong>Authors:</strong> J. Pablo Muoz, Jinjie Yuan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the performance of Large Language Models (LLMs) at inference, leveraging strategies such as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG). However, the optimal adaptation strategy varies across queries, and indiscriminate application of TTC strategy incurs substantial computational overhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a novel framework that adaptively selects the most effective TTC strategy for each query via a pretrained reward model, maximizing downstream accuracy across diverse domains and tasks. RTTC operates in a distributed server-client architecture, retrieving relevant samples from a remote knowledge base and applying RAG or lightweight fine-tuning on client devices only when necessary. To further mitigate redundant computation, we propose Query-State Caching, which enables the efficient reuse of historical query states at both retrieval and adaptation levels. Extensive experiments across multiple LLMs and benchmarks demonstrate that RTTC consistently achieves superior accuracy compared to vanilla RAG or TTT, validating the necessity of adaptive, reward-guided TTC selection and the potential of RTTC for scalable, high-performance language model adaptation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-07T21:18:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.IR</span><span>I.2.0</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10024v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10024v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 VFlowOpt: A Token Pruning Framework for LMMs with Visual Information
  Flow-Guided Optimization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sihan Yang, Runsen Xu, Chenhang Cui, Tai Wang, Dahua Lin, Jiangmiao Pang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging numerous visual tokens for fine-grained visual information, but this token redundancy results in significant computational costs. Previous research aimed at reducing visual tokens during inference typically leverages importance maps derived from attention scores among vision-only tokens or vision-language tokens to prune tokens across one or multiple pruning stages. Despite this progress, pruning frameworks and strategies remain simplistic and insufficiently explored, often resulting in substantial performance degradation. In this paper, we propose VFlowOpt, a token pruning framework that introduces an importance map derivation process and a progressive pruning module with a recycling mechanism. The hyperparameters of its pruning strategy are further optimized by a visual information flow-guided method. Specifically, we compute an importance map for image tokens based on their attention-derived context relevance and patch-level information entropy. We then decide which tokens to retain or prune and aggregate the pruned ones as recycled tokens to avoid potential information loss. Finally, we apply a visual information flow-guided method that regards the last token in the LMM as the most representative signal of text-visual interactions. This method minimizes the discrepancy between token representations in LMMs with and without pruning, thereby enabling superior pruning strategies tailored to different LMMs. Experiments demonstrate that VFlowOpt can prune 90% of visual tokens while maintaining comparable performance, leading to an 89% reduction in KV-Cache memory and 3.8 times faster inference.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-07T09:47:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.05211v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.05211v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human
  Video Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jingxuan He, Busheng Su, Finn Wong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Generating long, temporally coherent videos with precise control over subject identity and motion is a formidable challenge for current diffusion models, which often suffer from identity drift and are limited to short clips. We introduce PoseGen, a novel framework that generates arbitrarily long videos of a specific subject from a single reference image and a driving pose sequence. Our core innovation is an in-context LoRA finetuning strategy that injects subject appearance at the token level for identity preservation, while simultaneously conditioning on pose information at the channel level for fine-grained motion control. To overcome duration limits, PoseGen pioneers an interleaved segment generation method that seamlessly stitches video clips together, using a shared KV cache mechanism and a specialized transition process to ensure background consistency and temporal smoothness. Trained on a remarkably small 33-hour video dataset, extensive experiments show that PoseGen significantly outperforms state-of-the-art methods in identity fidelity, pose accuracy, and its unique ability to produce coherent, artifact-free videos of unlimited duration.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-07T07:19:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.05091v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.05091v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 Making Prompts First-Class Citizens for Adaptive LLM Pipelines</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ugur Cetintemel, Shu Chen, Alexander W. Lee, Deepti Raghavan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modern LLM pipelines increasingly resemble data-centric systems: they retrieve external context, compose intermediate outputs, validate results, and adapt based on runtime feedback. Yet, the central element guiding this process -- the prompt -- remains a brittle, opaque string, disconnected from the surrounding dataflow. This disconnect limits reuse, optimization, and runtime control.   In this paper, we describe our vision and an initial design for SPEAR, a language and runtime that fills this prompt management gap by making prompts structured, adaptive, and first-class components of the execution model. SPEAR enables (1) runtime prompt refinement -- modifying prompts dynamically in response to execution-time signals such as confidence, latency, or missing context; and (2) structured prompt management -- organizing prompt fragments into versioned views with support for introspection and logging.   SPEAR defines a prompt algebra that governs how prompts are constructed and adapted within a pipeline. It supports multiple refinement modes (manual, assisted, and automatic), giving developers a balance between control and automation. By treating prompt logic as structured data, SPEAR enables optimizations such as operator fusion, prefix caching, and view reuse. Preliminary experiments quantify the behavior of different refinement modes compared to static prompts and agentic retries, as well as the impact of prompt-level optimizations such as operator fusion.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-07T03:49:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.05012v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.05012v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jun Zhang, Desen Meng, Zhengming Zhang, Zhenpeng Huang, Tao Wu, Limin Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Despite the remarkable performance of multimodal large language models (MLLMs) across diverse tasks, the substantial training and inference costs impede their advancement. In this paper, we propose p-MoD, an efficient MLLM architecture that significantly reduces training and inference costs while maintaining model performance. The majority of computation in MLLMs stems from the overwhelming volume of vision tokens processed by the transformer-based LLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each LLM layer selects essential vision tokens to process while skipping redundant ones. However, integrating MoD into MLLMs is non-trivial. To address the challenges of training and inference stability as well as limited training data, we adapt the MoD module with two novel designs: tanh-gated weight normalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we observe that vision tokens exhibit higher redundancy in deeper layers and thus design a progressive ratio decay (PRD) strategy, which gradually reduces the token retention ratio layer by layer, employing a shifted cosine schedule. This crucial design fully unleashes the potential of MoD, significantly boosting the efficiency and performance of our models. Extensive experiments on two baseline models across 15 benchmarks show that our model matches or even surpasses the performance of corresponding baselines, while requiring only 55.6% TFLOPs and 53.7% KV cache storage during inference, and 77.7% GPU hours during training.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-06T16:57:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.04449v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.04449v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 Share Your Attention: Transformer Weight Sharing via Matrix-based
  Dictionary Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Magauiya Zhussip, Dmitriy Shopkhoev, Ammar Ali, Stamatios Lefkimmiatis
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have revolutionized AI applications, yet their high computational and memory demands hinder their widespread deployment. Existing compression techniques focus on intra-block optimizations (e.g. low-rank approximation, attention head pruning), while the repetitive layered structure of transformers implies significant inter-block redundancy - a dimension largely unexplored beyond key-value (KV) caching. Inspired by dictionary learning in CNNs, we propose a framework for structured weight sharing across transformer layers. Our approach decomposes attention projection matrices into shared dictionary atoms, reducing the attention module's parameters by 66.7% while achieving on-par performance. Unlike complex methods requiring distillation or architectural changes, MASA (Matrix Atom Sharing in Attention) operates as a drop-in replacement - trained with standard optimizers - and represents each layer's weights as linear combinations of shared matrix atoms. Experiments across scales (100M-700M parameters) show that MASA achieves better benchmark accuracy and perplexity than grouped-query attention (GQA), low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at comparable parameter budgets. Ablation studies confirm robustness to the dictionary size and the efficacy of shared representations in capturing cross-layer statistical regularities. Extending to Vision Transformers (ViT), MASA matches performance metrics on image classification and detection tasks with 66.7% fewer attention parameters. By combining dictionary learning strategies with transformer efficiency, MASA offers a scalable blueprint for parameter-efficient models without sacrificing performance. Finally, we investigate the possibility of employing MASA on pretrained LLMs to reduce their number of parameters without experiencing any significant drop in their performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-06T16:06:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.04581v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.04581v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 From FASTER to F2: Evolving Concurrent Key-Value Store Designs for Large
  Skewed Workloads</h2>
                <div class="authors">
                    <strong>Authors:</strong> Konstantinos Kanellis, Badrish Chandramouli, Ted Hart, Shivaram Venkataraman
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modern large-scale services such as search engines, messaging platforms, and serverless functions, rely on key-value (KV) stores to maintain high performance at scale. When such services are deployed in constrained memory environments, they present challenging requirements: point operations requiring high throughput, working sets much larger than main memory, and natural skew in key access patterns. Traditional KV stores, based on LSM- and B-Trees, have been widely used to handle such use cases, but they often suffer from suboptimal use of modern hardware resources. The FASTER project, developed as a high-performance open-source KV storage library, has demonstrated remarkable success in both in-memory and hybrid storage environments. However, when tasked with serving large skewed workloads, it faced challenges, including high indexing and compactions overheads, and inefficient management of non-overlapping read-hot and write-hot working sets.   In this paper, we introduce F2 (for FASTER v2), an evolution of FASTER designed to meet the requirements of large skewed workloads common in industry applications. F2 adopts a two-tier record-oriented design to handle larger-than-memory skewed workloads, along with new concurrent latch-free mechanisms and components to maximize performance on modern hardware. To realize this design, F2 tackles key challenges and introduces several innovations, including new latch-free algorithms for multi-threaded log compaction, a two-level hash index to reduce indexing overhead for cold records, and a read-cache for serving read-hot records. Our evaluation shows that F2 achieves 2-11.9x better throughput compared to existing KV stores, effectively serving the target workload. F2 is open-source and available as part of the FASTER project.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-06T15:38:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2305.01516v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2305.01516v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large
  Language Model Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Enyu Zhou, Kai Sheng, Hao Chen, Xin He
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Speculative decoding (SD), where an extra draft model first provides multiple draft tokens and the original target model then verifies these tokens in parallel, has shown great power for LLM inference acceleration. However, existing SD methods must adhere to the 'draft-then-verify' paradigm, which forces drafting and verification processes to execute sequentially during SD, resulting in inefficient inference performance and limiting the size of the draft model. Furthermore, once a single token in the candidate sequence is rejected during the drafting process, all subsequent candidate tokens must be discarded, leading to inefficient drafting. To address these challenges, we propose a cache-based parallel speculative decoding framework employing a 'query-and-correct' paradigm. Specifically, CARD decouples drafting and verification: the draft model generates candidate tokens to populate a shared cache, while the target model concurrently rectifies the draft model's generation direction. This effectively enables the target model to perform inference at speed approaching that of the draft model. Our approach achieves up to 4.83 speedup over vanilla decoding without requiring fine-tuning of either the draft or target models. Our code is available at https://github.com/hunzhizi/CARD.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-06T14:02:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.04462v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.04462v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 CITRAS: Covariate-Informed Transformer for Time Series Forecasting</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yosuke Yamaguchi, Issei Suemitsu, Wenpeng Wei
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In practical time series forecasting, covariates provide rich contextual information that can potentially enhance the forecast of target variables. Although some covariates extend into the future forecasting horizon (e.g., calendar events, discount schedules), most multivariate models fail to leverage this pivotal insight due to the length discrepancy with target variables. Additionally, capturing the dependency between target variables and covariates is non-trivial, as models must precisely reflect the local impact of covariates while also capturing global cross-variate dependencies. To overcome these challenges, we propose CITRAS, a decoder-only Transformer that flexibly leverages multiple targets, past covariates, and future covariates. While preserving strong autoregressive capabilities, CITRAS introduces two novel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and Attention Score Smoothing. KV Shift seamlessly incorporates future covariates into the forecasting of target variables based on their concurrent dependencies. Additionally, Attention Score Smoothing refines locally accurate patch-wise cross-variate dependencies into global variate-level dependencies by smoothing the past series of attention scores. Experimentally, CITRAS outperforms state-of-the-art models on thirteen real-world benchmarks from both covariate-informed and multivariate settings, demonstrating its versatile ability to leverage cross-variate and cross-time dependencies for improved forecasting accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-06T11:46:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.24007v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.24007v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 KVSink: Understanding and Enhancing the Preservation of Attention Sinks
  in KV Cache Quantization for LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zunhai Su, Kehong Yuan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Key-Value (KV) cache quantization has become a widely adopted optimization technique for efficient large language models (LLMs) inference by reducing KV cache memory usage and mitigating memory-bound constraints. Recent studies have emphasized the importance of preserving the original precision of KVs for the first few tokens to ensure the protection of attention sinks. While this approach has proven effective in mitigating performance degradation, its underlying principles remain insufficiently understood. Moreover, it fails to address the recent discovery that attention sinks can emerge beyond the initial token positions. In this work, we elucidate the underlying mechanisms of attention sinks during inference by examining their role in the cross-layer evolution of extreme activation outliers. Additionally, we provide a comprehensive analysis of the interplay between attention sinks and KV cache quantization. Based on our enhanced understanding, we introduce \textit{\textbf{KVSink}}, a plug-and-play method that effectively predicts sink tokens with negligible overhead, enabling more thorough preservation. Extensive experiments demonstrate that KVSink outperforms the existing Preserve-First-N (PFN) strategy, offering more effective preservation of attention sinks during KV cache quantization. Moreover, when applied to the well-established KVQuant method, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit numerical outliers.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-06T09:40:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.04257v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.04257v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level
  Efficiency for Edge Devices</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiyu Chen, Poh Seng Lim, Shuang Peng, Daxiong Luo, JungHau Foo, Yap Deep, Timothy Lee Jun Jie, Kelvin Teh Kae Wen, Fan Yang, Danyu Feng, Hao-Yun Chen, Peng-Wen Chen, Fangyuan Li, Xiaoxin Chen, Wong Wai Mun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Deploying Transformer-based large language models (LLMs) on resource-constrained edge devices for long-sequence tasks remains challenging due to the quadratic time complexity of self-attention and growing Key-Value (KV) cache demands. While existing KV cache optimizations improve memory efficiency, they often fail to reduce time to first token (TTFT) and may degrade performance through token pruning. Alternative sequence modeling architectures address some of these limitations, but typically require full retraining and lack infrastructure support. EdgeInfinite offers an efficient solution by fine-tuning only a small subset of parameters, maintaining quality while reducing both computational and memory costs, including improved TTFT. However, its instruction-following ability is limited, and it lacks mobile-specific optimizations. To address these issues, we propose EdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning (S-SFT) strategy tailored to long-sequence tasks such as summarization and question answering. We further optimized EdgeInfinite-Instruct for efficient deployment on edge NPUs by employing fine-grained post-training quantization (PTQ) to reduce computational demands while maintaining accuracy, and by implementing a fixed-shape computation graph that balances memory usage and on-device efficiency through scenario-specific customization of input token and cache sizes. Experiments on long-context benchmarks and real-world mobile tasks show that our approach improves domain-specific performance while maintaining efficiency on NPU-accelerated edge devices.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-06T08:32:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.00370v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.00370v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent
  Memory Subsystems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Davide Zoni, Andrea Galimberti, Adriano Guarisco
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Designing and validating efficient cache-coherent memory subsystems is a critical yet complex task in the development of modern multi-core system-on-chip architectures. Rhea is a unified framework that streamlines the design and system-level validation of RTL cache-coherent memory subsystems. On the design side, Rhea generates synthesizable, highly configurable RTL supporting various architectural parameters. On the validation side, Rhea integrates Verilator's cycle-accurate RTL simulation with gem5's full-system simulation, allowing realistic workloads and operating systems to run alongside the actual RTL under test. We apply Rhea to design MSI-based RTL memory subsystems with one and two levels of private caches and scaling up to sixteen cores. Their evaluation with 22 applications from state-of-the-art benchmark suites shows intermediate performance relative to gem5 Ruby's MI and MOESI models. The hybrid gem5-Verilator co-simulation flow incurs a moderate simulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher fidelity by simulating real RTL hardware. This overhead decreases with scale, down to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's effectiveness and scalability in enabling fast development of RTL cache-coherent memory subsystem designs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-05T18:34:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.03837v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.03837v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing
  Diffusion Transformer Caching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhen Zou, Feng Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion Transformer (DiT) has exhibited impressive generation capabilities but faces great challenges due to its high computational complexity. To address this issue, various methods, notably feature caching, have been introduced. However, these approaches focus on aligning non-cache diffusion without analyzing why caching damage the generation processes. In this paper, we first confirm that the cache greatly amplifies the exposure bias, resulting in a decline in the generation quality. However, directly applying noise scaling is challenging for this issue due to the non-smoothness of exposure bias. We found that this phenomenon stems from the mismatch between its frequency response characteristics and the simple cache of Attention and MLP. Since these two components exhibit unique preferences for frequency signals, which provides us with a caching strategy to separate Attention and MLP to achieve an enhanced fit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a joint caching strategy that aligns with the non-exposed bias diffusion process (which gives us a higher performance cap) of caching Attention and MLP based on the frequency-guided cache table. Our approach combines a comprehensive understanding of the caching mechanism and offers a new perspective on leveraging caching to accelerate the diffusion process. Empirical results indicate that FEB-Cache optimizes model performance while concurrently facilitating acceleration.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-05T16:17:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.07120v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.07120v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 Bidirectional TLS Handshake Caching for Constrained Industrial IoT
  Scenarios</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jrn Bodenhausen, Simon Mangel, Thomas Vogt, Martin Henze
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While TLS has become the de-facto standard for end-to-end security, its use to secure critical communication in evolving industrial IoT scenarios is severely limited by prevalent resource constraints of devices and networks. Most notably, the TLS handshake to establish secure connections incurs significant bandwidth and processing overhead that often cannot be handled in constrained environments. To alleviate this situation, we present BiTHaC which realizes bidirectional TLS handshake caching by exploiting that significant parts of repeated TLS handshakes, especially certificates, are static. Thus, redundant information neither needs to be transmitted nor corresponding computations performed, saving valuable bandwidth and processing resources. By implementing BiTHaC for wolfSSL, we show that we can reduce the bandwidth consumption of TLS handshakes by up to 61.1% and the computational overhead by up to 8.5%, while incurring only well-manageable memory overhead and preserving the strict security guarantees of TLS.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-05T11:00:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.03321v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.03321v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yueyue Liu, Hongyu Zhang, Yuantian Miao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable capabilities in a variety of software engineering tasks. Despite the advancements, their practical deployment faces challenges, including high financial costs, long response time, and varying performance, especially when handling a large number of queries (jobs). Existing optimization strategies for deploying LLMs for diverse tasks focus on static scheduling, which requires extensive training data for performance prediction, increasing the computational costs and limiting the applicability and flexibility. In this paper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective scheduling solution. The key idea is to learn LLMs' performance on diverse tasks and incorporate their real-time feedback to update strategies periodically. Specifically, SLS incorporates three key components, including an Adaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic Update Manager. The Cache Manager stores the outputs of previously processed queries and employs an adaptive strategy to reduce redundant computations and minimize response times. For queries not found in the cache, the Scheduler dynamically allocates them to the most suitable LLM based on the predicted performance and cost from models that take both query-specific and LLM-specific features as input. The Update Manager continuously refines the cache and scheduling strategies based on real-time feedback from the assigned queries to enhance decision-making and adapt to evolving task characteristics. To evaluate the effectiveness of SLS, we conduct extensive experiments on two LLM-based software engineering tasks, including log parsing and code generation. The results show that SLS significantly outperforms the baseline methods, achieving an average performance improvement of 198.82% and an average processing time reduction of 63.28%.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-05T09:35:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.03258v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.03258v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 Forecasting When to Forecast: Accelerating Diffusion Models with
  Confidence-Gated Taylor</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaoliu Guan, Lielin Jiang, Hanqi Chen, Xu Zhang, Jiaxing Yan, Guanzhong Wang, Yi Liu, Zetao Zhang, Yu Wu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion Transformers (DiTs) have demonstrated remarkable performance in visual generation tasks. However, their low inference speed limits their deployment in low-resource applications. Recent training-free approaches exploit the redundancy of features across timesteps by caching and reusing past representations to accelerate inference. Building on this idea, TaylorSeer instead uses cached features to predict future ones via Taylor expansion. However, its module-level prediction across all transformer blocks (e.g., attention or feedforward modules) requires storing fine-grained intermediate features, leading to notable memory and computation overhead. Moreover, it adopts a fixed caching schedule without considering the varying accuracy of predictions across timesteps, which can lead to degraded outputs when prediction fails. To address these limitations, we propose a novel approach to better leverage Taylor-based acceleration. First, we shift the Taylor prediction target from the module level to the last block level, significantly reducing the number of cached features. Furthermore, observing strong sequential dependencies among Transformer blocks, we propose to use the error between the Taylor-estimated and actual outputs of the first block as an indicator of prediction reliability. If the error is small, we trust the Taylor prediction for the last block; otherwise, we fall back to full computation, thereby enabling a dynamic caching mechanism. Empirical results show that our method achieves a better balance between speed and quality, achieving a 3.17x acceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible quality drop. The Project Page is \href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-05T02:13:39Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.02240v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.02240v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 GainSight: A Unified Framework for Data Lifetime Profiling and
  Heterogeneous Memory Composition</h2>
                <div class="authors">
                    <strong>Authors:</strong> Peijing Li, Matthew Hung, Yiming Tan, Konstantin Hofeld, Jake Cheng Jiajun, Shuhan Liu, Lixian Yan, Xinxin Wang, Philip Levis, H. -S. Philip Wong, Thierry Tambe
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As AI workloads drive increasing memory requirements, domain-specific accelerators need higher-density on-chip memory beyond what current SRAM scaling trends can provide. Simultaneously, the vast amounts of short-lived data in these workloads make SRAM overprovisioned in retention capability. To address this mismatch, we propose a wholesale shift from uniform SRAM arrays to heterogeneous on-chip memory, incorporating denser short-term RAM (StRAM) devices whose limited retention times align with transient data lifetimes. To facilitate this shift, we introduce GainSight, the first comprehensive, open-source framework that aligns dynamic, fine-grained workload lifetime profiles with memory device characteristics to enable generation of optimal StRAM memory compositions. GainSight combines retargetable profiling backends with an architecture-agnostic analytical frontend. The various backends capture cycle-accurate data lifetimes, while the frontend correlates workload patterns with StRAM retention properties to generate optimal memory compositions and project performance. GainSight elevates data lifetime to a first-class design consideration for next-generation AI accelerators, enabling systematic exploitation of data transience for improved on-chip memory density and efficiency. Applying GainSight to MLPerf Inference and PolyBench workloads reveals that 64.3% of first-level GPU cache accesses and 79.01% of systolic array scratchpad accesses exhibit sub-microsecond lifetimes suitable for high-density StRAM, with optimal heterogeneous on-chip memory compositions achieving up to 3x active energy and 4x area reductions compared to uniform SRAM hierarchies. To facilitate adoption and further research, GainSight is open-sourced at https://gainsight.stanford.edu/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-05T00:25:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>cs.ET</span><span>B.7.1; B.3.1; C.3; I.6; I.2.6</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.14866v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.14866v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuerong Song, Xiaoran Liu, Ruixiao Li, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and parallel decoding but suffer from prohibitive quadratic computational complexity and memory overhead during inference. Current caching techniques accelerate decoding by storing full-layer states, yet impose substantial memory usage that limit long-context applications. Our analysis of attention patterns in dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining salient across decoding steps and low-relevance tokens staying unimportant, motivating selective cache eviction. We propose Sparse-dLLM, the first training-free framework integrating dynamic cache eviction with sparse attention via delayed bidirectional sparse caching. By leveraging the stability of token saliency over steps, it retains critical tokens and dynamically evicts unimportant prefix/suffix entries using an attention-guided strategy. Extensive experiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to 10$\times$ higher throughput than vanilla dLLMs, with comparable performance and similar peak memory costs, outperforming previous methods in efficiency and effectiveness.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-04T16:14:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.02558v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.02558v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important
  Before Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiaolin Lin, Jingcun Wang, Olga Kondrateva, Yiyu Shi, Bing Li, Grace Li Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in large language models (LLMs) have significantly boosted long-context processing. However, the increasing key-value (KV) cache size poses critical challenges to memory and execution efficiency. Most KV cache compression methods rely on heuristic token eviction using all attention heads in Grouped Query Attention (GQA)-based LLMs. This method ignores the different functionalities of attention heads, leading to the eviction of critical tokens and thus degrades the performance of LLMs.   To address the issue above, instead of using all the attention heads in GQA-based LLMs to determine important tokens as in the previous work, we first identify the attention heads in each layer that are not only capable of retrieving the initial and final tokens of a prompt, but also capable of retrieving important tokens within the text and attending to their surrounding semantic context. Afterwards, we exploit such heads to determine the important tokens and retain their corresponding KV cache pairs. Furthermore, we analyze the cache eviction error of each layer individually and introduce a layer-adaptive KV cache allocation strategy. Experimental results demonstrate the proposed CompressKV consistently outperforms state-of-the-art approaches under various memory budgets on LongBench and Needle-in-a-Haystack benchmarks. Our code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-04T13:26:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.02401v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.02401v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 OnPair: Short Strings Compression for Fast Random Access</h2>
                <div class="authors">
                    <strong>Authors:</strong> Francesco Gargiulo, Rossano Venturini
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present OnPair, a dictionary-based compression algorithm designed to meet the needs of in-memory database systems that require both high compression and fast random access. Existing methods either achieve strong compression ratios at significant computational and memory cost (e.g., BPE) or prioritize speed at the expense of compression quality (e.g., FSST). OnPair bridges this gap by employing a cache-friendly dictionary construction technique that incrementally merges frequent adjacent substrings in a single sequential pass over a data sample. This enables fast, memory-efficient training without tracking global pair positions, as required by traditional BPE. We also introduce OnPair16, a variant that limits dictionary entries to 16 bytes, enabling faster parsing via optimized longest prefix matching. Both variants compress strings independently, supporting fine-grained random access without block-level overhead. Experiments on real-world datasets show that OnPair and OnPair16 achieve compression ratios comparable to BPE while significantly improving compression speed and memory usage.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-04T10:51:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>H.2.4; E.4; H.3.2</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.02280v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.02280v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 LeanK: Learnable K Cache Channel Pruning for Efficient Decoding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yike Zhang, Zhiyuan He, Huiqiang Jiang, Chengruidong Zhang, Yuqing Yang, Jianyong Wang, Lili Qiu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) enable long-context tasks but face efficiency challenges due to the growing key-value (KV) cache. We propose LeanK, a learning-based method that prunes unimportant key (K) cache channels by leveraging static channel sparsity. With a novel two-stage training process, LeanK learns channel-wise static mask that could satisfy specific sparsity ratio and hardware alignment requirement. LeanK reduces GPU memory and accelerates decoding without sacrificing accuracy. Experiments demonstrate up to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel enables 1.3x speedup for attention computation. We also provide insights into model channels and attention heads during long-context inference by analyzing the learned importance distribution. Our code is available at https://aka.ms/LeanK.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-04T09:08:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.02215v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.02215v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 CaliDrop: KV Cache Compression with Calibration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yi Su, Quantong Qiu, Yuechi Zhou, Juntao Li, Qingrong Xia, Ping Li, Xinyu Duan, Zhefeng Wang, Min Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) require substantial computational resources during generation. While the Key-Value (KV) cache significantly accelerates this process by storing attention intermediates, its memory footprint grows linearly with sequence length, batch size, and model size, creating a bottleneck in long-context scenarios. Various KV cache compression techniques, including token eviction, quantization, and low-rank projection, have been proposed to mitigate this bottleneck, often complementing each other. This paper focuses on enhancing token eviction strategies. Token eviction leverages the observation that the attention patterns are often sparse, allowing for the removal of less critical KV entries to save memory. However, this reduction usually comes at the cost of notable accuracy degradation, particularly under high compression ratios. To address this issue, we propose \textbf{CaliDrop}, a novel strategy that enhances token eviction through calibration. Our preliminary experiments show that queries at nearby positions exhibit high similarity. Building on this observation, CaliDrop performs speculative calibration on the discarded tokens to mitigate the accuracy loss caused by token eviction. Extensive experiments demonstrate that CaliDrop significantly improves the accuracy of existing token eviction methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-04T08:19:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.19906v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.19906v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 Bridging Cache-Friendliness and Concurrency: A Locality-Optimized
  In-Memory B-Skiplist</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yicong Luo, Senhe Hao, Brian Wheatman, Prashant Pandey, Helen Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Skiplists are widely used for in-memory indexing in many key-value stores, such as RocksDB and LevelDB, due to their ease of implementation and simple concurrency control mechanisms. However, traditional skiplists suffer from poor cache locality, as they store only a single element per node, leaving performance on the table. Minimizing last-level cache misses is key to maximizing in-memory index performance, making high cache locality essential. In this paper, we present a practical concurrent B-skiplist that enhances cache locality and performance while preserving the simplicity of traditional skiplist structures and concurrency control schemes. Our key contributions include a top-down, single-pass insertion algorithm for B-skiplists and a corresponding simple and efficient top-down concurrency control scheme. On 128 threads, the proposed concurrent B-skiplist achieves between 2x-9x higher throughput compared to state-of-the-art concurrent skiplist implementations, including Facebook's concurrent skiplist from Folly and the Java ConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves competitive (0.9x-1.7x) throughput on point workloads compared to state-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a more complete picture of the performance, we also measure the latency of skiplist and tree-based indices and find that the B-skiplist achieves between 3.5x-103x lower 99% latency compared to other concurrent skiplists and between 0.85x-64x lower 99% latency compared to tree-based indices on point workloads with inserts.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-04T04:48:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3754598.3754655' target='_blank'>doi</a><a href='http://arxiv.org/abs/2507.21492v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.21492v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 Timely Requesting for Time-Critical Content Users in Decentralized
  F-RANs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xingran Chen, Kai Li, Kun Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the rising demand for high-rate and timely communications, fog radio access networks (F-RANs) offer a promising solution. This work investigates age of information (AoI) performance in F-RANs, consisting of multiple content users (CUs), enhanced remote radio heads (eRRHs), and content providers (CPs). Time-critical CUs need rapid content updates from CPs but cannot communicate directly with them; instead, eRRHs act as intermediaries. CUs decide whether to request content from a CP and which eRRH to send the request to, while eRRHs decide whether to command CPs to update content or use cached content. We study two general classes of policies: (i) oblivious policies, where decision-making is independent of historical information, and (ii) non-oblivious policies, where decisions are influenced by historical information. First, we obtain closed-form expressions for the average AoI of eRRHs under both policy types. Due to the complexity of calculating closed-form expressions for CUs, we then derive general upper bounds for their average AoI. Next, we identify optimal policies for both types. Under both optimal policies, each CU requests content from each CP at an equal rate, consolidating all requests to a single eRRH when demand is low or resources are limited, and distributing requests evenly among eRRHs when demand is high and resources are ample. eRRHs command content from each CP at an equal rate under an optimal oblivious policy, while prioritize the CP with the highest age under an optimal non-oblivious policy. Our numerical results validate these theoretical findings.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-04T02:47:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.02930v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.02930v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 Compressing KV Cache for Long-Context LLM Inference with Inter-Layer
  Attention Similarity</h2>
                <div class="authors">
                    <strong>Authors:</strong> Da Ma, Lu Chen, Situo Zhang, Yuxun Miao, Su Zhu, Zhi Chen, Hongshen Xu, Hanqi Li, Shuai Fan, Lei Pan, Kai Yu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rapid expansion of context window sizes in Large Language Models~(LLMs) has enabled them to tackle increasingly complex tasks involving lengthy documents. However, this progress comes at the cost of a substantial increase in memory usage during inference, primarily due to the linear growth of the key-value~(KV) cache. Existing KV cache compression methods often discard less relevant tokens, which can lead to significant performance degradation when critical information is lost. In this paper, we propose \textsc{PoD}~(Proximal tokens over Distant tokens), a novel KV cache compression framework that allocates memory according to token importance, retaining less important tokens in a more compact, shared form rather than discarding them entirely. Our approach is motivated by two key observations: (1) proximal tokens -- those at the beginning and end of the context -- are significantly more important for next-token prediction, and (2) attention scores for distant tokens are highly redundant across consecutive layers. Leveraging these insights, \textsc{PoD} preserves the full KV cache for proximal tokens, while for distant tokens, it shares key states across layers. Since attention scores are determined by both queries and keys, sharing key states enables multiple layers to reuse a single set of keys for distant tokens, substantially reducing KV cache memory without discarding essential context. We further introduce a lightweight post-training adaptation to enable the model to adjust to this new attention-sharing structure. Extensive experiments on both synthetic~(Needle in a Haystack) and real-world long-context benchmarks demonstrate that \textsc{PoD} reduces KV cache memory usage by up to 35\% without compromising performance. Our method is orthogonal to existing token-selection-based techniques and can be combined with them for further KV cache compression.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-04T02:17:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.02252v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.02252v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 Revenue Optimization in Wireless Video Caching Networks: A
  Privacy-Preserving Two-Stage Solution</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yijing Zhang, Md-Ferdous Pervej, Andreas F. Molisch
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Video caching can significantly improve delivery efficiency and enhance quality of video streaming, which constitutes the majority of wireless communication traffic. Due to limited cache size, caching strategies must be designed to adapt to and dynamic user demand in order to maximize system revenue. The system revenue depends on the benefits of delivering the requested videos and costs for (a) transporting the files to the users and (b) cache replacement. Since the cache content at any point in time impacts the replacement costs in the future, demand predictions over multiple cache placement slots become an important prerequisite for efficient cache planning. Motivated by this, we introduce a novel two-stage privacy-preserving solution for revenue optimization in wireless video caching networks. First, we train a Transformer using privacy-preserving federated learning (FL) to predict multi-slot future demands. Given that prediction results are never entirely accurate, especially for longer horizons, we further combine global content popularity with per-user prediction results to estimate the content demand distribution. Then, in the second stage, we leverage these estimation results to find caching strategies that maximize the long-term system revenue. This latter problem takes on the form of a multi-stage knapsack problem, which we then transform to a integer linear program. Our extensive simulation results demonstrate that (i) our FL solution delivers nearly identical performance to that of the ideal centralized solution and outperforms other existing caching methods, and (ii) our novel revenue optimization approach provides deeper system performance insights than traditional cache hit ratio (CHR)-based optimization approaches.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-03T19:16:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.01898v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.01898v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 StreamAgent: Towards Anticipatory Agents for Streaming Video
  Understanding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haolin Yang, Feilong Tang, Linxiao Zhao, Xiang An, Ming Hu, Huifa Li, Xinlin Zhuang, Boqian Wang, Yifan Lu, Xiaofeng Zhang, Abdalla Swikir, Junjun He, Zongyuan Ge, Imran Razzak
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Real-time streaming video understanding in domains such as autonomous driving and intelligent surveillance poses challenges beyond conventional offline video processing, requiring continuous perception, proactive decision making, and responsive interaction based on dynamically evolving visual content. However, existing methods rely on alternating perception-reaction or asynchronous triggers, lacking task-driven planning and future anticipation, which limits their real-time responsiveness and proactive decision making in evolving video streams. To this end, we propose a StreamAgent that anticipates the temporal intervals and spatial regions expected to contain future task-relevant information to enable proactive and goal-driven responses. Specifically, we integrate question semantics and historical observations through prompting the anticipatory agent to anticipate the temporal progression of key events, align current observations with the expected future evidence, and subsequently adjust the perception action (e.g., attending to task-relevant regions or continuously tracking in subsequent frames). To enable efficient inference, we design a streaming KV-cache memory mechanism that constructs a hierarchical memory structure for selective recall of relevant tokens, enabling efficient semantic retrieval while reducing the overhead of storing all tokens in the traditional KV-cache. Extensive experiments on streaming and long video understanding tasks demonstrate that our method outperforms existing methods in response accuracy and real-time efficiency, highlighting its practical value for real-world streaming scenarios.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-03T18:15:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.01875v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.01875v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte
  Carlo Tree Search</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shuozhi Yuan, Limin Chen, Miaomiao Yuan, Jin Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at translating natural language questions into SQL queries. While recent advances in large language models have greatly improved performance, most existing approaches depend on models with tens of billions of parameters or costly APIs, limiting their applicability in resource-constrained environments. For real world, especially on edge devices, it is crucial for Text-to-SQL to ensure cost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL is of great practical significance. However, smaller LLMs often struggle with complicated user instruction, redundant schema linking or syntax correctness. To address these challenges, we propose MCTS-SQL, a novel framework that uses Monte Carlo Tree Search to guide SQL generation through multi-step refinement. Since the light-weight models' weak performance of single-shot prediction, we generate better results through several trials with feedback. However, directly applying MCTS-based methods inevitably leads to significant time and computational overhead. Driven by this issue, we propose a token-level prefix-cache mechanism that stores prior information during iterations, effectively improved the execution speed. Experiments results on the SPIDER and BIRD benchmarks demonstrate the effectiveness of our approach. Using a small open-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When leveraging a more powerful model Gemini 2.5 to explore the performance upper bound, we achieved results competitive with the SOTA. Our findings demonstrate that even small models can be effectively deployed in practical Text-to-SQL systems with the right strategy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-03T10:27:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.AI</span><span>cs.CL</span><span>cs.PL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.16607v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.16607v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 SmallKV: Small Model Assisted Compensation of KV Cache Compression for
  Efficient LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yi Zhao, Yajuan Peng, Cam-Tu Nguyen, Zuchao Li, Xiaoliang Wang, Hai Zhao, Xiaoming Fu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> KV cache eviction has emerged as an effective solution to alleviate resource constraints faced by LLMs in long-context scenarios. However, existing token-level eviction methods often overlook two critical aspects: (1) their irreversible eviction strategy fails to adapt to dynamic attention patterns during decoding (the saliency shift problem), and (2) they treat both marginally important tokens and truly unimportant tokens equally, despite the collective significance of marginal tokens to model performance (the marginal information over-compression problem). To address these issues, we design two compensation mechanisms based on the high similarity of attention matrices between LLMs of different scales. We propose SmallKV, a small model assisted compensation method for KV cache compression. SmallKV can maintain attention matching between different-scale LLMs to: 1) assist the larger model in perceiving globally important information of attention; and 2) use the smaller model's attention scores to approximate those of marginal tokens in the larger model. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and LongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency evaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than baseline methods, highlighting its potential for efficient and performant LLM inference in resource constrained environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-03T09:15:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.02751v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.02751v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D
  Gaussian Splatting</h2>
                <div class="authors">
                    <strong>Authors:</strong> David Bauer, Qi Wu, Hamid Gadirov, Kwan-Liu Ma
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Real-time path tracing is rapidly becoming the standard for rendering in entertainment and professional applications. In scientific visualization, volume rendering plays a crucial role in helping researchers analyze and interpret complex 3D data. Recently, photorealistic rendering techniques have gained popularity in scientific visualization, yet they face significant challenges. One of the most prominent issues is slow rendering performance and high pixel variance caused by Monte Carlo integration. In this work, we introduce a novel radiance caching approach for path-traced volume rendering. Our method leverages advances in volumetric scene representation and adapts 3D Gaussian splatting to function as a multi-level, path-space radiance cache. This cache is designed to be trainable on the fly, dynamically adapting to changes in scene parameters such as lighting configurations and transfer functions. By incorporating our cache, we achieve less noisy, higher-quality images without increasing rendering costs. To evaluate our approach, we compare it against a baseline path tracer that supports uniform sampling and next-event estimation and the state-of-the-art for neural radiance caching. Through both quantitative and qualitative analyses, we demonstrate that our path-space radiance cache is a robust solution that is easy to integrate and significantly enhances the rendering quality of volumetric visualization applications while maintaining comparable computational efficiency.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-02T23:59:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.GR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.19718v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.19718v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 PESTO: Real-Time Pitch Estimation with Self-supervised
  Transposition-equivariant Objective</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alain Riou, Bernardo Torres, Ben Hayes, Stefan Lattner, Gatan Hadjeres, Gal Richard, Geoffroy Peeters
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this paper, we introduce PESTO, a self-supervised learning approach for single-pitch estimation using a Siamese architecture. Our model processes individual frames of a Variable-$Q$ Transform (VQT) and predicts pitch distributions. The neural network is designed to be equivariant to translations, notably thanks to a Toeplitz fully-connected layer. In addition, we construct pitch-shifted pairs by translating and cropping the VQT frames and train our model with a novel class-based transposition-equivariant objective, eliminating the need for annotated data. Thanks to this architecture and training objective, our model achieves remarkable performances while being very lightweight ($130$k parameters). Evaluations on music and speech datasets (MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms self-supervised baselines but also competes with supervised methods, exhibiting superior cross-dataset generalization. Finally, we enhance PESTO's practical utility by developing a streamable VQT implementation using cached convolutions. Combined with our model's low latency (less than 10 ms) and minimal parameter count, this makes PESTO particularly suitable for real-time applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-02T21:00:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SD</span><span>cs.AI</span><span>cs.LG</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.01488v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.01488v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 Improving performance of content-centric networks via decentralized
  coded caching for multi-level popularity and access</h2>
                <div class="authors">
                    <strong>Authors:</strong> Azadeh Sadat Miraftab, Ahmadreza Montazerolghaem, Behrad Mahboobi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Content-Centric Networking (CCN) offers a novel architectural paradigm that seeks to address the inherent limitations of the prevailing Internet Protocol (IP)-based networking model. In contrast to the host-centric communication approach of IP networks, CCN prioritizes content by enabling direct addressing and routing based on content identifiers. The potential performance improvements of CCN can be further amplified through optimized management of coded data storage and transmission strategies. Decentralized Coded Caching (DCC) emerges as a promising technique that harnesses the collective caching power of distributed network elements. By strategically pre-positioning frequently accessed content closer to potential consumers during periods of low network utilization, DCC has the potential to mitigate content transfer rates during peak traffic periods. This paper proposes a series of fundamental modifications to the CCN architecture by integrating DCC. The proposed framework incorporates differentiated coding strategies tailored to user access privileges, thereby eliminating the overhead associated with queue-based searching. Additionally, the framework facilitates recoding of uncoded data encountered along the content delivery path. These combined methodologies demonstrably enhance network throughput, elevate cache hit ratios, and consequently, reduce content delivery latency compared to conventional CCN implementations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-02T10:12:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1007/s10586-025-05256-6' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.01298v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.01298v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 Unifying Mixture of Experts and Multi-Head Latent Attention for
  Efficient Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sushant Mehta, Raj Dandekar, Rajat Dandekar, Sreedath Panat
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present MoE-MLA-RoPE, a novel architecture combination that combines Mixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary Position Embeddings (RoPE) for efficient language modeling. Our approach addresses the fundamental trade-off between model capacity and computational efficiency through three key innovations: (1) fine-grained expert routing with 64 micro-experts and top-$k$ selection, enabling flexible specialization through 3.6 * 10^7 possible expert combinations; (2) shared expert isolation that dedicates 2 always active experts for common patterns while routing to 6 of 62 specialized experts; and (3) gradient-conflict-free load balancing that maintains expert utilization without interfering with primary loss optimization.   Extensive experiments on models ranging from 17M to 202M parameters demonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV cache memory reduction and 3.2x inference speedup while maintaining competitive perplexity (0.8% degradation). Compared to the parameters with 53.9M parameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla transformers while using 42% fewer active parameters per forward pass. FLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x inference acceleration. Automated evaluation using GPT-4 as a judge confirms quality improvements in generation, with higher scores on coherence (8.1/10), creativity (7.9/10) and grammatical correctness (8.2/10). Our results establish that architectural novelty, not parameter scaling, defines the efficiency frontier for resource-constrained language model deployment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-02T08:33:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.01261v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.01261v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion
  Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuchen Xia, Divyam Sharma, Yichao Yuan, Souvik Kundu, Nishil Talati
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion-based text-to-image generation models trade latency for quality: small models are fast but generate lower-quality images, while large models produce better images but are slow.   We present MoDM, a novel caching-based serving system for diffusion models that dynamically balances latency and quality through a mixture of diffusion models. Unlike prior approaches that rely on model-specific internal features, MoDM caches final images, allowing seamless retrieval and reuse across multiple diffusion model families.   This design enables adaptive serving by dynamically balancing latency and image quality: using smaller models for cache-hit requests to reduce latency while reserving larger models for cache-miss requests to maintain quality. Small model image quality is preserved using retrieved cached images.   We design a global monitor that optimally allocates GPU resources and balances inference workload, ensuring high throughput while meeting service-level objectives under varying request rates. Our evaluations show that MoDM significantly reduces average serving time by 2.5x while retaining image quality, making it a practical solution for scalable and resource-efficient model deployment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-02T06:50:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.11972v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.11972v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of
  Vision-Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xinyu Chen, Haotian Zhai, Can Zhang, Xiupeng Shi, Ruirui Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In zero-shot setting, test-time adaptation adjusts pre-trained models using unlabeled data from the test phase to enhance performance on unknown test distributions. Existing cache-enhanced TTA methods rely on a low-entropy criterion to select samples for prototype construction, assuming intra-class compactness. However, low-entropy samples may be unreliable under distribution shifts, and the resulting prototypes may not ensure compact intra-class distributions. This study identifies a positive correlation between cache-enhanced performance and intra-class compactness. Based on this observation, we propose a Multi-Cache enhanced Prototype-based Test-Time Adaptation (MCP) featuring three caches: an entropy cache for initializing prototype representations with low-entropy samples, an align cache for integrating visual and textual information to achieve compact intra-class distributions, and a negative cache for prediction calibration using high-entropy samples. We further developed MCP++, a framework incorporating cross-modal prototype alignment and residual learning, introducing prototype residual fine-tuning. Comparative and ablation experiments across 15 downstream tasks demonstrate that the proposed method and framework achieve state-of-the-art generalization performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-02T06:43:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.01225v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.01225v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 PiKV: KV Cache Management System for Mixture of Experts</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dong Liu, Yanxuan Yu, Ben Lengerich, Ying Nian Wu, Xuhong Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As large language models continue to scale up in both size and context length, the memory and communication cost of key-value (KV) cache storage has become a major bottleneck in multi-GPU and multi-node inference. While MoE-based architectures sparsify computation across experts, the corresponding KV caches remain dense and globally synchronized, resulting in significant overhead.   We introduce \textbf{PiKV}, a parallel and distributed KV cache serving framework tailored for MoE architecture. PiKV leverages \textit{expert-sharded KV storage} to partition caches across GPUs, \textit{PiKV routing} to reduce token-to-KV access, and a \textit{PiKV Scheduling} to adaptively retain query-relevant entries. To further reduce memory usage, PiKV integrates \textit{PiKV Compression} modules the caching pipeline for acceleration.   PiKV is recently publicly available as an open-source software library: \href{https://github.com/NoakLiu/PiKV}{https://github.com/NoakLiu/PiKV}. Experiments details is recorded at: \href{https://github.com/NoakLiu/PiKV/blob/main/downstream_tasks/README.md}{https://github.com/NoakLiu/PiKV/Experimental\_Results}. We also have PiKV integrated with Nvidia kvpress for acceleration, details see \href{https://github.com/NoakLiu/PiKVpress}{https://github.com/NoakLiu/PiKVpress}. PiKV is still a living project, aiming to become a comprehesive KV Cache management system for MoE Architectures.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-02T03:50:14Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span><span>cs.AI</span><span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.06526v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.06526v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for
  Pre-Ranking Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haoqiang Yang, Congde Yuan, Kun Bai, Mengzhuo Guo, Wei Yang, Chao Zhou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Online display advertising platforms rely on pre-ranking systems to efficiently filter and prioritize candidate ads from large corpora, balancing relevance to users with strict computational constraints. The prevailing two-tower architecture, though highly efficient due to its decoupled design and pre-caching, suffers from cross-domain interaction and coarse similarity metrics, undermining its capacity to model complex user-ad relationships. In this study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT) model, a new architecture that augments the two-tower paradigm with two key components: $\textit{generators}$ that pre-generate holistic vectors incorporating coarse-grained user-ad interactions through a dual-generator framework with a cosine-similarity-based generation loss as the training objective, and $\textit{multi-head representers}$ that project embeddings into multiple latent subspaces to capture fine-grained, multi-faceted user interests and multi-dimensional ad attributes. This design enhances modeling effectiveness without compromising inference efficiency. Extensive experiments on public datasets and large-scale online A/B testing on Tencent's advertising platform demonstrate that HIT significantly outperforms several baselines in relevance metrics, yielding a $1.66\%$ increase in Gross Merchandise Volume and a $1.55\%$ improvement in Return on Investment, alongside similar serving latency to the vanilla two-tower models. The HIT model has been successfully deployed in Tencent's online display advertising system, serving billions of impressions daily. The code is available at https://github.com/HarveyYang123/HIT_model.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-02T00:31:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.19849v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.19849v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 QPP-RNG: A Conceptual Quantum System for True Randomness</h2>
                <div class="authors">
                    <strong>Authors:</strong> Randy Kuang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We propose and experimentally demonstrate the \emph{Quasi-Superposition Quantum-inspired System (QSQS)} -- a conceptual quantum system for randomness generation built on measuring two conjugate observables of a permutation sorting process: the deterministic permutation count $n_p$ and the fundamentally non-deterministic sorting time $t$. By analogy with quantum systems, these observables are linked by an uncertainty-like constraint: algorithmic determinism ensures structural uniformity, while system-level fluctuations introduce irreducible unpredictability. We realize this framework concretely as \emph{QPP-RNG}, a system-embedded, software-based true random number generator (TRNG). In QPP-RNG, real-time measurements of sorting time $t$ -- shaped by CPU pipeline jitter, cache latency, and OS scheduling -- dynamically reseed the PRNG driving the permutation sequence. Crucially, QSQS transforms initially right-skewed raw distributions of $n_p$ and $t$ into nearly uniform outputs after modulo reduction, thanks to internal degeneracies that collapse many distinct states into the same output symbol. Empirical results show that as the repetition factor $m$ increases, output entropy converges toward theoretical maxima: Shannon and min-entropy values approach 8 bits, chi-squared statistics stabilize near ideal uniformity, and bell curves visually confirm the flattening from skewed to uniform distributions. Beyond practical implications, QSQS unifies deterministic algorithmic processes with non-deterministic physical fluctuations, offering a physics-based perspective for engineering true randomness in post-quantum cryptographic systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-01T20:08:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>quant-ph</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.01051v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.01051v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 Study of the HV power supply modules for the CUbesat Solar Polarimeter
  (CUSP)</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alessandro Lacerenza, Alda Rubini, Andrea Alimenti, Sergio Fabiani, Ettore Del Monte, Riccardo Campana, Mauro Centrone, Enrico Costa, Nicolas De Angelis, Giovanni De Cesare, Sergio Di Cosimo, Giuseppe Di Persio, Abhay Kumar, Pasqualino Loffredo, Giovanni Lombardi, Gabriele Minervini, Fabio Muleri, Paolo Romano, Emanuele Scalise, Enrico Silva, Paolo Soffitta, Davide Albanesi, Ilaria Baffo, Daniele Brienza, Valerio Campamaggiore, Giovanni Cucinella, Andrea Curatolo, Giulia de Iulis, Andrea Del Re, Vito Di Bari, Simone Di Filippo, Immacolata Donnarumma, Pierluigi Fanelli, Nicolas Gagliardi, Paolo Leonetti, Matteo Merge, Dario Modenini, Andrea Negri, Daniele Pecorella, Massimo Perelli, Alice Ponti, Francesca Sbop, Paolo Tortora, Alessandro Turchi, Valerio Vagelli, Emanuele Zaccagnino, Alessandro Zambardi, Costantino Zazza
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The CUbesat Solar Polarimeter (CUSP) project is a CubeSat mission orbiting the Earth aimed to measure the linear polarization of solar flares in the hard X-ray band by means of a Compton scattering polarimeter. CUSP will allow to study the magnetic reconnection and particle acceleration in the flaring magnetic structures of our star. CUSP is a project in the framework of the Alcor Program of the Italian Space Agency aimed to develop new CubeSat missions. CUSP undergoing the Phase B started in December 2024 that will last for 12 month. The Compton polarimeter of the CUSP payload performs coincidence measurements between plastic scintilaltors and GaGG(Ce) crystals to derive the polarization of X-rays. These sensors are readout by Multi Anode Photomultiplier Tubes (MAPMTs) and Avalanche Photodiodes (APDs) respectively. Both sensors need an HV power supply up to -1~kV (for the MAPMT) and +500~V (for the APD). We tested precision regulated High Voltage DC/DC Converters by HVM Technology Inc. with Sub-Miniature Case Size ($0.85''\times0.85''\times0.60''$) of the SMHV series. These modules are compact and suited for CubeSat missions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-01T14:05:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.IM</span><span>astro-ph.SR</span><span>physics.space-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.00647v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.00647v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight
  Approach</h2>
                <div class="authors">
                    <strong>Authors:</strong> Francisco Crespo, Javier Villegas, Carlos Baena, Eduardo Baena, Sergio Fortes, Raquel Barco
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The transition toward softwarized Radio Access Networks (RANs), driven by the Open RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through disaggregation and virtualization of base station functions. However, this shift introduces new challenges in managing CPU resources efficiently under strict real-time constraints. In particular, the interplay between latency-sensitive RAN workloads and general-purpose Operating System (OS) schedulers often leads to sub-optimal performance and unnecessary energy consumption. This work proposes a lightweight, programmable distributed application (dApp) deployed at the Distributed Unit (DU) level to dynamically orchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging thread-level telemetry like context switches, Instructions Per Cycle (IPC), and cache metrics, to adapt CPU thread affinity, core isolation, and frequency scaling in real time. Unlike existing solutions, it requires no access to proprietary RAN software, hardware-specific features, or kernel modifications. Fully compliant with the O-RAN architecture and agnostic to the underlying RAN stack, the proposed solution introduces negligible overhead while improving energy efficiency and CPU utilization. Experimental results using a commercial-grade srsRAN deployment demonstrate consistent power savings without compromising real-time processing performance, highlighting the potential of low-latency dApps for fine-grained resource control in next-generation networks
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-01T13:40:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span><span>cs.OS</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.00629v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.00629v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 Joint Association and Phase Shifts Design for UAV-mounted Stacked
  Intelligent Metasurfaces-assisted Communications</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mingzhe Fan, Geng Sun, Hongyang Pan, Jiacheng Wang, Jiancheng An, Hongyang Du, Chau Yuen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Stacked intelligent metasurfaces (SIMs) have emerged as a promising technology for realizing wave-domain signal processing, while the fixed SIMs will limit the communication performance of the system compared to the mobile SIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted communication system, where UAVs as base stations (BSs) can cache the data processed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance the communication performance. To this end, we formulate a UAV-SIM-based joint optimization problem (USBJOP) to comprehensively consider the association between UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of UAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and NP-hardness of USBJOP, we decompose it into three sub-optimization problems, which are the association between UAV-SIMs and users optimization problem (AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase shifts optimization problem (USPSOP). Then, these three sub-optimization problems are solved by an alternating optimization (AO) strategy. Specifically, AUUOP and ULOP are transformed to a convex form and then solved by the CVX tool, while we employ a layer-by-layer iterative optimization method for USPSOP. Simulation results verify the effectiveness of the proposed strategy under different simulation setups.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-01T13:25:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.00616v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.00616v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 Sortblock: Similarity-Aware Feature Reuse for Diffusion Model</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hanqi Chen, Xu Zhang, Xiaoliu Guan, Lielin Jiang, Guanzhong Wang, Zeyu Chen, Yi Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion Transformers (DiTs) have demonstrated remarkable generative capabilities, particularly benefiting from Transformer architectures that enhance visual and artistic fidelity. However, their inherently sequential denoising process results in high inference latency, limiting their deployment in real-time scenarios. Existing training-free acceleration approaches typically reuse intermediate features at fixed timesteps or layers, overlooking the evolving semantic focus across denoising stages and Transformer blocks.To address this, we propose Sortblock, a training-free inference acceleration framework that dynamically caches block-wise features based on their similarity across adjacent timesteps. By ranking the evolution of residuals, Sortblock adaptively determines a recomputation ratio, selectively skipping redundant computations while preserving generation quality. Furthermore, we incorporate a lightweight linear prediction mechanism to reduce accumulated errors in skipped blocks.Extensive experiments across various tasks and DiT architectures demonstrate that Sortblock achieves over 2$\times$ inference speedup with minimal degradation in output quality, offering an effective and generalizable solution for accelerating diffusion-based generative models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-01T08:10:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.00412v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.00412v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units
  with Precision Recovery</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weicheng Xue, Baisong Xu, Kai Yang, Yongxiang Liu, Dengdeng Fan, Pengxiang Xu, Yonghong Tian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Low-precision matrix engines, such as FP16 cube, offer high throughput but lack support for full-precision computation. In this work, we propose SGEMM-cube, a high-performance algorithm for emulating FP32 general matrix-matrix multiplication (GEMM) using only FP16 computation units on a representative AI accelerator. The method decomposes each FP32 operand into two FP16 values and compensates for numerical errors through a tunable scaling strategy. A detailed analysis of numerical errors, including underflow conditions and precision loss, guides the selection of scaling parameters to preserve up to 22 bits of mantissa accuracy. We further investigate the effect of computation order on accuracy and demonstrate that a term-wise accumulation scheme improves numerical stability over conventional FP32 GEMM in low-exponent regimes. Finally, a cache-aware blocking strategy and double-buffered pipeline are introduced to overlap memory transfers with computation, enabling SGEMM-cube to achieve up to 77\% of the theoretical FP32-equivalent peak performance on Ascend 910A NPU lacking native FP32 support. Extensive numerical experiments confirm that our method not only recovers the accuracy of native FP32 GEMM but also exhibits superior numerical stability under certain conditions, due to its structured and error-aware computation order.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-01T03:43:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.23387v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.23387v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 Next Tokens Denoising for Speech Synthesis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yanqing Liu, Ruiqing Xue, Chong Zhang, Yufei Liu, Gang Wang, Bohan Li, Yao Qian, Lei He, Shujie Liu, Sheng Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While diffusion and autoregressive (AR) models have significantly advanced generative modeling, they each present distinct limitations. AR models, which rely on causal attention, cannot exploit future context and suffer from slow generation speeds. Conversely, diffusion models struggle with key-value (KV) caching. To overcome these challenges, we introduce Dragon-FM, a novel text-to-speech (TTS) design that unifies AR and flow-matching. This model processes 48 kHz audio codec tokens in chunks at a compact rate of 12.5 tokens per second. This design enables AR modeling across chunks, ensuring global coherence, while parallel flow-matching within chunks facilitates fast iterative denoising. Thus, the model leverages KV-cache across chunks and utilizes bidirectional context within each chunk. Furthermore, it bridges continuous and discrete feature modeling, demonstrating that continuous AR flow-matching can predict discrete tokens with finite scalar quantizers. This efficient codec and fast chunk-autoregressive architecture also make the model highly effective for generating long-form content, such as podcasts. Experiments on podcast datasets demonstrate its capability to efficiently generate high-quality zero-shot podcasts.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-01T03:37:42Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SD</span><span>cs.CL</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.22746v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.22746v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device
  Speculative Decoding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ramchalam Kinattinkara Ramakrishnan, Zhaocong Yuan, Shaojie Zhuo, Chen Feng, Yicheng Lin, Chenzheng Su, Xiaopeng Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Speculative decoding generally dictates having a small, efficient draft model that is either pretrained or distilled offline to a particular target model series, for instance, Llama or Qwen models. However, within online deployment settings, there are two major challenges: 1) usage of a target model that is incompatible with the draft model; 2) expectation of latency improvements over usage and time. In this work, we propose OmniDraft, a unified framework that enables a single draft model to operate with any target model and adapt dynamically to user data. We introduce an online n-gram cache with hybrid distillation fine-tuning to address the cross-vocabulary mismatch across draft and target models; and further improve decoding speed by leveraging adaptive drafting techniques. OmniDraft is particularly suitable for on-device LLM applications where model cost, efficiency and user customization are the major points of contention. This further highlights the need to tackle the above challenges and motivates the \textit{``one drafter for all''} paradigm. We showcase the proficiency of the OmniDraft framework by performing online learning on math reasoning, coding and text generation tasks. Notably, OmniDraft enables a single Llama-68M model to pair with various target models including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding; and additionally provides up to 1.5-2x speedup.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-31T21:00:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.02659v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.02659v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haoran Zhang, Decheng Zuo, Yu Yan, Zhiyu Liang, Hongzhi Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The co-location of multiple database instances on resource constrained edge nodes creates significant cache contention, where traditional schemes are inefficient and unstable under dynamic workloads. To address this, we present SAM(a Stability-Aware Manager), an autonomic cache manager that establishes decision stability as a first-class design principle. It achieves this through its core control policy, AURA(Autonomic Utility-balancing Resource Allocator), which resolves the classic exploitation-exploration dilemma by synthesizing two orthogonal factors: the H-factor, representing proven historical efficiency (exploitation), and the V-factor, for estimated marginal gain (exploration). Through this practical synthesis and adaptive control, SAM achieves sustained high performance with strategic stability and robustness in volatile conditions.   Extensive experiments against 14 diverse baselines demonstrate SAM's superiority. It achieves top-tier throughput while being uniquely resilient to complex workload shifts and adversarial workloads like cache pollution. Furthermore, its decision latency is highly scalable, remaining nearly constant as the system grows to 120 databases. Crucially, SAM achieves superior decision stability -- maintaining consistent optimization directions despite noise, avoiding performance oscillations while ensuring predictable Quality of Service. These results prove that a principled, stability-aware design is essential for sustained high performance in real-world, large-scale systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-07-31T16:21:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>H.2.4; H.2.7</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.22701v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.22701v2' target='_blank'>pdf</a>
                </div>
            </div>
            <h1>Keyword: LLM Inference</h1>

            <div class="paper">
                <h2>#1 A Dataset for Distilling Knowledge Priors from Literature for
  Therapeutic Design</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haydn Thomas Jones, Natalie Maus, Josh Magnus Ludan, Maggie Ziyu Huan, Jiaming Liang, Marcelo Der Torossian Torres, Jiatao Liang, Zachary Ives, Yoseph Barash, Cesar de la Fuente-Nunez, Jacob R. Gardner, Mark Yatskar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> AI-driven discovery can greatly reduce design time and enhance new therapeutics' effectiveness. Models using simulators explore broad design spaces but risk violating implicit constraints due to a lack of experimental priors. For example, in a new analysis we performed on a diverse set of models on the GuacaMol benchmark using supervised classifiers, over 60\% of molecules proposed had high probability of being mutagenic. In this work, we introduce \ourdataset, a dataset of priors for design problems extracted from literature describing compounds used in lab settings. It is constructed with LLM pipelines for discovering therapeutic entities in relevant paragraphs and summarizing information in concise fair-use facts. \ourdataset~ consists of 32.3 million pairs of natural language facts, and appropriate entity representations (i.e. SMILES or refseq IDs). To demonstrate the potential of the data, we train LLM, CLIP, and LLava architectures to reason jointly about text and design targets and evaluate on tasks from the Therapeutic Data Commons (TDC). \ourdataset~is highly effective for creating models with strong priors: in supervised prediction problems that use our data as pretraining, our best models with 15M learnable parameters outperform larger 2B TxGemma on both regression and classification TDC tasks, and perform comparably to 9B models on average. Models built with \ourdataset~can be used as constraints while optimizing for novel molecules in GuacaMol, resulting in proposals that are safer and nearly as effective. We release our dataset at \href{https://huggingface.co/datasets/medexanon/Medex}{huggingface.co/datasets/medexanon/Medex}, and will provide expanded versions as available literature grows.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:59:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10899v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10899v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 Puppeteer: Rig and Animate Your 3D Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chaoyue Song, Xiu Li, Fan Yang, Zhongcong Xu, Jiacheng Wei, Fayao Liu, Jiashi Feng, Guosheng Lin, Jianfeng Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modern interactive applications increasingly demand dynamic 3D content, yet the transformation of static 3D models into animated assets constitutes a significant bottleneck in content creation pipelines. While recent advances in generative AI have revolutionized static 3D model creation, rigging and animation continue to depend heavily on expert intervention. We present Puppeteer, a comprehensive framework that addresses both automatic rigging and animation for diverse 3D objects. Our system first predicts plausible skeletal structures via an auto-regressive transformer that introduces a joint-based tokenization strategy for compact representation and a hierarchical ordering methodology with stochastic perturbation that enhances bidirectional learning capabilities. It then infers skinning weights via an attention-based architecture incorporating topology-aware joint attention that explicitly encodes inter-joint relationships based on skeletal graph distances. Finally, we complement these rigging advances with a differentiable optimization-based animation pipeline that generates stable, high-fidelity animations while being computationally more efficient than existing approaches. Extensive evaluations across multiple benchmarks demonstrate that our method significantly outperforms state-of-the-art techniques in both skeletal prediction accuracy and skinning quality. The system robustly processes diverse 3D content, ranging from professionally designed game assets to AI-generated shapes, producing temporally coherent animations that eliminate the jittering issues common in existing methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:59:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.GR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10898v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10898v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hongchao Jiang, Yiming Chen, Yushi Cao, Hung-yi Lee, Robby T. Tan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have significantly advanced the state-of-the-art in various coding tasks. Beyond directly answering user queries, LLMs can also serve as judges, assessing and comparing the quality of responses generated by other models. Such an evaluation capability is crucial both for benchmarking different LLMs and for improving response quality through response ranking. However, despite the growing adoption of the LLM-as-a-Judge paradigm, its effectiveness in coding scenarios remains underexplored due to the absence of dedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a benchmark explicitly designed to evaluate the performance of LLM-as-a-Judge models across three critical coding tasks: code generation, code repair, and unit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge models, we find that recent thinking models significantly outperform non-thinking models on our carefully designed code judging tasks. Notably, even relatively small thinking models, such as Qwen3-8B, can outperform specially trained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still exhibit significant randomness in their judgment of coding tasks. For pairwise judging tasks, simply changing the order in which responses are presented can substantially impact accuracy. In addition, when judging code and unit tests written by different LLMs, LLM-as-a-Judge models also show variance in performance. This sensitivity raises concerns about the reliability and consistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal prompting strategies for LLM-as-a-Judge. We find that using pair-wise comparison outperforms scalar point-wise judging. Furthermore, retaining comments and reasoning in the full, unprocessed LLM response leads to improved judge performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:58:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.10535v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.10535v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yushi Lan, Yihang Luo, Fangzhou Hong, Shangchen Zhou, Honghua Chen, Zhaoyang Lyu, Shuai Yang, Bo Dai, Chen Change Loy, Xingang Pan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present STream3R, a novel approach to 3D reconstruction that reformulates pointmap prediction as a decoder-only Transformer problem. Existing state-of-the-art methods for multi-view reconstruction either depend on expensive global optimization or rely on simplistic memory mechanisms that scale poorly with sequence length. In contrast, STream3R introduces an streaming framework that processes image sequences efficiently using causal attention, inspired by advances in modern language modeling. By learning geometric priors from large-scale 3D datasets, STream3R generalizes well to diverse and challenging scenarios, including dynamic scenes where traditional methods often fail. Extensive experiments show that our method consistently outperforms prior work across both static and dynamic scene benchmarks. Moreover, STream3R is inherently compatible with LLM-style training infrastructure, enabling efficient large-scale pretraining and fine-tuning for various downstream 3D tasks. Our results underscore the potential of causal Transformer models for online 3D perception, paving the way for real-time 3D understanding in streaming environments. More details can be found in our project page: https://nirvanalan.github.io/projects/stream3r.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:58:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10893v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10893v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 BiasGym: Fantastic LLM Biases and How to Find (and Remove) Them</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sekh Mainul Islam, Nadav Borenstein, Siddhesh Milind Pawar, Haeun Yu, Arnav Arora, Isabelle Augenstein
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Understanding biases and stereotypes encoded in the weights of Large Language Models (LLMs) is crucial for developing effective mitigation strategies. Biased behaviour is often subtle and non-trivial to isolate, even when deliberately elicited, making systematic analysis and debiasing particularly challenging. To address this, we introduce BiasGym, a simple, cost-effective, and generalizable framework for reliably injecting, analyzing, and mitigating conceptual associations within LLMs. BiasGym consists of two components: BiasInject, which injects specific biases into the model via token-based fine-tuning while keeping the model frozen, and BiasScope, which leverages these injected signals to identify and steer the components responsible for biased behavior. Our method enables consistent bias elicitation for mechanistic analysis, supports targeted debiasing without degrading performance on downstream tasks, and generalizes to biases unseen during token-based fine-tuning. We demonstrate the effectiveness of BiasGym in reducing real-world stereotypes (e.g., people from Italy being `reckless drivers') and in probing fictional associations (e.g., people from a fictional country having `blue skin'), showing its utility for both safety interventions and interpretability research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:57:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.08855v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.08855v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 Searching for Privacy Risks in LLM Agents via Simulation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yanzhe Zhang, Diyi Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The widespread deployment of LLM-based agents is likely to introduce a critical privacy threat: malicious agents that proactively engage others in multi-turn interactions to extract sensitive information. These dynamic dialogues enable adaptive attack strategies that can cause severe privacy violations, yet their evolving nature makes it difficult to anticipate and discover sophisticated vulnerabilities manually. To tackle this problem, we present a search-based framework that alternates between improving attacker and defender instructions by simulating privacy-critical agent interactions. Each simulation involves three roles: data subject, data sender, and data recipient. While the data subject's behavior is fixed, the attacker (data recipient) attempts to extract sensitive information from the defender (data sender) through persistent and interactive exchanges. To explore this interaction space efficiently, our search algorithm employs LLMs as optimizers, using parallel search with multiple threads and cross-thread propagation to analyze simulation trajectories and iteratively propose new instructions. Through this process, we find that attack strategies escalate from simple direct requests to sophisticated multi-turn tactics such as impersonation and consent forgery, while defenses advance from rule-based constraints to identity-verification state machines. The discovered attacks and defenses transfer across diverse scenarios and backbone models, demonstrating strong practical utility for building privacy-aware agents.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:49:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10880v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10880v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 Accelerating cosmological inference of interacting dark energy with
  neural emulators</h2>
                <div class="authors">
                    <strong>Authors:</strong> Karim Carrion
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The present thesis aims to tackle two critical aspects of present and future cosmological analysis of Large-Scale Structure (LSS): accurate modelling of the nonlinear matter power spectrum beyond $\Lambda$CDM, and efficient computational techniques for Bayesian parameter estimation. Both are crucial for testing alternative cosmologies and avoiding spurious results. We focus on the Dark Scattering (DS) model, describing pure momentum transfer between dark matter -- dark energy through the parameter $A_{\rm ds}$. To capture DS effects, we adopt the halo model reaction framework within $\tt{ReACT}$, compute the nonlinear DS spectrum, and validate it against $N$-body simulations. We further include baryonic feedback and massive neutrinos, finding degeneracies between DS and baryonic effects but not with neutrinos. We then constrain DS using cosmic shear from KiDS-1000, accelerated by neural emulators from $\tt{CosmoPower}$, which speed up predictions by $\mathcal{O}(10^4)$. Our DS emulator, trained on halo model reaction outputs, preserves percent-level accuracy and incorporates baryonic feedback. Analysing KiDS shear statistics, we obtain $\vert A_{\rm ds}\vert \lesssim 20$ b/GeV at $68 \%$ C.L. Combining KiDS with Planck CMB and BAO data, we find $A_{\rm ds}=10.6^{+4.5}_{-7.3}$ b/GeV at $68 \%$ C.L., suggesting the DS model as a promising resolution to the $S_8$ tension. Finally, we present weak lensing forecasts for Stage IV surveys using an automatically differentiable pipeline with $\tt{jax-cosmo}$ and gradient-based samplers in $\tt{NumPyro}$, reducing computational cost from months on CPUs to days on GPUs. Model evidence is evaluated with $\tt{harmonic}$ under multiple scale cuts. To put things into perspective, the modelling strategies and machine learning accelerations developed here provide powerful tools for the next generation of LSS cosmology.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:47:41Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.CO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10876v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10876v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 A Survey on Diffusion Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tianyi Li, Mingda Chen, Bowei Guo, Zhiqiang Shen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:47:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10875v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10875v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 SSRL: Self-Search Reinforcement Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuchen Fan, Kaiyan Zhang, Heng Zhou, Yuxin Zuo, Yanxu Chen, Yu Fu, Xinwei Long, Xuekai Zhu, Che Jiang, Yuchen Zhang, Li Kang, Gang Chen, Cheng Huang, Zhizhou He, Bingning Wang, Lei Bai, Ning Ding, Bowen Zhou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We investigate the potential of large language models (LLMs) to serve as efficient simulators for agentic search tasks in reinforcement learning (RL), thereby reducing dependence on costly interactions with external search engines. To this end, we first quantify the intrinsic search capability of LLMs via structured prompting and repeated sampling, which we term Self-Search. Our results reveal that LLMs exhibit strong scaling behavior with respect to the inference budget, achieving high pass@k on question-answering benchmarks, including the challenging BrowseComp task. Building on these observations, we introduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability through format-based and rule-based rewards. SSRL enables models to iteratively refine their knowledge utilization internally, without requiring access to external tools. Empirical evaluations demonstrate that SSRL-trained policy models provide a cost-effective and stable environment for search-driven RL training, reducing reliance on external search engines and facilitating robust sim-to-real transfer. We draw the following conclusions: 1) LLMs possess world knowledge that can be effectively elicited to achieve high performance; 2) SSRL demonstrates the potential of leveraging internal knowledge to reduce hallucination; 3) SSRL-trained models integrate seamlessly with external search engines without additional effort. Our findings highlight the potential of LLMs to support more scalable RL agent training.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:46:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10874v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10874v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 Performance of GPT-5 in Brain Tumor MRI Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mojtaba Safari, Shansong Wang, Mingzhe Hu, Zach Eidex, Qiang Li, Xiaofeng Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Accurate differentiation of brain tumor types on magnetic resonance imaging (MRI) is critical for guiding treatment planning in neuro-oncology. Recent advances in large language models (LLMs) have enabled visual question answering (VQA) approaches that integrate image interpretation with natural language reasoning. In this study, we evaluated GPT-4o, GPT-5-nano, GPT-5-mini, and GPT-5 on a curated brain tumor VQA benchmark derived from 3 Brain Tumor Segmentation (BraTS) datasets - glioblastoma (GLI), meningioma (MEN), and brain metastases (MET). Each case included multi-sequence MRI triplanar mosaics and structured clinical features transformed into standardized VQA items. Models were assessed in a zero-shot chain-of-thought setting for accuracy on both visual and reasoning tasks. Results showed that GPT-5-mini achieved the highest macro-average accuracy (44.19%), followed by GPT-5 (43.71%), GPT-4o (41.49%), and GPT-5-nano (35.85%). Performance varied by tumor subtype, with no single model dominating across all cohorts. These findings suggest that GPT-5 family models can achieve moderate accuracy in structured neuro-oncological VQA tasks, but not at a level acceptable for clinical use.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:35:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10865v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10865v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 DAmodel: Hierarchical Bayesian Modelling of DA White Dwarfs for
  Spectrophotometric Calibration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Benjamin M. Boyd, Gautham Narayan, Kaisey S. Mandel, Matthew Grayling, Abhijit Saha, Tim Axelrod, Thomas Matheson, Edward W. Olszewski, Annalisa Calamida, Aaron Do, Ralph C. Bohlin, Jay B. Holberg, Ivan Hubeny, Susana Deustua, Armin Rest, Christopher W. Stubbs, Aidan Berres, Mai Li, John W. Mackenty, Elena Sabbi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We use hierarchical Bayesian modelling to calibrate a network of 32 all-sky faint DA white dwarf (DA WD) spectrophotometric standards ($16.5 < V < 19.5$) alongside three CALSPEC standards, from 912 \r{A} to 32 $\mu$m. The framework is the first of its kind to jointly infer photometric zeropoints and WD parameters (surface gravity $\log g$, effective temperature $T_{\text{eff}}$, extinction $A_V$, dust relation parameter $R_V$) by simultaneously modelling both photometric and spectroscopic data. We model panchromatic Hubble Space Telescope Wide Field Camera 3 (HST/WFC3) UVIS and IR photometry, HST/STIS UV spectroscopy and ground-based optical spectroscopy to sub-percent precision. Photometric residuals for the sample are the lowest yet yielding $<0.004$ mag RMS on average from the UV to the NIR, achieved by jointly inferring time-dependent changes in system sensitivity and WFC3/IR count-rate nonlinearity. Our GPU-accelerated implementation enables efficient sampling via Hamiltonian Monte Carlo, critical for exploring the high-dimensional posterior space. The hierarchical nature of the model enables population analysis of intrinsic WD and dust parameters. Inferred spectral energy distributions from this model will be essential for calibrating the James Webb Space Telescope as well as next-generation surveys, including Vera Rubin Observatory's Legacy Survey of Space and Time and the Nancy Grace Roman Space Telescope.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:23:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.IM</span><span>astro-ph.CO</span><span>astro-ph.SR</span><span>stat.AP</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1093/mnras/staf629' target='_blank'>doi</a><a href='http://arxiv.org/abs/2412.08809v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.08809v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy,
  Expertise, and Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chongyuan Dai, Jinpeng Hu, Hongchang Shi, Zhuo Li, Xun Yang, Meng Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Amidst a shortage of qualified mental health professionals, the integration of large language models (LLMs) into psychological applications offers a promising way to alleviate the growing burden of mental health disorders. Recent reasoning-augmented LLMs have achieved remarkable performance in mathematics and programming, while research in the psychological domain has predominantly emphasized emotional support and empathetic dialogue, with limited attention to reasoning mechanisms that are beneficial to generating reliable responses. Therefore, in this paper, we propose Psyche-R1, the first Chinese psychological LLM that jointly integrates empathy, psychological expertise, and reasoning, built upon a novel data curation pipeline. Specifically, we design a comprehensive data synthesis pipeline that produces over 75k high-quality psychological questions paired with detailed rationales, generated through chain-of-thought (CoT) reasoning and iterative prompt-rationale optimization, along with 73k empathetic dialogues. Subsequently, we employ a hybrid training strategy wherein challenging samples are identified through a multi-LLM cross-selection strategy for group relative policy optimization (GRPO) to improve reasoning ability, while the remaining data is used for supervised fine-tuning (SFT) to enhance empathetic response generation and psychological domain knowledge. Extensive experiment results demonstrate the effectiveness of the Psyche-R1 across several psychological benchmarks, where our 7B Psyche-R1 achieves comparable results to 671B DeepSeek-R1.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:18:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10848v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10848v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 iFairy: the First 2-bit Complex LLM with All Parameters in $\{\pm1, \pm
  i\}$</h2>
                <div class="authors">
                    <strong>Authors:</strong> Feiyu Wang, Guoan Wang, Yihao Zhang, Shengfan Wang, Weitao Li, Bokai Huang, Shimao Chen, Zihan Jiang, Rui Xu, Tong Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Quantization-Aware Training (QAT) integrates quantization into the training loop, enabling LLMs to learn robust low-bit representations, and is widely recognized as one of the most promising research directions. All current QAT research focuses on minimizing quantization error on full-precision models, where the full-precision accuracy acts as an upper bound (accuracy ceiling). No existing method has even attempted to surpass this ceiling. To break this ceiling, we propose a new paradigm: raising the ceiling (full-precision model), and then still quantizing it efficiently into 2 bits. We propose Fairy$\pm i$, the first 2-bit quantization framework for complex-valued LLMs. Specifically, our method leverages the representational advantages of the complex domain to boost full-precision accuracy. We map weights to the fourth roots of unity $\{\pm1, \pm i\}$, forming a perfectly symmetric and information-theoretically optimal 2-bit representation. Importantly, each quantized weight has either a zero real or imaginary part, enabling multiplication-free inference using only additions and element swaps. Experimental results show that Fairy$\pm i$ outperforms the ceiling of existing 2-bit quantization approaches in terms of both PPL and downstream tasks, while maintaining strict storage and compute efficiency. This work opens a new direction for building highly accurate and practical LLMs under extremely low-bit constraints.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:13:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.05571v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.05571v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 Reinforced Language Models for Sequential Decision Making</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jim Dilkes, Vahid Yazdanpanah, Sebastian Stein
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) show potential as sequential decision-making agents, but their application is often limited due to a reliance on large, computationally expensive models. This creates a need to improve smaller models, yet existing post-training methods are designed for single-turn interactions and cannot handle credit assignment in multi-step agentic tasks. To address this, we introduce Multi-Step Group-Relative Policy Optimization (MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal Text-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP) frameworks. For credit assignment, MS-GRPO attributes the entire cumulative episode reward to each individual episode step. We supplement this algorithm with a novel absolute-advantage-weighted episode sampling strategy that we show improves training performance. We evaluate our approach by post-training a 3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate that the method is effective in improving decision-making performance: our post-trained 3B parameter model outperforms a 72B parameter baseline by 50% on the Frozen Lake task. This work demonstrates that targeted post-training is a practical and efficient alternative to relying on model scale for creating sequential decision-making agents using LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:05:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span><span>I.2.7; I.2.8</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10839v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10839v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 Interpretable Neural ODEs for Gene Regulatory Network Discovery under
  Perturbations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zaikang Lin, Sei Chang, Aaron Zweig, Minseo Kang, Elham Azizi, David A. Knowles
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Modern high-throughput biological datasets with thousands of perturbations provide the opportunity for large-scale discovery of causal graphs that represent the regulatory interactions between genes. Differentiable causal graphical models have been proposed to infer a gene regulatory network (GRN) from large scale interventional datasets, capturing the causal gene regulatory relationships from genetic perturbations. However, existing models are limited in their expressivity and scalability while failing to address the dynamic nature of biological processes such as cellular differentiation. We propose PerturbODE, a novel framework that incorporates biologically informative neural ordinary differential equations (neural ODEs) to model cell state trajectories under perturbations and derive the causal GRN from the neural ODE's parameters. We demonstrate PerturbODE's efficacy in trajectory prediction and GRN inference across simulated and real over-expression datasets.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:04:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CE</span><span>q-bio.MN</span><span>stat.ME</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2501.02409v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2501.02409v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 Beyond "Not Novel Enough": Enriching Scholarly Critique with
  LLM-Assisted Feedback</h2>
                <div class="authors">
                    <strong>Authors:</strong> Osama Mohammed Afzal, Preslav Nakov, Tom Hope, Iryna Gurevych
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Novelty assessment is a central yet understudied aspect of peer review, particularly in high volume fields like NLP where reviewer capacity is increasingly strained. We present a structured approach for automated novelty evaluation that models expert reviewer behavior through three stages: content extraction from submissions, retrieval and synthesis of related work, and structured comparison for evidence based assessment. Our method is informed by a large scale analysis of human written novelty reviews and captures key patterns such as independent claim verification and contextual reasoning. Evaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty assessments, the approach achieves 86.5% alignment with human reasoning and 75.3% agreement on novelty conclusions - substantially outperforming existing LLM based baselines. The method produces detailed, literature aware analyses and improves consistency over ad hoc reviewer judgments. These results highlight the potential for structured LLM assisted approaches to support more rigorous and transparent peer review without displacing human expertise. Data and code are made available.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T16:18:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10795v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10795v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guangda Liu, Chengwei Li, Zhenyu Ning, Minyi Guo, Jieru Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have been widely deployed with rapidly expanding context windows to support increasingly demanding applications. However, long contexts pose significant deployment challenges, primarily due to the KV cache whose size grows proportionally with context length. While KV cache compression methods are proposed to address this issue, KV dropping methods incur considerable accuracy loss, and KV retrieval methods suffer from significant efficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization framework to enhance KV retrieval efficiency while preserving accuracy. On the algorithm side, FreeKV introduces speculative retrieval to shift the KV selection and recall processes out of the critical path, combined with fine-grained correction to ensure accuracy. On the system side, FreeKV employs hybrid KV layouts across CPU and GPU memory to eliminate fragmented data transfers, and leverages double-buffered streamed recall to further improve efficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy across various scenarios and models, delivering up to 13$\times$ speedup compared to SOTA KV retrieval methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T16:12:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.13109v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.13109v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in
  Clinical Natural Language Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mal Jullien, Marco Valentino, Andr Freitas
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models are often assumed to acquire increasingly structured, generalizable internal representations simply by scaling data and parameters. We interrogate this assumption by introducing a Clinical Trial Natural Language Inference benchmark comprising four reasoning families, Causal Attribution, Compositional Grounding, Epistemic Verification, and Risk State Abstraction. Each item is paired with a targeted Ground Knowledge and Meta-Level Reasoning Verification (GKMRV) probe, allowing us to dissociate failures of factual access from failures of inference. We evaluate six contemporary LLMs under both direct and chain of thought prompting.   Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform poorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy, output inferences are highly consistent across samples (mean 0.87), indicating a systematic application of underlying heuristics and shortcuts.   These results reveal fundamental structural and representational limitations: current LLMs often possess the relevant clinical knowledge but lack the structured, composable internal representations needed to deploy it reliably (e.g., integrating constraints, weighing evidence, or simulating counterfactuals). Decoupling knowledge from reasoning with GKMRV makes this dissociation explicit and measurable, providing an effective framework for probing the reliability of LLMs in high-stakes domains.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T16:01:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10777v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10777v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 Video-BLADE: Block-Sparse Attention Meets Step Distillation for
  Efficient Video Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Youping Gu, Xiaolong Li, Yuhao Hu, Bohan Zhuang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion transformers currently lead the field in high-quality video generation, but their slow iterative denoising process and prohibitive quadratic attention costs for long sequences create significant inference bottlenecks. While both step distillation and sparse attention mechanisms have shown promise as independent acceleration strategies, effectively combining these approaches presents critical challenges -- training-free integration yields suboptimal results, while separately training sparse attention after step distillation requires prohibitively expensive high-quality video data. To overcome these limitations, we propose BLADE, an innovative data-free joint training framework that introduces: (1) an Adaptive Block-Sparse Attention (ASA) mechanism for dynamically generating content-aware sparsity masks to focus computation on salient spatiotemporal features, and (2) a sparsity-aware step distillation paradigm built upon Trajectory Distribution Matching (TDM) that directly incorporates sparsity into the distillation process rather than treating it as a separate compression step, with fast convergence. We validate BLADE on text-to-video models like CogVideoX-5B and Wan2.1-1.3B. Our framework demonstrates remarkable efficiency gains across different scales. On Wan2.1-1.3B, BLADE achieves a 14.10x end-to-end inference acceleration over a 50-step baseline. Moreover, on models such as CogVideoX-5B with short video sequence lengths, our framework delivers a robust 8.89x speedup. Crucially, the acceleration is accompanied by a consistent quality improvement. On the VBench-2.0 benchmark, BLADE boosts the score of CogVideoX-5B to 0.569 (from 0.534) and Wan2.1-1.3B to 0.570 (from 0.563), results that are further corroborated by superior ratings in human evaluations. Our code and model weights are publicly available at: http://ziplab.co/BLADE-Homepage/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T15:58:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10774v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10774v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 Modeling Human Responses to Multimodal AI Content</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhiqi Shen, Shaojing Fan, Danni Xu, Terence Sim, Mohan Kankanhalli
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As AI-generated content becomes widespread, so does the risk of misinformation. While prior research has primarily focused on identifying whether content is authentic, much less is known about how such content influences human perception and behavior. In domains like trading or the stock market, predicting how people react (e.g., whether a news post will go viral), can be more critical than verifying its factual accuracy. To address this, we take a human-centered approach and introduce the MhAIM Dataset, which contains 154,552 online posts (111,153 of them AI-generated), enabling large-scale analysis of how people respond to AI-generated content. Our human study reveals that people are better at identifying AI content when posts include both text and visuals, particularly when inconsistencies exist between the two. We propose three new metrics: trustworthiness, impact, and openness, to quantify how users judge and engage with online content. We present T-Lens, an LLM-based agent system designed to answer user queries by incorporating predicted human responses to multimodal information. At its core is HR-MCP (Human Response Model Context Protocol), built on the standardized Model Context Protocol (MCP), enabling seamless integration with any LLM. This integration allows T-Lens to better align with human reactions, enhancing both interpretability and interaction capabilities. Our work provides empirical insights and practical tools to equip LLMs with human-awareness capabilities. By highlighting the complex interplay among AI, human cognition, and information reception, our findings suggest actionable strategies for mitigating the risks of AI-driven misinformation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T15:55:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.MM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10769v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10769v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit
  KV Cache</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dayou Du, Shijie Cao, Jianyi Cheng, Luo Mai, Ting Cao, Mao Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rise of long-context Large Language Models (LLMs) amplifies memory and bandwidth demands during autoregressive decoding, as the Key-Value (KV) cache grows with each generated token. Low-bit KV-cache quantization (e.g., 4-bit or 2-bit) can reduce memory footprint while preserving accuracy, but existing systems suffer from slow decoding due to their exclusive reliance on CUDA cores, neglecting Tensor Cores (the primary source of compute on modern GPUs). We present BitDecoding, a new long-context LLM inference system with a low-bit KV cache. BitDecoding enables efficient low-bit KV-cache decoding by cooperatively leveraging CUDA cores and Tensor Cores. It introduces methods for automatically inducing optimized layouts to exploit Tensor Cores, along with warp-level parallelization strategies for dequantization. For unified system support, BitDecoding includes a query transformation module supporting diverse attention variants, a quantization kernel that supports both tensor-wise and channel-wise scaling used in various quantization algorithms with high performance, and a dequantization kernel with a software-defined pipeline to coordinate CUDA and Tensor Cores execution for mixed-precision operations. Evaluated on RTX 4090, A100, and H100, BitDecoding accelerates decoding by up to 7.5x, 4.8x, and 8.9x, respectively, over FP16 FlashDecoding-v2, and surpasses the state-of-the-art low-bit system QServe by up to 4.3x. On LLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding latency by 3x, showing substantial improvements for long-context generation. The code is available at https://github.com/DD-DuDa/BitDecoding.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T15:37:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>cs.AI</span><span>cs.CL</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.18773v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.18773v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 Stepwise Decomposition and Dual-stream Focus: A Novel Approach for
  Training-free Camouflaged Object Segmentation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chao Yin, Hao Li, Kequan Yang, Jide Li, Pinpin Zhu, Xiaoqiang Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While promptable segmentation (\textit{e.g.}, SAM) has shown promise for various segmentation tasks, it still requires manual visual prompts for each object to be segmented. In contrast, task-generic promptable segmentation aims to reduce the need for such detailed prompts by employing only a task-generic prompt to guide segmentation across all test samples. However, when applied to Camouflaged Object Segmentation (COS), current methods still face two critical issues: 1) \textit{\textbf{semantic ambiguity in getting instance-specific text prompts}}, which arises from insufficient discriminative cues in holistic captions, leading to foreground-background confusion; 2) \textit{\textbf{semantic discrepancy combined with spatial separation in getting instance-specific visual prompts}}, which results from global background sampling far from object boundaries with low feature correlation, causing SAM to segment irrelevant regions. To address the issues above, we propose \textbf{RDVP-MSD}, a novel training-free test-time adaptation framework that synergizes \textbf{R}egion-constrained \textbf{D}ual-stream \textbf{V}isual \textbf{P}rompting (RDVP) via \textbf{M}ultimodal \textbf{S}tepwise \textbf{D}ecomposition Chain of Thought (MSD-CoT). MSD-CoT progressively disentangles image captions to eliminate semantic ambiguity, while RDVP injects spatial constraints into visual prompting and independently samples visual prompts for foreground and background points, effectively mitigating semantic discrepancy and spatial separation. Without requiring any training or supervision, RDVP-MSD achieves a state-of-the-art segmentation result on multiple COS benchmarks and delivers a faster inference speed than previous methods, demonstrating significantly improved accuracy and efficiency. The codes will be available at \href{https://github.com/ycyinchao/RDVP-MSD}{https://github.com/ycyinchao/RDVP-MSD}
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T15:36:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.06818v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.06818v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 Signature of the explosion mechanism of Type-Ia SNe in substructures of
  the remnant: the case of Tycho's SNR</h2>
                <div class="authors">
                    <strong>Authors:</strong> Soham Mandal, Nria Torres-Alb, Carles Badenes, Shazrene Mohamed
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Type-Ia supernovae (SNe), or runaway thermonuclear explosions of white dwarfs (WDs), play a critical role in the chemical evolution of galaxies, and are important cosmological distance indicators due to their 'standardizable' lightcurves. Growing evidence, however, suggests greater diversity in their observed lightcurves (and spectra) than thought previously. This is usually attributed to a variety of WD explosion mechanisms and progenitor system properties, but a direct link between the explosion mechanisms and Type-Ia SN observables remains elusive. Here we present a novel approach to identify explosion mechanisms of Type-Ia SNe, by analyzing the sizes of small-scale turbulent substructures of different elements in their extended ejecta, i.e., in Supernova Remnants (SNRs). Our three-dimensional hydrodynamical models show that substructures in an SNR dominated by iron-group elements may have a typical size different from substructures dominated by intermediate mass elements (e.g., Si, S) in the same SNR. This size difference is governed by the explosion mechanism. Applying this approach to Tycho's SNR, we find that its observed structure is most consistent with the explosion of a sub-Chandrasekhar mass WD via the double-detonation mechanism. Extending this method to other well-characterized SNRs can let us connect the inferred explosion mechanism to the associated historical SNe, which often have spectra reconstructed through light echo observations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T15:35:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.HE</span><span>astro-ph.GA</span><span>astro-ph.SR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10752v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10752v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 Pass@k Training for Adaptively Balancing Exploration and Exploitation of
  Large Reasoning Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhipeng Chen, Xiaobo Qin, Youbin Wu, Yue Ling, Qinghao Ye, Wayne Xin Zhao, Guang Shi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reinforcement learning with verifiable rewards (RLVR), which typically adopts Pass@1 as the reward, has faced the issues in balancing exploration and exploitation, causing policies to prefer conservative actions, converging to a local optimum. Identifying an appropriate reward metric is therefore crucial. Regarding the prior work, although Pass@k has been used in evaluation, its connection to LLM exploration ability in RLVR remains largely overlooked. To investigate this, we first use Pass@k as the reward to train the policy model (i.e., $\textbf{Pass@k Training}$), and observe the improvement on its exploration ability. Next, we derive an analytical solution for the advantage of Pass@k Training, leading to an efficient and effective process. Building on this, our analysis reveals that exploration and exploitation are not inherently conflicting objectives, while they can mutually enhance each other. Moreover, Pass@k Training with analytical derivation essentially involves directly designing the advantage function. Inspired by this, we preliminarily explore the advantage design for RLVR, showing promising results and highlighting a potential future direction.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T15:34:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10751v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10751v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 An Efficient Model-Driven Groupwise Approach for Atlas Construction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ziwei Zou, Bei Zou, Xiaoyan Kui, Wenqi Lu, Haoran Dou, Arezoo Zakeri, Timothy Cootes, Alejandro F Frangi, Jinming Duan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Atlas construction is fundamental to medical image analysis, offering a standardized spatial reference for tasks such as population-level anatomical modeling. While data-driven registration methods have recently shown promise in pairwise settings, their reliance on large training datasets, limited generalizability, and lack of true inference phases in groupwise contexts hinder their practical use. In contrast, model-driven methods offer training-free, theoretically grounded, and data-efficient alternatives, though they often face scalability and optimization challenges when applied to large 3D datasets. In this work, we introduce DARC (Diffeomorphic Atlas Registration via Coordinate descent), a novel model-driven groupwise registration framework for atlas construction. DARC supports a broad range of image dissimilarity metrics and efficiently handles arbitrary numbers of 3D images without incurring GPU memory issues. Through a coordinate descent strategy and a centrality-enforcing activation function, DARC produces unbiased, diffeomorphic atlases with high anatomical fidelity. Beyond atlas construction, we demonstrate two key applications: (1) One-shot segmentation, where labels annotated only on the atlas are propagated to subjects via inverse deformations, outperforming state-of-the-art few-shot methods; and (2) shape synthesis, where new anatomical variants are generated by warping the atlas mesh using synthesized diffeomorphic deformation fields. Overall, DARC offers a flexible, generalizable, and resource-efficient framework for atlas construction and applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T15:28:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>math.OC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10743v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10743v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiangqi Jin, Yuxuan Wang, Yifeng Gao, Zichen Wen, Biqing Qi, Dongrui Liu, Linfeng Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Despite large language models (LLMs) have achieved remarkable success, their prefix-only prompting paradigm and sequential generation process offer limited flexibility for bidirectional information. Diffusion large language models (dLLMs) present new opportunities through their bidirectional attention mechanisms and iterative refinement processes, enabling more flexible in-place prompting strategies. We introduce ICE (In-Place Chain-of-Thought Prompting with Early Exit), a novel framework that transforms prefix-only prompting into in-place prompting specifically designed for dLLMs. ICE integrates in-place prompts directly within masked token positions during iterative refinement and employs a confidence-aware early exit mechanism to significantly reduce computational overhead. Extensive experiments demonstrate ICE's effectiveness, achieving up to 17.29% accuracy improvement with 4.12$\times$ speedup on GSM8K, and up to 276.67$\times$ acceleration on MMLU while maintaining competitive performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T15:16:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10736v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10736v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 Partial identification via conditional linear programs: estimation and
  policy learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Eli Ben-Michael
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Many important quantities of interest are only partially identified from observable data: the data can limit them to a set of plausible values, but not uniquely determine them. This paper develops a unified framework for covariate-assisted estimation, inference, and decision making in partial identification problems where the parameter of interest satisfies a series of linear constraints, conditional on covariates. In such settings, bounds on the parameter can be written as expectations of solutions to conditional linear programs that optimize a linear function subject to linear constraints, where both the objective function and the constraints may depend on covariates and need to be estimated from data. Examples include estimands involving the joint distributions of potential outcomes, policy learning with inequality-aware value functions, and instrumental variable settings. We propose two de-biased estimators for bounds defined by conditional linear programs. The first directly solves the conditional linear programs with plugin estimates and uses output from standard LP solvers to de-bias the plugin estimate, avoiding the need for computationally demanding vertex enumeration of all possible solutions for symbolic bounds. The second uses entropic regularization to create smooth approximations to the conditional linear programs, trading a small amount of approximation error for improved estimation and computational efficiency. We establish conditions for asymptotic normality of both estimators, show that both estimators are robust to first-order errors in estimating the conditional constraints and objectives, and construct Wald-type confidence intervals for the partially identified parameters. These results also extend to policy learning problems where the value of a decision policy is only partially identified. We apply our methods to a study on the effects of Medicaid enrollment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T15:13:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>econ.EM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.12215v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.12215v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 Sample-efficient LLM Optimization with Reset Replay</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zichuan Liu, Jinyu Wang, Lei Song, Jiang Bian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in post-training Large Language Models (LLMs), particularly through Reinforcement Learning (RL) and preference optimization methods, are key drivers for enhancing their reasoning capabilities. However, these methods are often plagued by low sample efficiency and a susceptibility to primacy bias, where overfitting to initial experiences degrades policy quality and damages the learning process. To address these challenges, we introduce LLM optimization with Reset Replay (LoRR), a general and powerful plugin designed to enhance sample efficiency in any preference-based optimization framework. LoRR core mechanism enables training at a high replay number, maximizing the utility of each collected data batch. To counteract the risk of overfitting inherent in high-replay training, LoRR incorporates a periodic reset strategy with reusing initial data, which preserves network plasticity. Furthermore, it leverages a hybrid optimization objective, combining supervised fine-tuning (SFT) and preference-based losses to further bolster data exploitation. Our extensive experiments demonstrate that LoRR significantly boosts the performance of various preference optimization methods on both mathematical and general reasoning benchmarks. Notably, an iterative DPO approach augmented with LoRR achieves comparable performance on challenging math tasks, outperforming some complex and computationally intensive RL-based algorithms. These findings highlight that LoRR offers a practical, sample-efficient, and highly effective paradigm for LLM finetuning, unlocking greater performance from limited data.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:59:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.06412v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.06412v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 Lightweight CNNs for Embedded SAR Ship Target Detection and
  Classification</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fabian Kresse, Georgios Pilikos, Mario Azcueta, Nicolas Floury
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Synthetic Aperture Radar (SAR) data enables large-scale surveillance of maritime vessels. However, near-real-time monitoring is currently constrained by the need to downlink all raw data, perform image focusing, and subsequently analyze it on the ground. On-board processing to generate higher-level products could reduce the data volume that needs to be downlinked, alleviating bandwidth constraints and minimizing latency. However, traditional image focusing and processing algorithms face challenges due to the satellite's limited memory, processing power, and computational resources. This work proposes and evaluates neural networks designed for real-time inference on unfocused SAR data acquired in Stripmap and Interferometric Wide (IW) modes captured with Sentinel-1. Our results demonstrate the feasibility of using one of our models for on-board processing and deployment on an FPGA. Additionally, by investigating a binary classification task between ships and windmills, we demonstrate that target classification is possible.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:55:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10712v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10712v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 CountCluster: Training-Free Object Quantity Guidance with
  Cross-Attention Map Clustering for Text-to-Image Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Joohyeon Lee, Jin-Seop Lee, Jee-Hyong Lee
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion-based text-to-image generation models have demonstrated strong performance in terms of image quality and diversity. However, they still struggle to generate images that accurately reflect the number of objects specified in the input prompt. Several approaches have been proposed that rely on either external counting modules for iterative refinement or quantity representations derived from learned tokens or latent features. However, they still have limitations in accurately reflecting the specified number of objects and overlook an important structural characteristic--The number of object instances in the generated image is largely determined in the early timesteps of the denoising process. To correctly reflect the object quantity for image generation, the highly activated regions in the object cross-attention map at the early timesteps should match the input object quantity, while each region should be clearly separated. To address this issue, we propose \textit{CountCluster}, a method that guides the object cross-attention map to be clustered according to the specified object count in the input, without relying on any external tools or additional training. The proposed method partitions the object cross-attention map into $k$ clusters at inference time based on attention scores, defines an ideal distribution in which each cluster is spatially well-separated, and optimizes the latent to align with this target distribution. Our method achieves an average improvement of 18.5\%p in object count accuracy compared to existing methods, and demonstrates superior quantity control performance across a variety of prompts. Code will be released at: https://github.com/JoohyeonL22/CountCluster .
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:53:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10710v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10710v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 GenOM: Ontology Matching with Description Generation and Large Language
  Model</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yiping Song, Jiaoyan Chen, Renate A. Schmidt
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Ontology matching (OM) plays an essential role in enabling semantic interoperability and integration across heterogeneous knowledge sources, particularly in the biomedical domain which contains numerous complex concepts related to diseases and pharmaceuticals. This paper introduces GenOM, a large language model (LLM)-based ontology alignment framework, which enriches the semantic representations of ontology concepts via generating textual definitions, retrieves alignment candidates with an embedding model, and incorporates exact matching-based tools to improve precision. Extensive experiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often achieve competitive performance, surpassing many baselines including traditional OM systems and recent LLM-based methods. Further ablation studies confirm the effectiveness of semantic enrichment and few-shot prompting, highlighting the framework's robustness and adaptability.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:48:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10703v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10703v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 Encoding and inference on separable effects for sustained treatments</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ignacio Gonzalez-Perez, Kerollos Nashat Wanis, Aaron Leor Sarvet, Mats Julius Stensrud
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Sustained treatment strategies are common in many domains, particularly in medicine, where many treatment are delivered repeatedly over time. The effects of adherence to a treatment strategy throughout follow-up are often more relevant to decision-makers than effects of treatment assignment or initiation. Here we consider the separable effect of sustained use of a time-varying treatment. Despite the potential usefulness of this estimand, the theory of separable effects has yet to be extended to settings with sustained treatment strategies. To derive our results, we use an unconventional encoding of time-varying treatment strategies. This allows us to obtain concise formulations of identifying assumptions with better practical properties; for example, they admit frugal graphical representations and formulations of identifying functionals. These functionals are used to motivate doubly robust semiparametrically efficient estimators. The results are applied to the Systolic Blood Pressure Intervention Trial (SPRINT), where we estimate a separable effect of modified blood pressure treatments on the risk of acute kidney injury.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:46:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10702v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10702v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 REFN: A Reinforcement-Learning-From-Network Framework against
  1-day/n-day Exploitations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tianlong Yu, Lihong Liu, Ziyi Zhou, Fudu Xing, Kailong Wang, Yang Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The exploitation of 1 day or n day vulnerabilities poses severe threats to networked devices due to massive deployment scales and delayed patching (average Mean Time To Patch exceeds 60 days). Existing defenses, including host based patching and network based filtering, are inadequate due to limited scalability across diverse devices, compatibility issues especially with embedded or legacy systems, and error prone deployment process (manual patch validation). To address these issues, we introduce REFN (Reinforcement Learning From Network), a novel framework that trains Large Language Models (LLMs) to autonomously generate network filters to prevent 1 day or n day exploitations. REFN ensures scalability by uniquely employs Reinforcement Learning (RL) driven by online network rewards instead of traditional Human Feedback (RLHF). REFN guarantees compatibility via unified deployment on edge security gateways (Amazon Eero). REFN provides robustness via online validation using real network traffic. Crucially, REFN addresses three core challenges in training LLMs for exploit prevention: 1) expanding current LLMs limited vulnerability fixing expertise via Agentic RAG based Knowledge Distillation, 2) bridging current LLMs language to network gaps through an RL From VNF Pipeline that translates language context (vulnerability description) into network enforcement, 3) addressing the LLM hallucination and non determinism via the Online Agentic Validation that penalizes erroneous outputs. Evaluated across 22 families of 1 day or n day exploits, REFN demonstrates effectiveness (21.1 percent higher accuracy than alternatives), efficiency (Mean Time To Patch of 3.65 hours) and scalability (easily scale to 10K devices). REFN serves as an initial step toward training LLMs to rapidly prevent massive scale 1 day or n day exploitations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:45:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10701v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10701v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 Chem3DLLM: 3D Multimodal Large Language Models for Chemistry</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lei Jiang, Shuzhou Sun, Biqing Qi, Yuchen Fu, Xiaohua Xu, Yuqiang Li, Dongzhan Zhou, Tianfan Fu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the real world, a molecule is a 3D geometric structure. Compared to 1D SMILES sequences and 2D molecular graphs, 3D molecules represent the most informative molecular modality. Despite the rapid progress of autoregressive-based language models, they cannot handle the generation of 3D molecular conformation due to several challenges: 1) 3D molecular structures are incompatible with LLMs' discrete token space, 2) integrating heterogeneous inputs like proteins, ligands, and text remains difficult within a unified model, and 3) LLMs lack essential scientific priors, hindering the enforcement of physical and chemical constraints during generation. To tackle these issues, we present Chem3DLLM, a unified protein-conditioned multimodal large language model. Our approach designs a novel reversible text encoding for 3D molecular structures using run-length compression, achieving 3x size reduction while preserving complete structural information. This enables seamless integration of molecular geometry with protein pocket features in a single LLM architecture. We employ reinforcement learning with stability-based rewards to optimize chemical validity and incorporate a lightweight protein embedding projector for end-to-end training. Experimental results on structure-based drug design demonstrate state-of-the-art performance with a Vina score of -7.21, validating our unified multimodal approach for practical drug discovery applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:37:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10696v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10696v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 Learning from Natural Language Feedback for Personalized Question
  Answering</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alireza Salemi, Hamed Zamani
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Personalization is crucial for enhancing both the effectiveness and user satisfaction of language technologies, particularly in information-seeking tasks like question answering. Current approaches for personalizing large language models (LLMs) often rely on retrieval-augmented generation (RAG), followed by reinforcement learning with scalar reward signals to teach models how to use retrieved personal context. We believe that these scalar rewards sometimes provide weak, non-instructive feedback, limiting learning efficiency and personalization quality. We introduce VAC, a novel framework for personalized response generation that replaces scalar rewards with natural language feedback (NLF) that are generated conditioned on the user profiles and the question narratives. NLF serves as a rich and actionable supervision signal, allowing the policy model to iteratively refine its outputs and internalize effective personalization strategies. Training alternates between optimizing the feedback model and fine-tuning the policy model on the improved responses, resulting in a policy model that no longer requires feedback at inference. Evaluation on the LaMP-QA benchmark that consists of three diverse domains demonstrates consistent and significant improvements over the state-of-the-art results. Human evaluations further confirm the superior quality of the generated responses. These results demonstrate that NLF provides more effective signals for optimizing personalized question answering.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:36:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10695v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10695v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 THERMOS: Thermally-Aware Multi-Objective Scheduling of AI Workloads on
  Heterogeneous Multi-Chiplet PIM Architectures</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alish Kanani, Lukas Pfromm, Harsh Sharma, Janardhan Rao Doppa, Partha Pratim Pande, Umit Y. Ogras
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Chiplet-based integration enables large-scale systems that combine diverse technologies, enabling higher yield, lower costs, and scalability, making them well-suited to AI workloads. Processing-in-Memory (PIM) has emerged as a promising solution for AI inference, leveraging technologies such as ReRAM, SRAM, and FeFET, each offering unique advantages and trade-offs. A heterogeneous chiplet-based PIM architecture can harness the complementary strengths of these technologies to enable higher performance and energy efficiency. However, scheduling AI workloads across such a heterogeneous system is challenging due to competing performance objectives, dynamic workload characteristics, and power and thermal constraints. To address this need, we propose THERMOS, a thermally-aware, multi-objective scheduling framework for AI workloads on heterogeneous multi-chiplet PIM architectures. THERMOS trains a single multi-objective reinforcement learning (MORL) policy that is capable of achieving Pareto-optimal execution time, energy, or a balanced objective at runtime, depending on the target preferences. Comprehensive evaluations show that THERMOS achieves up to 89% faster average execution time and 57% lower average energy consumption than baseline AI workload scheduling algorithms with only 0.14% runtime and 0.022% energy overhead.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:35:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10691v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10691v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 Novel View Synthesis using DDIM Inversion</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sehajdeep SIngh, A V Subramanyam
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Synthesizing novel views from a single input image is a challenging task. It requires extrapolating the 3D structure of a scene while inferring details in occluded regions, and maintaining geometric consistency across viewpoints. Many existing methods must fine-tune large diffusion backbones using multiple views or train a diffusion model from scratch, which is extremely expensive. Additionally, they suffer from blurry reconstruction and poor generalization. This gap presents the opportunity to explore an explicit lightweight view translation framework that can directly utilize the high-fidelity generative capabilities of a pretrained diffusion model while reconstructing a scene from a novel view. Given the DDIM-inverted latent of a single input image, we employ a camera pose-conditioned translation U-Net, TUNet, to predict the inverted latent corresponding to the desired target view. However, the image sampled using the predicted latent may result in a blurry reconstruction. To this end, we propose a novel fusion strategy that exploits the inherent noise correlation structure observed in DDIM inversion. The proposed fusion strategy helps preserve the texture and fine-grained details. To synthesize the novel view, we use the fused latent as the initial condition for DDIM sampling, leveraging the generative prior of the pretrained diffusion model. Extensive experiments on MVImgNet demonstrate that our method outperforms existing methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:32:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10688v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10688v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 From Actions to Words: Towards Abstractive-Textual Policy Summarization
  in RL</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sahar Admoni, Assaf Hallak, Yftah Ziser, Omer Ben-Porat, Ofra Amir
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Policies generated by Reinforcement Learning (RL) algorithms are difficult to explain to users, as they emerge from the interaction of complex reward structures and neural network representations. Consequently, analyzing and predicting agent behavior can be challenging, undermining user trust in real-world applications. To facilitate user understanding, current methods for global policy summarization typically rely on videos that demonstrate agent behavior in a subset of world states. However, users can only watch a limited number of demonstrations, constraining their understanding. Moreover, these methods place the burden of interpretation on users by presenting raw behaviors rather than synthesizing them into coherent patterns. To resolve these issues, we introduce SySLLM (Synthesized Summary using Large Language Models), advocating for a new paradigm of abstractive-textual policy explanations. By leveraging Large Language Models (LLMs)-which possess extensive world knowledge and pattern synthesis capabilities-SySLLM generates textual summaries that provide structured and comprehensible explanations of agent policies. SySLLM demonstrates that LLMs can interpret spatio-temporally structured descriptions of state-action trajectories from an RL agent and generate valuable policy insights in a zero-shot setting, without any prior knowledge or fine-tuning. Our evaluation shows that SySLLM captures key insights, such as goal preferences and exploration strategies, that were also identified by human experts. Furthermore, in a large-scale user study (with 200 participants), SySLLM summaries were preferred over demonstration-based summaries (HIGHLIGHTS) by a clear majority (75.5%) of participants.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:31:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.10509v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.10509v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 Advancing Autonomous Incident Response: Leveraging LLMs and Cyber Threat
  Intelligence</h2>
                <div class="authors">
                    <strong>Authors:</strong> Amine Tellache, Abdelaziz Amara Korba, Amdjed Mokhtari, Horea Moldovan, Yacine Ghamri-Doudane
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Effective incident response (IR) is critical for mitigating cyber threats, yet security teams are overwhelmed by alert fatigue, high false-positive rates, and the vast volume of unstructured Cyber Threat Intelligence (CTI) documents. While CTI holds immense potential for enriching security operations, its extensive and fragmented nature makes manual analysis time-consuming and resource-intensive. To bridge this gap, we introduce a novel Retrieval-Augmented Generation (RAG)-based framework that leverages Large Language Models (LLMs) to automate and enhance IR by integrating dynamically retrieved CTI. Our approach introduces a hybrid retrieval mechanism that combines NLP-based similarity searches within a CTI vector database with standardized queries to external CTI platforms, facilitating context-aware enrichment of security alerts. The augmented intelligence is then leveraged by an LLM-powered response generation module, which formulates precise, actionable, and contextually relevant incident mitigation strategies. We propose a dual evaluation paradigm, wherein automated assessment using an auxiliary LLM is systematically cross-validated by cybersecurity experts. Empirical validation on real-world and simulated alerts demonstrates that our approach enhances the accuracy, contextualization, and efficiency of IR, alleviating analyst workload and reducing response latency. This work underscores the potential of LLM-driven CTI fusion in advancing autonomous security operations and establishing a foundation for intelligent, adaptive cybersecurity frameworks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:20:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10677v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10677v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 Knowledge-based Consistency Testing of Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sai Sathiesh Rajan, Ezekiel Soremekun, Sudipta Chattopadhyay
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this work, we systematically expose and measure the inconsistency and knowledge gaps of Large Language Models (LLMs). Specifically, we propose an automated testing framework (called KonTest) which leverages a knowledge graph to construct test cases. KonTest probes and measures the inconsistencies in the LLM's knowledge of the world via a combination of semantically-equivalent queries and test oracles (metamorphic or ontological oracle). KonTest further mitigates knowledge gaps via a weighted LLM model ensemble. Using four state-of-the-art LLMs (Falcon, Gemini, GPT3.5, and Llama2), we show that KonTest generates 19.2% error inducing inputs (1917 errors from 9979 test inputs). It also reveals a 16.5% knowledge gap across all tested LLMs. A mitigation method informed by KonTest's test suite reduces LLM knowledge gap by 32.48%. Our ablation study further shows that GPT3.5 is not suitable for knowledge-based consistency testing because it is only 60%-68% effective in knowledge construction.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:12:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.12830v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.12830v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 AEGISS -- Atomic orbital and Entropy-based Guided Inference for Space
  Selection -- A novel semi-automated active space selection workflow for
  quantum chemistry and quantum computing applications</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fabio Tarocco, Pi A. B. Haase, Fabijan Pavoevi, Vijay Krishna, Leonardo Guidoni, Stefan Knecht, Martina Stella
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The selection of a balanced active space is a critical step in multi-reference quantum chemistry calculations, particularly for systems with strong electron correlation. Likewise, active space selection is a key to unlock the potential of contemporary quantum computing in quantum chemistry. Albeit recent progress, there remains a lack of a unified, robust, and fully automated framework for active space selection that performs reliably across a wide range of molecular systems.   In this work, we present a novel approach inspired by both the AVAS (Atomic Valence Active Space) and AutoCAS methods. Our method unifies orbital entropy analysis with atomic orbital projections to guide the construction of chemically and physically meaningful active spaces. This integrated scheme enables a more consistent and flexible selection of active orbitals while retaining automation and scalability. We validate our approach on a set of molecular systems relevant to photodynamic therapy, in particular a set of Ru(II)-complexes, selected to span increasing levels of electron correlation and structural complexity. These molecules serve as challenging test cases due to the presence of strong static correlation and the need for highly accurate electronic structure descriptions. Our results demonstrate that the method can reliably identify compact, chemically intuitive active spaces that capture the essential physics, making it suitable for both classical and quantum computational frameworks.   Furthermore, we have developed this approach in a package that is intuitive to use for users and can be interfaced with both standard quantum chemistry and quantum computing applications, making it accessible to a broad research community.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:11:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>physics.chem-ph</span><span>cond-mat.str-el</span><span>physics.comp-ph</span><span>quant-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10671v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10671v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for
  Document Question-Answering with Hierarchical Index and Multi-Granularity
  Retrieval</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ziyu Gong, Yihua Huang, Chengcheng Mai
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The multi-modal long-context document question-answering task aims to locate and integrate multi-modal evidences (such as texts, tables, charts, images, and layouts) distributed across multiple pages, for question understanding and answer generation. The existing methods can be categorized into Large Vision-Language Model (LVLM)-based and Retrieval-Augmented Generation (RAG)-based methods. However, the former were susceptible to hallucinations, while the latter struggled for inter-modal disconnection and cross-page fragmentation. To address these challenges, a novel multi-modal RAG model, named MMRAG-DocQA, was proposed, leveraging both textual and visual information across long-range pages to facilitate accurate question answering. A hierarchical indexing method with the integration of flattened in-page chunks and topological cross-page chunks was designed to jointly establish in-page multi-modal associations and long-distance cross-page dependencies. By means of joint similarity evaluation and large language model (LLM)-based re-ranking, a multi-granularity semantic retrieval method, including the page-level parent page retrieval and document-level summary retrieval, was proposed to foster multi-modal evidence connection and long-distance evidence integration and reasoning. Experimental results performed on public datasets, MMLongBench-Doc and LongDocURL, demonstrated the superiority of our MMRAG-DocQA method in understanding and answering modality-rich and multi-page documents.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:11:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MM</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.00579v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.00579v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 Serial Over Parallel: Learning Continual Unification for Multi-Modal
  Visual Object Tracking and Benchmarking</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhangyong Tang, Tianyang Xu, Xuefeng Zhu, Chunyang Cheng, Tao Zhou, Xiaojun Wu, Josef Kittler
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Unifying multiple multi-modal visual object tracking (MMVOT) tasks draws increasing attention due to the complementary nature of different modalities in building robust tracking systems. Existing practices mix all data sensor types in a single training procedure, structuring a parallel paradigm from the data-centric perspective and aiming for a global optimum on the joint distribution of the involved tasks. However, the absence of a unified benchmark where all types of data coexist forces evaluations on separated benchmarks, causing \textit{inconsistency} between training and testing, thus leading to performance \textit{degradation}. To address these issues, this work advances in two aspects: \ding{182} A unified benchmark, coined as UniBench300, is introduced to bridge the inconsistency by incorporating multiple task data, reducing inference passes from three to one and cutting time consumption by 27\%. \ding{183} The unification process is reformulated in a serial format, progressively integrating new tasks. In this way, the performance degradation can be specified as knowledge forgetting of previous tasks, which naturally aligns with the philosophy of continual learning (CL), motivating further exploration of injecting CL into the unification process. Extensive experiments conducted on two baselines and four benchmarks demonstrate the significance of UniBench300 and the superiority of CL in supporting a stable unification process. Moreover, while conducting dedicated analyses, the performance degradation is found to be negatively correlated with network capacity. Additionally, modality discrepancies contribute to varying degradation levels across tasks (RGBT > RGBD > RGBE in MMVOT), offering valuable insights for future multi-modal vision research. Source codes and the proposed benchmark is available at \textit{https://github.com/Zhangyong-Tang/UniBench300}.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T13:54:04Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10655v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10655v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 SemPT: Semantic Prompt Tuning for Vision-Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiao Shi, Yangjun Ou, Zhenzhong Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Visual transfer learning for unseen categories presents an active research topic yet a challenging task, due to the inherent conflict between preserving category-specific representations and acquiring transferable knowledge. Vision-Language Models (VLMs) pre-trained on large amounts of image-text pairs offer a promising solution. However, existing prompt tuning methods rely on sparse category labels or disparate LLM-generated descriptions, which fragment knowledge representation and hinder transferability. To address this limitation, we introduce Semantic Prompt Tuning (SemPT), a novel framework that tackles the generalization challenge by leveraging shared attribute-level knowledge across categories. Specifically, SemPT adopts a two-step prompting strategy to guide LLM in extracting shared visual attributes and generating attribute-level descriptions, capturing transferable semantic cues beyond labels while ensuring coherent structure. Then, visually guided weighting is applied to the embeddings of attribute-level descriptions to reduce noise from irrelevant attributes and enhance the text embeddings. Additionally, image embeddings are jointly aligned with both label and attribute-enhanced text embeddings, balancing discrimination for seen categories and transferability to unseen ones. Considering the availability of category exposure, our inference dynamically selects between standard label embeddings for seen categories and attribute-enhanced embeddings for unseen ones to ensure effective adaptation. Extensive experiments on 15 benchmark datasets demonstrate that SemPT achieves state-of-the-art performance across various settings, including base-to-novel generalization, cross-dataset transfer, cross-domain transfer, and few-shot learning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T13:41:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10645v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10645v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 Mathematical Computation and Reasoning Errors by Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Liang Zhang, Edith Aurora Graf
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are increasingly utilized in AI-driven educational instruction and assessment, particularly within mathematics education. The capability of LLMs to generate accurate answers and detailed solutions for math problem-solving tasks is foundational for ensuring reliable and precise feedback and assessment in math education practices. Our study focuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1, DeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including arithmetic, algebra, and number theory, and identifies step-level reasoning errors within their solutions. Instead of relying on standard benchmarks, we intentionally build math tasks (via item models) that are challenging for LLMs and prone to errors. The accuracy of final answers and the presence of errors in individual solution steps were systematically analyzed and coded. Both single-agent and dual-agent configurations were tested. It is observed that the reasoning-enhanced OpenAI o1 model consistently achieved higher or nearly perfect accuracy across all three math task categories. Analysis of errors revealed that procedural slips were the most frequent and significantly impacted overall performance, while conceptual misunderstandings were less frequent. Deploying dual-agent configurations substantially improved overall performance. These findings offer actionable insights into enhancing LLM performance and underscore effective strategies for integrating LLMs into mathematics education, thereby advancing AI-driven instructional practices and assessment precision.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T13:25:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.09932v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.09932v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 Comment on "Energy-speed relationship of quantum particles challenges
  Bohmian mechanics"</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aurlien Drezet, Dustin Lazarovici, Bernard Michael Nabet
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In their recent paper [Nature 643, 67 (2025)], Sharaglazova et al. report an optical microcavity experiment yielding an "energy-speed relationship" for quantum particles in evanescent states, which they infer from the observed population transfer between two coupled waveguides. The authors argue that their findings challenge the validity of Bohmian particle dynamics because, according to the Bohmian guiding equation, the velocities in the classically forbidden region would be zero. In this note, we explain why this claim is false and the experimental findings are in perfect agreement with Bohmian mechanics. We also clarify why the operationally defined speeds reported in the paper are unrelated to particle velocities in the sense described by Bohmian mechanics. In contrast to other recent replies, our analysis relies solely on the standard Bohmian guidance equation for single particles.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T13:21:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>quant-ph</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.04756v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.04756v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 FuXi-: Towards a Lightweight and Fast Large-Scale Generative
  Recommendation Model</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yufei Ye, Wei Guo, Hao Wang, Hong Zhu, Yuyang Ye, Yong Liu, Huifeng Guo, Ruiming Tang, Defu Lian, Enhong Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Scaling laws for autoregressive generative recommenders reveal potential for larger, more versatile systems but mean greater latency and training costs. To accelerate training and inference, we investigated the recent generative recommendation models HSTU and FuXi-$\alpha$, identifying two efficiency bottlenecks: the indexing operations in relative temporal attention bias and the computation of the query-key attention map. Additionally, we observed that relative attention bias in self-attention mechanisms can also serve as attention maps. Previous works like Synthesizer have shown that alternative forms of attention maps can achieve similar performance, naturally raising the question of whether some attention maps are redundant. Through empirical experiments, we discovered that using the query-key attention map might degrade the model's performance in recommendation tasks. To address these bottlenecks, we propose a new framework applicable to Transformer-like recommendation models. On one hand, we introduce Functional Relative Attention Bias, which avoids the time-consuming operations of the original relative attention bias, thereby accelerating the process. On the other hand, we remove the query-key attention map from the original self-attention layer and design a new Attention-Free Token Mixer module. Furthermore, by applying this framework to FuXi-$\alpha$, we introduce a new model, FuXi-$\beta$. Experiments across multiple datasets demonstrate that FuXi-$\beta$ outperforms previous state-of-the-art models and achieves significant acceleration compared to FuXi-$\alpha$, while also adhering to the scaling law. Notably, FuXi-$\beta$ shows an improvement of 27% to 47% in the NDCG@10 metric on large-scale industrial datasets compared to FuXi-$\alpha$. Our code is available in a public repository: https://github.com/USTC-StarTeam/FuXi-beta
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T13:12:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10615v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10615v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 Exploration of features in the black hole mass spectrum inspired by
  non-parametric analyses of gravitational wave observations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Stefano Rinaldi, Yajie Liang, Gabriele Demasi, Michela Mapelli, Walter Del Pozzo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Current gravitational-wave data reveal structures in the mass function of binary compact objects. Properly modelling and deciphering such structures is the ultimate goal of gravitational-wave population analysis: in this context, non-parametric models are a powerful tool to infer the distribution of black holes from gravitational waves without committing to any specific functional form. Here, we aim to quantitatively corroborate the findings of non-parametric methods with parametrised models incorporating the features found in such analyses. We propose two modifications of the currently favoured PowerLaw+Peak model, inspired by non-parametric studies, and use them to analyse the third Gravitational Wave Transient Catalogue. Our analysis marginally supports the existence of two distinct, differently redshift-evolving subpopulations in the black hole primary mass function, and suggests that, to date, we are still unable to robustly assess the shape of the mass ratio distribution for symmetric ($q>0.7$) binaries.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T13:02:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.HE</span><span>gr-qc</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.05929v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.05929v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 Goal-Oriented Time-Series Forecasting: Foundation Framework Design</h2>
                <div class="authors">
                    <strong>Authors:</strong> Luca-Andrei Fechete, Mohamed Sana, Fadhel Ayed, Nicola Piovesan, Wenjie Li, Antonio De Domenico, Tareq Si Salem
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Conventional time-series forecasting methods typically aim to minimize overall prediction error, without accounting for the varying importance of different forecast ranges in downstream applications. We propose a training methodology that enables forecasting models to adapt their focus to application-specific regions of interest at inference time, without retraining. The approach partitions the prediction space into fine-grained segments during training, which are dynamically reweighted and aggregated to emphasize the target range specified by the application. Unlike prior methods that predefine these ranges, our framework supports flexible, on-demand adjustments. Experiments on standard benchmarks and a newly collected wireless communication dataset demonstrate that our method not only improves forecast accuracy within regions of interest but also yields measurable gains in downstream task performance. These results highlight the potential for closer integration between predictive modeling and decision-making in real-world systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T13:00:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.17493v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.17493v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 NAVER: A Neuro-Symbolic Compositional Automaton for Visual Grounding
  with Explicit Logic Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhixi Cai, Fucai Ke, Simindokht Jahangard, Maria Garcia de la Banda, Reza Haffari, Peter J. Stuckey, Hamid Rezatofighi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Visual Grounding (VG) tasks, such as referring expression detection and segmentation tasks are important for linking visual entities to context, especially in complex reasoning tasks that require detailed query interpretation. This paper explores VG beyond basic perception, highlighting challenges for methods that require reasoning like human cognition. Recent advances in large language methods (LLMs) and Vision-Language methods (VLMs) have improved abilities for visual comprehension, contextual understanding, and reasoning. These methods are mainly split into end-to-end and compositional methods, with the latter offering more flexibility. Compositional approaches that integrate LLMs and foundation models show promising performance but still struggle with complex reasoning with language-based logical representations. To address these limitations, we propose NAVER, a compositional visual grounding method that integrates explicit probabilistic logic reasoning within a finite-state automaton, equipped with a self-correcting mechanism. This design improves robustness and interpretability in inference through explicit logic reasoning. Our results show that NAVER achieves SoTA performance comparing to recent end-to-end and compositional baselines. The code is available at https://github.com/ControlNet/NAVER .
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T12:58:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.00372v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.00372v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 Towards Embodied Agentic AI: Review and Classification of LLM- and
  VLM-Driven Robot Autonomy and Interaction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sahar Salimpour, Lei Fu, Farhad Keramat, Leonardo Militano, Giovanni Toffetti, Harry Edelman, Jorge Pea Queralta
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Foundation models, including large language models (LLMs) and vision-language models (VLMs), have recently enabled novel approaches to robot autonomy and human-robot interfaces. In parallel, vision-language-action models (VLAs) or large behavior models (LBMs) are increasing the dexterity and capabilities of robotic systems. This survey paper focuses on those works advancing towards agentic applications and architectures. This includes initial efforts exploring GPT-style interfaces to tooling, as well as more complex system where AI agents are coordinators, planners, perception actors, or generalist interfaces. Such agentic architectures allow robots to reason over natural language instructions, invoke APIs, plan task sequences, or assist in operations and diagnostics. In addition to peer-reviewed research, due to the fast-evolving nature of the field, we highlight and include community-driven projects, ROS packages, and industrial frameworks that show emerging trends. We propose a taxonomy for classifying model integration approaches and present a comparative analysis of the role that agents play in different solutions in today's literature.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T12:55:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.05294v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.05294v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 Why Report Failed Interactions With Robots?! Towards Vignette-based
  Interaction Quality</h2>
                <div class="authors">
                    <strong>Authors:</strong> Agnes Axelsson, Merle Reimann, Ronald Cumbal, Hannah Pelikan, Divesh Lala
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Although the quality of human-robot interactions has improved with the advent of LLMs, there are still various factors that cause systems to be sub-optimal when compared to human-human interactions. The nature and criticality of failures are often dependent on the context of the interaction and so cannot be generalized across the wide range of scenarios and experiments which have been implemented in HRI research. In this work we propose the use of a technique overlooked in the field of HRI, ethnographic vignettes, to clearly highlight these failures, particularly those that are rarely documented. We describe the methodology behind the process of writing vignettes and create our own based on our personal experiences with failures in HRI systems. We emphasize the strength of vignettes as the ability to communicate failures from a multi-disciplinary perspective, promote transparency about the capabilities of robots, and document unexpected behaviours which would otherwise be omitted from research reports. We encourage the use of vignettes to augment existing interaction evaluation methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-15T16:07:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10603v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10603v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 MSRS: Adaptive Multi-Subspace Representation Steering for Attribute
  Alignment in Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xinyan Jiang, Lin Zhang, Jiayi Zhang, Qingsong Yang, Guimin Hu, Di Wang, Lijie Hu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Activation steering offers a promising approach to controlling the behavior of Large Language Models by directly manipulating their internal activations. However, most existing methods struggle to jointly steer multiple attributes, often resulting in interference and undesirable trade-offs. To address this challenge, we propose Multi-Subspace Representation Steering (MSRS), a novel framework for effective multi-attribute steering via subspace representation fine-tuning. MSRS reduces inter-attribute interference by allocating orthogonal subspaces to each attribute, isolating their influence within the model's representation space. MSRS also incorporates a hybrid subspace composition strategy: it combines attribute-specific subspaces for unique steering directions with a shared subspace for common steering directions. A dynamic weighting function learns to efficiently integrate these components for precise control. During inference, MSRS introduces a token-level steering mechanism that dynamically identifies and intervenes on the most semantically relevant tokens, enabling fine-grained behavioral modulation. Experimental results show that MSRS significantly reduces attribute conflicts, surpasses existing methods across a range of attributes, and generalizes effectively to diverse downstream tasks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T12:40:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10599v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10599v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 Debiasing Multimodal Large Language Models via Penalization of Language
  Priors</h2>
                <div class="authors">
                    <strong>Authors:</strong> YiFan Zhang, Yang Shi, Weichen Yu, Qingsong Wen, Xue Wang, Wenjing Yang, Zhang Zhang, Liang Wang, Rong Jin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the realms of computer vision and natural language processing, Multimodal Large Language Models (MLLMs) have become indispensable tools, proficient in generating textual responses based on visual inputs. Despite their advancements, our investigation reveals a noteworthy bias: the generated content is often driven more by the inherent priors of the underlying Large Language Models (LLMs) than by the input image. Empirical experiments underscore the persistence of this bias, as MLLMs often provide confident answers even in the absence of relevant images or given incongruent visual inputs. To rectify these biases and redirect the model's focus toward visual information, we propose two simple, training-free strategies. First, for tasks such as classification or multi-choice question answering, we introduce a "Post-Hoc Debias" method using an affine calibration step to adjust the output distribution. This approach ensures uniform answer scores when the image is absent, acting as an effective regularization technique to alleviate the influence of LLM priors. For more intricate open-ended generation tasks, we extend this method to "Visual Debias Decoding", which mitigates bias by contrasting token log-probabilities conditioned on a correct image versus a meaningless one. Additionally, our investigation sheds light on the instability of MLLMs across various decoding configurations. Through systematic exploration of different settings, we achieve significant performance improvements--surpassing previously reported results--and raise concerns about the fairness of current evaluation practices. Comprehensive experiments substantiate the effectiveness of our proposed strategies in mitigating biases. These strategies not only prove beneficial in minimizing hallucinations but also contribute to the generation of more helpful and precise illustrations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T12:30:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2403.05262v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2403.05262v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 Technical Report: Facilitating the Adoption of Causal Inference Methods
  Through LLM-Empowered Co-Pilot</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jeroen Berrevoets, Julianna Piskorz, Robert Davis, Harry Amad, Jim Weatherall, Mihaela van der Schaar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Estimating treatment effects (TE) from observational data is a critical yet complex task in many fields, from healthcare and economics to public policy. While recent advances in machine learning and causal inference have produced powerful estimation techniques, their adoption remains limited due to the need for deep expertise in causal assumptions, adjustment strategies, and model selection. In this paper, we introduce CATE-B, an open-source co-pilot system that uses large language models (LLMs) within an agentic framework to guide users through the end-to-end process of treatment effect estimation. CATE-B assists in (i) constructing a structural causal model via causal discovery and LLM-based edge orientation, (ii) identifying robust adjustment sets through a novel Minimal Uncertainty Adjustment Set criterion, and (iii) selecting appropriate regression methods tailored to the causal structure and dataset characteristics. To encourage reproducibility and evaluation, we release a suite of benchmark tasks spanning diverse domains and causal complexities. By combining causal inference with intelligent, interactive assistance, CATE-B lowers the barrier to rigorous causal analysis and lays the foundation for a new class of benchmarks in automated treatment effect estimation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T12:20:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10581v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10581v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alessio Buscemi, Daniele Proverbio, Alessandro Di Stefano, The Anh Han, German Castignani, Pietro Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Letting AI agents interact in multi-agent applications adds a layer of complexity to the interpretability and prediction of AI outcomes, with profound implications for their trustworthy adoption in research and society. Game theory offers powerful models to capture and interpret strategic interaction among agents, but requires the support of reproducible, standardized and user-friendly IT frameworks to enable comparison and interpretation of results. To this end, we present FAIRGAME, a Framework for AI Agents Bias Recognition using Game Theory. We describe its implementation and usage, and we employ it to uncover biased outcomes in popular games among AI agents, depending on the employed Large Language Model (LLM) and used language, as well as on the personality trait or strategic knowledge of the agents. Overall, FAIRGAME allows users to reliably and easily simulate their desired games and scenarios and compare the results across simulation campaigns and with game-theoretic predictions, enabling the systematic discovery of biases, the anticipation of emerging behavior out of strategic interplays, and empowering further research into strategic decision-making using LLM agents.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T12:12:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.14325v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.14325v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 Towards Agentic AI for Multimodal-Guided Video Object Segmentation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tuyen Tran, Thao Minh Le, Truyen Tran
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Referring-based Video Object Segmentation is a multimodal problem that requires producing fine-grained segmentation results guided by external cues. Traditional approaches to this task typically involve training specialized models, which come with high computational complexity and manual annotation effort. Recent advances in vision-language foundation models open a promising direction toward training-free approaches. Several studies have explored leveraging these general-purpose models for fine-grained segmentation, achieving performance comparable to that of fully supervised, task-specific models. However, existing methods rely on fixed pipelines that lack the flexibility needed to adapt to the dynamic nature of the task. To address this limitation, we propose Multi-Modal Agent, a novel agentic system designed to solve this task in a more flexible and adaptive manner. Specifically, our method leverages the reasoning capabilities of large language models (LLMs) to generate dynamic workflows tailored to each input. This adaptive procedure iteratively interacts with a set of specialized tools designed for low-level tasks across different modalities to identify the target object described by the multimodal cues. Our agentic approach demonstrates clear improvements over prior methods on two multimodal-conditioned VOS tasks: RVOS and Ref-AVS.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T12:11:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10572v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10572v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 M-estimation for Gaussian processes with time-inhomogeneous drifts from
  high-frequency data</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yasutaka Shimizu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We propose a contrast-based estimation method for Gaussian processes with time-inhomogeneous drifts, observed under high-frequency sampling. The process is modeled as the sum of a deterministic drift function and a stationary Gaussian component with a parametric kernel. Our method constructs a local contrast function from adjacent increments, which avoids inversion of large covariance matrices and allows for efficient computation. We prove consistency and asymptotic normality of the resulting estimators under general ergodicity conditions. A distinctive feature of our approach is that the drift estimator attains a nonstandard convergence rate, stemming from the direct Riemann integrability of the drift density. This highlights a fundamental difference from standard estimation regimes. Furthermore, when the local contrast fails to identify all parameters in the covariance kernel, moment-based corrections can be incorporated to recover identifiability. The proposed framework is simple, flexible, and particularly well suited for high-frequency inference with time-inhomogeneous structure.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T11:53:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>math.ST</span><span>math.PR</span><span>stat.TH</span><span>62M10, 62F12, 60G15</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.01164v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.01164v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 Curse of High Dimensionality Issue in Transformer for Long-context
  Modeling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shuhai Zhang, Zeng You, Yaofo Chen, Zhiquan Wen, Qianyue Wang, Zhijie Qiu, Yuanqing Li, Mingkui Tan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformer-based large language models (LLMs) excel in natural language processing tasks by capturing long-range dependencies through self-attention mechanisms. However, long-context modeling faces significant computational inefficiencies due to \textit{redundant} attention computations: while attention weights are often \textit{sparse}, all tokens consume \textit{equal} computational resources. In this paper, we reformulate traditional probabilistic sequence modeling as a \textit{supervised learning task}, enabling the separation of relevant and irrelevant tokens and providing a clearer understanding of redundancy. Based on this reformulation, we theoretically analyze attention sparsity, revealing that only a few tokens significantly contribute to predictions. Building on this, we formulate attention optimization as a linear coding problem and propose a \textit{group coding strategy}, theoretically showing its ability to improve robustness against random noise and enhance learning efficiency. Motivated by this, we propose \textit{Dynamic Group Attention} (DGA), which leverages the group coding to explicitly reduce redundancy by aggregating less important tokens during attention computation. Empirical results show that our DGA significantly reduces computational costs while maintaining competitive performance.Code is available at https://github.com/bolixinyu/DynamicGroupAttention.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T11:51:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span><span>stat.ML</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.22107v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.22107v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 eDIF: A European Deep Inference Fabric for Remote Interpretability of
  LLM</h2>
                <div class="authors">
                    <strong>Authors:</strong> Irma Heithoff. Marc Guggenberger, Sandra Kalogiannis, Susanne Mayer, Fabian Maag, Sigurd Schacht, Carsten Lanquillon
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper presents a feasibility study on the deployment of a European Deep Inference Fabric (eDIF), an NDIF-compatible infrastructure designed to support mechanistic interpretability research on large language models. The need for widespread accessibility of LLM interpretability infrastructure in Europe drives this initiative to democratize advanced model analysis capabilities for the research community. The project introduces a GPU-based cluster hosted at Ansbach University of Applied Sciences and interconnected with partner institutions, enabling remote model inspection via the NNsight API. A structured pilot study involving 16 researchers from across Europe evaluated the platform's technical performance, usability, and scientific utility. Users conducted interventions such as activation patching, causal tracing, and representation analysis on models including GPT-2 and DeepSeek-R1-70B. The study revealed a gradual increase in user engagement, stable platform performance throughout, and a positive reception of the remote experimentation capabilities. It also marked the starting point for building a user community around the platform. Identified limitations such as prolonged download durations for activation data as well as intermittent execution interruptions are addressed in the roadmap for future development. This initiative marks a significant step towards widespread accessibility of LLM interpretability infrastructure in Europe and lays the groundwork for broader deployment, expanded tooling, and sustained community collaboration in mechanistic interpretability research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T11:45:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10553v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10553v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 When Language Overrules: Revealing Text Dominance in Multimodal Large
  Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Huyu Wu, Meng Tang, Xinhan Zheng, Haiyun Jiang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across a diverse range of multimodal tasks. However, these models suffer from a core problem known as text dominance: they depend heavily on text for their inference, while underutilizing other modalities. While prior work has acknowledged this phenomenon in vision-language tasks, often attributing it to data biases or model architectures. In this paper, we conduct the first systematic investigation of text dominance across diverse data modalities, including images, videos, audio, time-series, and graphs. To measure this imbalance, we propose two evaluation metrics: the Modality Dominance Index (MDI) and the Attention Efficiency Index (AEI). Our comprehensive analysis reveals that text dominance is both significant and pervasive across all tested modalities. Our in-depth analysis identifies three underlying causes: attention dilution from severe token redundancy in non-textual modalities, the influence of fusion architecture design, and task formulations that implicitly favor textual inputs. Furthermore, we propose a simple token compression method that effectively rebalances model attention. Applying this method to LLaVA-7B, for instance, drastically reduces its MDI from 10.23 to a well-balanced value of 0.86. Our analysis and methodological framework offer a foundation for the development of more equitable and comprehensive multimodal language models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T11:44:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10552v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10552v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 From Data Center IoT Telemetry to Data Analytics Chatbots -- Virtual
  Knowledge Graph is All You Need</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junaid Ahmed Khan, Hiari Pizzini Cavagna, Andrea Proia, Andrea Bartolini
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Industry 5.0 demands IoT systems that support seamless human-machine collaboration, yet current IoT data analysis requires deep domain, deployment, and query expertise. We show that combining Large Language Models (LLMs) with Knowledge Graphs (KGs) enables natural language access to heterogeneous IoT data. Focusing on data center IoT telemetry, we introduce a rule-based Virtual Knowledge Graph (VKG) construction process and an on-premise LLM inference service to create an end-to-end Data Analytics (DA) chatbot. Our system dynamically generates VKGs per query and translates user input into SPARQL, achieving 92.5% accuracy (vs. 25% for LLM-to-NoSQL) while reducing latency by 85% (20.36s to 3.03s) and keeping VKG sizes under 179 MiB. This work demonstrates that VKG-powered LLM interfaces deliver accurate, low-latency, and relationship-aware access to large-scale telemetry, bridging the gap between users and complex IoT systems in Industry 5.0.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T11:40:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.22267v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.22267v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 Dark Star Clusters or Ultra-Faint Dwarf galaxies? Revisiting UMa3/U1</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ali Rostami-Shirazi, Hosein Haghi, Akram Hasani Zonoozi, Pavel Kroupa
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Owing to sparse spectroscopic observations, the classification of faint satellites as either dark matter-dominated dwarf galaxies or self-gravitating star clusters remains unresolved. The recently discovered Ursa Major III/UNIONS 1 (UMa3/U1) object, with its measured velocity dispersion, provides a rare observational anchor in this regime. Despite its cluster-like compactness, its inferred dynamical mass-to-light ratio (M_dyn/L) suggests a dark matter-dominated nature, prompting interpretations of UMa3/U1 as a microgalaxy, though current measurements remain inconclusive. Thousand-level M_dyn/L values are not unique to galaxies; self-gravitating dark star clusters (DSCs) can reach comparable levels via energy injection driven by a centrally segregated black hole subsystem (BHSub), which accelerates the evaporation of luminous stars and leads to a super-virial appearance with elevated velocity dispersion. To assess whether UMa3/U1 is a DSC, we conducted direct N-body simulations and identified a model that successfully reproduces both its compact structure and elevated M_dyn/L, supporting a self-gravitating cluster origin. We find the cluster entered the DSC phase around 4 Gyr ago, with its luminous stars expected to be depleted within the next 1 Gyr, followed by the gradual disruption of the central BHSub over the subsequent Gyr. We broaden our analysis by mapping DSC evolutionary tracks in the size versus total luminosity (L) and M_dyn/L-L spaces, showing that DSCs occupy a region overlapping with faint, ambiguous satellites. In the M_dyn/L-L diagram, DSCs trace a transitional channel bridging globular clusters and dwarf galaxies as they rise from M_dyn/L ~ 2 to 10^4 M_sun/L_sun.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T11:32:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.GA</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.3847/2041-8213/adf320' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.10543v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10543v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 Improving Value-based Process Verifier via Low-Cost Variance Reduction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zetian Sun, Dongfang Li, Baotian Hu, Min Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have achieved remarkable success in a wide range of tasks. However, their reasoning capabilities, particularly in complex domains like mathematics, remain a significant challenge. Value-based process verifiers, which estimate the probability of a partial reasoning chain leading to a correct solution, are a promising approach for improving reasoning. Nevertheless, their effectiveness is often hindered by estimation error in their training annotations, a consequence of the limited number of Monte Carlo (MC) samples feasible due to the high cost of LLM inference. In this paper, we identify that the estimation error primarily arises from high variance rather than bias, and the MC estimator is a Minimum Variance Unbiased Estimator (MVUE). To address the problem, we propose the \textsc{Com}pound \textsc{M}onte \textsc{C}arlo \textsc{S}ampling (ComMCS) method, which constructs an unbiased estimator by linearly combining the MC estimators from the current and subsequent steps. Theoretically, we show that our method leads to a predictable reduction in variance, while maintaining an unbiased estimation without additional LLM inference cost. We also perform empirical experiments on the MATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method. Notably, ComMCS outperforms regression-based optimization method by 2.8 points, the non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32 sampling experiment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T11:22:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10539v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10539v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 Active Automata Learning with Advice</h2>
                <div class="authors">
                    <strong>Authors:</strong> Micha Fica, Jan Otop
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present an extended automata learning framework that combines active automata learning with deductive inference. The learning algorithm asks membership and equivalence queries as in the original framework, but it is also given advice, which is used to infer answers to queries when possible and reduce the burden on the teacher. We consider advice given via string rewriting systems, which specify equivalence of words w.r.t. the target languages. The main motivation for the proposed framework is to reduce the number of queries. We show how to adapt Angluin-style learning algorithms to this framework with low overhead. Finally, we present empirical evaluation of our approach and observe substantial improvement in query complexity.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T11:13:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.FL</span><span>F.4.3; I.2.2</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10535v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10535v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 Mitigating Exponential Mixed Frequency Growth through Frequency
  Selection and Dimensional Separation in Quantum Machine Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Michael Poppel, David Bucher, Maximilian Zorn, Nico Kraus, Jonas Stein, Claudia Linnhoff-Popien
                </div>
                <div class="summary">
                    <strong>Summary:</strong> To leverage the potential computational speedup of quantum computing (QC), research in quantum machine learning (QML) has gained increasing prominence. Angle encoding techniques in QML models have been shown to generate truncated Fourier series, offering asymptotically universal function approximation capabilities. By selecting efficient feature maps (FMs) within quantum circuits, one can leverage the exponential growth of Fourier frequencies for improved approximation. In multi-dimensional settings, additional input dimensions induce further exponential scaling via mixed frequencies. In practice, however, quantum models frequently fail at regression tasks. Through two white-box experiments, we show that such failures can occur even when the relevant frequencies are present, due to an insufficient number of trainable parameters.   In order to mitigate the double-exponential parameter growth resulting from double-exponentially growing frequencies, we propose frequency selection and dimensional separation as techniques to constrain the number of parameters, thereby improving trainability. By restricting the QML model to essential frequencies and permitting mixed frequencies only among feature dimensions with known interdependence, we expand the set of tractable problems on current hardware. We demonstrate the reduced parameter requirements by fitting two white-box functions with known frequency spectrum and dimensional interdependencies that could not be fitted with the default methods. The reduced parameter requirements permit us to perform training on a noisy quantum simulator and to demonstrate inference on real quantum hardware.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T11:10:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>quant-ph</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10533v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10533v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 An Explainable Transformer-based Model for Phishing Email Detection: A
  Large Language Model Approach</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mohammad Amaz Uddin, Md Mahiuddin, Iqbal H. Sarker
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Phishing email is a serious cyber threat that tries to deceive users by sending false emails with the intention of stealing confidential information or causing financial harm. Attackers, often posing as trustworthy entities, exploit technological advancements and sophistication to make detection and prevention of phishing more challenging. Despite extensive academic research, phishing detection remains an ongoing and formidable challenge in the cybersecurity landscape. Large Language Models (LLMs) and Masked Language Models (MLMs) possess immense potential to offer innovative solutions to address long-standing challenges. In this research paper, we present an optimized, fine-tuned transformer-based DistilBERT model designed for the detection of phishing emails. In the detection process, we work with a phishing email dataset and utilize the preprocessing techniques to clean and solve the imbalance class issues. Through our experiments, we found that our model effectively achieves high accuracy, demonstrating its capability to perform well. Finally, we demonstrate our fine-tuned model using Explainable-AI (XAI) techniques such as Local Interpretable Model-Agnostic Explanations (LIME) and Transformer Interpret to explain how our model makes predictions in the context of text classification for phishing emails.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T10:59:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.13871v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.13871v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 Bridging Solidity Evolution Gaps: An LLM-Enhanced Approach for Smart
  Contract Compilation Error Resolution</h2>
                <div class="authors">
                    <strong>Authors:</strong> Likai Ye, Mengliang Li, Dehai Zhao, Jiamou Sun, Xiaoxue Ren
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Solidity, the dominant smart contract language for Ethereum, has rapidly evolved with frequent version updates to enhance security, functionality, and developer experience. However, these continual changes introduce significant challenges, particularly in compilation errors, code migration, and maintenance. Therefore, we conduct an empirical study to investigate the challenges in the Solidity version evolution and reveal that 81.68% of examined contracts encounter errors when compiled across different versions, with 86.92% of compilation errors.   To mitigate these challenges, we conducted a systematic evaluation of large language models (LLMs) for resolving Solidity compilation errors during version migrations. Our empirical analysis across both open-source (LLaMA3, DeepSeek) and closed-source (GPT-4o, GPT-3.5-turbo) LLMs reveals that although these models exhibit error repair capabilities, their effectiveness diminishes significantly for semantic-level issues and shows strong dependency on prompt engineering strategies. This underscores the critical need for domain-specific adaptation in developing reliable LLM-based repair systems for smart contracts.   Building upon these insights, we introduce SMCFIXER, a novel framework that systematically integrates expert knowledge retrieval with LLM-based repair mechanisms for Solidity compilation error resolution. The architecture comprises three core phases: (1) context-aware code slicing that extracts relevant error information; (2) expert knowledge retrieval from official documentation; and (3) iterative patch generation for Solidity migration. Experimental validation across Solidity version migrations demonstrates our approach's statistically significant 24.24% improvement over baseline GPT-4o on real-world datasets, achieving near-perfect 96.97% accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T10:42:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10517v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10517v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 SIFThinker: Spatially-Aware Image Focus for Visual Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhangquan Chen, Ruihui Zhao, Chuwei Luo, Mingze Sun, Xinlei Yu, Yangyang Kang, Ruqi Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Current multimodal large language models (MLLMs) still face significant challenges in complex visual tasks (e.g., spatial understanding, fine-grained perception). Prior methods have tried to incorporate visual reasoning, however, they fail to leverage attention correction with spatial cues to iteratively refine their focus on prompt-relevant regions. In this paper, we introduce SIFThinker, a spatially-aware "think-with-images" framework that mimics human visual perception. Specifically, SIFThinker enables attention correcting and image region focusing by interleaving depth-enhanced bounding boxes and natural language. Our contributions are twofold: First, we introduce a reverse-expansion-forward-inference strategy that facilitates the generation of interleaved image-text chains of thought for process-level supervision, which in turn leads to the construction of the SIF-50K dataset. Besides, we propose GRPO-SIF, a reinforced training paradigm that integrates depth-informed visual grounding into a unified reasoning pipeline, teaching the model to dynamically correct and focus on prompt-relevant regions. Extensive experiments demonstrate that SIFThinker outperforms state-of-the-art methods in spatial understanding and fine-grained visual perception, while maintaining strong general capabilities, highlighting the effectiveness of our method. Code: https://github.com/zhangquanchen/SIFThinker.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T10:34:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>I.2.10</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.06259v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.06259v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 Yan: Foundational Interactive Video Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Deheng Ye, Fangyun Zhou, Jiacheng Lv, Jianqi Ma, Jun Zhang, Junyan Lv, Junyou Li, Minwen Deng, Mingyu Yang, Qiang Fu, Wei Yang, Wenkai Lv, Yangbin Yu, Yewen Wang, Yonghang Guan, Zhihao Hu, Zhongbin Fang, Zhongqian Sun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present Yan, a foundational framework for interactive video generation, covering the entire pipeline from simulation and generation to editing. Specifically, Yan comprises three core modules. AAA-level Simulation: We design a highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based shift-window denoising inference process, achieving real-time 1080P/60FPS interactive simulation. Multi-Modal Generation: We introduce a hierarchical autoregressive caption method that injects game-specific knowledge into open-domain multi-modal video diffusion models (VDMs), then transforming the VDM into a frame-wise, action-controllable, real-time infinite interactive video generator. Notably, when the textual and visual prompts are sourced from different domains, the model demonstrates strong generalization, allowing it to blend and compose the style and mechanics across domains flexibly according to user prompts. Multi-Granularity Editing: We propose a hybrid model that explicitly disentangles interactive mechanics simulation from visual rendering, enabling multi-granularity video content editing during interaction through text. Collectively, Yan offers an integration of these modules, pushing interactive video generation beyond isolated capabilities toward a comprehensive AI-driven interactive creation paradigm, paving the way for the next generation of creative tools, media, and entertainment. The project page is: https://greatx3.github.io/Yan/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T10:26:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.08601v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.08601v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 Accelerating the time-domain LISA response model with central finite
  differences and hybridization techniques</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jorge Valencia, Sascha Husa
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Accurate and efficient modeling of the Laser Interferometer Space Antenna (LISA) response is crucial for gravitational-wave (GW) data analysis. A key computational challenge lies in evaluating time-delay interferometry (TDI) variables, which require projecting GW polarizations onto the LISA arms at different retarded times. Without approximations, the full LISA response is computationally expensive, and traditional approaches, such as the long-wavelength approximation, accelerate the response calculation at the cost of reducing accuracy at high frequencies. In this work, we introduce a novel hybrid time-domain response for LISA that balances computational efficiency and accuracy across the binary's evolution. Our method is applicable to massive black hole binaries and implements a fast low-frequency approximation during the early inspiral$\unicode{x2013}$where most of these binaries spend most of the time in the sensitive frequency band of LISA$\unicode{x2013}$while reserving the computationally intensive full-response calculations for the late inspiral, merger, and ringdown phases. The low-frequency approximation (LFA) is based on Taylor expanding the response quantities around a chosen evaluation time such that time delays correspond to central finite differences. Our hybrid approach supports CPU and GPU implementations, TDI generations 1.5 and 2.0, and flexible time-delay complexity, and has the potential to accelerate parts of the global fit and reduce energy consumption. We also test our LFA and hybrid responses on eccentric binaries, and we perform parameter estimation for a "golden" binary. Additionally, we assess the efficacy of our low-frequency response for "deep alerts" by performing inspiral-only Bayesian inference.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T10:23:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>gr-qc</span><span>astro-ph.IM</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1103/lk4h-lz7y' target='_blank'>doi</a><a href='http://arxiv.org/abs/2505.09600v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.09600v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 A Unified Multi-Agent Framework for Universal Multimodal Understanding
  and Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiulin Li, Ping Huang, Yexin Li, Shuo Chen, Juewen Hu, Ye Tian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Real-world multimodal applications often require any-to-any capabilities, enabling both understanding and generation across modalities including text, image, audio, and video. However, integrating the strengths of autoregressive language models (LLMs) for reasoning and diffusion models for high-fidelity generation remains challenging. Existing approaches rely on rigid pipelines or tightly coupled architectures, limiting flexibility and scalability. We propose MAGUS (Multi-Agent Guided Unified Multimodal System), a modular framework that unifies multimodal understanding and generation via two decoupled phases: Cognition and Deliberation. MAGUS enables symbolic multi-agent collaboration within a shared textual workspace. In the Cognition phase, three role-conditioned multimodal LLM agents - Perceiver, Planner, and Reflector - engage in collaborative dialogue to perform structured understanding and planning. The Deliberation phase incorporates a Growth-Aware Search mechanism that orchestrates LLM-based reasoning and diffusion-based generation in a mutually reinforcing manner. MAGUS supports plug-and-play extensibility, scalable any-to-any modality conversion, and semantic alignment - all without the need for joint training. Experiments across multiple benchmarks, including image, video, and audio generation, as well as cross-modal instruction following, demonstrate that MAGUS outperforms strong baselines and state-of-the-art systems. Notably, on the MME benchmark, MAGUS surpasses the powerful closed-source model GPT-4o.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T09:52:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10494v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10494v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 Reverse Physician-AI Relationship: Full-process Clinical Diagnosis
  Driven by a Large Language Model</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shicheng Xu, Xin Huang, Zihao Wei, Liang Pang, Huawei Shen, Xueqi Cheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Full-process clinical diagnosis in the real world encompasses the entire diagnostic workflow that begins with only an ambiguous chief complaint. While artificial intelligence (AI), particularly large language models (LLMs), is transforming clinical diagnosis, its role remains largely as an assistant to physicians. This AI-assisted working pattern makes AI can only answer specific medical questions at certain parts within the diagnostic process, but lack the ability to drive the entire diagnostic process starting from an ambiguous complaint, which still relies heavily on human physicians. This gap limits AI's ability to fully reduce physicians' workload and enhance diagnostic efficiency. To address this, we propose a paradigm shift that reverses the relationship between physicians and AI: repositioning AI as the primary director, with physicians serving as its assistants. So we present DxDirector-7B, an LLM endowed with advanced deep thinking capabilities, enabling it to drive the full-process diagnosis with minimal physician involvement. Furthermore, DxDirector-7B establishes a robust accountability framework for misdiagnoses, delineating responsibility between AI and human physicians. In evaluations across rare, complex, and real-world cases under full-process diagnosis setting, DxDirector-7B not only achieves significant superior diagnostic accuracy but also substantially reduces physician workload than state-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained analyses across multiple clinical departments and tasks validate its efficacy, with expert evaluations indicating its potential to serve as a viable substitute for medical specialists. These findings mark a new era where AI, traditionally a physicians' assistant, now drives the entire diagnostic process to drastically reduce physicians' workload, indicating an efficient and accurate diagnostic solution.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T09:51:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CE</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10492v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10492v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 SEQ-GPT: LLM-assisted Spatial Query via Example</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ivan Khai Ze Lim, Ningyi Liao, Yiming Yang, Gerald Wei Yong Yip, Siqiang Luo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Contemporary spatial services such as online maps predominantly rely on user queries for location searches. However, the user experience is limited when performing complex tasks, such as searching for a group of locations simultaneously. In this study, we examine the extended scenario known as Spatial Exemplar Query (SEQ), where multiple relevant locations are jointly searched based on user-specified examples. We introduce SEQ-GPT, a spatial query system powered by Large Language Models (LLMs) towards more versatile SEQ search using natural language. The language capabilities of LLMs enable unique interactive operations in the SEQ process, including asking users to clarify query details and dynamically adjusting the search based on user feedback. We also propose a tailored LLM adaptation pipeline that aligns natural language with structured spatial data and queries through dialogue synthesis and multi-model cooperation. SEQ-GPT offers an end-to-end demonstration for broadening spatial search with realistic data and application scenarios.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T09:41:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10486v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10486v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 M2DAO-Talker: Harmonizing Multi-granular Motion Decoupling and
  Alternating Optimization for Talking-head Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kui Jiang, Shiyu Liu, Junjun Jiang, Hongxun Yao, Xiaopeng Fan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Audio-driven talking head generation holds significant potential for film production. While existing 3D methods have advanced motion modeling and content synthesis, they often produce rendering artifacts, such as motion blur, temporal jitter, and local penetration, due to limitations in representing stable, fine-grained motion fields. Through systematic analysis, we reformulate talking head generation into a unified framework comprising three steps: video preprocessing, motion representation, and rendering reconstruction. This framework underpins our proposed M2DAO-Talker, which addresses current limitations via multi-granular motion decoupling and alternating optimization. Specifically, we devise a novel 2D portrait preprocessing pipeline to extract frame-wise deformation control conditions (motion region segmentation masks, and camera parameters) to facilitate motion representation. To ameliorate motion modeling, we elaborate a multi-granular motion decoupling strategy, which independently models non-rigid (oral and facial) and rigid (head) motions for improved reconstruction accuracy. Meanwhile, a motion consistency constraint is developed to ensure head-torso kinematic consistency, thereby mitigating penetration artifacts caused by motion aliasing. In addition, an alternating optimization strategy is designed to iteratively refine facial and oral motion parameters, enabling more realistic video generation. Experiments across multiple datasets show that M2DAO-Talker achieves state-of-the-art performance, with the 2.43 dB PSNR improvement in generation quality and 0.64 gain in user-evaluated video realness versus TalkingGaussian while with 150 FPS inference speed. Our project homepage is https://m2dao-talker.github.io/M2DAO-Talk.github.io.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T09:36:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.08307v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.08307v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 Pinet: Optimizing hard-constrained neural networks with orthogonal
  projection layers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Panagiotis D. Grontas, Antonio Terpin, Efe C. Balta, Raffaello D'Andrea, John Lygeros
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We introduce an output layer for neural networks that ensures satisfaction of convex constraints. Our approach, $\Pi$net, leverages operator splitting for rapid and reliable projections in the forward pass, and the implicit function theorem for backpropagation. We deploy $\Pi$net as a feasible-by-design optimization proxy for parametric constrained optimization problems and obtain modest-accuracy solutions faster than traditional solvers when solving a single problem, and significantly faster for a batch of problems. We surpass state-of-the-art learning approaches in terms of training time, solution quality, and robustness to hyperparameter tuning, while maintaining similar inference times. Finally, we tackle multi-vehicle motion planning with non-convex trajectory preferences and provide $\Pi$net as a GPU-ready package implemented in JAX with effective tuning heuristics.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T09:32:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>math.OC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10480v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10480v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 Semantic IDs for Joint Generative Search and Recommendation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Gustavo Penha, Edoardo D'Amico, Marco De Nadai, Enrico Palumbo, Alexandre Tamborrino, Ali Vardasbi, Max Lefarov, Shawn Lin, Timothy Heath, Francesco Fabbri, Hugues Bouchard
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Generative models powered by Large Language Models (LLMs) are emerging as a unified solution for powering both recommendation and search tasks. A key design choice in these models is how to represent items, traditionally through unique identifiers (IDs) and more recently with Semantic IDs composed of discrete codes, obtained from embeddings. While task-specific embedding models can improve performance for individual tasks, they may not generalize well in a joint setting. In this paper, we explore how to construct Semantic IDs that perform well both in search and recommendation when using a unified model. We compare a range of strategies to construct Semantic IDs, looking into task-specific and cross-tasks approaches, and also whether each task should have its own semantic ID tokens in a joint search and recommendation generative model. Our results show that using a bi-encoder model fine-tuned on both search and recommendation tasks to obtain item embeddings, followed by the construction of a unified Semantic ID space provides an effective trade-off, enabling strong performance in both tasks. We hope these findings spark follow-up work on generalisable, semantically grounded ID schemes and inform the next wave of unified generative recommender architectures.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T09:28:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10478v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10478v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over
  Scholarly Knowledge Graphs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xueli Pan, Victor de Boer, Jacco van Ossenbruggen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Question answering over Scholarly Knowledge Graphs (SKGs) remains a challenging task due to the complexity of scholarly content and the intricate structure of these graphs. Large Language Model (LLM) approaches could be used to translate natural language questions (NLQs) into SPARQL queries; however, these LLM-based approaches struggle with SPARQL query generation due to limited exposure to SKG-specific content and the underlying schema. We identified two main types of errors in the LLM-generated SPARQL queries: (i) structural inconsistencies, such as missing or redundant triples in the queries, and (ii) semantic inaccuracies, where incorrect entities or properties are shown in the queries despite a correct query structure. To address these issues, we propose FIRESPARQL, a modular framework that supports fine-tuned LLMs as a core component, with optional context provided via retrieval-augmented generation (RAG) and a SPARQL query correction layer. We evaluate the framework on the SciQA Benchmark using various configurations (zero-shot, zero-shot with RAG, one-shot, fine-tuning, and fine-tuning with RAG) and compare the performance with baseline and state-of-the-art approaches. We measure query accuracy using BLEU and ROUGE metrics, and query result accuracy using relaxed exact match(RelaxedEM), with respect to the gold standards containing the NLQs, SPARQL queries, and the results of the queries. Experimental results demonstrate that fine-tuning achieves the highest overall performance, reaching 0.90 ROUGE-L for query accuracy and 0.85 RelaxedEM for result accuracy on the test set.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T09:08:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.DL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10467v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10467v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 Boosting Cross-problem Generalization in Diffusion-Based Neural
  Combinatorial Solver via Inference Time Adaptation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haoyu Lei, Kaiwen Zhou, Yinchuan Li, Zhitang Chen, Farzan Farnia
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Diffusion-based Neural Combinatorial Optimization (NCO) has demonstrated effectiveness in solving NP-complete (NPC) problems by learning discrete diffusion models for solution generation, eliminating hand-crafted domain knowledge. Despite their success, existing NCO methods face significant challenges in both cross-scale and cross-problem generalization, and high training costs compared to traditional solvers. While recent studies on diffusion models have introduced training-free guidance approaches that leverage pre-defined guidance functions for conditional generation, such methodologies have not been extensively explored in combinatorial optimization. To bridge this gap, we propose a training-free inference time adaptation framework (DIFU-Ada) that enables both the zero-shot cross-problem transfer and cross-scale generalization capabilities of diffusion-based NCO solvers without requiring additional training. We provide theoretical analysis that helps understanding the cross-problem transfer capability. Our experimental results demonstrate that a diffusion solver, trained exclusively on the Traveling Salesman Problem (TSP), can achieve competitive zero-shot transfer performance across different problem scales on TSP variants, such as Prize Collecting TSP (PCTSP) and the Orienteering Problem (OP), through inference time adaptation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T09:07:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.12188v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.12188v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic
  Parallelism in LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Keyu Chen, Zhifeng Shen, Daohai Yu, Haoqian Wu, Wei Wen, Jianfeng He, Ruizhi Qiao, Xing Sun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The increasing scale and complexity of large language models (LLMs) pose significant inference latency challenges, primarily due to their autoregressive decoding paradigm characterized by the sequential nature of next-token prediction. By re-examining the outputs of autoregressive models, we observed that some segments exhibit parallelizable structures, which we term intrinsic parallelism. Decoding each parallelizable branch simultaneously (i.e. parallel decoding) can significantly improve the overall inference speed of LLMs. In this paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which addresses two core challenges: automated construction of parallelizable data and efficient parallel decoding mechanism. More specifically, we introduce a non-invasive pipeline that automatically extracts and validates parallelizable structures from the responses of autoregressive models. To empower efficient adaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which enables seamless transitions between serial and parallel decoding modes while maintaining a reusable KV cache, maximizing computational efficiency. Extensive evaluations across General Tasks, Retrieval-Augmented Generation, Mathematical Reasoning, demonstrate that ASPD achieves unprecedented performance in both effectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up to 3.19x speedup (1.85x on average) while maintaining response quality within 1% difference compared to autoregressive models, realizing significant acceleration without compromising generation quality. Our framework sets a groundbreaking benchmark for efficient LLM parallel inference, paving the way for its deployment in latency-sensitive applications such as AI-powered customer service bots and answer retrieval engines.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T09:04:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.08895v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.08895v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 X-Node: Self-Explanation is All We Need</h2>
                <div class="authors">
                    <strong>Authors:</strong> Prajit Sengupta, Islem Rekik
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Graph neural networks (GNNs) have achieved state-of-the-art results in computer vision and medical image classification tasks by capturing structural dependencies across data instances. However, their decision-making remains largely opaque, limiting their trustworthiness in high-stakes clinical applications where interpretability is essential. Existing explainability techniques for GNNs are typically post-hoc and global, offering limited insight into individual node decisions or local reasoning. We introduce X-Node, a self-explaining GNN framework in which each node generates its own explanation as part of the prediction process. For every node, we construct a structured context vector encoding interpretable cues such as degree, centrality, clustering, feature saliency, and label agreement within its local topology. A lightweight Reasoner module maps this context into a compact explanation vector, which serves three purposes: (1) reconstructing the node's latent embedding via a decoder to enforce faithfulness, (2) generating a natural language explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3) guiding the GNN itself via a "text-injection" mechanism that feeds explanations back into the message-passing pipeline. We evaluate X-Node on two graph datasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT, and GIN backbones. Our results show that X-Node maintains competitive classification accuracy while producing faithful, per-node explanations. Repository: https://github.com/basiralab/X-Node.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T09:00:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10461v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10461v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 VERCATION: Precise Vulnerable Open-source Software Version
  Identification based on Static Analysis and LLM</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yiran Cheng, Ting Zhang, Lwin Khin Shar, Shouguo Yang, Chaopeng Dong, David Lo, Shichao Lv, Zhiqiang Shi, Limin Sun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Open-source software (OSS) has experienced a surge in popularity, attributed to its collaborative development model and cost-effective nature. However, the adoption of specific software versions in development projects may introduce security risks when these versions bring along vulnerabilities. Current methods of identifying vulnerable versions typically analyze and extract the code features involved in vulnerability patches using static analysis with pre-defined rules. They then use code clone detection to identify the vulnerable versions. These methods are hindered by imprecision due to (1) the exclusion of vulnerability-irrelevant code in the analysis and (2) the inadequacy of code clone detection. This paper presents VERCATION, an approach designed to identify vulnerable versions of OSS written in C/C++. VERCATION combines program slicing with a Large Language Model (LLM) to identify vulnerability-relevant code from vulnerability patches. It then backtracks historical commits to gather previous modifications of identified vulnerability-relevant code. We propose code clone detection based on expanded and normalized ASTs to compare the differences between pre-modification and post-modification code, thereby locating the vulnerability-introducing commit (vic) and enabling the identification of the vulnerable versions between the vulnerability-fixing commit and the vic. We curate a dataset linking 122 OSS vulnerabilities and 1,211 versions to evaluate VERCATION. On this dataset, our approach achieves an F1 score of 93.1%, outperforming current state-of-the-art methods. More importantly, VERCATION detected 202 incorrect vulnerable OSS versions in NVD reports.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T09:00:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07321v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07321v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 Efficient Methods for Accurate Sparse Trajectory Recovery and Map
  Matching</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wei Tian, Jieming Shi, Man Lung Yiu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Real-world trajectories are often sparse with low-sampling rates (i.e., long intervals between consecutive GPS points) and misaligned with road networks, yet many applications demand high-quality data for optimal performance. To improve data quality with sparse trajectories as input, we systematically study two related research problems: trajectory recovery on road network, which aims to infer missing points to recover high-sampling trajectories, and map matching, which aims to map GPS points to road segments to determine underlying routes. In this paper, we present efficient methods TRMMA and MMA for accurate trajectory recovery and map matching, respectively, where MMA serves as the first step of TRMMA. In MMA, we carefully formulate a classification task to map a GPS point from sparse trajectories to a road segment over a small candidate segment set, rather than the entire road network. We develop techniques in MMA to generate effective embeddings that capture the patterns of GPS data, directional information, and road segments, to accurately align sparse trajectories to routes. For trajectory recovery, TRMMA focuses on the segments in the route returned by MMA to infer missing points with position ratios on road segments, producing high-sampling trajectories efficiently by avoiding evaluation of all road segments. Specifically, in TRMMA, we design a dual-transformer encoding process to cohesively capture latent patterns in trajectories and routes, and an effective decoding technique to sequentially predict the position ratios and road segments of missing points. We conduct extensive experiments to compare TRMMA and MMA with numerous existing methods for trajectory recovery and map matching, respectively, on 4 large real-world datasets. TRMMA and MMA consistently achieve the best result quality, often by a significant margin.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T09:00:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DB</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/ICDE65448.2025.00034' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.10460v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10460v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 Building Instruction-Tuning Datasets from Human-Written Instructions
  with Open-Weight Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Youmi Ma, Sakae Mizuki, Kazuki Fujii, Taishi Nakamura, Masanari Ohi, Hinari Shimada, Taihei Shiotani, Koshiro Saito, Koki Maeda, Kakeru Hattori, Takumi Okamoto, Shigeki Ishida, Rio Yokota, Hiroya Takamura, Naoaki Okazaki
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Instruction tuning is crucial for enabling Large Language Models (LLMs) to solve real-world tasks. Prior work has shown the effectiveness of instruction-tuning data synthesized solely from LLMs, raising a fundamental question: Do we still need human-originated signals for instruction tuning? This work answers the question affirmatively: we build state-of-the-art instruction-tuning datasets sourced from human-written instructions, by simply pairing them with LLM-generated responses. LLMs fine-tuned on our datasets consistently outperform those fine-tuned on existing ones. Our data construction approach can be easily adapted to other languages; we build datasets for Japanese and confirm that LLMs tuned with our data reach state-of-the-art performance. Analyses suggest that instruction-tuning in a new language allows LLMs to follow instructions, while the tuned models exhibit a notable lack of culture-specific knowledge in that language. The datasets and fine-tuned models will be publicly available. Our datasets, synthesized with open-weight LLMs, are openly distributed under permissive licenses, allowing for diverse use cases.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T08:59:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.23714v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.23714v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 Multi-Label Plant Species Prediction with Metadata-Enhanced Multi-Head
  Vision Transformers</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hanna Herasimchyk, Robin Labryga, Tomislav Prusina
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present a multi-head vision transformer approach for multi-label plant species prediction in vegetation plot images, addressing the PlantCLEF 2025 challenge. The task involves training models on single-species plant images while testing on multi-species quadrat images, creating a drastic domain shift. Our methodology leverages a pre-trained DINOv2 Vision Transformer Base (ViT-B/14) backbone with multiple classification heads for species, genus, and family prediction, utilizing taxonomic hierarchies. Key contributions include multi-scale tiling to capture plants at different scales, dynamic threshold optimization based on mean prediction length, and ensemble strategies through bagging and Hydra model architectures. The approach incorporates various inference techniques including image cropping to remove non-plant artifacts, top-n filtering for prediction constraints, and logit thresholding strategies. Experiments were conducted on approximately 1.4 million training images covering 7,806 plant species. Results demonstrate strong performance, making our submission 3rd best on the private leaderboard. Our code is available at https://github.com/geranium12/plant-clef-2025/tree/v1.0.0.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T08:56:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.IR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10457v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10457v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 TikZero: Zero-Shot Text-Guided Graphics Program Synthesis</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jonas Belouadi, Eddy Ilg, Margret Keuper, Hideki Tanaka, Masao Utiyama, Raj Dabre, Steffen Eger, Simone Paolo Ponzetto
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Automatically synthesizing figures from text captions is a compelling capability. However, achieving high geometric precision and editability requires representing figures as graphics programs in languages like TikZ, and aligned training data (i.e., graphics programs with captions) remains scarce. Meanwhile, large amounts of unaligned graphics programs and captioned raster images are more readily available. We reconcile these disparate data sources by presenting TikZero, which decouples graphics program generation from text understanding by using image representations as an intermediary bridge. It enables independent training on graphics programs and captioned images and allows for zero-shot text-guided graphics program synthesis during inference. We show that our method substantially outperforms baselines that can only operate with caption-aligned graphics programs. Furthermore, when leveraging caption-aligned graphics programs as a complementary training signal, TikZero matches or exceeds the performance of much larger models, including commercial systems like GPT-4o. Our code, datasets, and select models are publicly available.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T08:52:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.11509v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.11509v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 BKP: An R Package for Beta Kernel Process Modeling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiangyan Zhao, Kunhai Qing, Jin Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present BKP, a user-friendly and extensible R package that implements the Beta Kernel Process (BKP) -- a fully nonparametric and computationally efficient framework for modeling spatially varying binomial probabilities. The BKP model combines localized kernel-weighted likelihoods with conjugate beta priors, resulting in closed-form posterior inference without requiring latent variable augmentation or intensive MCMC sampling. The package supports binary and aggregated binomial responses, allows flexible choices of kernel functions and prior specification, and provides loss-based kernel hyperparameter tuning procedures. In addition, BKP extends naturally to the Dirichlet Kernel Process (DKP) for modeling spatially varying multinomial or compositional data. To our knowledge, this is the first publicly available R package for implementing BKP-based methods. We illustrate the use of BKP through several synthetic and real-world datasets, highlighting its interpretability, accuracy, and scalability. The package aims to facilitate practical application and future methodological development of kernel-based beta modeling in statistics and machine learning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T08:34:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.CO</span><span>stat.ML</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10447v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10447v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On
  Offline Knowledge Base</h2>
                <div class="authors">
                    <strong>Authors:</strong> Song Mao, Lejun Cheng, Pinlong Cai, Guohang Yan, Ding Wang, Botian Shi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have demonstrated remarkable capabilities in various applications. However, their use as writing assistants in specialized domains like finance, medicine, and law is often hampered by a lack of deep domain-specific knowledge and a tendency to hallucinate. Existing solutions, such as Retrieval-Augmented Generation (RAG), can suffer from inconsistency across multiple retrieval steps, while online search-based methods often degrade quality due to unreliable web content. To address these challenges, we introduce DeepWriter, a customizable, multimodal, long-form writing assistant that operates on a curated, offline knowledge base. DeepWriter leverages a novel pipeline that involves task decomposition, outline generation, multimodal retrieval, and section-by-section composition with reflection. By deeply mining information from a structured corpus and incorporating both textual and visual elements, DeepWriter generates coherent, factually grounded, and professional-grade documents. We also propose a hierarchical knowledge representation to enhance retrieval efficiency and accuracy. Our experiments on financial report generation demonstrate that DeepWriter produces high-quality, verifiable articles that surpasses existing baselines in factual accuracy and generated content quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T08:14:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.14189v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.14189v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy
  Optimization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xingyu Wu, Yuchen Yan, Shangke Lyu, Linjuan Wu, Yiwen Qiu, Yongliang Shen, Weiming Lu, Jian Shao, Jun Xiao, Yueting Zhuang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large reasoning models have achieved remarkable performance through extended chain-of-thought sequences, yet this computational freedom leads to excessive token generation even for simple problems. We present Length-Adaptive Policy Optimization (LAPO), a novel framework that transforms reasoning length control from an external constraint into an intrinsic model capability. Unlike existing approaches that impose rigid limits or rely on post-hoc interventions, LAPO enables models to internalize an understanding of appropriate reasoning depth through a two-stage reinforcement learning process. In the first stage, models learn natural reasoning patterns by discovering the statistical distribution of successful solution lengths. The second stage leverages these patterns as meta-cognitive guidance, embedding them directly within the model's reasoning context to ensure inference-time flexibility. Experiments on mathematical reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9% while improving accuracy by 2.3%. Our analysis reveals that models trained with LAPO develop emergent abilities to allocate computational resources based on problem complexity, achieving efficient reasoning without sacrificing quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T08:13:36Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.15758v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.15758v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 Inequality Restricted Minimum Density Power Divergence Estimation in
  Panel Count Data</h2>
                <div class="authors">
                    <strong>Authors:</strong> Udita Goswami, Shuvashree Mondal
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Analysis of panel count data has garnered a considerable amount of attention in the literature, leading to the development of multiple statistical techniques. In inferential analysis, most of the works focus on leveraging estimating equations-based techniques or conventional maximum likelihood estimation. However, the robustness of these methods is largely questionable. In this paper, we present the robust density power divergence estimation for panel count data arising from nonhomogeneous Poisson processes, correlated through a latent frailty variable. In order to cope with real-world incidents, it is often desired to impose certain inequality constraints on the parameter space, giving rise to the restricted minimum density power divergence estimator. The significant contribution of this study lies in deriving its asymptotic properties. The proposed method ensures high efficiency in the model estimation while providing reliable inference despite data contamination. Moreover, the density power divergence measure is governed by a tuning parameter $\gamma$, which controls the trade-off between robustness and efficiency. To effectively determine the optimal value of $\gamma$, this study employs a generalized score-matching technique, marking considerable progress in the data analysis. Simulation studies and real data examples are provided to illustrate the performance of the estimator and to substantiate the theory developed.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T08:12:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ME</span><span>stat.AP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.21534v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.21534v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 SC2Arena and StarEvolve: Benchmark and Self-Improvement Framework for
  LLMs in Complex Decision-Making Tasks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pengbo Shen, Yaqing Wang, Ni Mu, Yao Luan, Runpeng Xie, Senhao Yang, Lexiang Wang, Hao Hu, Shuang Xu, Yiqin Yang, Bo Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Evaluating large language models (LLMs) in complex decision-making is essential for advancing AI's ability for strategic planning and real-time adaptation. However, existing benchmarks for tasks like StarCraft II fail to capture the game's full complexity, such as its complete game context, diverse action spaces, and all playable races. To address this gap, we present SC2Arena, a benchmark that fully supports all playable races, low-level action spaces, and optimizes text-based observations to tackle spatial reasoning challenges. Complementing this, we introduce StarEvolve, a hierarchical framework that integrates strategic planning with tactical execution, featuring iterative self-correction and continuous improvement via fine-tuning on high-quality gameplay data. Its key components include a Planner-Executor-Verifier structure to break down gameplay, and a scoring system for selecting high-quality training samples. Comprehensive analysis using SC2Arena provides valuable insights into developing generalist agents that were not possible with previous benchmarks. Experimental results also demonstrate that our proposed StarEvolve achieves superior performance in strategic planning. Our code, environment, and algorithms are publicly available.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T07:58:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10428v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10428v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 Computational Economics in Large Language Models: Exploring Model
  Behavior and Incentive Design under Resource Constraints</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sandeep Reddy, Kabir Khan, Rohit Patil, Ananya Chakraborty, Faizan A. Khan, Swati Kulkarni, Arjun Verma, Neha Singh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) are limited by substantial computational cost. We introduce a "computational economics" framework that treats an LLM as an internal economy of resource-constrained agents (attention heads and neuron blocks) that must allocate scarce computation to maximize task utility. First, we show empirically that when computation is scarce, standard LLMs reallocate attention toward high-value tokens while preserving accuracy. Building on this observation, we propose an incentive-driven training paradigm that augments the task loss with a differentiable computation cost term, encouraging sparse and efficient activations. On GLUE (MNLI, STS-B, CoLA) and WikiText-103, the method yields a family of models that trace a Pareto frontier and consistently dominate post-hoc pruning; for a similar accuracy we obtain roughly a forty percent reduction in FLOPS and lower latency, together with more interpretable attention patterns. These results indicate that economic principles offer a principled route to designing efficient, adaptive, and more transparent LLMs under strict resource constraints.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T07:55:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>I.2.6; I.2.7; I.5.1</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10426v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10426v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 Evaluating LLMs on Chinese Idiom Translation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Cai Yang, Yao Dou, David Heineman, Xiaofeng Wu, Wei Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Idioms, whose figurative meanings usually differ from their literal interpretations, are common in everyday language, especially in Chinese, where they often contain historical references and follow specific structural patterns. Despite recent progress in machine translation with large language models, little is known about Chinese idiom translation. In this work, we introduce IdiomEval, a framework with a comprehensive error taxonomy for Chinese idiom translation. We annotate 900 translation pairs from nine modern systems, including GPT-4o and Google Translate, across four domains: web, news, Wikipedia, and social media. We find these systems fail at idiom translation, producing incorrect, literal, partial, or even missing translations. The best-performing system, GPT-4, makes errors in 28% of cases. We also find that existing evaluation metrics measure idiom quality poorly with Pearson correlation below 0.48 with human ratings. We thus develop improved models that achieve F$_1$ scores of 0.68 for detecting idiom translation errors.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T07:52:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10421v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10421v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long
  Narrative Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Juyuan Wang, Rongchen Zhao, Wei Wei, Yufeng Wang, Mo Yu, Jie Zhou, Jin Xu, Liyan Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Narrative comprehension on long stories and novels has been a challenging domain attributed to their intricate plotlines and entangled, often evolving relations among characters and entities. Given the LLM's diminished reasoning over extended context and high computational cost, retrieval-based approaches remain a pivotal role in practice. However, traditional RAG methods can fall short due to their stateless, single-step retrieval process, which often overlooks the dynamic nature of capturing interconnected relations within long-range context. In this work, we propose ComoRAG, holding the principle that narrative reasoning is not a one-shot process, but a dynamic, evolving interplay between new evidence acquisition and past knowledge consolidation, analogous to human cognition when reasoning with memory-related signals in the brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes iterative reasoning cycles while interacting with a dynamic memory workspace. In each cycle, it generates probing queries to devise new exploratory paths, then integrates the retrieved evidence of new aspects into a global memory pool, thereby supporting the emergence of a coherent context for the query resolution. Across four challenging long-context narrative benchmarks (200K+ tokens), ComoRAG outperforms strong RAG baselines with consistent relative gains up to 11% compared to the strongest baseline. Further analysis reveals that ComoRAG is particularly advantageous for complex queries requiring global comprehension, offering a principled, cognitively motivated paradigm for retrieval-based long context comprehension towards stateful reasoning. Our code is publicly released at https://github.com/EternityJune25/ComoRAG
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T07:52:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10419v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10419v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 MCP2OSC: Parametric Control by Natural Language</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuan-Yi Fan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Text prompts enable intuitive content creation but may fall short in achieving high precision for intricate tasks; knob or slider controls offer precise adjustments at the cost of increased complexity. To address the gap between knobs and prompts, a new MCP (Model Context Protocol) server and a unique set of prompt design criteria are presented to enable exploring parametric OSC (OpenSoundControl) control by natural language prompts. Demonstrated by 14 practical QA examples with best practices and the generalized prompt templates, this study finds Claude integrated with the MCP2OSC server effective in generating OSC messages by natural language, interpreting, searching, and visualizing OSC messages, validating and debugging OSC messages, and managing OSC address patterns. MCP2OSC enhances human-machine collaboration by leveraging LLM (Large Language Model) to handle intricate OSC development tasks, and by empowering human creativity with an intuitive language interface featuring flexible precision controls: a prompt-based OSC tool. This study provides a novel perspective on the creative MCP application at the network protocol level by utilizing LLM's strength in directly processing and generating human-readable OSC messages. The results suggest its potential for a LLM-based universal control mechanism for multimedia devices.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T07:38:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span><span>cs.SD</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10414v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10414v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 CS-Agent: LLM-based Community Search via Dual-agent Collaboration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiahao Hua, Long Yuan, Qingshuai Feng, Qiang Fan, Shan Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks, yet their application to graph structure analysis, particularly in community search, remains underexplored. Community search, a fundamental task in graph analysis, aims to identify groups of nodes with dense interconnections, which is crucial for understanding the macroscopic structure of graphs. In this paper, we propose GraphCS, a comprehensive benchmark designed to evaluate the performance of LLMs in community search tasks. Our experiments reveal that while LLMs exhibit preliminary potential, they frequently fail to return meaningful results and suffer from output bias. To address these limitations, we introduce CS-Agent, a dual-agent collaborative framework to enhance LLM-based community search. CS-Agent leverages the complementary strengths of two LLMs acting as Solver and Validator. Through iterative feedback and refinement, CS-Agent dynamically refines initial results without fine-tuning or additional training. After the multi-round dialogue, Decider module selects the optimal community. Extensive experiments demonstrate that CS-Agent significantly improves the quality and stability of identified communities compared to baseline methods. To our knowledge, this is the first work to apply LLMs to community search, bridging the gap between LLMs and graph analysis while providing a robust and adaptive solution for real-world applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T07:26:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.09549v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.09549v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 Semantic-Enhanced Time-Series Forecasting via Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hao Liu, Chun Yang, Zhang xiaoxing, Xiaobin Zhu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Time series forecasting plays a significant role in finance, energy, meteorology, and IoT applications. Recent studies have leveraged the generalization capabilities of large language models (LLMs) to adapt to time series forecasting, achieving promising performance. However, existing studies focus on token-level modal alignment, instead of bridging the intrinsic modality gap between linguistic knowledge structures and time series data patterns, greatly limiting the semantic representation. To address this issue, we propose a novel Semantic-Enhanced LLM (SE-LLM) that explores the inherent periodicity and anomalous characteristics of time series to embed into the semantic space to enhance the token embedding. This process enhances the interpretability of tokens for LLMs, thereby activating the potential of LLMs for temporal sequence analysis. Moreover, existing Transformer-based LLMs excel at capturing long-range dependencies but are weak at modeling short-term anomalies in time-series data. Hence, we propose a plugin module embedded within self-attention that models long-term and short-term dependencies to effectively adapt LLMs to time-series analysis. Our approach freezes the LLM and reduces the sequence dimensionality of tokens, greatly reducing computational consumption. Experiments demonstrate the superiority performance of our SE-LLM against the state-of-the-art (SOTA) methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T07:25:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.07697v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.07697v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 Rollout Roulette: A Probabilistic Inference Approach to Inference-Time
  Scaling of LLMs using Particle-Based Monte Carlo Methods</h2>
                <div class="authors">
                    <strong>Authors:</strong> Isha Puri, Shivchander Sudalairaj, Guangxuan Xu, Kai Xu, Akash Srivastava
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have achieved significant performance gains via scaling up model sizes and/or data. However, recent evidence suggests diminishing returns from such approaches, motivating scaling the computation spent at inference time. Existing inference-time scaling methods, usually with reward models, cast the task as a search problem, which tends to be vulnerable to reward hacking as a consequence of approximation errors in reward models. In this paper, we instead cast inference-time scaling as a probabilistic inference task and leverage sampling-based techniques to explore the typical set of the state distribution of a state-space model with an approximate likelihood, rather than optimize for its mode directly. We propose a novel inference-time scaling approach by adapting particle-based Monte Carlo methods to this task. Our empirical evaluation demonstrates that our methods have a 4-16x better scaling rate over our deterministic search counterparts on various challenging mathematical reasoning tasks. Using our approach, we show that Qwen2.5-Math-1.5B-Instruct can surpass GPT-4o accuracy in only 4 rollouts, while Qwen2.5-Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts. Our work not only presents an effective method to inference-time scaling, but also connects the rich literature in probabilistic inference with inference-time scaling of LLMs to develop more robust algorithms in future work. Code, videos, and further information available at https://probabilistic-inference-scaling.github.io.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T07:21:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.01618v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.01618v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 Measuring Diversity in Synthetic Datasets</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuchang Zhu, Huizhe Zhang, Bingzhe Wu, Jintang Li, Zibin Zheng, Peilin Zhao, Liang Chen, Yatao Bian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) are widely adopted to generate synthetic datasets for various natural language processing (NLP) tasks, such as text classification and summarization. However, accurately measuring the diversity of these synthetic datasets-an aspect crucial for robust model performance-remains a significant challenge. In this paper, we introduce DCScore, a novel method for measuring synthetic dataset diversity from a classification perspective. Specifically, DCScore formulates diversity evaluation as a sample classification task, leveraging mutual relationships among samples. We further provide theoretical verification of the diversity-related axioms satisfied by DCScore, highlighting its role as a principled diversity evaluation method. Experimental results on synthetic datasets reveal that DCScore enjoys a stronger correlation with multiple diversity pseudo-truths of evaluated datasets, underscoring its effectiveness. Moreover, both empirical and theoretical evidence demonstrate that DCScore substantially reduces computational costs compared to existing methods. Code is available at: https://github.com/bluewhalelab/dcscore.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T07:15:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.08512v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.08512v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with
  Location-Election-Disjoint</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qianli Ma, Dongrui Liu, Qian Chen, Linfeng Zhang, Jing Shao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Fine-tuning pre-trained Large Language Models (LLMs) for specialized tasks incurs substantial computational and data costs. While model merging offers a training-free solution to integrate multiple task-specific models, existing methods suffer from safety-utility conflicts where enhanced general capabilities degrade safety safeguards. We identify two root causes: $\textbf{neuron misidentification}$ due to simplistic parameter magnitude-based selection, and $\textbf{cross-task neuron interference}$ during merging. To address these challenges, we propose $\textbf{LED-Merging}$, a three-stage framework that $\textbf{L}$ocates task-specific neurons via gradient-based attribution, dynamically $\textbf{E}$lects critical neurons through multi-model importance fusion, and $\textbf{D}$isjoints conflicting updates through parameter isolation. Extensive experiments on Llama-3-8B, Mistral-7B, and Llama2-13B demonstrate that LED-Merging effectively reduces harmful response rates, showing a 31.4\% decrease on Llama-3-8B-Instruct on HarmBench, while simultaneously preserving 95\% of utility performance, such as achieving 52.39\% accuracy on GSM8K. LED-Merging resolves safety-utility conflicts and provides a lightweight, training-free paradigm for constructing reliable multi-task LLMs. Code is available at $\href{https://github.com/MqLeet/LED-Merging}{GitHub}$.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T07:15:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.18653/v1/2025.acl-long.1055' target='_blank'>doi</a><a href='http://arxiv.org/abs/2502.16770v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.16770v2' target='_blank'>pdf</a>
                </div>
            </div>
            <h1>Keyword: LLM Deployment</h1>

            <div class="paper">
                <h2>#1 A Dataset for Distilling Knowledge Priors from Literature for
  Therapeutic Design</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haydn Thomas Jones, Natalie Maus, Josh Magnus Ludan, Maggie Ziyu Huan, Jiaming Liang, Marcelo Der Torossian Torres, Jiatao Liang, Zachary Ives, Yoseph Barash, Cesar de la Fuente-Nunez, Jacob R. Gardner, Mark Yatskar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> AI-driven discovery can greatly reduce design time and enhance new therapeutics' effectiveness. Models using simulators explore broad design spaces but risk violating implicit constraints due to a lack of experimental priors. For example, in a new analysis we performed on a diverse set of models on the GuacaMol benchmark using supervised classifiers, over 60\% of molecules proposed had high probability of being mutagenic. In this work, we introduce \ourdataset, a dataset of priors for design problems extracted from literature describing compounds used in lab settings. It is constructed with LLM pipelines for discovering therapeutic entities in relevant paragraphs and summarizing information in concise fair-use facts. \ourdataset~ consists of 32.3 million pairs of natural language facts, and appropriate entity representations (i.e. SMILES or refseq IDs). To demonstrate the potential of the data, we train LLM, CLIP, and LLava architectures to reason jointly about text and design targets and evaluate on tasks from the Therapeutic Data Commons (TDC). \ourdataset~is highly effective for creating models with strong priors: in supervised prediction problems that use our data as pretraining, our best models with 15M learnable parameters outperform larger 2B TxGemma on both regression and classification TDC tasks, and perform comparably to 9B models on average. Models built with \ourdataset~can be used as constraints while optimizing for novel molecules in GuacaMol, resulting in proposals that are safer and nearly as effective. We release our dataset at \href{https://huggingface.co/datasets/medexanon/Medex}{huggingface.co/datasets/medexanon/Medex}, and will provide expanded versions as available literature grows.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:59:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10899v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10899v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#2 CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hongchao Jiang, Yiming Chen, Yushi Cao, Hung-yi Lee, Robby T. Tan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have significantly advanced the state-of-the-art in various coding tasks. Beyond directly answering user queries, LLMs can also serve as judges, assessing and comparing the quality of responses generated by other models. Such an evaluation capability is crucial both for benchmarking different LLMs and for improving response quality through response ranking. However, despite the growing adoption of the LLM-as-a-Judge paradigm, its effectiveness in coding scenarios remains underexplored due to the absence of dedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a benchmark explicitly designed to evaluate the performance of LLM-as-a-Judge models across three critical coding tasks: code generation, code repair, and unit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge models, we find that recent thinking models significantly outperform non-thinking models on our carefully designed code judging tasks. Notably, even relatively small thinking models, such as Qwen3-8B, can outperform specially trained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still exhibit significant randomness in their judgment of coding tasks. For pairwise judging tasks, simply changing the order in which responses are presented can substantially impact accuracy. In addition, when judging code and unit tests written by different LLMs, LLM-as-a-Judge models also show variance in performance. This sensitivity raises concerns about the reliability and consistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal prompting strategies for LLM-as-a-Judge. We find that using pair-wise comparison outperforms scalar point-wise judging. Furthermore, retaining comments and reasoning in the full, unprocessed LLM response leads to improved judge performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:58:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.10535v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.10535v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#3 STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yushi Lan, Yihang Luo, Fangzhou Hong, Shangchen Zhou, Honghua Chen, Zhaoyang Lyu, Shuai Yang, Bo Dai, Chen Change Loy, Xingang Pan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We present STream3R, a novel approach to 3D reconstruction that reformulates pointmap prediction as a decoder-only Transformer problem. Existing state-of-the-art methods for multi-view reconstruction either depend on expensive global optimization or rely on simplistic memory mechanisms that scale poorly with sequence length. In contrast, STream3R introduces an streaming framework that processes image sequences efficiently using causal attention, inspired by advances in modern language modeling. By learning geometric priors from large-scale 3D datasets, STream3R generalizes well to diverse and challenging scenarios, including dynamic scenes where traditional methods often fail. Extensive experiments show that our method consistently outperforms prior work across both static and dynamic scene benchmarks. Moreover, STream3R is inherently compatible with LLM-style training infrastructure, enabling efficient large-scale pretraining and fine-tuning for various downstream 3D tasks. Our results underscore the potential of causal Transformer models for online 3D perception, paving the way for real-time 3D understanding in streaming environments. More details can be found in our project page: https://nirvanalan.github.io/projects/stream3r.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:58:05Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10893v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10893v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#4 BiasGym: Fantastic LLM Biases and How to Find (and Remove) Them</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sekh Mainul Islam, Nadav Borenstein, Siddhesh Milind Pawar, Haeun Yu, Arnav Arora, Isabelle Augenstein
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Understanding biases and stereotypes encoded in the weights of Large Language Models (LLMs) is crucial for developing effective mitigation strategies. Biased behaviour is often subtle and non-trivial to isolate, even when deliberately elicited, making systematic analysis and debiasing particularly challenging. To address this, we introduce BiasGym, a simple, cost-effective, and generalizable framework for reliably injecting, analyzing, and mitigating conceptual associations within LLMs. BiasGym consists of two components: BiasInject, which injects specific biases into the model via token-based fine-tuning while keeping the model frozen, and BiasScope, which leverages these injected signals to identify and steer the components responsible for biased behavior. Our method enables consistent bias elicitation for mechanistic analysis, supports targeted debiasing without degrading performance on downstream tasks, and generalizes to biases unseen during token-based fine-tuning. We demonstrate the effectiveness of BiasGym in reducing real-world stereotypes (e.g., people from Italy being `reckless drivers') and in probing fictional associations (e.g., people from a fictional country having `blue skin'), showing its utility for both safety interventions and interpretability research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:57:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.08855v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.08855v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#5 Searching for Privacy Risks in LLM Agents via Simulation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yanzhe Zhang, Diyi Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The widespread deployment of LLM-based agents is likely to introduce a critical privacy threat: malicious agents that proactively engage others in multi-turn interactions to extract sensitive information. These dynamic dialogues enable adaptive attack strategies that can cause severe privacy violations, yet their evolving nature makes it difficult to anticipate and discover sophisticated vulnerabilities manually. To tackle this problem, we present a search-based framework that alternates between improving attacker and defender instructions by simulating privacy-critical agent interactions. Each simulation involves three roles: data subject, data sender, and data recipient. While the data subject's behavior is fixed, the attacker (data recipient) attempts to extract sensitive information from the defender (data sender) through persistent and interactive exchanges. To explore this interaction space efficiently, our search algorithm employs LLMs as optimizers, using parallel search with multiple threads and cross-thread propagation to analyze simulation trajectories and iteratively propose new instructions. Through this process, we find that attack strategies escalate from simple direct requests to sophisticated multi-turn tactics such as impersonation and consent forgery, while defenses advance from rule-based constraints to identity-verification state machines. The discovered attacks and defenses transfer across diverse scenarios and backbone models, demonstrating strong practical utility for building privacy-aware agents.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:49:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10880v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10880v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#6 SSRL: Self-Search Reinforcement Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuchen Fan, Kaiyan Zhang, Heng Zhou, Yuxin Zuo, Yanxu Chen, Yu Fu, Xinwei Long, Xuekai Zhu, Che Jiang, Yuchen Zhang, Li Kang, Gang Chen, Cheng Huang, Zhizhou He, Bingning Wang, Lei Bai, Ning Ding, Bowen Zhou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We investigate the potential of large language models (LLMs) to serve as efficient simulators for agentic search tasks in reinforcement learning (RL), thereby reducing dependence on costly interactions with external search engines. To this end, we first quantify the intrinsic search capability of LLMs via structured prompting and repeated sampling, which we term Self-Search. Our results reveal that LLMs exhibit strong scaling behavior with respect to the inference budget, achieving high pass@k on question-answering benchmarks, including the challenging BrowseComp task. Building on these observations, we introduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability through format-based and rule-based rewards. SSRL enables models to iteratively refine their knowledge utilization internally, without requiring access to external tools. Empirical evaluations demonstrate that SSRL-trained policy models provide a cost-effective and stable environment for search-driven RL training, reducing reliance on external search engines and facilitating robust sim-to-real transfer. We draw the following conclusions: 1) LLMs possess world knowledge that can be effectively elicited to achieve high performance; 2) SSRL demonstrates the potential of leveraging internal knowledge to reduce hallucination; 3) SSRL-trained models integrate seamlessly with external search engines without additional effort. Our findings highlight the potential of LLMs to support more scalable RL agent training.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:46:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10874v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10874v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#7 TLE-Based A2C Agent for Terrestrial Coverage Orbital Path Planning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anantha Narayanan, Battu Bhanu Teja, Pruthwik Mishra
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The increasing congestion of Low Earth Orbit (LEO) poses persistent challenges to the efficient deployment and safe operation of Earth observation satellites. Mission planners must now account not only for mission-specific requirements but also for the increasing collision risk with active satellites and space debris. This work presents a reinforcement learning framework using the Advantage Actor-Critic (A2C) algorithm to optimize satellite orbital parameters for precise terrestrial coverage within predefined surface radii. By formulating the problem as a Markov Decision Process (MDP) within a custom OpenAI Gymnasium environment, our method simulates orbital dynamics using classical Keplerian elements. The agent progressively learns to adjust five of the orbital parameters - semi-major axis, eccentricity, inclination, right ascension of ascending node, and the argument of perigee-to achieve targeted terrestrial coverage. Comparative evaluation against Proximal Policy Optimization (PPO) demonstrates A2C's superior performance, achieving 5.8x higher cumulative rewards (10.0 vs 9.263025) while converging in 31.5x fewer timesteps (2,000 vs 63,000). The A2C agent consistently meets mission objectives across diverse target coordinates while maintaining computational efficiency suitable for real-time mission planning applications. Key contributions include: (1) a TLE-based orbital simulation environment incorporating physics constraints, (2) validation of actor-critic methods' superiority over trust region approaches in continuous orbital control, and (3) demonstration of rapid convergence enabling adaptive satellite deployment. This approach establishes reinforcement learning as a computationally efficient alternative for scalable and intelligent LEO mission planning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:44:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10872v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10872v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#8 Performance of GPT-5 in Brain Tumor MRI Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mojtaba Safari, Shansong Wang, Mingzhe Hu, Zach Eidex, Qiang Li, Xiaofeng Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Accurate differentiation of brain tumor types on magnetic resonance imaging (MRI) is critical for guiding treatment planning in neuro-oncology. Recent advances in large language models (LLMs) have enabled visual question answering (VQA) approaches that integrate image interpretation with natural language reasoning. In this study, we evaluated GPT-4o, GPT-5-nano, GPT-5-mini, and GPT-5 on a curated brain tumor VQA benchmark derived from 3 Brain Tumor Segmentation (BraTS) datasets - glioblastoma (GLI), meningioma (MEN), and brain metastases (MET). Each case included multi-sequence MRI triplanar mosaics and structured clinical features transformed into standardized VQA items. Models were assessed in a zero-shot chain-of-thought setting for accuracy on both visual and reasoning tasks. Results showed that GPT-5-mini achieved the highest macro-average accuracy (44.19%), followed by GPT-5 (43.71%), GPT-4o (41.49%), and GPT-5-nano (35.85%). Performance varied by tumor subtype, with no single model dominating across all cohorts. These findings suggest that GPT-5 family models can achieve moderate accuracy in structured neuro-oncological VQA tasks, but not at a level acceptable for clinical use.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:35:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10865v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10865v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#9 A Reference Architecture for Governance of Cloud Native Applications</h2>
                <div class="authors">
                    <strong>Authors:</strong> William Pourmajidi, Lei Zhang, John Steinbacher, Tony Erwin, Andriy Miranskyy
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The evolution of cloud computing has given rise to Cloud Native Applications (CNAs), presenting new challenges in governance, particularly when faced with strict compliance requirements. This work explores the unique characteristics of CNAs and their impact on governance. We introduce a comprehensive reference architecture designed to streamline governance across CNAs, along with a sample implementation, offering insights for both single and multi-cloud environments. Our architecture seamlessly integrates governance within the CNA framework, adhering to a ``battery-included'' philosophy. Tailored for both expansive and compact CNA deployments across various industries, this design enables cloud practitioners to prioritize product development by alleviating the complexities associated with governance. In addition, it provides a building block for academic exploration of generic CNA frameworks, highlighting their relevance in the evolving cloud computing landscape.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:31:02Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/TCC.2025.3578557' target='_blank'>doi</a><a href='http://arxiv.org/abs/2302.11617v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2302.11617v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#10 Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy,
  Expertise, and Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chongyuan Dai, Jinpeng Hu, Hongchang Shi, Zhuo Li, Xun Yang, Meng Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Amidst a shortage of qualified mental health professionals, the integration of large language models (LLMs) into psychological applications offers a promising way to alleviate the growing burden of mental health disorders. Recent reasoning-augmented LLMs have achieved remarkable performance in mathematics and programming, while research in the psychological domain has predominantly emphasized emotional support and empathetic dialogue, with limited attention to reasoning mechanisms that are beneficial to generating reliable responses. Therefore, in this paper, we propose Psyche-R1, the first Chinese psychological LLM that jointly integrates empathy, psychological expertise, and reasoning, built upon a novel data curation pipeline. Specifically, we design a comprehensive data synthesis pipeline that produces over 75k high-quality psychological questions paired with detailed rationales, generated through chain-of-thought (CoT) reasoning and iterative prompt-rationale optimization, along with 73k empathetic dialogues. Subsequently, we employ a hybrid training strategy wherein challenging samples are identified through a multi-LLM cross-selection strategy for group relative policy optimization (GRPO) to improve reasoning ability, while the remaining data is used for supervised fine-tuning (SFT) to enhance empathetic response generation and psychological domain knowledge. Extensive experiment results demonstrate the effectiveness of the Psyche-R1 across several psychological benchmarks, where our 7B Psyche-R1 achieves comparable results to 671B DeepSeek-R1.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:18:35Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10848v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10848v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#11 iFairy: the First 2-bit Complex LLM with All Parameters in $\{\pm1, \pm
  i\}$</h2>
                <div class="authors">
                    <strong>Authors:</strong> Feiyu Wang, Guoan Wang, Yihao Zhang, Shengfan Wang, Weitao Li, Bokai Huang, Shimao Chen, Zihan Jiang, Rui Xu, Tong Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Quantization-Aware Training (QAT) integrates quantization into the training loop, enabling LLMs to learn robust low-bit representations, and is widely recognized as one of the most promising research directions. All current QAT research focuses on minimizing quantization error on full-precision models, where the full-precision accuracy acts as an upper bound (accuracy ceiling). No existing method has even attempted to surpass this ceiling. To break this ceiling, we propose a new paradigm: raising the ceiling (full-precision model), and then still quantizing it efficiently into 2 bits. We propose Fairy$\pm i$, the first 2-bit quantization framework for complex-valued LLMs. Specifically, our method leverages the representational advantages of the complex domain to boost full-precision accuracy. We map weights to the fourth roots of unity $\{\pm1, \pm i\}$, forming a perfectly symmetric and information-theoretically optimal 2-bit representation. Importantly, each quantized weight has either a zero real or imaginary part, enabling multiplication-free inference using only additions and element swaps. Experimental results show that Fairy$\pm i$ outperforms the ceiling of existing 2-bit quantization approaches in terms of both PPL and downstream tasks, while maintaining strict storage and compute efficiency. This work opens a new direction for building highly accurate and practical LLMs under extremely low-bit constraints.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:13:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.05571v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.05571v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#12 TAR: Teacher-Aligned Representations via Contrastive Learning for
  Quadrupedal Locomotion</h2>
                <div class="authors">
                    <strong>Authors:</strong> Amr Mousa, Neil Karavis, Michele Caprio, Wei Pan, Richard Allmendinger
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Quadrupedal locomotion via Reinforcement Learning (RL) is commonly addressed using the teacher-student paradigm, where a privileged teacher guides a proprioceptive student policy. However, key challenges such as representation misalignment between privileged teacher and proprioceptive-only student, covariate shift due to behavioral cloning, and lack of deployable adaptation; lead to poor generalization in real-world scenarios. We propose Teacher-Aligned Representations via Contrastive Learning (TAR), a framework that leverages privileged information with self-supervised contrastive learning to bridge this gap. By aligning representations to a privileged teacher in simulation via contrastive objectives, our student policy learns structured latent spaces and exhibits robust generalization to Out-of-Distribution (OOD) scenarios, surpassing the fully privileged "Teacher". Results showed accelerated training by 2x compared to state-of-the-art baselines to achieve peak performance. OOD scenarios showed better generalization by 40% on average compared to existing methods. Moreover, TAR transitions seamlessly into learning during deployment without requiring privileged states, setting a new benchmark in sample-efficient, adaptive locomotion and enabling continual fine-tuning in real-world scenarios. Open-source code and videos are available at https://amrmousa.com/TARLoco/.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:09:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.LG</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.20839v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.20839v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#13 Generalizable Federated Learning using Client Adaptive Focal Modulation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tajamul Ashraf, Iqra Altaf Gillani
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Federated learning (FL) has proven essential for privacy-preserving, collaborative training across distributed clients. Our prior work, TransFed, introduced a robust transformer-based FL framework that leverages a learn-to-adapt hypernetwork to generate personalized focal modulation layers per client, outperforming traditional methods in non-IID and cross-domain settings. In this extended version, we propose AdaptFED, where we deepen the investigation of focal modulation in generalizable FL by incorporating: (1) a refined adaptation strategy that integrates task-aware client embeddings to personalize modulation dynamics further, (2) enhanced theoretical bounds on adaptation performance, and (3) broader empirical validation across additional modalities, including time-series and multilingual data. We also introduce an efficient variant of TransFed that reduces server-client communication overhead via low-rank hypernetwork conditioning, enabling scalable deployment in resource-constrained environments. Extensive experiments on eight diverse datasets reaffirm the superiority of our method over state-of-the-art baselines, particularly in source-free and cross-task federated setups. Our findings not only extend the capabilities of focal modulation in FL but also pave the way for more adaptive, scalable, and generalizable transformer-based federated systems. The code is available at http://github.com/Tajamul21/TransFed
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:06:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10840v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10840v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#14 Reinforced Language Models for Sequential Decision Making</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jim Dilkes, Vahid Yazdanpanah, Sebastian Stein
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) show potential as sequential decision-making agents, but their application is often limited due to a reliance on large, computationally expensive models. This creates a need to improve smaller models, yet existing post-training methods are designed for single-turn interactions and cannot handle credit assignment in multi-step agentic tasks. To address this, we introduce Multi-Step Group-Relative Policy Optimization (MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal Text-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP) frameworks. For credit assignment, MS-GRPO attributes the entire cumulative episode reward to each individual episode step. We supplement this algorithm with a novel absolute-advantage-weighted episode sampling strategy that we show improves training performance. We evaluate our approach by post-training a 3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate that the method is effective in improving decision-making performance: our post-trained 3B parameter model outperforms a 72B parameter baseline by 50% on the Frozen Lake task. This work demonstrates that targeted post-training is a practical and efficient alternative to relying on model scale for creating sequential decision-making agents using LLMs.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T17:05:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span><span>I.2.7; I.2.8</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10839v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10839v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#15 Mobile-Friendly Deep Learning for Plant Disease Detection: A Lightweight
  CNN Benchmark Across 101 Classes of 33 Crops</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anand Kumar, Harminder Pal Monga, Tapasi Brahma, Satyam Kalra, Navas Sherif
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Plant diseases are a major threat to food security globally. It is important to develop early detection systems which can accurately detect. The advancement in computer vision techniques has the potential to solve this challenge. We have developed a mobile-friendly solution which can accurately classify 101 plant diseases across 33 crops. We built a comprehensive dataset by combining different datasets, Plant Doc, PlantVillage, and PlantWild, all of which are for the same purpose. We evaluated performance across several lightweight architectures - MobileNetV2, MobileNetV3, MobileNetV3-Large, and EfficientNet-B0, B1 - specifically chosen for their efficiency on resource-constrained devices. The results were promising, with EfficientNet-B1 delivering our best performance at 94.7% classification accuracy. This architecture struck an optimal balance between accuracy and computational efficiency, making it well-suited for real-world deployment on mobile devices.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T16:43:27Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10817v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10817v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#16 The SET Perceptual Factors Framework: Towards Assured Perception for
  Autonomous Systems</h2>
                <div class="authors">
                    <strong>Authors:</strong> Troi Williams
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Future autonomous systems promise significant societal benefits, yet their deployment raises concerns about safety and trustworthiness. A key concern is assuring the reliability of robot perception, as perception seeds safe decision-making. Failures in perception are often due to complex yet common environmental factors and can lead to accidents that erode public trust. To address this concern, we introduce the SET (Self, Environment, and Target) Perceptual Factors Framework. We designed the framework to systematically analyze how factors such as weather, occlusion, or sensor limitations negatively impact perception. To achieve this, the framework employs SET State Trees to categorize where such factors originate and SET Factor Trees to model how these sources and factors impact perceptual tasks like object detection or pose estimation. Next, we develop Perceptual Factor Models using both trees to quantify the uncertainty for a given task. Our framework aims to promote rigorous safety assurances and cultivate greater public understanding and trust in autonomous systems by offering a transparent and standardized method for identifying, modeling, and communicating perceptual risks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T16:22:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10798v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10798v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#17 Beyond "Not Novel Enough": Enriching Scholarly Critique with
  LLM-Assisted Feedback</h2>
                <div class="authors">
                    <strong>Authors:</strong> Osama Mohammed Afzal, Preslav Nakov, Tom Hope, Iryna Gurevych
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Novelty assessment is a central yet understudied aspect of peer review, particularly in high volume fields like NLP where reviewer capacity is increasingly strained. We present a structured approach for automated novelty evaluation that models expert reviewer behavior through three stages: content extraction from submissions, retrieval and synthesis of related work, and structured comparison for evidence based assessment. Our method is informed by a large scale analysis of human written novelty reviews and captures key patterns such as independent claim verification and contextual reasoning. Evaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty assessments, the approach achieves 86.5% alignment with human reasoning and 75.3% agreement on novelty conclusions - substantially outperforming existing LLM based baselines. The method produces detailed, literature aware analyses and improves consistency over ad hoc reviewer judgments. These results highlight the potential for structured LLM assisted approaches to support more rigorous and transparent peer review without displacing human expertise. Data and code are made available.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T16:18:37Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10795v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10795v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#18 FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Guangda Liu, Chengwei Li, Zhenyu Ning, Minyi Guo, Jieru Zhao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have been widely deployed with rapidly expanding context windows to support increasingly demanding applications. However, long contexts pose significant deployment challenges, primarily due to the KV cache whose size grows proportionally with context length. While KV cache compression methods are proposed to address this issue, KV dropping methods incur considerable accuracy loss, and KV retrieval methods suffer from significant efficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization framework to enhance KV retrieval efficiency while preserving accuracy. On the algorithm side, FreeKV introduces speculative retrieval to shift the KV selection and recall processes out of the critical path, combined with fine-grained correction to ensure accuracy. On the system side, FreeKV employs hybrid KV layouts across CPU and GPU memory to eliminate fragmented data transfers, and leverages double-buffered streamed recall to further improve efficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy across various scenarios and models, delivering up to 13$\times$ speedup compared to SOTA KV retrieval methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T16:12:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.13109v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.13109v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#19 The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in
  Clinical Natural Language Inference</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mal Jullien, Marco Valentino, Andr Freitas
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models are often assumed to acquire increasingly structured, generalizable internal representations simply by scaling data and parameters. We interrogate this assumption by introducing a Clinical Trial Natural Language Inference benchmark comprising four reasoning families, Causal Attribution, Compositional Grounding, Epistemic Verification, and Risk State Abstraction. Each item is paired with a targeted Ground Knowledge and Meta-Level Reasoning Verification (GKMRV) probe, allowing us to dissociate failures of factual access from failures of inference. We evaluate six contemporary LLMs under both direct and chain of thought prompting.   Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform poorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy, output inferences are highly consistent across samples (mean 0.87), indicating a systematic application of underlying heuristics and shortcuts.   These results reveal fundamental structural and representational limitations: current LLMs often possess the relevant clinical knowledge but lack the structured, composable internal representations needed to deploy it reliably (e.g., integrating constraints, weighing evidence, or simulating counterfactuals). Decoupling knowledge from reasoning with GKMRV makes this dissociation explicit and measurable, providing an effective framework for probing the reliability of LLMs in high-stakes domains.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T16:01:10Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10777v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10777v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#20 Modeling Human Responses to Multimodal AI Content</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhiqi Shen, Shaojing Fan, Danni Xu, Terence Sim, Mohan Kankanhalli
                </div>
                <div class="summary">
                    <strong>Summary:</strong> As AI-generated content becomes widespread, so does the risk of misinformation. While prior research has primarily focused on identifying whether content is authentic, much less is known about how such content influences human perception and behavior. In domains like trading or the stock market, predicting how people react (e.g., whether a news post will go viral), can be more critical than verifying its factual accuracy. To address this, we take a human-centered approach and introduce the MhAIM Dataset, which contains 154,552 online posts (111,153 of them AI-generated), enabling large-scale analysis of how people respond to AI-generated content. Our human study reveals that people are better at identifying AI content when posts include both text and visuals, particularly when inconsistencies exist between the two. We propose three new metrics: trustworthiness, impact, and openness, to quantify how users judge and engage with online content. We present T-Lens, an LLM-based agent system designed to answer user queries by incorporating predicted human responses to multimodal information. At its core is HR-MCP (Human Response Model Context Protocol), built on the standardized Model Context Protocol (MCP), enabling seamless integration with any LLM. This integration allows T-Lens to better align with human reactions, enhancing both interpretability and interaction capabilities. Our work provides empirical insights and practical tools to equip LLMs with human-awareness capabilities. By highlighting the complex interplay among AI, human cognition, and information reception, our findings suggest actionable strategies for mitigating the risks of AI-driven misinformation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T15:55:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.MM</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10769v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10769v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#21 BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit
  KV Cache</h2>
                <div class="authors">
                    <strong>Authors:</strong> Dayou Du, Shijie Cao, Jianyi Cheng, Luo Mai, Ting Cao, Mao Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The rise of long-context Large Language Models (LLMs) amplifies memory and bandwidth demands during autoregressive decoding, as the Key-Value (KV) cache grows with each generated token. Low-bit KV-cache quantization (e.g., 4-bit or 2-bit) can reduce memory footprint while preserving accuracy, but existing systems suffer from slow decoding due to their exclusive reliance on CUDA cores, neglecting Tensor Cores (the primary source of compute on modern GPUs). We present BitDecoding, a new long-context LLM inference system with a low-bit KV cache. BitDecoding enables efficient low-bit KV-cache decoding by cooperatively leveraging CUDA cores and Tensor Cores. It introduces methods for automatically inducing optimized layouts to exploit Tensor Cores, along with warp-level parallelization strategies for dequantization. For unified system support, BitDecoding includes a query transformation module supporting diverse attention variants, a quantization kernel that supports both tensor-wise and channel-wise scaling used in various quantization algorithms with high performance, and a dequantization kernel with a software-defined pipeline to coordinate CUDA and Tensor Cores execution for mixed-precision operations. Evaluated on RTX 4090, A100, and H100, BitDecoding accelerates decoding by up to 7.5x, 4.8x, and 8.9x, respectively, over FP16 FlashDecoding-v2, and surpasses the state-of-the-art low-bit system QServe by up to 4.3x. On LLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding latency by 3x, showing substantial improvements for long-context generation. The code is available at https://github.com/DD-DuDa/BitDecoding.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T15:37:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AR</span><span>cs.AI</span><span>cs.CL</span><span>cs.PF</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.18773v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.18773v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#22 Pass@k Training for Adaptively Balancing Exploration and Exploitation of
  Large Reasoning Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhipeng Chen, Xiaobo Qin, Youbin Wu, Yue Ling, Qinghao Ye, Wayne Xin Zhao, Guang Shi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Reinforcement learning with verifiable rewards (RLVR), which typically adopts Pass@1 as the reward, has faced the issues in balancing exploration and exploitation, causing policies to prefer conservative actions, converging to a local optimum. Identifying an appropriate reward metric is therefore crucial. Regarding the prior work, although Pass@k has been used in evaluation, its connection to LLM exploration ability in RLVR remains largely overlooked. To investigate this, we first use Pass@k as the reward to train the policy model (i.e., $\textbf{Pass@k Training}$), and observe the improvement on its exploration ability. Next, we derive an analytical solution for the advantage of Pass@k Training, leading to an efficient and effective process. Building on this, our analysis reveals that exploration and exploitation are not inherently conflicting objectives, while they can mutually enhance each other. Moreover, Pass@k Training with analytical derivation essentially involves directly designing the advantage function. Inspired by this, we preliminarily explore the advantage design for RLVR, showing promising results and highlighting a potential future direction.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T15:34:47Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10751v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10751v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#23 Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiangqi Jin, Yuxuan Wang, Yifeng Gao, Zichen Wen, Biqing Qi, Dongrui Liu, Linfeng Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Despite large language models (LLMs) have achieved remarkable success, their prefix-only prompting paradigm and sequential generation process offer limited flexibility for bidirectional information. Diffusion large language models (dLLMs) present new opportunities through their bidirectional attention mechanisms and iterative refinement processes, enabling more flexible in-place prompting strategies. We introduce ICE (In-Place Chain-of-Thought Prompting with Early Exit), a novel framework that transforms prefix-only prompting into in-place prompting specifically designed for dLLMs. ICE integrates in-place prompts directly within masked token positions during iterative refinement and employs a confidence-aware early exit mechanism to significantly reduce computational overhead. Extensive experiments demonstrate ICE's effectiveness, achieving up to 17.29% accuracy improvement with 4.12$\times$ speedup on GSM8K, and up to 276.67$\times$ acceleration on MMLU while maintaining competitive performance.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T15:16:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10736v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10736v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#24 EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain
  Egocentric Video Question Answering</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yanjun Li, Yuqian Fu, Tianwen Qian, Qi'ao Xu, Silong Dai, Danda Pani Paudel, Luc Van Gool, Xiaoling Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances in Multimodal Large Language Models (MLLMs) have significantly pushed the frontier of egocentric video question answering (EgocentricQA). However, existing benchmarks and studies are mainly limited to common daily activities such as cooking and cleaning. In contrast, real-world deployment inevitably encounters domain shifts, where target domains differ substantially in both visual style and semantic content. To bridge this gap, we introduce \textbf{EgoCross}, a comprehensive benchmark designed to evaluate the cross-domain generalization of MLLMs in EgocentricQA. EgoCross covers four diverse and challenging domains, including surgery, industry, extreme sports, and animal perspective, representing realistic and high-impact application scenarios. It comprises approximately 1,000 QA pairs across 798 video clips, spanning four key QA tasks: prediction, recognition, localization, and counting. Each QA pair provides both OpenQA and CloseQA formats to support fine-grained evaluation. Extensive experiments show that most existing MLLMs, whether general-purpose or egocentric-specialized, struggle to generalize to domains beyond daily life, highlighting the limitations of current models. Furthermore, we conduct several pilot studies, \eg, fine-tuning and reinforcement learning, to explore potential improvements. We hope EgoCross and our accompanying analysis will serve as a foundation for advancing domain-adaptive, robust egocentric video understanding. Data and codes will be released at: \href{https://github.com/MyUniverse0726/EgoCross}{https://github.com/MyUniverse0726/EgoCross.}
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T15:11:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10729v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10729v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#25 Predictive Position Control for Movable Antenna Arrays in UAV
  Communications: A Spatio-Temporal Transformer-LSTM Framework</h2>
                <div class="authors">
                    <strong>Authors:</strong> Kan Yu, Kaixuan Li, Xiaowu Liu, Qixun Zhang, Zhiyong Feng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In complex urban environments, dynamic obstacles and multipath effects lead to significant link attenuation and pervasive coverage blind spots. Conventional approaches based on large-scale fixed antenna arrays and UAV trajectory optimization struggle to balance energy efficiency, real-time adaptation, and spatial flexibility. The movable antenna (MA) technology has emerged as a promising solution, offering enhanced spatial flexibility and reduced energy consumption to overcome the bottlenecks of urban low-altitude communications. However, MA deployment faces a critical velocity mismatch between UAV mobility and mechanical repositioning latency, undermining real-time link optimization and security assurance. To overcome this, we propose a predictive MA-UAV collaborative control framework. First, optimal antenna positions are derived via secrecy rate maximization. Second, a Transformer-enhanced long short-term memory (LSTM) network predicts future MA positions by capturing spatio-temporal correlations in antenna trajectories. Extensive simulations demonstrate superior prediction accuracy (NMSE reduction exceeds 49\%) and communication reliability versus current popular benchmarks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T15:00:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IT</span><span>math.IT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10720v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10720v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#26 Sample-efficient LLM Optimization with Reset Replay</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zichuan Liu, Jinyu Wang, Lei Song, Jiang Bian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advancements in post-training Large Language Models (LLMs), particularly through Reinforcement Learning (RL) and preference optimization methods, are key drivers for enhancing their reasoning capabilities. However, these methods are often plagued by low sample efficiency and a susceptibility to primacy bias, where overfitting to initial experiences degrades policy quality and damages the learning process. To address these challenges, we introduce LLM optimization with Reset Replay (LoRR), a general and powerful plugin designed to enhance sample efficiency in any preference-based optimization framework. LoRR core mechanism enables training at a high replay number, maximizing the utility of each collected data batch. To counteract the risk of overfitting inherent in high-replay training, LoRR incorporates a periodic reset strategy with reusing initial data, which preserves network plasticity. Furthermore, it leverages a hybrid optimization objective, combining supervised fine-tuning (SFT) and preference-based losses to further bolster data exploitation. Our extensive experiments demonstrate that LoRR significantly boosts the performance of various preference optimization methods on both mathematical and general reasoning benchmarks. Notably, an iterative DPO approach augmented with LoRR achieves comparable performance on challenging math tasks, outperforming some complex and computationally intensive RL-based algorithms. These findings highlight that LoRR offers a practical, sample-efficient, and highly effective paradigm for LLM finetuning, unlocking greater performance from limited data.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:59:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.06412v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.06412v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#27 Lightweight CNNs for Embedded SAR Ship Target Detection and
  Classification</h2>
                <div class="authors">
                    <strong>Authors:</strong> Fabian Kresse, Georgios Pilikos, Mario Azcueta, Nicolas Floury
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Synthetic Aperture Radar (SAR) data enables large-scale surveillance of maritime vessels. However, near-real-time monitoring is currently constrained by the need to downlink all raw data, perform image focusing, and subsequently analyze it on the ground. On-board processing to generate higher-level products could reduce the data volume that needs to be downlinked, alleviating bandwidth constraints and minimizing latency. However, traditional image focusing and processing algorithms face challenges due to the satellite's limited memory, processing power, and computational resources. This work proposes and evaluates neural networks designed for real-time inference on unfocused SAR data acquired in Stripmap and Interferometric Wide (IW) modes captured with Sentinel-1. Our results demonstrate the feasibility of using one of our models for on-board processing and deployment on an FPGA. Additionally, by investigating a binary classification task between ships and windmills, we demonstrate that target classification is possible.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:55:19Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10712v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10712v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#28 GenOM: Ontology Matching with Description Generation and Large Language
  Model</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yiping Song, Jiaoyan Chen, Renate A. Schmidt
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Ontology matching (OM) plays an essential role in enabling semantic interoperability and integration across heterogeneous knowledge sources, particularly in the biomedical domain which contains numerous complex concepts related to diseases and pharmaceuticals. This paper introduces GenOM, a large language model (LLM)-based ontology alignment framework, which enriches the semantic representations of ontology concepts via generating textual definitions, retrieves alignment candidates with an embedding model, and incorporates exact matching-based tools to improve precision. Extensive experiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often achieve competitive performance, surpassing many baselines including traditional OM systems and recent LLM-based methods. Further ablation studies confirm the effectiveness of semantic enrichment and few-shot prompting, highlighting the framework's robustness and adaptability.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:48:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10703v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10703v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#29 REFN: A Reinforcement-Learning-From-Network Framework against
  1-day/n-day Exploitations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tianlong Yu, Lihong Liu, Ziyi Zhou, Fudu Xing, Kailong Wang, Yang Yang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The exploitation of 1 day or n day vulnerabilities poses severe threats to networked devices due to massive deployment scales and delayed patching (average Mean Time To Patch exceeds 60 days). Existing defenses, including host based patching and network based filtering, are inadequate due to limited scalability across diverse devices, compatibility issues especially with embedded or legacy systems, and error prone deployment process (manual patch validation). To address these issues, we introduce REFN (Reinforcement Learning From Network), a novel framework that trains Large Language Models (LLMs) to autonomously generate network filters to prevent 1 day or n day exploitations. REFN ensures scalability by uniquely employs Reinforcement Learning (RL) driven by online network rewards instead of traditional Human Feedback (RLHF). REFN guarantees compatibility via unified deployment on edge security gateways (Amazon Eero). REFN provides robustness via online validation using real network traffic. Crucially, REFN addresses three core challenges in training LLMs for exploit prevention: 1) expanding current LLMs limited vulnerability fixing expertise via Agentic RAG based Knowledge Distillation, 2) bridging current LLMs language to network gaps through an RL From VNF Pipeline that translates language context (vulnerability description) into network enforcement, 3) addressing the LLM hallucination and non determinism via the Online Agentic Validation that penalizes erroneous outputs. Evaluated across 22 families of 1 day or n day exploits, REFN demonstrates effectiveness (21.1 percent higher accuracy than alternatives), efficiency (Mean Time To Patch of 3.65 hours) and scalability (easily scale to 10K devices). REFN serves as an initial step toward training LLMs to rapidly prevent massive scale 1 day or n day exploitations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:45:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10701v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10701v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#30 Towards Hybrid Lunar PNT: Error Models, Lower Bounds and Algorithms</h2>
                <div class="authors">
                    <strong>Authors:</strong> Robert Phlmann, Emanuel Staudinger, Gonzalo Seco-Granados
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Accurate positioning, navigation and timing (PNT) are crucial for upcoming lunar surface missions. Lunar satellite navigation systems are being developed, but lack coverage during early deployment phases. Hybrid lunar PNT combining cooperative navigation, satellite systems, and an optional reference station offers improved accuracy and availability. This study develops realistic error models that incorporate temporal correlations often ignored in existing works. We derive a cooperative navigation error model considering fading and pseudorange bias from multipath propagation, and compare three error models for lunar satellite pseudorange and pseudorange rate signal-in-space error. These temporal error correlation models integrate easily into Kalman filters and provide realistic performance predictions essential for robust navigation engines. We perform case studies to demonstrate that hybrid navigation significantly improves accuracy, particularly with static users present. Most notably, hybrid navigation enables optimal performance when using a lunar reference station, achieving sub-meter accuracy with only two visible satellites.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:43:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10699v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10699v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#31 Chem3DLLM: 3D Multimodal Large Language Models for Chemistry</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lei Jiang, Shuzhou Sun, Biqing Qi, Yuchen Fu, Xiaohua Xu, Yuqiang Li, Dongzhan Zhou, Tianfan Fu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the real world, a molecule is a 3D geometric structure. Compared to 1D SMILES sequences and 2D molecular graphs, 3D molecules represent the most informative molecular modality. Despite the rapid progress of autoregressive-based language models, they cannot handle the generation of 3D molecular conformation due to several challenges: 1) 3D molecular structures are incompatible with LLMs' discrete token space, 2) integrating heterogeneous inputs like proteins, ligands, and text remains difficult within a unified model, and 3) LLMs lack essential scientific priors, hindering the enforcement of physical and chemical constraints during generation. To tackle these issues, we present Chem3DLLM, a unified protein-conditioned multimodal large language model. Our approach designs a novel reversible text encoding for 3D molecular structures using run-length compression, achieving 3x size reduction while preserving complete structural information. This enables seamless integration of molecular geometry with protein pocket features in a single LLM architecture. We employ reinforcement learning with stability-based rewards to optimize chemical validity and incorporate a lightweight protein embedding projector for end-to-end training. Experimental results on structure-based drug design demonstrate state-of-the-art performance with a Vina score of -7.21, validating our unified multimodal approach for practical drug discovery applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:37:54Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10696v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10696v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#32 Learning from Natural Language Feedback for Personalized Question
  Answering</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alireza Salemi, Hamed Zamani
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Personalization is crucial for enhancing both the effectiveness and user satisfaction of language technologies, particularly in information-seeking tasks like question answering. Current approaches for personalizing large language models (LLMs) often rely on retrieval-augmented generation (RAG), followed by reinforcement learning with scalar reward signals to teach models how to use retrieved personal context. We believe that these scalar rewards sometimes provide weak, non-instructive feedback, limiting learning efficiency and personalization quality. We introduce VAC, a novel framework for personalized response generation that replaces scalar rewards with natural language feedback (NLF) that are generated conditioned on the user profiles and the question narratives. NLF serves as a rich and actionable supervision signal, allowing the policy model to iteratively refine its outputs and internalize effective personalization strategies. Training alternates between optimizing the feedback model and fine-tuning the policy model on the improved responses, resulting in a policy model that no longer requires feedback at inference. Evaluation on the LaMP-QA benchmark that consists of three diverse domains demonstrates consistent and significant improvements over the state-of-the-art results. Human evaluations further confirm the superior quality of the generated responses. These results demonstrate that NLF provides more effective signals for optimizing personalized question answering.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:36:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10695v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10695v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#33 An Attempt to Devise a Pairwise Ising-Type Maximum Entropy Model
  Integrated Cost Function for Optimizing SNN Deployment</h2>
                <div class="authors">
                    <strong>Authors:</strong> Wanhong Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Spiking Neural Networks (SNNs) emulate the spiking behavior of biological neurons and are typically deployed on distributed-memory neuromorphic hardware. The deployment of a SNN usually requires partitioning the network and mapping these partitions onto the hardware's processing units.   However, finding optimal deployment configurations is an NP-hard problem, often addressed through optimization algorithms. While some objectives (e.g., memory utilization and chip count) are static, others (e.g., communication latency and energy efficiency) depend on the network's dynamic behavior, necessitating dynamic-aware optimization.   To address this, we model SNN dynamics using an Ising-type pairwise interaction framework, bridging microscopic neuron interactions with macroscopic network behavior. We optimize deployment by exploring the parameter and configuration spaces of the Ising model.   We evaluate our approach on two SNNs deployed on the sPyNNaker neuromorphic platform. Initial results suggest that the method underperforms, potentially due to the Ising model's equilibrium assumptions and the architectural complexity of real-world neuromorphic hardware, highlighting limitations in its current formulation.   Update: The method proposed is with a equilibrium-dynamics SNN assumption, and the original paper does not mention this. The paper needs to be revisited and reuploaded after further experiments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:34:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.07014v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.07014v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#34 Reinforcement Learning in Vision: A Survey</h2>
                <div class="authors">
                    <strong>Authors:</strong> Weijia Wu, Chen Gao, Joya Chen, Kevin Qinghong Lin, Qingwei Meng, Yiming Zhang, Yuke Qiu, Hong Zhou, Mike Zheng Shou
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent advances at the intersection of reinforcement learning (RL) and visual intelligence have enabled agents that not only perceive complex visual scenes but also reason, generate, and act within them. This survey offers a critical and up-to-date synthesis of the field. We first formalize visual RL problems and trace the evolution of policy-optimization strategies from RLHF to verifiable reward paradigms, and from Proximal Policy Optimization to Group Relative Policy Optimization. We then organize more than 200 representative works into four thematic pillars: multi-modal large language models, visual generation, unified model frameworks, and vision-language-action models. For each pillar we examine algorithmic design, reward engineering, benchmark progress, and we distill trends such as curriculum-driven training, preference-aligned diffusion, and unified reward modeling. Finally, we review evaluation protocols spanning set-level fidelity, sample-level preference, and state-level stability, and we identify open challenges that include sample efficiency, generalization, and safe deployment. Our goal is to provide researchers and practitioners with a coherent map of the rapidly expanding landscape of visual RL and to highlight promising directions for future inquiry. Resources are available at: https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:32:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.08189v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.08189v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#35 From Actions to Words: Towards Abstractive-Textual Policy Summarization
  in RL</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sahar Admoni, Assaf Hallak, Yftah Ziser, Omer Ben-Porat, Ofra Amir
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Policies generated by Reinforcement Learning (RL) algorithms are difficult to explain to users, as they emerge from the interaction of complex reward structures and neural network representations. Consequently, analyzing and predicting agent behavior can be challenging, undermining user trust in real-world applications. To facilitate user understanding, current methods for global policy summarization typically rely on videos that demonstrate agent behavior in a subset of world states. However, users can only watch a limited number of demonstrations, constraining their understanding. Moreover, these methods place the burden of interpretation on users by presenting raw behaviors rather than synthesizing them into coherent patterns. To resolve these issues, we introduce SySLLM (Synthesized Summary using Large Language Models), advocating for a new paradigm of abstractive-textual policy explanations. By leveraging Large Language Models (LLMs)-which possess extensive world knowledge and pattern synthesis capabilities-SySLLM generates textual summaries that provide structured and comprehensible explanations of agent policies. SySLLM demonstrates that LLMs can interpret spatio-temporally structured descriptions of state-action trajectories from an RL agent and generate valuable policy insights in a zero-shot setting, without any prior knowledge or fine-tuning. Our evaluation shows that SySLLM captures key insights, such as goal preferences and exploration strategies, that were also identified by human experts. Furthermore, in a large-scale user study (with 200 participants), SySLLM summaries were preferred over demonstration-based summaries (HIGHLIGHTS) by a clear majority (75.5%) of participants.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:31:22Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.10509v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.10509v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#36 Advancing Autonomous Incident Response: Leveraging LLMs and Cyber Threat
  Intelligence</h2>
                <div class="authors">
                    <strong>Authors:</strong> Amine Tellache, Abdelaziz Amara Korba, Amdjed Mokhtari, Horea Moldovan, Yacine Ghamri-Doudane
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Effective incident response (IR) is critical for mitigating cyber threats, yet security teams are overwhelmed by alert fatigue, high false-positive rates, and the vast volume of unstructured Cyber Threat Intelligence (CTI) documents. While CTI holds immense potential for enriching security operations, its extensive and fragmented nature makes manual analysis time-consuming and resource-intensive. To bridge this gap, we introduce a novel Retrieval-Augmented Generation (RAG)-based framework that leverages Large Language Models (LLMs) to automate and enhance IR by integrating dynamically retrieved CTI. Our approach introduces a hybrid retrieval mechanism that combines NLP-based similarity searches within a CTI vector database with standardized queries to external CTI platforms, facilitating context-aware enrichment of security alerts. The augmented intelligence is then leveraged by an LLM-powered response generation module, which formulates precise, actionable, and contextually relevant incident mitigation strategies. We propose a dual evaluation paradigm, wherein automated assessment using an auxiliary LLM is systematically cross-validated by cybersecurity experts. Empirical validation on real-world and simulated alerts demonstrates that our approach enhances the accuracy, contextualization, and efficiency of IR, alleviating analyst workload and reducing response latency. This work underscores the potential of LLM-driven CTI fusion in advancing autonomous security operations and establishing a foundation for intelligent, adaptive cybersecurity frameworks.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:20:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10677v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10677v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#37 Knowledge-based Consistency Testing of Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sai Sathiesh Rajan, Ezekiel Soremekun, Sudipta Chattopadhyay
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In this work, we systematically expose and measure the inconsistency and knowledge gaps of Large Language Models (LLMs). Specifically, we propose an automated testing framework (called KonTest) which leverages a knowledge graph to construct test cases. KonTest probes and measures the inconsistencies in the LLM's knowledge of the world via a combination of semantically-equivalent queries and test oracles (metamorphic or ontological oracle). KonTest further mitigates knowledge gaps via a weighted LLM model ensemble. Using four state-of-the-art LLMs (Falcon, Gemini, GPT3.5, and Llama2), we show that KonTest generates 19.2% error inducing inputs (1917 errors from 9979 test inputs). It also reveals a 16.5% knowledge gap across all tested LLMs. A mitigation method informed by KonTest's test suite reduces LLM knowledge gap by 32.48%. Our ablation study further shows that GPT3.5 is not suitable for knowledge-based consistency testing because it is only 60%-68% effective in knowledge construction.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:12:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2407.12830v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2407.12830v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#38 MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for
  Document Question-Answering with Hierarchical Index and Multi-Granularity
  Retrieval</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ziyu Gong, Yihua Huang, Chengcheng Mai
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The multi-modal long-context document question-answering task aims to locate and integrate multi-modal evidences (such as texts, tables, charts, images, and layouts) distributed across multiple pages, for question understanding and answer generation. The existing methods can be categorized into Large Vision-Language Model (LVLM)-based and Retrieval-Augmented Generation (RAG)-based methods. However, the former were susceptible to hallucinations, while the latter struggled for inter-modal disconnection and cross-page fragmentation. To address these challenges, a novel multi-modal RAG model, named MMRAG-DocQA, was proposed, leveraging both textual and visual information across long-range pages to facilitate accurate question answering. A hierarchical indexing method with the integration of flattened in-page chunks and topological cross-page chunks was designed to jointly establish in-page multi-modal associations and long-distance cross-page dependencies. By means of joint similarity evaluation and large language model (LLM)-based re-ranking, a multi-granularity semantic retrieval method, including the page-level parent page retrieval and document-level summary retrieval, was proposed to foster multi-modal evidence connection and long-distance evidence integration and reasoning. Experimental results performed on public datasets, MMLongBench-Doc and LongDocURL, demonstrated the superiority of our MMRAG-DocQA method in understanding and answering modality-rich and multi-page documents.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T14:11:07Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.MM</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.00579v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.00579v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#39 SemPT: Semantic Prompt Tuning for Vision-Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xiao Shi, Yangjun Ou, Zhenzhong Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Visual transfer learning for unseen categories presents an active research topic yet a challenging task, due to the inherent conflict between preserving category-specific representations and acquiring transferable knowledge. Vision-Language Models (VLMs) pre-trained on large amounts of image-text pairs offer a promising solution. However, existing prompt tuning methods rely on sparse category labels or disparate LLM-generated descriptions, which fragment knowledge representation and hinder transferability. To address this limitation, we introduce Semantic Prompt Tuning (SemPT), a novel framework that tackles the generalization challenge by leveraging shared attribute-level knowledge across categories. Specifically, SemPT adopts a two-step prompting strategy to guide LLM in extracting shared visual attributes and generating attribute-level descriptions, capturing transferable semantic cues beyond labels while ensuring coherent structure. Then, visually guided weighting is applied to the embeddings of attribute-level descriptions to reduce noise from irrelevant attributes and enhance the text embeddings. Additionally, image embeddings are jointly aligned with both label and attribute-enhanced text embeddings, balancing discrimination for seen categories and transferability to unseen ones. Considering the availability of category exposure, our inference dynamically selects between standard label embeddings for seen categories and attribute-enhanced embeddings for unseen ones to ensure effective adaptation. Extensive experiments on 15 benchmark datasets demonstrate that SemPT achieves state-of-the-art performance across various settings, including base-to-novel generalization, cross-dataset transfer, cross-domain transfer, and few-shot learning.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T13:41:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10645v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10645v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#40 Synthesis of Deep Neural Networks with Safe Robust Adaptive Control for
  Reliable Operation of Wheeled Mobile Robots</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mehdi Heydari Shahna, Jouni Mattila
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Deep neural networks (DNNs) can enable precise control while maintaining low computational costs by circumventing the need for dynamic modeling. However, the deployment of such black-box approaches remains challenging for heavy-duty wheeled mobile robots (WMRs), which are subject to strict international standards and prone to faults and disturbances. We designed a hierarchical control policy for heavy-duty WMRs, monitored by two safety layers with differing levels of authority. To this end, a DNN policy was trained and deployed as the primary control strategy, providing high-precision performance under nominal operating conditions. When external disturbances arise and reach a level of intensity such that the system performance falls below a predefined threshold, a low-level safety layer intervenes by deactivating the primary control policy and activating a model-free robust adaptive control (RAC) policy. This transition enables the system to continue operating while ensuring stability by effectively managing the inherent trade-off between system robustness and responsiveness. Regardless of the control policy in use, a high-level safety layer continuously monitors system performance during operation. It initiates a shutdown only when disturbances become sufficiently severe such that compensation is no longer viable and continued operation would jeopardize the system or its environment. The proposed synthesis of DNN and RAC policy guarantees uniform exponential stability of the entire WMR system while adhering to safety standards to some extent. The effectiveness of the proposed approach was further validated through real-time experiments using a 6,000 kg WMR.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T13:33:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.SY</span><span>eess.SY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10634v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10634v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#41 Mathematical Computation and Reasoning Errors by Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Liang Zhang, Edith Aurora Graf
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) are increasingly utilized in AI-driven educational instruction and assessment, particularly within mathematics education. The capability of LLMs to generate accurate answers and detailed solutions for math problem-solving tasks is foundational for ensuring reliable and precise feedback and assessment in math education practices. Our study focuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1, DeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including arithmetic, algebra, and number theory, and identifies step-level reasoning errors within their solutions. Instead of relying on standard benchmarks, we intentionally build math tasks (via item models) that are challenging for LLMs and prone to errors. The accuracy of final answers and the presence of errors in individual solution steps were systematically analyzed and coded. Both single-agent and dual-agent configurations were tested. It is observed that the reasoning-enhanced OpenAI o1 model consistently achieved higher or nearly perfect accuracy across all three math task categories. Analysis of errors revealed that procedural slips were the most frequent and significantly impacted overall performance, while conceptual misunderstandings were less frequent. Deploying dual-agent configurations substantially improved overall performance. These findings offer actionable insights into enhancing LLM performance and underscore effective strategies for integrating LLMs into mathematics education, thereby advancing AI-driven instructional practices and assessment precision.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T13:25:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.09932v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.09932v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#42 NAVER: A Neuro-Symbolic Compositional Automaton for Visual Grounding
  with Explicit Logic Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zhixi Cai, Fucai Ke, Simindokht Jahangard, Maria Garcia de la Banda, Reza Haffari, Peter J. Stuckey, Hamid Rezatofighi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Visual Grounding (VG) tasks, such as referring expression detection and segmentation tasks are important for linking visual entities to context, especially in complex reasoning tasks that require detailed query interpretation. This paper explores VG beyond basic perception, highlighting challenges for methods that require reasoning like human cognition. Recent advances in large language methods (LLMs) and Vision-Language methods (VLMs) have improved abilities for visual comprehension, contextual understanding, and reasoning. These methods are mainly split into end-to-end and compositional methods, with the latter offering more flexibility. Compositional approaches that integrate LLMs and foundation models show promising performance but still struggle with complex reasoning with language-based logical representations. To address these limitations, we propose NAVER, a compositional visual grounding method that integrates explicit probabilistic logic reasoning within a finite-state automaton, equipped with a self-correcting mechanism. This design improves robustness and interpretability in inference through explicit logic reasoning. Our results show that NAVER achieves SoTA performance comparing to recent end-to-end and compositional baselines. The code is available at https://github.com/ControlNet/NAVER .
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T12:58:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.00372v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.00372v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#43 Towards Embodied Agentic AI: Review and Classification of LLM- and
  VLM-Driven Robot Autonomy and Interaction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sahar Salimpour, Lei Fu, Farhad Keramat, Leonardo Militano, Giovanni Toffetti, Harry Edelman, Jorge Pea Queralta
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Foundation models, including large language models (LLMs) and vision-language models (VLMs), have recently enabled novel approaches to robot autonomy and human-robot interfaces. In parallel, vision-language-action models (VLAs) or large behavior models (LBMs) are increasing the dexterity and capabilities of robotic systems. This survey paper focuses on those works advancing towards agentic applications and architectures. This includes initial efforts exploring GPT-style interfaces to tooling, as well as more complex system where AI agents are coordinators, planners, perception actors, or generalist interfaces. Such agentic architectures allow robots to reason over natural language instructions, invoke APIs, plan task sequences, or assist in operations and diagnostics. In addition to peer-reviewed research, due to the fast-evolving nature of the field, we highlight and include community-driven projects, ROS packages, and industrial frameworks that show emerging trends. We propose a taxonomy for classifying model integration approaches and present a comparative analysis of the role that agents play in different solutions in today's literature.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T12:55:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.05294v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.05294v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#44 Why Report Failed Interactions With Robots?! Towards Vignette-based
  Interaction Quality</h2>
                <div class="authors">
                    <strong>Authors:</strong> Agnes Axelsson, Merle Reimann, Ronald Cumbal, Hannah Pelikan, Divesh Lala
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Although the quality of human-robot interactions has improved with the advent of LLMs, there are still various factors that cause systems to be sub-optimal when compared to human-human interactions. The nature and criticality of failures are often dependent on the context of the interaction and so cannot be generalized across the wide range of scenarios and experiments which have been implemented in HRI research. In this work we propose the use of a technique overlooked in the field of HRI, ethnographic vignettes, to clearly highlight these failures, particularly those that are rarely documented. We describe the methodology behind the process of writing vignettes and create our own based on our personal experiences with failures in HRI systems. We emphasize the strength of vignettes as the ability to communicate failures from a multi-disciplinary perspective, promote transparency about the capabilities of robots, and document unexpected behaviours which would otherwise be omitted from research reports. We encourage the use of vignettes to augment existing interaction evaluation methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-15T16:07:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span><span>cs.HC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10603v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10603v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#45 Towards Powerful and Practical Patch Attacks for 2D Object Detection in
  Autonomous Driving</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuxin Cao, Yedi Zhang, Wentao He, Yifan Liao, Yan Xiao, Chang Li, Zhiyong Huang, Jin Song Dong
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Learning-based autonomous driving systems remain critically vulnerable to adversarial patches, posing serious safety and security risks in their real-world deployment. Black-box attacks, notable for their high attack success rate without model knowledge, are especially concerning, with their transferability extensively studied to reduce computational costs compared to query-based attacks. Previous transferability-based black-box attacks typically adopt mean Average Precision (mAP) as the evaluation metric and design training loss accordingly. However, due to the presence of multiple detected bounding boxes and the relatively lenient Intersection over Union (IoU) thresholds, the attack effectiveness of these approaches is often overestimated, resulting in reduced success rates in practical attacking scenarios. Furthermore, patches trained on low-resolution data often fail to maintain effectiveness on high-resolution images, limiting their transferability to autonomous driving datasets. To fill this gap, we propose P$^3$A, a Powerful and Practical Patch Attack framework for 2D object detection in autonomous driving, specifically optimized for high-resolution datasets. First, we introduce a novel metric, Practical Attack Success Rate (PASR), to more accurately quantify attack effectiveness with greater relevance for pedestrian safety. Second, we present a tailored Localization-Confidence Suppression Loss (LCSL) to improve attack transferability under PASR. Finally, to maintain the transferability for high-resolution datasets, we further incorporate the Probabilistic Scale-Preserving Padding (PSPP) into the patch attack pipeline as a data preprocessing step. Extensive experiments show that P$^3$A outperforms state-of-the-art attacks on unseen models and unseen high-resolution datasets, both under the proposed practical IoU-based evaluation metric and the previous mAP-based metrics.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T12:41:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10600v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10600v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#46 FreeGAD: A Training-Free yet Effective Approach for Graph Anomaly
  Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yunfeng Zhao, Yixin Liu, Shiyuan Li, Qingfeng Chen, Yu Zheng, Shirui Pan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Graph Anomaly Detection (GAD) aims to identify nodes that deviate from the majority within a graph, playing a crucial role in applications such as social networks and e-commerce. Despite the current advancements in deep learning-based GAD, existing approaches often suffer from high deployment costs and poor scalability due to their complex and resource-intensive training processes. Surprisingly, our empirical findings suggest that the training phase of deep GAD methods, commonly perceived as crucial, may actually contribute less to anomaly detection performance than expected. Inspired by this, we propose FreeGAD, a novel training-free yet effective GAD method. Specifically, it leverages an affinity-gated residual encoder to generate anomaly-aware representations. Meanwhile, FreeGAD identifies anchor nodes as pseudo-normal and anomalous guides, followed by calculating anomaly scores through anchor-guided statistical deviations. Extensive experiments demonstrate that FreeGAD achieves superior anomaly detection performance, efficiency, and scalability on multiple benchmark datasets from diverse domains, without any training or iterative optimization.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T12:37:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10594v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10594v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#47 A Comprehensive Tutorial and Survey of O-RAN: Exploring Slicing-aware
  Architecture, Deployment Options, Use Cases, and Challenges</h2>
                <div class="authors">
                    <strong>Authors:</strong> Khurshid Alam, Mohammad Asif Habibi, Matthias Tammen, Dennis Krummacker, Walid Saad, Marco Di Renzo, Tommaso Melodia, Xavier Costa-Prez, Mrouane Debbah, Ashutosh Dutta, Hans D. Schotten
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Open-radio access network (O-RAN) seeks to establish the principles of openness, programmability, automation, intelligence, and hardware-software disaggregation with interoperable and standard-compliant interfaces. It advocates for multi-vendorism and multi-stakeholderism within a cloudified and virtualized wireless infrastructure, aimed at enhancing the deployment, operation, and management of RAN architecture. These enhancements promise increased flexibility, performance optimization, service innovation, energy efficiency, and cost effectiveness across fifth-generation (5G), sixth-generation (6G), and beyond networks. A silent feature of O-RAN architecture is its support for network slicing, which entails interaction with other domains of the cellular network, notably the transport network (TN) and the core network (CN), to realize end-to-end (E2E) network slicing. The study of this feature requires exploring the stances and contributions of diverse standards development organizations (SDOs). In this context, we note that despite the ongoing industrial deployments and standardization efforts, the research and standardization communities have yet to comprehensively address network slicing in O-RAN. To address this gap, this paper provides a comprehensive exploration of network slicing in O-RAN through an in-depth review of specification documents from O-RAN Alliance and research papers from leading industry and academic institutions. The paper commences with an overview of the relevant standardization and open source contributions, subsequently delving into the latest O-RAN architecture with an emphasis on its slicing aspects. Furthermore, the paper explores O-RAN deployment scenarios, examining options for the deployment and orchestration of RAN and TN slice subnets. It also discusses the slicing of the underlying infrastructure and provides an overview of various use cases related...
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T12:30:30Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1109/COMST.2025.3598406' target='_blank'>doi</a><a href='http://arxiv.org/abs/2405.03555v6' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2405.03555v6' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#48 Debiasing Multimodal Large Language Models via Penalization of Language
  Priors</h2>
                <div class="authors">
                    <strong>Authors:</strong> YiFan Zhang, Yang Shi, Weichen Yu, Qingsong Wen, Xue Wang, Wenjing Yang, Zhang Zhang, Liang Wang, Rong Jin
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the realms of computer vision and natural language processing, Multimodal Large Language Models (MLLMs) have become indispensable tools, proficient in generating textual responses based on visual inputs. Despite their advancements, our investigation reveals a noteworthy bias: the generated content is often driven more by the inherent priors of the underlying Large Language Models (LLMs) than by the input image. Empirical experiments underscore the persistence of this bias, as MLLMs often provide confident answers even in the absence of relevant images or given incongruent visual inputs. To rectify these biases and redirect the model's focus toward visual information, we propose two simple, training-free strategies. First, for tasks such as classification or multi-choice question answering, we introduce a "Post-Hoc Debias" method using an affine calibration step to adjust the output distribution. This approach ensures uniform answer scores when the image is absent, acting as an effective regularization technique to alleviate the influence of LLM priors. For more intricate open-ended generation tasks, we extend this method to "Visual Debias Decoding", which mitigates bias by contrasting token log-probabilities conditioned on a correct image versus a meaningless one. Additionally, our investigation sheds light on the instability of MLLMs across various decoding configurations. Through systematic exploration of different settings, we achieve significant performance improvements--surpassing previously reported results--and raise concerns about the fairness of current evaluation practices. Comprehensive experiments substantiate the effectiveness of our proposed strategies in mitigating biases. These strategies not only prove beneficial in minimizing hallucinations but also contribute to the generation of more helpful and precise illustrations.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T12:30:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2403.05262v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2403.05262v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#49 Technical Report: Facilitating the Adoption of Causal Inference Methods
  Through LLM-Empowered Co-Pilot</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jeroen Berrevoets, Julianna Piskorz, Robert Davis, Harry Amad, Jim Weatherall, Mihaela van der Schaar
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Estimating treatment effects (TE) from observational data is a critical yet complex task in many fields, from healthcare and economics to public policy. While recent advances in machine learning and causal inference have produced powerful estimation techniques, their adoption remains limited due to the need for deep expertise in causal assumptions, adjustment strategies, and model selection. In this paper, we introduce CATE-B, an open-source co-pilot system that uses large language models (LLMs) within an agentic framework to guide users through the end-to-end process of treatment effect estimation. CATE-B assists in (i) constructing a structural causal model via causal discovery and LLM-based edge orientation, (ii) identifying robust adjustment sets through a novel Minimal Uncertainty Adjustment Set criterion, and (iii) selecting appropriate regression methods tailored to the causal structure and dataset characteristics. To encourage reproducibility and evaluation, we release a suite of benchmark tasks spanning diverse domains and causal complexities. By combining causal inference with intelligent, interactive assistance, CATE-B lowers the barrier to rigorous causal analysis and lays the foundation for a new class of benchmarks in automated treatment effect estimation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T12:20:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10581v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10581v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#50 FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory</h2>
                <div class="authors">
                    <strong>Authors:</strong> Alessio Buscemi, Daniele Proverbio, Alessandro Di Stefano, The Anh Han, German Castignani, Pietro Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Letting AI agents interact in multi-agent applications adds a layer of complexity to the interpretability and prediction of AI outcomes, with profound implications for their trustworthy adoption in research and society. Game theory offers powerful models to capture and interpret strategic interaction among agents, but requires the support of reproducible, standardized and user-friendly IT frameworks to enable comparison and interpretation of results. To this end, we present FAIRGAME, a Framework for AI Agents Bias Recognition using Game Theory. We describe its implementation and usage, and we employ it to uncover biased outcomes in popular games among AI agents, depending on the employed Large Language Model (LLM) and used language, as well as on the personality trait or strategic knowledge of the agents. Overall, FAIRGAME allows users to reliably and easily simulate their desired games and scenarios and compare the results across simulation campaigns and with game-theoretic predictions, enabling the systematic discovery of biases, the anticipation of emerging behavior out of strategic interplays, and empowering further research into strategic decision-making using LLM agents.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T12:12:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.14325v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.14325v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#51 Towards Agentic AI for Multimodal-Guided Video Object Segmentation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Tuyen Tran, Thao Minh Le, Truyen Tran
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Referring-based Video Object Segmentation is a multimodal problem that requires producing fine-grained segmentation results guided by external cues. Traditional approaches to this task typically involve training specialized models, which come with high computational complexity and manual annotation effort. Recent advances in vision-language foundation models open a promising direction toward training-free approaches. Several studies have explored leveraging these general-purpose models for fine-grained segmentation, achieving performance comparable to that of fully supervised, task-specific models. However, existing methods rely on fixed pipelines that lack the flexibility needed to adapt to the dynamic nature of the task. To address this limitation, we propose Multi-Modal Agent, a novel agentic system designed to solve this task in a more flexible and adaptive manner. Specifically, our method leverages the reasoning capabilities of large language models (LLMs) to generate dynamic workflows tailored to each input. This adaptive procedure iteratively interacts with a set of specialized tools designed for low-level tasks across different modalities to identify the target object described by the multimodal cues. Our agentic approach demonstrates clear improvements over prior methods on two multimodal-conditioned VOS tasks: RVOS and Ref-AVS.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T12:11:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10572v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10572v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#52 PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D
  Perception Tasks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xinhao Wang, Zhiwei Lin, Zhongyu Xia, Yongtao Wang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) represent two mainstream model quantization approaches. However, PTQ often leads to unacceptable performance degradation in quantized models, while QAT imposes substantial GPU memory requirements and extended training time due to weight fine-tuning. In this paper, we propose PTQAT, a novel general hybrid quantization algorithm for the efficient deployment of 3D perception networks. To address the speed accuracy trade-off between PTQ and QAT, our method selects critical layers for QAT fine-tuning and performs PTQ on the remaining layers. Contrary to intuition, fine-tuning the layers with smaller output discrepancies before and after quantization, rather than those with larger discrepancies, actually leads to greater improvements in the model's quantization accuracy. This means we better compensate for quantization errors during their propagation, rather than addressing them at the point where they occur. The proposed PTQAT achieves similar performance to QAT with more efficiency by freezing nearly 50% of quantifiable layers. Additionally, PTQAT is a universal quantization method that supports various quantization bit widths (4 bits) as well as different model architectures, including CNNs and Transformers. The experimental results on nuScenes across diverse 3D perception tasks, including object detection, semantic segmentation, and occupancy prediction, show that our method consistently outperforms QAT-only baselines. Notably, it achieves 0.2%-0.9% NDS and 0.3%-1.0% mAP gains in object detection, 0.3%-2.0% mIoU gains in semantic segmentation and occupancy prediction while fine-tuning fewer weights.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-15T06:20:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10557v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10557v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#53 xApp Conflict Mitigation with Scheduler</h2>
                <div class="authors">
                    <strong>Authors:</strong> Idris Cinemre, Toktam Mahmoodi, Amirmohammad Farzaneh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Open RAN (O-RAN) fosters multi-vendor interoperability and data-driven control but simultaneously introduces the challenge of coordinating pre-trained xApps that may produce conflicting actions. Although O-RAN specifications mandate offline training and validation to prevent the deployment of untrained or inadequately tested models, operational conflicts can still arise under dynamic and context-dependent conditions.This work proposes a scheduler-based conflict mitigation framework to address these challenges without requiring training xApps together or further xApp re-training. By examining an indirect conflict involving power and resource block allocation xApps and employing an Advantage Actor-Critic (A2C) approach to train both xApps and the scheduler, we illustrate that a straightforward A2C-based scheduler improves performance relative to independently deployed xApps and conflicting cases. Notably, among all tested deployment scenarios (including individual xApp deployment, multiple conflicting xApps, and limited scheduler configurations), augmenting the system with baseline xApps and enabling the scheduler to select from a broader pool achieves the highest total transmission rate, thereby underscoring the importance of adaptive scheduling mechanisms. These findings highlight the context-dependent nature of conflicts in automated network management, as two xApps may conflict under certain conditions but coexist under others. Consequently, the ability to dynamically update and adapt the scheduler to accommodate diverse operational intents is vital for future network deployments. By offering dynamic scheduling without re-training xApps, this framework advances practical conflict resolution solutions while supporting real-world scalability.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T11:54:11Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>eess.SP</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.06867v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.06867v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#54 Retrieval-Augmented Prompt for OOD Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ruisong Han, Zongbo Han, Jiahao Zhang, Mingyue Cheng, Changqing Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Out-of-Distribution (OOD) detection is crucial for the reliable deployment of machine learning models in-the-wild, enabling accurate identification of test samples that differ from the training data distribution. Existing methods rely on auxiliary outlier samples or in-distribution (ID) data to generate outlier information for training, but due to limited outliers and their mismatch with real test OOD samples, they often fail to provide sufficient semantic supervision, leading to suboptimal performance. To address this, we propose a novel OOD detection method called Retrieval-Augmented Prompt (RAP). RAP augments a pre-trained vision-language model's prompts by retrieving external knowledge, offering enhanced semantic supervision for OOD detection. During training, RAP retrieves descriptive words for outliers based on joint similarity with external textual knowledge and uses them to augment the model's OOD prompts. During testing, RAP dynamically updates OOD prompts in real-time based on the encountered OOD samples, enabling the model to rapidly adapt to the test environment. Our extensive experiments demonstrate that RAP achieves state-of-the-art performance on large-scale OOD detection benchmarks. For example, in 1-shot OOD detection on the ImageNet-1k dataset, RAP reduces the average FPR95 by 7.05% and improves the AUROC by 1.71% compared to previous methods. Additionally, comprehensive ablation studies validate the effectiveness of each module and the underlying motivations of our approach.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T11:52:43Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10556v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10556v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#55 Curse of High Dimensionality Issue in Transformer for Long-context
  Modeling</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shuhai Zhang, Zeng You, Yaofo Chen, Zhiquan Wen, Qianyue Wang, Zhijie Qiu, Yuanqing Li, Mingkui Tan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Transformer-based large language models (LLMs) excel in natural language processing tasks by capturing long-range dependencies through self-attention mechanisms. However, long-context modeling faces significant computational inefficiencies due to \textit{redundant} attention computations: while attention weights are often \textit{sparse}, all tokens consume \textit{equal} computational resources. In this paper, we reformulate traditional probabilistic sequence modeling as a \textit{supervised learning task}, enabling the separation of relevant and irrelevant tokens and providing a clearer understanding of redundancy. Based on this reformulation, we theoretically analyze attention sparsity, revealing that only a few tokens significantly contribute to predictions. Building on this, we formulate attention optimization as a linear coding problem and propose a \textit{group coding strategy}, theoretically showing its ability to improve robustness against random noise and enhance learning efficiency. Motivated by this, we propose \textit{Dynamic Group Attention} (DGA), which leverages the group coding to explicitly reduce redundancy by aggregating less important tokens during attention computation. Empirical results show that our DGA significantly reduces computational costs while maintaining competitive performance.Code is available at https://github.com/bolixinyu/DynamicGroupAttention.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T11:51:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.LG</span><span>stat.ML</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.22107v4' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.22107v4' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#56 eDIF: A European Deep Inference Fabric for Remote Interpretability of
  LLM</h2>
                <div class="authors">
                    <strong>Authors:</strong> Irma Heithoff. Marc Guggenberger, Sandra Kalogiannis, Susanne Mayer, Fabian Maag, Sigurd Schacht, Carsten Lanquillon
                </div>
                <div class="summary">
                    <strong>Summary:</strong> This paper presents a feasibility study on the deployment of a European Deep Inference Fabric (eDIF), an NDIF-compatible infrastructure designed to support mechanistic interpretability research on large language models. The need for widespread accessibility of LLM interpretability infrastructure in Europe drives this initiative to democratize advanced model analysis capabilities for the research community. The project introduces a GPU-based cluster hosted at Ansbach University of Applied Sciences and interconnected with partner institutions, enabling remote model inspection via the NNsight API. A structured pilot study involving 16 researchers from across Europe evaluated the platform's technical performance, usability, and scientific utility. Users conducted interventions such as activation patching, causal tracing, and representation analysis on models including GPT-2 and DeepSeek-R1-70B. The study revealed a gradual increase in user engagement, stable platform performance throughout, and a positive reception of the remote experimentation capabilities. It also marked the starting point for building a user community around the platform. Identified limitations such as prolonged download durations for activation data as well as intermittent execution interruptions are addressed in the roadmap for future development. This initiative marks a significant step towards widespread accessibility of LLM interpretability infrastructure in Europe and lays the groundwork for broader deployment, expanded tooling, and sustained community collaboration in mechanistic interpretability research.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T11:45:34Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10553v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10553v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#57 From Data Center IoT Telemetry to Data Analytics Chatbots -- Virtual
  Knowledge Graph is All You Need</h2>
                <div class="authors">
                    <strong>Authors:</strong> Junaid Ahmed Khan, Hiari Pizzini Cavagna, Andrea Proia, Andrea Bartolini
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Industry 5.0 demands IoT systems that support seamless human-machine collaboration, yet current IoT data analysis requires deep domain, deployment, and query expertise. We show that combining Large Language Models (LLMs) with Knowledge Graphs (KGs) enables natural language access to heterogeneous IoT data. Focusing on data center IoT telemetry, we introduce a rule-based Virtual Knowledge Graph (VKG) construction process and an on-premise LLM inference service to create an end-to-end Data Analytics (DA) chatbot. Our system dynamically generates VKGs per query and translates user input into SPARQL, achieving 92.5% accuracy (vs. 25% for LLM-to-NoSQL) while reducing latency by 85% (20.36s to 3.03s) and keeping VKG sizes under 179 MiB. This work demonstrates that VKG-powered LLM interfaces deliver accurate, low-latency, and relationship-aware access to large-scale telemetry, bridging the gap between users and complex IoT systems in Industry 5.0.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T11:40:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.DC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2506.22267v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2506.22267v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#58 Improving Value-based Process Verifier via Low-Cost Variance Reduction</h2>
                <div class="authors">
                    <strong>Authors:</strong> Zetian Sun, Dongfang Li, Baotian Hu, Min Zhang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have achieved remarkable success in a wide range of tasks. However, their reasoning capabilities, particularly in complex domains like mathematics, remain a significant challenge. Value-based process verifiers, which estimate the probability of a partial reasoning chain leading to a correct solution, are a promising approach for improving reasoning. Nevertheless, their effectiveness is often hindered by estimation error in their training annotations, a consequence of the limited number of Monte Carlo (MC) samples feasible due to the high cost of LLM inference. In this paper, we identify that the estimation error primarily arises from high variance rather than bias, and the MC estimator is a Minimum Variance Unbiased Estimator (MVUE). To address the problem, we propose the \textsc{Com}pound \textsc{M}onte \textsc{C}arlo \textsc{S}ampling (ComMCS) method, which constructs an unbiased estimator by linearly combining the MC estimators from the current and subsequent steps. Theoretically, we show that our method leads to a predictable reduction in variance, while maintaining an unbiased estimation without additional LLM inference cost. We also perform empirical experiments on the MATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method. Notably, ComMCS outperforms regression-based optimization method by 2.8 points, the non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32 sampling experiment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T11:22:29Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10539v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10539v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#59 MLM: Learning Multi-task Loco-Manipulation Whole-Body Control for
  Quadruped Robot with Arm</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xin Liu, Bida Ma, Chenkun Qi, Yan Ding, Zhaxizhuoma, Guorong Zhang, Pengan Chen, Kehui Liu, Zhongjie Jia, Chuyue Guan, Yule Mo, Jiaqi Liu, Feng Gao, Jiangwei Zhong, Bin Zhao, Xuelong Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Whole-body loco-manipulation for quadruped robots with arm remains a challenging problem, particularly in achieving multi-task control. To address this, we propose MLM, a reinforcement learning framework driven by both real-world and simulation data. It enables a six-DoF robotic arm--equipped quadruped robot to perform whole-body loco-manipulation for multiple tasks autonomously or under human teleoperation. To address the problem of balancing multiple tasks during the learning of loco-manipulation, we introduce a trajectory library with an adaptive, curriculum-based sampling mechanism. This approach allows the policy to efficiently leverage real-world collected trajectories for learning multi-task loco-manipulation. To address deployment scenarios with only historical observations and to enhance the performance of policy execution across tasks with different spatial ranges, we propose a Trajectory-Velocity Prediction policy network. It predicts unobservable future trajectories and velocities. By leveraging extensive simulation data and curriculum-based rewards, our controller achieves whole-body behaviors in simulation and zero-shot transfer to real-world deployment. Ablation studies in simulation verify the necessity and effectiveness of our approach, while real-world experiments on the Go2 robot with an Airbot robotic arm demonstrate the policy's good performance in multi-task execution.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T11:18:32Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.RO</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10538v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10538v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#60 Adversarial Robustness in Two-Stage Learning-to-Defer: Algorithms and
  Guarantees</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yannis Montreuil, Axel Carlier, Lai Xing Ng, Wei Tsang Ooi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Two-stage Learning-to-Defer (L2D) enables optimal task delegation by assigning each input to either a fixed main model or one of several offline experts, supporting reliable decision-making in complex, multi-agent environments. However, existing L2D frameworks assume clean inputs and are vulnerable to adversarial perturbations that can manipulate query allocation--causing costly misrouting or expert overload. We present the first comprehensive study of adversarial robustness in two-stage L2D systems. We introduce two novel attack strategie--untargeted and targeted--which respectively disrupt optimal allocations or force queries to specific agents. To defend against such threats, we propose SARD, a convex learning algorithm built on a family of surrogate losses that are provably Bayes-consistent and $(\mathcal{R}, \mathcal{G})$-consistent. These guarantees hold across classification, regression, and multi-task settings. Empirical results demonstrate that SARD significantly improves robustness under adversarial attacks while maintaining strong clean performance, marking a critical step toward secure and trustworthy L2D deployment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T11:00:13Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>stat.ML</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.01027v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.01027v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#61 An Explainable Transformer-based Model for Phishing Email Detection: A
  Large Language Model Approach</h2>
                <div class="authors">
                    <strong>Authors:</strong> Mohammad Amaz Uddin, Md Mahiuddin, Iqbal H. Sarker
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Phishing email is a serious cyber threat that tries to deceive users by sending false emails with the intention of stealing confidential information or causing financial harm. Attackers, often posing as trustworthy entities, exploit technological advancements and sophistication to make detection and prevention of phishing more challenging. Despite extensive academic research, phishing detection remains an ongoing and formidable challenge in the cybersecurity landscape. Large Language Models (LLMs) and Masked Language Models (MLMs) possess immense potential to offer innovative solutions to address long-standing challenges. In this research paper, we present an optimized, fine-tuned transformer-based DistilBERT model designed for the detection of phishing emails. In the detection process, we work with a phishing email dataset and utilize the preprocessing techniques to clean and solve the imbalance class issues. Through our experiments, we found that our model effectively achieves high accuracy, demonstrating its capability to perform well. Finally, we demonstrate our fine-tuned model using Explainable-AI (XAI) techniques such as Local Interpretable Model-Agnostic Explanations (LIME) and Transformer Interpret to explain how our model makes predictions in the context of text classification for phishing emails.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T10:59:03Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2402.13871v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2402.13871v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#62 Bridging Solidity Evolution Gaps: An LLM-Enhanced Approach for Smart
  Contract Compilation Error Resolution</h2>
                <div class="authors">
                    <strong>Authors:</strong> Likai Ye, Mengliang Li, Dehai Zhao, Jiamou Sun, Xiaoxue Ren
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Solidity, the dominant smart contract language for Ethereum, has rapidly evolved with frequent version updates to enhance security, functionality, and developer experience. However, these continual changes introduce significant challenges, particularly in compilation errors, code migration, and maintenance. Therefore, we conduct an empirical study to investigate the challenges in the Solidity version evolution and reveal that 81.68% of examined contracts encounter errors when compiled across different versions, with 86.92% of compilation errors.   To mitigate these challenges, we conducted a systematic evaluation of large language models (LLMs) for resolving Solidity compilation errors during version migrations. Our empirical analysis across both open-source (LLaMA3, DeepSeek) and closed-source (GPT-4o, GPT-3.5-turbo) LLMs reveals that although these models exhibit error repair capabilities, their effectiveness diminishes significantly for semantic-level issues and shows strong dependency on prompt engineering strategies. This underscores the critical need for domain-specific adaptation in developing reliable LLM-based repair systems for smart contracts.   Building upon these insights, we introduce SMCFIXER, a novel framework that systematically integrates expert knowledge retrieval with LLM-based repair mechanisms for Solidity compilation error resolution. The architecture comprises three core phases: (1) context-aware code slicing that extracts relevant error information; (2) expert knowledge retrieval from official documentation; and (3) iterative patch generation for Solidity migration. Experimental validation across Solidity version migrations demonstrates our approach's statistically significant 24.24% improvement over baseline GPT-4o on real-world datasets, achieving near-perfect 96.97% accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T10:42:26Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10517v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10517v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#63 Error dependencies in the space-based CNEOS fireball database</h2>
                <div class="authors">
                    <strong>Authors:</strong> Eloy Pea-Asensio, Hector Socas-Navarro, Darryl Z. Seligman
                </div>
                <div class="summary">
                    <strong>Summary:</strong> We evaluate the reliability of CNEOS-derived ephemerides of fireball events given the absence of the underlying data. We analyzed 18 events that have both (i) sufficient satellite information to derive orbits and (ii) ground-based observational counterparts. We quantify the uncertainties on these calibrated events using the orbital similarity criterion $D_D$. We also examine the velocity components imbalance and identify discriminants that can indicate the accuracy of an event. We identify two groups in the CNEOS database. CNEOS data produces ephemeris determinations with $D_D$<0.1 for fireballs reported either (i) after late 2017 or (ii) with impact energies above 0.45 kt with 74-78% of events having $D_D$=0.03$\pm$0.02, while ~11% show $D_D$<0.008. Our statistical test confirms these two parameters as the only reliable discriminants that, when combined, explain the two accuracy groups. Daylight, z-velocity component, low altitude, long duration, and latitude might also indicate errors, although the limited dataset may obscure correlations. No clear discriminants are identified for more restrictive $D_D$ cut-offs. We provide estimates of orbital uncertainties for calibrated events. The hyperbolic fireball subset in the CNEOS database appears as an outlier in the velocity imbalance test. Our results confirm that the fidelity of CNEOS fireball data improved significantly from 2018, likely due to the deployment of next-generation space sensors, and show a growing number of high-velocity events. Hyperbolic candidates should be interpreted with caution, as their velocities and inclinations likely reflect measurement errors. Accuracy constraints remain limited by the dataset size, as evidenced by the lack of statistically significant dependence on duration, preventing strong conclusions from being drawn.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T10:23:18Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>astro-ph.EP</span><span>astro-ph.IM</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1051/0004-6361/202554224' target='_blank'>doi</a><a href='http://arxiv.org/abs/2508.01454v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.01454v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#64 A Unified Multi-Agent Framework for Universal Multimodal Understanding
  and Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiulin Li, Ping Huang, Yexin Li, Shuo Chen, Juewen Hu, Ye Tian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Real-world multimodal applications often require any-to-any capabilities, enabling both understanding and generation across modalities including text, image, audio, and video. However, integrating the strengths of autoregressive language models (LLMs) for reasoning and diffusion models for high-fidelity generation remains challenging. Existing approaches rely on rigid pipelines or tightly coupled architectures, limiting flexibility and scalability. We propose MAGUS (Multi-Agent Guided Unified Multimodal System), a modular framework that unifies multimodal understanding and generation via two decoupled phases: Cognition and Deliberation. MAGUS enables symbolic multi-agent collaboration within a shared textual workspace. In the Cognition phase, three role-conditioned multimodal LLM agents - Perceiver, Planner, and Reflector - engage in collaborative dialogue to perform structured understanding and planning. The Deliberation phase incorporates a Growth-Aware Search mechanism that orchestrates LLM-based reasoning and diffusion-based generation in a mutually reinforcing manner. MAGUS supports plug-and-play extensibility, scalable any-to-any modality conversion, and semantic alignment - all without the need for joint training. Experiments across multiple benchmarks, including image, video, and audio generation, as well as cross-modal instruction following, demonstrate that MAGUS outperforms strong baselines and state-of-the-art systems. Notably, on the MME benchmark, MAGUS surpasses the powerful closed-source model GPT-4o.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T09:52:51Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.MA</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10494v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10494v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#65 AlDBaran: Towards Blazingly Fast State Commitments for Blockchains</h2>
                <div class="authors">
                    <strong>Authors:</strong> Bernhard Kauer, Aleksandr Petrosyan, Benjamin Livshits
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The fundamental basis for maintaining integrity within contemporary blockchain systems is provided by authenticated databases. Our analysis indicates that a significant portion of the approaches applied in this domain fail to sufficiently meet the stringent requirements of systems processing transactions at rates of multi-million TPS. AlDBaran signifies a substantial advancement in authenticated databases. By eliminating disk I/O operations from the critical path, implementing prefetching strategies, and refining the update mechanism of the Merkle tree, we have engineered an authenticated data structure capable of handling state updates efficiently at a network throughput of 50 Gbps. This throughput capacity significantly surpasses any empirically documented blockchain throughput, guaranteeing the ability of even the most high-throughput blockchains to generate state commitments effectively.   AlDBaran provides support for historical state proofs, which facilitates a wide array of novel applications. For instance, the deployment of AlDBaran could enable blockchains that do not currently support state commitments to offer functionalities for light clients and/or implement rollups.   When benchmarked against alternative authenticated data structure projects, AlDBaran exhibits superior performance and simplicity. In particular, AlDBaran achieves speeds of approximately 48 million updates per second using an identical machine configuration. This characteristic renders AlDBaran an attractive solution for resource-limited environments, as its historical data capabilities can be modularly isolated (and deactivated), which further enhances performance. On consumer-level portable hardware, it achieves approximately 8 million updates/s in an in-memory setting and 5 million updates/s with snapshots at sub-second intervals, illustrating compelling and cost-effective scalability.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T09:52:15Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.GT</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10493v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10493v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#66 Reverse Physician-AI Relationship: Full-process Clinical Diagnosis
  Driven by a Large Language Model</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shicheng Xu, Xin Huang, Zihao Wei, Liang Pang, Huawei Shen, Xueqi Cheng
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Full-process clinical diagnosis in the real world encompasses the entire diagnostic workflow that begins with only an ambiguous chief complaint. While artificial intelligence (AI), particularly large language models (LLMs), is transforming clinical diagnosis, its role remains largely as an assistant to physicians. This AI-assisted working pattern makes AI can only answer specific medical questions at certain parts within the diagnostic process, but lack the ability to drive the entire diagnostic process starting from an ambiguous complaint, which still relies heavily on human physicians. This gap limits AI's ability to fully reduce physicians' workload and enhance diagnostic efficiency. To address this, we propose a paradigm shift that reverses the relationship between physicians and AI: repositioning AI as the primary director, with physicians serving as its assistants. So we present DxDirector-7B, an LLM endowed with advanced deep thinking capabilities, enabling it to drive the full-process diagnosis with minimal physician involvement. Furthermore, DxDirector-7B establishes a robust accountability framework for misdiagnoses, delineating responsibility between AI and human physicians. In evaluations across rare, complex, and real-world cases under full-process diagnosis setting, DxDirector-7B not only achieves significant superior diagnostic accuracy but also substantially reduces physician workload than state-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained analyses across multiple clinical departments and tasks validate its efficacy, with expert evaluations indicating its potential to serve as a viable substitute for medical specialists. These findings mark a new era where AI, traditionally a physicians' assistant, now drives the entire diagnostic process to drastically reduce physicians' workload, indicating an efficient and accurate diagnostic solution.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T09:51:20Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.CE</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10492v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10492v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#67 Two Means to an End Goal: Connecting Explainability and Contestability
  in the Regulation of Public Sector AI</h2>
                <div class="authors">
                    <strong>Authors:</strong> Timothe Schmude, Mireia Yurrita, Kars Alfrink, Thomas Le Goff, Sebastian Tschiatschek, Tiphaine Viard
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Explainability and its emerging counterpart contestability have become important normative and design principles for trustworthy AI as they enable users and subjects to understand and challenge AI decisions. However, realizing these principles is difficult, as they assume different meanings in technical, legal, and organizational dimensions of AI regulation. To resolve this conceptual polysemy, in this paper, we present the findings of an interview study with 14 experts to examine the intersection and implementation of explainability and contestability, and their understanding in different research communities. We outline differentiations between descriptive and normative explainability, judicial and non-judicial channels of contestation, and individual and collective contestation action. We further describe the main points of friction in the realization of both principles, including the alignment between top-down and bottom-up regulation, the assignment of responsibility, and the need for interdisciplinary collaboration. Lastly, we formulate three recommendations for AI policy to implement both principles through a Regulation by Design perspective. We believe our contributions can inform policy-making and regulation of these core principles and enable more effective and equitable design, development, and deployment of trustworthy public AI systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T09:48:31Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CY</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2504.18236v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2504.18236v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#68 SEQ-GPT: LLM-assisted Spatial Query via Example</h2>
                <div class="authors">
                    <strong>Authors:</strong> Ivan Khai Ze Lim, Ningyi Liao, Yiming Yang, Gerald Wei Yong Yip, Siqiang Luo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Contemporary spatial services such as online maps predominantly rely on user queries for location searches. However, the user experience is limited when performing complex tasks, such as searching for a group of locations simultaneously. In this study, we examine the extended scenario known as Spatial Exemplar Query (SEQ), where multiple relevant locations are jointly searched based on user-specified examples. We introduce SEQ-GPT, a spatial query system powered by Large Language Models (LLMs) towards more versatile SEQ search using natural language. The language capabilities of LLMs enable unique interactive operations in the SEQ process, including asking users to clarify query details and dynamically adjusting the search based on user feedback. We also propose a tailored LLM adaptation pipeline that aligns natural language with structured spatial data and queries through dialogue synthesis and multi-model cooperation. SEQ-GPT offers an end-to-end demonstration for broadening spatial search with realistic data and application scenarios.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T09:41:55Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10486v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10486v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#69 Semantic IDs for Joint Generative Search and Recommendation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Gustavo Penha, Edoardo D'Amico, Marco De Nadai, Enrico Palumbo, Alexandre Tamborrino, Ali Vardasbi, Max Lefarov, Shawn Lin, Timothy Heath, Francesco Fabbri, Hugues Bouchard
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Generative models powered by Large Language Models (LLMs) are emerging as a unified solution for powering both recommendation and search tasks. A key design choice in these models is how to represent items, traditionally through unique identifiers (IDs) and more recently with Semantic IDs composed of discrete codes, obtained from embeddings. While task-specific embedding models can improve performance for individual tasks, they may not generalize well in a joint setting. In this paper, we explore how to construct Semantic IDs that perform well both in search and recommendation when using a unified model. We compare a range of strategies to construct Semantic IDs, looking into task-specific and cross-tasks approaches, and also whether each task should have its own semantic ID tokens in a joint search and recommendation generative model. Our results show that using a bi-encoder model fine-tuned on both search and recommendation tasks to obtain item embeddings, followed by the construction of a unified Semantic ID space provides an effective trade-off, enabling strong performance in both tasks. We hope these findings spark follow-up work on generalisable, semantically grounded ID schemes and inform the next wave of unified generative recommender architectures.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T09:28:49Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10478v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10478v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#70 EDAPT: Towards Calibration-Free BCIs with Continual Online Adaptation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Lisa Haxel, Jaivardhan Kapoor, Ulf Ziemann, Jakob H. Macke
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Brain-computer interfaces (BCIs) suffer from accuracy degradation as neural signals drift over time and vary across users, requiring frequent recalibration that limits practical deployment. We introduce EDAPT, a task- and model-agnostic framework that eliminates calibration through continual model adaptation. EDAPT first trains a baseline decoder using data from multiple users, then continually personalizes this model via supervised finetuning as the neural patterns evolve during use. We tested EDAPT across nine datasets covering three BCI tasks, and found that it consistently improved accuracy over conventional, static methods. These improvements primarily stem from combining population-level pretraining and online continual finetuning, with unsupervised domain adaptation providing further gains on some datasets. EDAPT runs efficiently, updating models within 200 milliseconds on consumer-grade hardware. Finally, decoding accuracy scales with total data budget rather than its allocation between subjects and trials. EDAPT provides a practical pathway toward calibration-free BCIs, reducing a major barrier to BCI deployment.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T09:23:25Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.HC</span><span>q-bio.NC</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10474v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10474v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#71 Hummingbird: Fast, Flexible, and Fair Inter-Domain Bandwidth
  Reservations</h2>
                <div class="authors">
                    <strong>Authors:</strong> Karl Wst, Giacomo Giuliari, Markus Legner, Jean-Pierre Smith, Marc Wyss, Jules Bachmann, Juan A. Garcia-Pardo, Adrian Perrig
                </div>
                <div class="summary">
                    <strong>Summary:</strong> To realize the long-standing vision of providing quality-of-service (QoS) guarantees on a public Internet, this paper introduces Hummingbird: a lightweight QoS-system that provides fine-grained inter-domain reservations for end hosts.   Hummingbird enables flexible and composable reservations with end-to-end guarantees, and addresses an often overlooked, but crucial, aspect of bandwidth-reservation systems: incentivization of network providers. Hummingbird represents bandwidth reservations as tradable assets, allowing markets to emerge. These markets then ensure fair and efficient resource allocation and encourage deployment by remunerating providers. This incentivization is facilitated by decoupling reservations from network identities, which enables novel control-plane mechanisms and allows the design of a control plane based on smart contracts.   Hummingbird also provides an efficient reservation data plane, which streamlines the processing on routers and thus simplifies the implementation, deployment, and traffic policing, while maintaining robust security properties. Our prototype implementation demonstrates the efficiency and scalability of Hummingbird's asset-based control plane, and our high-speed software implementation can fill a 160 Gbps link with Hummingbird packets on commodity hardware.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T09:16:33Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.NI</span><span>cs.CR</span><span>C.2.1; C.2.2; C.2.6</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.1145/3718958.3750495' target='_blank'>doi</a><a href='http://arxiv.org/abs/2308.09959v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2308.09959v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#72 FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over
  Scholarly Knowledge Graphs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Xueli Pan, Victor de Boer, Jacco van Ossenbruggen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Question answering over Scholarly Knowledge Graphs (SKGs) remains a challenging task due to the complexity of scholarly content and the intricate structure of these graphs. Large Language Model (LLM) approaches could be used to translate natural language questions (NLQs) into SPARQL queries; however, these LLM-based approaches struggle with SPARQL query generation due to limited exposure to SKG-specific content and the underlying schema. We identified two main types of errors in the LLM-generated SPARQL queries: (i) structural inconsistencies, such as missing or redundant triples in the queries, and (ii) semantic inaccuracies, where incorrect entities or properties are shown in the queries despite a correct query structure. To address these issues, we propose FIRESPARQL, a modular framework that supports fine-tuned LLMs as a core component, with optional context provided via retrieval-augmented generation (RAG) and a SPARQL query correction layer. We evaluate the framework on the SciQA Benchmark using various configurations (zero-shot, zero-shot with RAG, one-shot, fine-tuning, and fine-tuning with RAG) and compare the performance with baseline and state-of-the-art approaches. We measure query accuracy using BLEU and ROUGE metrics, and query result accuracy using relaxed exact match(RelaxedEM), with respect to the gold standards containing the NLQs, SPARQL queries, and the results of the queries. Experimental results demonstrate that fine-tuning achieves the highest overall performance, reaching 0.90 ROUGE-L for query accuracy and 0.85 RelaxedEM for result accuracy on the test set.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T09:08:50Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.DL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10467v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10467v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#73 ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic
  Parallelism in LLMs</h2>
                <div class="authors">
                    <strong>Authors:</strong> Keyu Chen, Zhifeng Shen, Daohai Yu, Haoqian Wu, Wei Wen, Jianfeng He, Ruizhi Qiao, Xing Sun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> The increasing scale and complexity of large language models (LLMs) pose significant inference latency challenges, primarily due to their autoregressive decoding paradigm characterized by the sequential nature of next-token prediction. By re-examining the outputs of autoregressive models, we observed that some segments exhibit parallelizable structures, which we term intrinsic parallelism. Decoding each parallelizable branch simultaneously (i.e. parallel decoding) can significantly improve the overall inference speed of LLMs. In this paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which addresses two core challenges: automated construction of parallelizable data and efficient parallel decoding mechanism. More specifically, we introduce a non-invasive pipeline that automatically extracts and validates parallelizable structures from the responses of autoregressive models. To empower efficient adaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which enables seamless transitions between serial and parallel decoding modes while maintaining a reusable KV cache, maximizing computational efficiency. Extensive evaluations across General Tasks, Retrieval-Augmented Generation, Mathematical Reasoning, demonstrate that ASPD achieves unprecedented performance in both effectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up to 3.19x speedup (1.85x on average) while maintaining response quality within 1% difference compared to autoregressive models, realizing significant acceleration without compromising generation quality. Our framework sets a groundbreaking benchmark for efficient LLM parallel inference, paving the way for its deployment in latency-sensitive applications such as AI-powered customer service bots and answer retrieval engines.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T09:04:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.08895v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.08895v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#74 X-Node: Self-Explanation is All We Need</h2>
                <div class="authors">
                    <strong>Authors:</strong> Prajit Sengupta, Islem Rekik
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Graph neural networks (GNNs) have achieved state-of-the-art results in computer vision and medical image classification tasks by capturing structural dependencies across data instances. However, their decision-making remains largely opaque, limiting their trustworthiness in high-stakes clinical applications where interpretability is essential. Existing explainability techniques for GNNs are typically post-hoc and global, offering limited insight into individual node decisions or local reasoning. We introduce X-Node, a self-explaining GNN framework in which each node generates its own explanation as part of the prediction process. For every node, we construct a structured context vector encoding interpretable cues such as degree, centrality, clustering, feature saliency, and label agreement within its local topology. A lightweight Reasoner module maps this context into a compact explanation vector, which serves three purposes: (1) reconstructing the node's latent embedding via a decoder to enforce faithfulness, (2) generating a natural language explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3) guiding the GNN itself via a "text-injection" mechanism that feeds explanations back into the message-passing pipeline. We evaluate X-Node on two graph datasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT, and GIN backbones. Our results show that X-Node maintains competitive classification accuracy while producing faithful, per-node explanations. Repository: https://github.com/basiralab/X-Node.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T09:00:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10461v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10461v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#75 VERCATION: Precise Vulnerable Open-source Software Version
  Identification based on Static Analysis and LLM</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yiran Cheng, Ting Zhang, Lwin Khin Shar, Shouguo Yang, Chaopeng Dong, David Lo, Shichao Lv, Zhiqiang Shi, Limin Sun
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Open-source software (OSS) has experienced a surge in popularity, attributed to its collaborative development model and cost-effective nature. However, the adoption of specific software versions in development projects may introduce security risks when these versions bring along vulnerabilities. Current methods of identifying vulnerable versions typically analyze and extract the code features involved in vulnerability patches using static analysis with pre-defined rules. They then use code clone detection to identify the vulnerable versions. These methods are hindered by imprecision due to (1) the exclusion of vulnerability-irrelevant code in the analysis and (2) the inadequacy of code clone detection. This paper presents VERCATION, an approach designed to identify vulnerable versions of OSS written in C/C++. VERCATION combines program slicing with a Large Language Model (LLM) to identify vulnerability-relevant code from vulnerability patches. It then backtracks historical commits to gather previous modifications of identified vulnerability-relevant code. We propose code clone detection based on expanded and normalized ASTs to compare the differences between pre-modification and post-modification code, thereby locating the vulnerability-introducing commit (vic) and enabling the identification of the vulnerable versions between the vulnerability-fixing commit and the vic. We curate a dataset linking 122 OSS vulnerabilities and 1,211 versions to evaluate VERCATION. On this dataset, our approach achieves an F1 score of 93.1%, outperforming current state-of-the-art methods. More importantly, VERCATION detected 202 incorrect vulnerable OSS versions in NVD reports.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T09:00:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SE</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2408.07321v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2408.07321v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#76 Building Instruction-Tuning Datasets from Human-Written Instructions
  with Open-Weight Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Youmi Ma, Sakae Mizuki, Kazuki Fujii, Taishi Nakamura, Masanari Ohi, Hinari Shimada, Taihei Shiotani, Koshiro Saito, Koki Maeda, Kakeru Hattori, Takumi Okamoto, Shigeki Ishida, Rio Yokota, Hiroya Takamura, Naoaki Okazaki
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Instruction tuning is crucial for enabling Large Language Models (LLMs) to solve real-world tasks. Prior work has shown the effectiveness of instruction-tuning data synthesized solely from LLMs, raising a fundamental question: Do we still need human-originated signals for instruction tuning? This work answers the question affirmatively: we build state-of-the-art instruction-tuning datasets sourced from human-written instructions, by simply pairing them with LLM-generated responses. LLMs fine-tuned on our datasets consistently outperform those fine-tuned on existing ones. Our data construction approach can be easily adapted to other languages; we build datasets for Japanese and confirm that LLMs tuned with our data reach state-of-the-art performance. Analyses suggest that instruction-tuning in a new language allows LLMs to follow instructions, while the tuned models exhibit a notable lack of culture-specific knowledge in that language. The datasets and fine-tuned models will be publicly available. Our datasets, synthesized with open-weight LLMs, are openly distributed under permissive licenses, allowing for diverse use cases.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T08:59:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2503.23714v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2503.23714v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#77 System design and realisation towards optimising secure key bits in free
  space QKD</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pooja Chandravanshi, Jayanth Ramakrishnan, Tanya Sharma, Ayan Biswas, Ravindra P. Singh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Quantum Key Distribution (QKD) is rapidly transitioning from cutting-edge laboratory research to real-world deployment in established communication networks. Although QKD promises future-proof security, practical challenges stil exist due to imperfections in physical devices. Many protocols offer strong security guarantees, but their implementation can be complex and difficult. To bridge this gap, we present a practical and systematic framework for implementing QKD, focused on the BB84 protocol but designed with broader applicability in mind. The article includes key concepts for device calibration, synchronisation,optical alignment, and key post-processing. We outline a simple algorithm for key sifting that is easily implementable in hardware. Our results highlight the importance of selecting the temporal window to optimise both the key rate and the quantum bit error rate (QBER). In addition, we show that random sampling of the sifted key bits for error estimation yields more reliable results than sequential sampling. We also integrate the Entrapped Pulse Coincidence Detection (EPCD) protocol to boost key generation rates, further enhancing performance. Although our work focuses on BB84, the techniques and practices outlined are general enough to support a wide range of QKD protocols. This makes our framework a valuable tool for both research and real-world deployment of secure quantum communication systems.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T08:57:59Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>quant-ph</span><span>physics.optics</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10458v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10458v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#78 A Lightweight Transformer with Phase-Only Cross-Attention for
  Illumination-Invariant Biometric Authentication</h2>
                <div class="authors">
                    <strong>Authors:</strong> Arun K. Sharma, Shubhobrata Bhattacharya, Motahar Reza, Bishakh Bhattacharya
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Traditional biometric systems have encountered significant setbacks due to various unavoidable factors, for example, wearing of face masks in face recognition-based biometrics and hygiene concerns in fingerprint-based biometrics. This paper proposes a novel lightweight vision transformer with phase-only cross-attention (POC-ViT) using dual biometric traits of forehead and periocular portions of the face, capable of performing well even with face masks and without any physical touch, offering a promising alternative to traditional methods. The POC-ViT framework is designed to handle two biometric traits and to capture inter-dependencies in terms of relative structural patterns. Each channel consists of a Cross-Attention using phase-only correlation (POC) that captures both their individual and correlated structural patterns. The computation of cross-attention using POC extracts the phase correlation in the spatial features. Therefore, it is robust against variations in resolution and intensity, as well as illumination changes in the input images. The lightweight model is suitable for edge device deployment. The performance of the proposed framework was successfully demonstrated using the Forehead Subcutaneous Vein Pattern and Periocular Biometric Pattern (FSVP-PBP) database, having 350 subjects. The POC-ViT framework outperformed state-of-the-art methods with an outstanding classification accuracy of $98.8\%$ with the dual biometric traits.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T08:27:24Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2412.19160v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2412.19160v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#79 DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On
  Offline Knowledge Base</h2>
                <div class="authors">
                    <strong>Authors:</strong> Song Mao, Lejun Cheng, Pinlong Cai, Guohang Yan, Ding Wang, Botian Shi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have demonstrated remarkable capabilities in various applications. However, their use as writing assistants in specialized domains like finance, medicine, and law is often hampered by a lack of deep domain-specific knowledge and a tendency to hallucinate. Existing solutions, such as Retrieval-Augmented Generation (RAG), can suffer from inconsistency across multiple retrieval steps, while online search-based methods often degrade quality due to unreliable web content. To address these challenges, we introduce DeepWriter, a customizable, multimodal, long-form writing assistant that operates on a curated, offline knowledge base. DeepWriter leverages a novel pipeline that involves task decomposition, outline generation, multimodal retrieval, and section-by-section composition with reflection. By deeply mining information from a structured corpus and incorporating both textual and visual elements, DeepWriter generates coherent, factually grounded, and professional-grade documents. We also propose a hierarchical knowledge representation to enhance retrieval efficiency and accuracy. Our experiments on financial report generation demonstrate that DeepWriter produces high-quality, verifiable articles that surpasses existing baselines in factual accuracy and generated content quality.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T08:14:12Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.14189v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.14189v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#80 SC2Arena and StarEvolve: Benchmark and Self-Improvement Framework for
  LLMs in Complex Decision-Making Tasks</h2>
                <div class="authors">
                    <strong>Authors:</strong> Pengbo Shen, Yaqing Wang, Ni Mu, Yao Luan, Runpeng Xie, Senhao Yang, Lexiang Wang, Hao Hu, Shuang Xu, Yiqin Yang, Bo Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Evaluating large language models (LLMs) in complex decision-making is essential for advancing AI's ability for strategic planning and real-time adaptation. However, existing benchmarks for tasks like StarCraft II fail to capture the game's full complexity, such as its complete game context, diverse action spaces, and all playable races. To address this gap, we present SC2Arena, a benchmark that fully supports all playable races, low-level action spaces, and optimizes text-based observations to tackle spatial reasoning challenges. Complementing this, we introduce StarEvolve, a hierarchical framework that integrates strategic planning with tactical execution, featuring iterative self-correction and continuous improvement via fine-tuning on high-quality gameplay data. Its key components include a Planner-Executor-Verifier structure to break down gameplay, and a scoring system for selecting high-quality training samples. Comprehensive analysis using SC2Arena provides valuable insights into developing generalist agents that were not possible with previous benchmarks. Experimental results also demonstrate that our proposed StarEvolve achieves superior performance in strategic planning. Our code, environment, and algorithms are publicly available.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T07:58:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10428v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10428v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#81 Computational Economics in Large Language Models: Exploring Model
  Behavior and Incentive Design under Resource Constraints</h2>
                <div class="authors">
                    <strong>Authors:</strong> Sandeep Reddy, Kabir Khan, Rohit Patil, Ananya Chakraborty, Faizan A. Khan, Swati Kulkarni, Arjun Verma, Neha Singh
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) are limited by substantial computational cost. We introduce a "computational economics" framework that treats an LLM as an internal economy of resource-constrained agents (attention heads and neuron blocks) that must allocate scarce computation to maximize task utility. First, we show empirically that when computation is scarce, standard LLMs reallocate attention toward high-value tokens while preserving accuracy. Building on this observation, we propose an incentive-driven training paradigm that augments the task loss with a differentiable computation cost term, encouraging sparse and efficient activations. On GLUE (MNLI, STS-B, CoLA) and WikiText-103, the method yields a family of models that trace a Pareto frontier and consistently dominate post-hoc pruning; for a similar accuracy we obtain roughly a forty percent reduction in FLOPS and lower latency, together with more interpretable attention patterns. These results indicate that economic principles offer a principled route to designing efficient, adaptive, and more transparent LLMs under strict resource constraints.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T07:55:45Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>I.2.6; I.2.7; I.5.1</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10426v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10426v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#82 Evaluating LLMs on Chinese Idiom Translation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Cai Yang, Yao Dou, David Heineman, Xiaofeng Wu, Wei Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Idioms, whose figurative meanings usually differ from their literal interpretations, are common in everyday language, especially in Chinese, where they often contain historical references and follow specific structural patterns. Despite recent progress in machine translation with large language models, little is known about Chinese idiom translation. In this work, we introduce IdiomEval, a framework with a comprehensive error taxonomy for Chinese idiom translation. We annotate 900 translation pairs from nine modern systems, including GPT-4o and Google Translate, across four domains: web, news, Wikipedia, and social media. We find these systems fail at idiom translation, producing incorrect, literal, partial, or even missing translations. The best-performing system, GPT-4, makes errors in 28% of cases. We also find that existing evaluation metrics measure idiom quality poorly with Pearson correlation below 0.48 with human ratings. We thus develop improved models that achieve F$_1$ scores of 0.68 for detecting idiom translation errors.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T07:52:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10421v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10421v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#83 ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long
  Narrative Reasoning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Juyuan Wang, Rongchen Zhao, Wei Wei, Yufeng Wang, Mo Yu, Jie Zhou, Jin Xu, Liyan Xu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Narrative comprehension on long stories and novels has been a challenging domain attributed to their intricate plotlines and entangled, often evolving relations among characters and entities. Given the LLM's diminished reasoning over extended context and high computational cost, retrieval-based approaches remain a pivotal role in practice. However, traditional RAG methods can fall short due to their stateless, single-step retrieval process, which often overlooks the dynamic nature of capturing interconnected relations within long-range context. In this work, we propose ComoRAG, holding the principle that narrative reasoning is not a one-shot process, but a dynamic, evolving interplay between new evidence acquisition and past knowledge consolidation, analogous to human cognition when reasoning with memory-related signals in the brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes iterative reasoning cycles while interacting with a dynamic memory workspace. In each cycle, it generates probing queries to devise new exploratory paths, then integrates the retrieved evidence of new aspects into a global memory pool, thereby supporting the emergence of a coherent context for the query resolution. Across four challenging long-context narrative benchmarks (200K+ tokens), ComoRAG outperforms strong RAG baselines with consistent relative gains up to 11% compared to the strongest baseline. Further analysis reveals that ComoRAG is particularly advantageous for complex queries requiring global comprehension, offering a principled, cognitively motivated paradigm for retrieval-based long context comprehension towards stateful reasoning. Our code is publicly released at https://github.com/EternityJune25/ComoRAG
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T07:52:09Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span><span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10419v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10419v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#84 MCP2OSC: Parametric Control by Natural Language</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuan-Yi Fan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Text prompts enable intuitive content creation but may fall short in achieving high precision for intricate tasks; knob or slider controls offer precise adjustments at the cost of increased complexity. To address the gap between knobs and prompts, a new MCP (Model Context Protocol) server and a unique set of prompt design criteria are presented to enable exploring parametric OSC (OpenSoundControl) control by natural language prompts. Demonstrated by 14 practical QA examples with best practices and the generalized prompt templates, this study finds Claude integrated with the MCP2OSC server effective in generating OSC messages by natural language, interpreting, searching, and visualizing OSC messages, validating and debugging OSC messages, and managing OSC address patterns. MCP2OSC enhances human-machine collaboration by leveraging LLM (Large Language Model) to handle intricate OSC development tasks, and by empowering human creativity with an intuitive language interface featuring flexible precision controls: a prompt-based OSC tool. This study provides a novel perspective on the creative MCP application at the network protocol level by utilizing LLM's strength in directly processing and generating human-readable OSC messages. The results suggest its potential for a LLM-based universal control mechanism for multimedia devices.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T07:38:01Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.HC</span><span>cs.AI</span><span>cs.SD</span><span>eess.AS</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10414v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10414v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#85 CS-Agent: LLM-based Community Search via Dual-agent Collaboration</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jiahao Hua, Long Yuan, Qingshuai Feng, Qiang Fan, Shan Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks, yet their application to graph structure analysis, particularly in community search, remains underexplored. Community search, a fundamental task in graph analysis, aims to identify groups of nodes with dense interconnections, which is crucial for understanding the macroscopic structure of graphs. In this paper, we propose GraphCS, a comprehensive benchmark designed to evaluate the performance of LLMs in community search tasks. Our experiments reveal that while LLMs exhibit preliminary potential, they frequently fail to return meaningful results and suffer from output bias. To address these limitations, we introduce CS-Agent, a dual-agent collaborative framework to enhance LLM-based community search. CS-Agent leverages the complementary strengths of two LLMs acting as Solver and Validator. Through iterative feedback and refinement, CS-Agent dynamically refines initial results without fine-tuning or additional training. After the multi-round dialogue, Decider module selects the optimal community. Extensive experiments demonstrate that CS-Agent significantly improves the quality and stability of identified communities compared to baseline methods. To our knowledge, this is the first work to apply LLMs to community search, bridging the gap between LLMs and graph analysis while providing a robust and adaptive solution for real-world applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T07:26:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.SI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.09549v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.09549v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#86 Semantic-Enhanced Time-Series Forecasting via Large Language Models</h2>
                <div class="authors">
                    <strong>Authors:</strong> Hao Liu, Chun Yang, Zhang xiaoxing, Xiaobin Zhu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Time series forecasting plays a significant role in finance, energy, meteorology, and IoT applications. Recent studies have leveraged the generalization capabilities of large language models (LLMs) to adapt to time series forecasting, achieving promising performance. However, existing studies focus on token-level modal alignment, instead of bridging the intrinsic modality gap between linguistic knowledge structures and time series data patterns, greatly limiting the semantic representation. To address this issue, we propose a novel Semantic-Enhanced LLM (SE-LLM) that explores the inherent periodicity and anomalous characteristics of time series to embed into the semantic space to enhance the token embedding. This process enhances the interpretability of tokens for LLMs, thereby activating the potential of LLMs for temporal sequence analysis. Moreover, existing Transformer-based LLMs excel at capturing long-range dependencies but are weak at modeling short-term anomalies in time-series data. Hence, we propose a plugin module embedded within self-attention that models long-term and short-term dependencies to effectively adapt LLMs to time-series analysis. Our approach freezes the LLM and reduces the sequence dimensionality of tokens, greatly reducing computational consumption. Experiments demonstrate the superiority performance of our SE-LLM against the state-of-the-art (SOTA) methods.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T07:25:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.CE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.07697v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.07697v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#87 Rollout Roulette: A Probabilistic Inference Approach to Inference-Time
  Scaling of LLMs using Particle-Based Monte Carlo Methods</h2>
                <div class="authors">
                    <strong>Authors:</strong> Isha Puri, Shivchander Sudalairaj, Guangxuan Xu, Kai Xu, Akash Srivastava
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) have achieved significant performance gains via scaling up model sizes and/or data. However, recent evidence suggests diminishing returns from such approaches, motivating scaling the computation spent at inference time. Existing inference-time scaling methods, usually with reward models, cast the task as a search problem, which tends to be vulnerable to reward hacking as a consequence of approximation errors in reward models. In this paper, we instead cast inference-time scaling as a probabilistic inference task and leverage sampling-based techniques to explore the typical set of the state distribution of a state-space model with an approximate likelihood, rather than optimize for its mode directly. We propose a novel inference-time scaling approach by adapting particle-based Monte Carlo methods to this task. Our empirical evaluation demonstrates that our methods have a 4-16x better scaling rate over our deterministic search counterparts on various challenging mathematical reasoning tasks. Using our approach, we show that Qwen2.5-Math-1.5B-Instruct can surpass GPT-4o accuracy in only 4 rollouts, while Qwen2.5-Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts. Our work not only presents an effective method to inference-time scaling, but also connects the rich literature in probabilistic inference with inference-time scaling of LLMs to develop more robust algorithms in future work. Code, videos, and further information available at https://probabilistic-inference-scaling.github.io.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T07:21:40Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.01618v5' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.01618v5' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#88 Decentralized Weather Forecasting via Distributed Machine Learning and
  Blockchain-Based Model Validation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Rilwan Umar, Aydin Abadi, Basil Aldali, Benito Vincent, Elliot A. J. Hurley, Hotoon Aljazaeri, Jamie Hedley-Cook, Jamie-Lee Bell, Lambert Uwuigbusun, Mujeeb Ahmed, Shishir Nagaraja, Suleiman Sabo, Weaam Alrbeiqi
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Weather forecasting plays a vital role in disaster preparedness, agriculture, and resource management, yet current centralized forecasting systems are increasingly strained by security vulnerabilities, limited scalability, and susceptibility to single points of failure. To address these challenges, we propose a decentralized weather forecasting framework that integrates Federated Learning (FL) with blockchain technology. FL enables collaborative model training without exposing sensitive local data; this approach enhances privacy and reduces data transfer overhead. Meanwhile, the Ethereum blockchain ensures transparent and dependable verification of model updates. To further enhance the system's security, we introduce a reputation-based voting mechanism that assesses the trustworthiness of submitted models while utilizing the Interplanetary File System (IPFS) for efficient off-chain storage. Experimental results demonstrate that our approach not only improves forecasting accuracy but also enhances system resilience and scalability, making it a viable candidate for deployment in real-world, security-critical environments.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T07:18:06Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.09299v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.09299v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#89 Measuring Diversity in Synthetic Datasets</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yuchang Zhu, Huizhe Zhang, Bingzhe Wu, Jintang Li, Zibin Zheng, Peilin Zhao, Liang Chen, Yatao Bian
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large language models (LLMs) are widely adopted to generate synthetic datasets for various natural language processing (NLP) tasks, such as text classification and summarization. However, accurately measuring the diversity of these synthetic datasets-an aspect crucial for robust model performance-remains a significant challenge. In this paper, we introduce DCScore, a novel method for measuring synthetic dataset diversity from a classification perspective. Specifically, DCScore formulates diversity evaluation as a sample classification task, leveraging mutual relationships among samples. We further provide theoretical verification of the diversity-related axioms satisfied by DCScore, highlighting its role as a principled diversity evaluation method. Experimental results on synthetic datasets reveal that DCScore enjoys a stronger correlation with multiple diversity pseudo-truths of evaluated datasets, underscoring its effectiveness. Moreover, both empirical and theoretical evidence demonstrate that DCScore substantially reduces computational costs compared to existing methods. Code is available at: https://github.com/bluewhalelab/dcscore.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T07:15:48Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2502.08512v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.08512v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#90 LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with
  Location-Election-Disjoint</h2>
                <div class="authors">
                    <strong>Authors:</strong> Qianli Ma, Dongrui Liu, Qian Chen, Linfeng Zhang, Jing Shao
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Fine-tuning pre-trained Large Language Models (LLMs) for specialized tasks incurs substantial computational and data costs. While model merging offers a training-free solution to integrate multiple task-specific models, existing methods suffer from safety-utility conflicts where enhanced general capabilities degrade safety safeguards. We identify two root causes: $\textbf{neuron misidentification}$ due to simplistic parameter magnitude-based selection, and $\textbf{cross-task neuron interference}$ during merging. To address these challenges, we propose $\textbf{LED-Merging}$, a three-stage framework that $\textbf{L}$ocates task-specific neurons via gradient-based attribution, dynamically $\textbf{E}$lects critical neurons through multi-model importance fusion, and $\textbf{D}$isjoints conflicting updates through parameter isolation. Extensive experiments on Llama-3-8B, Mistral-7B, and Llama2-13B demonstrate that LED-Merging effectively reduces harmful response rates, showing a 31.4\% decrease on Llama-3-8B-Instruct on HarmBench, while simultaneously preserving 95\% of utility performance, such as achieving 52.39\% accuracy on GSM8K. LED-Merging resolves safety-utility conflicts and provides a lightweight, training-free paradigm for constructing reliable multi-task LLMs. Code is available at $\href{https://github.com/MqLeet/LED-Merging}{GitHub}$.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T07:15:21Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://dx.doi.org/10.18653/v1/2025.acl-long.1055' target='_blank'>doi</a><a href='http://arxiv.org/abs/2502.16770v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2502.16770v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#91 Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text
  Generation</h2>
                <div class="authors">
                    <strong>Authors:</strong> Huizhen Shu, Xuying Li, Qirui Wang, Yuji Kosuga, Mengqiu Tian, Zhuo Li
                </div>
                <div class="summary">
                    <strong>Summary:</strong> With the rapid proliferation of Natural Language Processing (NLP), especially Large Language Models (LLMs), generating adversarial examples to jailbreak LLMs remains a key challenge for understanding model vulnerabilities and improving robustness. In this context, we propose a new black-box attack method that leverages the interpretability of large models. We introduce the Sparse Feature Perturbation Framework (SFPF), a novel approach for adversarial text generation that utilizes sparse autoencoders to identify and manipulate critical features in text. After using the SAE model to reconstruct hidden layer representations, we perform feature clustering on the successfully attacked texts to identify features with higher activations. These highly activated features are then perturbed to generate new adversarial texts. This selective perturbation preserves the malicious intent while amplifying safety signals, thereby increasing their potential to evade existing defenses. Our method enables a new red-teaming strategy that balances adversarial effectiveness with safety alignment. Experimental results demonstrate that adversarial texts generated by SFPF can bypass state-of-the-art defense mechanisms, revealing persistent vulnerabilities in current NLP systems.However, the method's effectiveness varies across prompts and layers, and its generalizability to other architectures and larger models remains to be validated.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T07:12:44Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10404v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10404v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#92 Compass-Thinker-7B Technical Report</h2>
                <div class="authors">
                    <strong>Authors:</strong> Anxiang Zeng, Haibo Zhang, Kaixiang Mo, Long Zhang, Shuman Liu, Yanhui Huang, Yawen Liu, Yuepeng Sheng, Yuwei Huang
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Recent R1-Zero-like research further demonstrates that reasoning extension has given large language models (LLMs) unprecedented reasoning capabilities, and Reinforcement Learning is the core technology to elicit its complex reasoning. However, conducting RL experiments directly on hyperscale models involves high computational costs and resource demands, posing significant risks. We propose the Compass-Thinker-7B model, which aims to explore the potential of Reinforcement Learning with less computational resources and costs, and provides insights for further research into RL recipes for larger models. Compass-Thinker-7B is trained from an open source model through a specially designed Reinforcement Learning Pipeline. We curate a dataset of 30k verifiable mathematics problems for the Reinforcement Learning Pipeline. By configuring data and training settings with different difficulty distributions for different stages, the potential of the model is gradually released and the training efficiency is improved. Extensive evaluations show that Compass-Thinker-7B possesses exceptional reasoning potential, and achieves superior performance on mathematics compared to the same-sized RL model. Especially in the challenging AIME2024 evaluation, Compass-Thinker-7B achieves 40% accuracy.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T07:12:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.08909v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.08909v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#93 Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation
  for Quantization for Free</h2>
                <div class="authors">
                    <strong>Authors:</strong> Euntae Choi, Sumin Song, Woosang Lim, Sungjoo Yoo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Large Language Models (LLMs) face deployment challenges due to high computational costs, and while Post-Training Quantization (PTQ) offers a solution, existing rotation-based methods struggle at very low bit-widths like 2-bit. We introduce a novel, training-free approach to construct an improved rotation matrix, addressing the limitations of current methods. The key contributions include leveraging the Walsh-Hadamard transform with sequency ordering, which clusters similar frequency components to reduce quantization error compared to standard Hadamard matrices, significantly improving performance. Furthermore, we propose a Grouped Sequency-arranged Rotation (GSR) using block-diagonal matrices with smaller Walsh blocks, effectively isolating outlier impacts and achieving performance comparable to optimization-based methods without requiring any training. Our method demonstrates robust performance on reasoning tasks and Perplexity (PPL) score on WikiText-2. Our method also enhances results even when applied over existing learned rotation techniques.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T07:02:58Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>cs.AI</span><span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2505.03810v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2505.03810v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#94 LinguaFluid: Language Guided Fluid Control via Semantic Rewards in
  Reinforcement Learning</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aoming Liang, Chi Cheng, Dashuai Chen, Boai Sun, Dixia Fan
                </div>
                <div class="summary">
                    <strong>Summary:</strong> In the domain of scientific machine learning, designing effective reward functions remains a challenge in reinforcement learning (RL), particularly in environments where task goals are difficult to specify numerically. Reward functions in existing work are predominantly based on heuristics, manual engineering, or task-specific tuning. In this work, we introduce a semantically aligned reinforcement learning method where rewards are computed by aligning the current state with a target semantic instruction using a Sentence-Bidirectional Encoder Representations from Transformers (SBERT). Instead of relying on manually defined reward functions, the policy receives feedback based on the reward, which is a cosine similarity between the goal textual description and the statement description in the episode. We evaluated our approach in several environments and showed that semantic reward can guide learning to achieve competitive control behavior, even in the absence of hand-crafted reward functions. Our study demonstrates a correlation between the language embedding space and the conventional Euclidean space. This framework opens new horizons for aligning agent behavior with natural language goals and lays the groundwork for a more seamless integration of larger language models (LLMs) and fluid control applications.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T07:01:16Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span><span>physics.flu-dyn</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.05977v2' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.05977v2' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#95 PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce
  Driver Distraction Detection</h2>
                <div class="authors">
                    <strong>Authors:</strong> Haibin Sun, Xinghui Song
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Driver distraction detection is essential for improving traffic safety and reducing road accidents. However, existing models often suffer from degraded generalization when deployed in real-world scenarios. This limitation primarily arises from the few-shot learning challenge caused by the high cost of data annotation in practical environments, as well as the substantial domain shift between training datasets and target deployment conditions. To address these issues, we propose a Pose-driven Quality-controlled Data Augmentation Framework (PQ-DAF) that leverages a vision-language model for sample filtering to cost-effectively expand training data and enhance cross-domain robustness. Specifically, we employ a Progressive Conditional Diffusion Model (PCDMs) to accurately capture key driver pose features and synthesize diverse training examples. A sample quality assessment module, built upon the CogVLM vision-language model, is then introduced to filter out low-quality synthetic samples based on a confidence threshold, ensuring the reliability of the augmented dataset. Extensive experiments demonstrate that PQ-DAF substantially improves performance in few-shot driver distraction detection, achieving significant gains in model generalization under data-scarce conditions.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T06:54:28Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CV</span><span>cs.AI</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10397v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10397v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#96 XQuant: Breaking the Memory Wall for LLM Inference with KV Cache
  Rematerialization</h2>
                <div class="authors">
                    <strong>Authors:</strong> Aditya Tomar, Coleman Hooper, Minjae Lee, Haocheng Xi, Rishabh Tiwari, Wonjun Kang, Luca Manolache, Michael W. Mahoney, Kurt Keutzer, Amir Gholami
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Although LLM inference has emerged as a critical workload for many downstream applications, efficiently inferring LLMs is challenging due to the substantial memory footprint and bandwidth requirements. In parallel, compute capabilities have steadily outpaced both memory capacity and bandwidth over the last few decades, a trend that remains evident in modern GPU hardware and exacerbates the challenge of LLM inference. As such, new algorithms are emerging that trade increased computation for reduced memory operations. To that end, we present XQuant, which takes advantage of this trend, enabling an order-of-magnitude reduction in memory consumption through low-bit quantization with substantial accuracy benefits relative to state-of-the-art KV cache quantization methods. We accomplish this by quantizing and caching the layer input activations X, instead of using standard KV caching, and then rematerializing the Keys and Values on-the-fly during inference. This results in an immediate 2$\times$ memory savings compared to KV caching. By applying XQuant, we achieve up to $\sim 7.7\times$ memory savings with $<0.1$ perplexity degradation compared to the FP16 baseline. Furthermore, our approach leverages the fact that X values are similar across layers. Building on this observation, we introduce XQuant-CL, which exploits the cross-layer similarity in the X embeddings for extreme compression. Across different models, XQuant-CL attains up to 10$\times$ memory savings relative to the FP16 baseline with only 0.01 perplexity degradation, and 12.5$\times$ memory savings with only $0.1$ perplexity degradation. XQuant exploits the rapidly increasing compute capabilities of hardware platforms to eliminate the memory bottleneck, while surpassing state-of-the-art KV cache quantization methods and achieving near-FP16 accuracy across a wide range of models.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T06:52:38Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.LG</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10395v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10395v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#97 Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts</h2>
                <div class="authors">
                    <strong>Authors:</strong> Chiyu Zhang, Lu Zhou, Xiaogang Xu, Jiafei Wu, Liming Fang, Zhe Liu
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Evaluating jailbreak attacks is challenging when prompts are not overtly harmful or fail to induce harmful outputs. Unfortunately, many existing red-teaming datasets contain such unsuitable prompts. To evaluate attacks accurately, these datasets need to be assessed and cleaned for maliciousness. However, existing malicious content detection methods rely on either manual annotation, which is labor-intensive, or large language models (LLMs), which have inconsistent accuracy in harmful types. To balance accuracy and efficiency, we propose a hybrid evaluation framework named MDH (Malicious content Detection based on LLMs with Human assistance) that combines LLM-based annotation with minimal human oversight, and apply it to dataset cleaning and detection of jailbroken responses. Furthermore, we find that well-crafted developer messages can significantly boost jailbreak success, leading us to propose two new strategies: D-Attack, which leverages context simulation, and DH-CoT, which incorporates hijacked chains of thought. The Codes, datasets, judgements, and detection results will be released in github repository: https://github.com/AlienZhang1996/DH-CoT.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T06:46:56Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span><span>cs.CR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10390v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10390v1' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#98 Out of Distribution, Out of Luck: How Well Can LLMs Trained on
  Vulnerability Datasets Detect Top 25 CWE Weaknesses?</h2>
                <div class="authors">
                    <strong>Authors:</strong> Yikun Li, Ngoc Tan Bui, Ting Zhang, Martin Weyssow, Chengran Yang, Xin Zhou, Jinfeng Jiang, Junkai Chen, Huihui Huang, Huu Hung Nguyen, Chiok Yew Ho, Jie Tan, Ruiyin Li, Yide Yin, Han Wei Ang, Frank Liauw, Eng Lieh Ouh, Lwin Khin Shar, David Lo
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Automated vulnerability detection research has made substantial progress, yet its real-world impact remains limited. Current vulnerability datasets suffer from issues including label inaccuracy rates of 20-71%, extensive duplication, and poor coverage of critical CWE types. These issues create a significant "generalization gap" where models achieve misleading self-testing performance (measured on held-out data from the same dataset for training) by exploiting spurious correlations rather than learning true vulnerability patterns. Our analysis reveals that many models experience substantial performance drops of up to 33% when evaluated on independent data, with some performing close to random guessing. To address these limitations, we present a three-part solution. First, we introduce a manually curated test dataset, BenchVul, covering the MITRE Top 25 Most Dangerous CWEs. Second, we construct a high-quality training dataset, TitanVul, comprising 38,863 functions by aggregating seven public sources and applying deduplication and validation using a novel multi-agent LLM framework. Third, we propose a Realistic Vulnerability Generation (RVG) framework, which synthesizes context-aware vulnerability examples for underrepresented but critical CWE types through simulated development workflows. Our evaluation shows the strengths of each component in closing the generalization gap. First, BenchVul shows the limitations of self-testing: models trained on existing datasets, such as BigVul and CVEfixes, experience performance drops on BenchVul (from 0.776 to 0.519 and from 0.713 to 0.607). Second, training models on TitanVul demonstrates improved generalization, with model performance increasing from 0.584 when evaluated on the same dataset to 0.767 when tested on BenchVul. Third, supplementing TitanVul with RVG-generated data yields further gains, increasing model performance by 14.0% to 0.874.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T06:16:52Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CR</span><span>cs.SE</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2507.21817v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2507.21817v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#99 Leveraging Large Language Models for Relevance Judgments in Legal Case
  Retrieval</h2>
                <div class="authors">
                    <strong>Authors:</strong> Shengjie Ma, Qi Chu, Jiaxin Mao, Xuhui Jiang, Haozhe Duan, Chong Chen
                </div>
                <div class="summary">
                    <strong>Summary:</strong> Determining which legal cases are relevant to a given query involves navigating lengthy texts and applying nuanced legal reasoning. Traditionally, this task has demanded significant time and domain expertise to identify key Legal Facts and reach sound juridical conclusions. In addition, existing data with legal case similarities often lack interpretability, making it difficult to understand the rationale behind relevance judgments. With the growing capabilities of large language models (LLMs), researchers have begun investigating their potential in this domain. Nonetheless, the method of employing a general large language model for reliable relevance judgments in legal case retrieval remains largely unexplored. To address this gap in research, we propose a novel few-shot approach where LLMs assist in generating expert-aligned interpretable relevance judgments. The proposed approach decomposes the judgment process into several stages, mimicking the workflow of human annotators and allowing for the flexible incorporation of expert reasoning to improve the accuracy of relevance judgments. Importantly, it also ensures interpretable data labeling, providing transparency and clarity in the relevance assessment process. Through a comparison of relevance judgments made by LLMs and human experts, we empirically demonstrate that the proposed approach can yield reliable and valid relevance assessments. Furthermore, we demonstrate that with minimal expert supervision, our approach enables a large language model to acquire case analysis expertise and subsequently transfers this ability to a smaller model via annotation-based knowledge distillation.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T06:13:17Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.AI</span><span>cs.IR</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2403.18405v3' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2403.18405v3' target='_blank'>pdf</a>
                </div>
            </div>
            
            <div class="paper">
                <h2>#100 Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with
  Constrained Decoding</h2>
                <div class="authors">
                    <strong>Authors:</strong> Jakub md, Pavel Pib, Pavel Krl
                </div>
                <div class="summary">
                    <strong>Summary:</strong> While aspect-based sentiment analysis (ABSA) has made substantial progress, challenges remain for low-resource languages, which are often overlooked in favour of English. Current cross-lingual ABSA approaches focus on limited, less complex tasks and often rely on external translation tools. This paper introduces a novel approach using constrained decoding with sequence-to-sequence models, eliminating the need for unreliable translation tools and improving cross-lingual performance by 5\% on average for the most complex task. The proposed method also supports multi-tasking, which enables solving multiple ABSA tasks with a single model, with constrained decoding boosting results by more than 10\%.   We evaluate our approach across seven languages and six ABSA tasks, surpassing state-of-the-art methods and setting new benchmarks for previously unexplored tasks. Additionally, we assess large language models (LLMs) in zero-shot, few-shot, and fine-tuning scenarios. While LLMs perform poorly in zero-shot and few-shot settings, fine-tuning achieves competitive results compared to smaller multilingual models, albeit at the cost of longer training and inference times.   We provide practical recommendations for real-world applications, enhancing the understanding of cross-lingual ABSA methodologies. This study offers valuable insights into the strengths and limitations of cross-lingual ABSA approaches, advancing the state-of-the-art in this challenging research domain.
                </div>
                <div class="updated">
                    <strong>Updated:</strong> 2025-08-14T06:07:53Z
                </div>
                <div class="tags">
                    <strong>Tags:</strong> <span>cs.CL</span>
                </div>
                <div class="links">
                    <a href='http://arxiv.org/abs/2508.10369v1' target='_blank'>HTML</a><a href='http://arxiv.org/pdf/2508.10369v1' target='_blank'>pdf</a>
                </div>
            </div>
            
        </div>
    </body>
    </html>
    